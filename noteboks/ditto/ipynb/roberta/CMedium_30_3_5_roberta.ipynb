{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PSgSb9vUtCyX",
        "outputId": "46053842-98f5-4cb6-f070-0382b934809e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 21.12 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "Checking out files: 100% (128/128), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 50.9 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 74.7 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 90.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 32.8 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 9.37 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.7 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.7 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 67.7 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 58.3 MB/s \n",
            "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 54.3 MB/s \n",
            "\u001b[?25hCollecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 78.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 34.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449922 sha256=832b1d196cef49e7939711f3acae7a156ed63f4c1980a28aa631aa74513df0ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=716c0effde8b2d1f4cf48fe7355e0e9e37250cefc6b17ea77071d672d47e767b\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_hfO6D_uLby",
        "outputId": "b90ecf48-161d-41b5-8d90-fc87df1fa30c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10117, done.\u001b[K\n",
            "remote: Counting objects: 100% (233/233), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 10117 (delta 119), reused 152 (delta 62), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10117/10117), 14.97 MiB | 22.18 MiB/s, done.\n",
            "Resolving deltas: 100% (6927/6927), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-phnt_2lk\n",
            "Created temporary directory: /tmp/pip-req-tracker-w38l4sjl\n",
            "Initialized build tracking at /tmp/pip-req-tracker-w38l4sjl\n",
            "Created build tracker: /tmp/pip-req-tracker-w38l4sjl\n",
            "Entered build tracker: /tmp/pip-req-tracker-w38l4sjl\n",
            "Created temporary directory: /tmp/pip-install-hgvcfdto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-hwrkrgc4\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-w38l4sjl'\n",
            "    Running setup.py (path:/tmp/pip-req-build-hwrkrgc4/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-5qluhenf\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-5qluhenf/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-5qluhenf/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-5qluhenf/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-5qluhenf/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-5qluhenf/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-5qluhenf/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-hwrkrgc4 has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-w38l4sjl'\n",
            "Created temporary directory: /tmp/pip-unpack-jwepvgo6\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-r5hm9hlk\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-r5hm9hlk\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-hwrkrgc4/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-hwrkrgc4/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-r5hm9hlk\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-r5hm9hlk/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298244 sha256=77b0d3406ff2bcd255fd8d4b3cff90c91327a96c5d71b0f3542c58b719a097c5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-phnt_2lk/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-w38l4sjl'\n",
            "/content/ditto\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKSZwBG_uyzV",
        "outputId": "071da489-7a04-40a3-bcbb-73cbeafd6d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.48-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 62.6 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting botocore==1.27.48\n",
            "  Downloading botocore-1.27.48-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 56.0 MB/s \n",
            "\u001b[?25hCollecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.48->awscli) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.48->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.48 botocore-1.27.48 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ],
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR84V9pFRkw8",
        "outputId": "bf96a915-ca68-4f3c-ff81-421930b1b430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 18 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAyGpUo9ifJM"
      },
      "source": [
        "## importa config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xawOMn6icU7",
        "outputId": "c34a80a7-8b7d-43e8-9680-f9643a0572dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 1014, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 1014 (delta 27), reused 7 (delta 0), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (1014/1014), 254.12 MiB | 27.00 MiB/s, done.\n",
            "Resolving deltas: 100% (611/611), done.\n",
            "Checking out files: 100% (1284/1284), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw03GW7dmkqy"
      },
      "source": [
        "## remove e move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQVym9vwmx-g",
        "outputId": "557b02f8-f459-4648-8758-0bd701704d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "outputs": [],
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/CMedium_30_3_5/configs.json /content/ditto/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm4nohJxf9bD"
      },
      "source": [
        "# DA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDvm9a1dIlo"
      },
      "source": [
        "## DA STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qxLFPNvcGgH",
        "outputId": "e02aad35-c46a-4009-9a57-8418bf0ce24a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 653kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 28.0MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 22.8MB/s]\n",
            "Downloading: 100% 501M/501M [00:10<00:00, 49.8MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4370655417442322\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.2692307692307693\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4294983744621277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.2692307692307693, f1=0.2666666666666667, best_f1=0.2692307692307693\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38396021723747253\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.34210526315789475, f1=0.3111111111111111, best_f1=0.3111111111111111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25043460726737976\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.4, f1=0.4799999999999999, best_f1=0.4799999999999999\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26980870962142944\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.49122807017543857, f1=0.6, best_f1=0.6\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16603343188762665\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7222222222222223, f1=0.6857142857142857, best_f1=0.6857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3379935324192047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.41791044776119407, f1=0.5957446808510638, best_f1=0.6857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28282415866851807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7878787878787878, f1=0.7058823529411764, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20394828915596008\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7741935483870968, f1=0.7096774193548386, best_f1=0.7058823529411764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19168208539485931\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7999999999999999, f1=0.7096774193548386, best_f1=0.7096774193548386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09319324791431427\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.8387096774193549, f1=0.75, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03616790473461151\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8235294117647058, f1=0.7058823529411764, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05354250967502594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8125000000000001, f1=0.7272727272727273, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03842749819159508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8387096774193549, f1=0.75, best_f1=0.75\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04991879686713219\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8387096774193549, f1=0.75, best_f1=0.75\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 108586.53it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7058823529411764\n",
            "real_f1 = 0.7027027027027025\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:19, 221.77it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjO-q4GLeCE1"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ7mparQevgM",
        "outputId": "eae0c544-24a0-405b-83c0-25ec9ae65ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6005876064300537\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.4345407783985138\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.5726630091667175\n",
            "step: 30, loss: 0.2759717106819153\n",
            "step: 40, loss: 0.39234980940818787\n",
            "step: 50, loss: 0.5565707087516785\n",
            "step: 60, loss: 0.32151076197624207\n",
            "step: 70, loss: 0.10444100201129913\n",
            "step: 80, loss: 0.1567116230726242\n",
            "step: 90, loss: 0.11700677871704102\n",
            "step: 100, loss: 0.2192908525466919\n",
            "step: 110, loss: 0.12317076325416565\n",
            "step: 120, loss: 0.13849472999572754\n",
            "step: 130, loss: 0.015688356012105942\n",
            "step: 140, loss: 0.011002880521118641\n",
            "step: 150, loss: 0.015072227455675602\n",
            "step: 160, loss: 0.017418980598449707\n",
            "step: 170, loss: 0.021292302757501602\n",
            "step: 180, loss: 0.17858392000198364\n",
            "step: 190, loss: 0.0257926844060421\n",
            "step: 200, loss: 0.007198851555585861\n",
            "step: 210, loss: 0.1176840215921402\n",
            "step: 220, loss: 0.11066537350416183\n",
            "step: 230, loss: 0.0036469013430178165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9718785151856018, f1=0.9705215419501134, best_f1=0.9705215419501134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021314110606908798\n",
            "step: 10, loss: 0.210979163646698\n",
            "step: 20, loss: 0.032166652381420135\n",
            "step: 30, loss: 0.007537170313298702\n",
            "step: 40, loss: 0.07984242588281631\n",
            "step: 50, loss: 0.02074011228978634\n",
            "step: 60, loss: 0.021502966061234474\n",
            "step: 70, loss: 0.01209726370871067\n",
            "step: 80, loss: 0.004089401103556156\n",
            "step: 90, loss: 0.0014624291798099875\n",
            "step: 100, loss: 0.007274358533322811\n",
            "step: 110, loss: 0.00824993010610342\n",
            "step: 120, loss: 0.031239449977874756\n",
            "step: 130, loss: 0.004872123245149851\n",
            "step: 140, loss: 0.00410580774769187\n",
            "step: 150, loss: 0.05795073136687279\n",
            "step: 160, loss: 0.003787001594901085\n",
            "step: 170, loss: 0.0013388121733441949\n",
            "step: 180, loss: 0.0032426209654659033\n",
            "step: 190, loss: 0.020421907305717468\n",
            "step: 200, loss: 0.007969401776790619\n",
            "step: 210, loss: 0.029007911682128906\n",
            "step: 220, loss: 0.0009602121426723897\n",
            "step: 230, loss: 0.0008238042937591672\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9819413092550789, f1=0.9762174405436014, best_f1=0.9762174405436014\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004797401372343302\n",
            "step: 10, loss: 0.018113939091563225\n",
            "step: 20, loss: 0.0008949976181611419\n",
            "step: 30, loss: 0.03685460612177849\n",
            "step: 40, loss: 0.006290160119533539\n",
            "step: 50, loss: 0.014760306105017662\n",
            "step: 60, loss: 0.002940040547400713\n",
            "step: 70, loss: 0.0019560011569410563\n",
            "step: 80, loss: 0.0007664301083423197\n",
            "step: 90, loss: 0.004268209915608168\n",
            "step: 100, loss: 0.01636551320552826\n",
            "step: 110, loss: 0.018563903868198395\n",
            "step: 120, loss: 0.0007193775963969529\n",
            "step: 130, loss: 0.002732613356783986\n",
            "step: 140, loss: 0.0005390682490542531\n",
            "step: 150, loss: 0.06360502541065216\n",
            "step: 160, loss: 0.00397457554936409\n",
            "step: 170, loss: 0.003246545558795333\n",
            "step: 180, loss: 0.018105067312717438\n",
            "step: 190, loss: 0.006986362859606743\n",
            "step: 200, loss: 0.008408332243561745\n",
            "step: 210, loss: 0.0038704685866832733\n",
            "step: 220, loss: 0.00395547179505229\n",
            "step: 230, loss: 0.016887042671442032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9820627802690582, f1=0.9774266365688488, best_f1=0.9774266365688488\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006014863029122353\n",
            "step: 10, loss: 0.009329511784017086\n",
            "step: 20, loss: 0.01084950566291809\n",
            "step: 30, loss: 0.0006429193308576941\n",
            "step: 40, loss: 0.0932057574391365\n",
            "step: 50, loss: 0.016049103811383247\n",
            "step: 60, loss: 0.0036980623845010996\n",
            "step: 70, loss: 0.0004507332341745496\n",
            "step: 80, loss: 0.0009915796108543873\n",
            "step: 90, loss: 0.0029768485110253096\n",
            "step: 100, loss: 0.0011398118222132325\n",
            "step: 110, loss: 0.0008047238225117326\n",
            "step: 120, loss: 0.001163144945167005\n",
            "step: 130, loss: 0.0020306340884417295\n",
            "step: 140, loss: 0.01600836031138897\n",
            "step: 150, loss: 0.00043084603385068476\n",
            "step: 160, loss: 0.00784617941826582\n",
            "step: 170, loss: 0.003213489195331931\n",
            "step: 180, loss: 0.16715411841869354\n",
            "step: 190, loss: 0.012621724046766758\n",
            "step: 200, loss: 0.006686868146061897\n",
            "step: 210, loss: 0.0012244838289916515\n",
            "step: 220, loss: 0.010524558834731579\n",
            "step: 230, loss: 0.00223249732516706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9807909604519773, f1=0.9818594104308391, best_f1=0.9774266365688488\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005019110161811113\n",
            "step: 10, loss: 0.0015497268177568913\n",
            "step: 20, loss: 0.0006851240759715438\n",
            "step: 30, loss: 0.00045669975224882364\n",
            "step: 40, loss: 0.010013842955231667\n",
            "step: 50, loss: 0.0006710966699756682\n",
            "step: 60, loss: 0.004164968151599169\n",
            "step: 70, loss: 0.0029700424056500196\n",
            "step: 80, loss: 0.03206334263086319\n",
            "step: 90, loss: 0.04701048135757446\n",
            "step: 100, loss: 0.0008758658659644425\n",
            "step: 110, loss: 0.0017598087433725595\n",
            "step: 120, loss: 0.00651391688734293\n",
            "step: 130, loss: 0.001331530511379242\n",
            "step: 140, loss: 0.028779273852705956\n",
            "step: 150, loss: 0.09230829030275345\n",
            "step: 160, loss: 0.004474223591387272\n",
            "step: 170, loss: 0.006289544049650431\n",
            "step: 180, loss: 0.007936153560876846\n",
            "step: 190, loss: 0.008575700223445892\n",
            "step: 200, loss: 0.0006032832898199558\n",
            "step: 210, loss: 0.002926694927737117\n",
            "step: 220, loss: 0.0027988243382424116\n",
            "step: 230, loss: 0.004166203085333109\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.987598647125141, f1=0.9852774631936579, best_f1=0.9852774631936579\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002251311670988798\n",
            "step: 10, loss: 0.00565325329080224\n",
            "step: 20, loss: 0.0013113563181832433\n",
            "step: 30, loss: 0.0006370781920850277\n",
            "step: 40, loss: 0.0003273081674706191\n",
            "step: 50, loss: 0.0004846765077672899\n",
            "step: 60, loss: 0.0016171283787116408\n",
            "step: 70, loss: 0.0003597326285671443\n",
            "step: 80, loss: 0.0014025007840245962\n",
            "step: 90, loss: 0.0069318669848144054\n",
            "step: 100, loss: 0.00046490353997796774\n",
            "step: 110, loss: 0.006715166382491589\n",
            "step: 120, loss: 0.0004134780610911548\n",
            "step: 130, loss: 0.000460866023786366\n",
            "step: 140, loss: 0.0005954676889814436\n",
            "step: 150, loss: 0.00014420623483601958\n",
            "step: 160, loss: 0.02133793756365776\n",
            "step: 170, loss: 0.00038187854806892574\n",
            "step: 180, loss: 0.00013791551464237273\n",
            "step: 190, loss: 0.00029695374541915953\n",
            "step: 200, loss: 0.001597303431481123\n",
            "step: 210, loss: 0.001894557848572731\n",
            "step: 220, loss: 0.004947966430336237\n",
            "step: 230, loss: 0.0018305904231965542\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9864864864864865, f1=0.9876819708846584, best_f1=0.9852774631936579\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0025600362569093704\n",
            "step: 10, loss: 0.0008557740366086364\n",
            "step: 20, loss: 0.0012596484739333391\n",
            "step: 30, loss: 0.0013585002161562443\n",
            "step: 40, loss: 0.004612314980477095\n",
            "step: 50, loss: 0.0011229225201532245\n",
            "step: 60, loss: 0.00037918283487670124\n",
            "step: 70, loss: 0.2610381543636322\n",
            "step: 80, loss: 0.012396267615258694\n",
            "step: 90, loss: 0.003896792884916067\n",
            "step: 100, loss: 0.0007229336188174784\n",
            "step: 110, loss: 0.0011276145232841372\n",
            "step: 120, loss: 0.0026218260172754526\n",
            "step: 130, loss: 0.005723252426832914\n",
            "step: 140, loss: 0.0009960067691281438\n",
            "step: 150, loss: 0.006469155661761761\n",
            "step: 160, loss: 0.00241175782866776\n",
            "step: 170, loss: 0.0010705966269597411\n",
            "step: 180, loss: 0.0004863765207119286\n",
            "step: 190, loss: 0.0002935347147285938\n",
            "step: 200, loss: 0.0008458061493001878\n",
            "step: 210, loss: 0.01103464886546135\n",
            "step: 220, loss: 0.00037351137143559754\n",
            "step: 230, loss: 0.0009353089844807982\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9887640449438202, f1=0.9864864864864865, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008967758622020483\n",
            "step: 10, loss: 0.003050091676414013\n",
            "step: 20, loss: 0.0029388568364083767\n",
            "step: 30, loss: 0.0019612316973507404\n",
            "step: 40, loss: 0.0028780223801732063\n",
            "step: 50, loss: 0.0007206428563222289\n",
            "step: 60, loss: 0.0008670032257214189\n",
            "step: 70, loss: 0.00042915227822959423\n",
            "step: 80, loss: 0.0308036170899868\n",
            "step: 90, loss: 0.0008557414403185248\n",
            "step: 100, loss: 0.0011593851959332824\n",
            "step: 110, loss: 0.0017056787619367242\n",
            "step: 120, loss: 0.0011613320093601942\n",
            "step: 130, loss: 0.002455356763675809\n",
            "step: 140, loss: 0.0007610503816977143\n",
            "step: 150, loss: 0.06920036673545837\n",
            "step: 160, loss: 0.0004990280140191317\n",
            "step: 170, loss: 0.09357504546642303\n",
            "step: 180, loss: 0.0010613296180963516\n",
            "step: 190, loss: 0.001373936072923243\n",
            "step: 200, loss: 0.0045599378645420074\n",
            "step: 210, loss: 0.0017897762591019273\n",
            "step: 220, loss: 0.0005800689104944468\n",
            "step: 230, loss: 0.0003949144738726318\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9876819708846584, f1=0.9876265466816648, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004747141501866281\n",
            "step: 10, loss: 0.0011017777724191546\n",
            "step: 20, loss: 0.0021728717256337404\n",
            "step: 30, loss: 0.00441161822527647\n",
            "step: 40, loss: 0.0004743289027828723\n",
            "step: 50, loss: 0.0007446764502674341\n",
            "step: 60, loss: 0.0008930402109399438\n",
            "step: 70, loss: 0.06636980921030045\n",
            "step: 80, loss: 0.00016021048941183835\n",
            "step: 90, loss: 0.03333020955324173\n",
            "step: 100, loss: 0.0003302470431663096\n",
            "step: 110, loss: 0.0004557677311822772\n",
            "step: 120, loss: 0.07740059494972229\n",
            "step: 130, loss: 0.0026429120916873217\n",
            "step: 140, loss: 0.003210898954421282\n",
            "step: 150, loss: 0.0008226664504036307\n",
            "step: 160, loss: 0.029661642387509346\n",
            "step: 170, loss: 0.0004341225721873343\n",
            "step: 180, loss: 0.00020575267262756824\n",
            "step: 190, loss: 0.00010558625945122913\n",
            "step: 200, loss: 0.00017473996558692306\n",
            "step: 210, loss: 0.003002662444487214\n",
            "step: 220, loss: 0.00017252190446015447\n",
            "step: 230, loss: 0.00010284086602041498\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9876819708846584, f1=0.9821029082774049, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011465993884485215\n",
            "step: 10, loss: 0.00014861967065371573\n",
            "step: 20, loss: 9.902405145112425e-05\n",
            "step: 30, loss: 6.406661123037338e-05\n",
            "step: 40, loss: 0.002797531196847558\n",
            "step: 50, loss: 0.00030398895614780486\n",
            "step: 60, loss: 0.0007370676612481475\n",
            "step: 70, loss: 0.0005117832915857434\n",
            "step: 80, loss: 6.718617078149691e-05\n",
            "step: 90, loss: 9.967006917577237e-05\n",
            "step: 100, loss: 6.493658293038607e-05\n",
            "step: 110, loss: 0.0013986864360049367\n",
            "step: 120, loss: 6.319220119621605e-05\n",
            "step: 130, loss: 0.00037138917832635343\n",
            "step: 140, loss: 0.0006586899980902672\n",
            "step: 150, loss: 0.002052045427262783\n",
            "step: 160, loss: 5.372720625018701e-05\n",
            "step: 170, loss: 0.00012608521501533687\n",
            "step: 180, loss: 0.00016960236825980246\n",
            "step: 190, loss: 9.724592382553965e-05\n",
            "step: 200, loss: 0.0001658116962062195\n",
            "step: 210, loss: 0.00025064218789339066\n",
            "step: 220, loss: 0.00013299273268785328\n",
            "step: 230, loss: 7.298769196495414e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9898989898989898, f1=0.9832402234636871, best_f1=0.9832402234636871\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.256498116068542e-05\n",
            "step: 10, loss: 0.00010610974277369678\n",
            "step: 20, loss: 0.00011511362390592694\n",
            "step: 30, loss: 6.699843652313575e-05\n",
            "step: 40, loss: 3.04480672639329e-05\n",
            "step: 50, loss: 4.5621625758940354e-05\n",
            "step: 60, loss: 0.00010268612822983414\n",
            "step: 70, loss: 5.732563295168802e-05\n",
            "step: 80, loss: 0.002026147674769163\n",
            "step: 90, loss: 0.10545475780963898\n",
            "step: 100, loss: 0.00763375498354435\n",
            "step: 110, loss: 0.01618030294775963\n",
            "step: 120, loss: 0.00019410514505580068\n",
            "step: 130, loss: 9.920438606059179e-05\n",
            "step: 140, loss: 0.0001346975623164326\n",
            "step: 150, loss: 0.00012475985568016768\n",
            "step: 160, loss: 0.005691020283848047\n",
            "step: 170, loss: 0.0004641428531613201\n",
            "step: 180, loss: 0.0002887898008339107\n",
            "step: 190, loss: 7.441818888764828e-05\n",
            "step: 200, loss: 0.0021955121774226427\n",
            "step: 210, loss: 0.004495653323829174\n",
            "step: 220, loss: 0.0001569599553477019\n",
            "step: 230, loss: 5.5826872994657606e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9899216125419933, f1=0.9854096520763187, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.292902930406854e-05\n",
            "step: 10, loss: 4.8918962420430034e-05\n",
            "step: 20, loss: 0.023660490289330482\n",
            "step: 30, loss: 0.01659618318080902\n",
            "step: 40, loss: 6.70664303470403e-05\n",
            "step: 50, loss: 0.00011103779979748651\n",
            "step: 60, loss: 6.451793160522357e-05\n",
            "step: 70, loss: 0.00019883387722074986\n",
            "step: 80, loss: 5.16153231728822e-05\n",
            "step: 90, loss: 0.0005344322999008\n",
            "step: 100, loss: 6.870077777421102e-05\n",
            "step: 110, loss: 4.437848474481143e-05\n",
            "step: 120, loss: 5.504641740117222e-05\n",
            "step: 130, loss: 7.584707054775208e-05\n",
            "step: 140, loss: 5.740107371821068e-05\n",
            "step: 150, loss: 6.907341594342142e-05\n",
            "step: 160, loss: 0.010878631845116615\n",
            "step: 170, loss: 0.0013155206106603146\n",
            "step: 180, loss: 4.2538493289612234e-05\n",
            "step: 190, loss: 0.01315239630639553\n",
            "step: 200, loss: 3.131993435090408e-05\n",
            "step: 210, loss: 8.153261296683922e-05\n",
            "step: 220, loss: 0.016986606642603874\n",
            "step: 230, loss: 6.560638576047495e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9899216125419933, f1=0.9854423292273236, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.784575528698042e-05\n",
            "step: 10, loss: 0.0003249151341151446\n",
            "step: 20, loss: 6.226230470929295e-05\n",
            "step: 30, loss: 2.6808929760591127e-05\n",
            "step: 40, loss: 5.4164625908015296e-05\n",
            "step: 50, loss: 5.886247890884988e-05\n",
            "step: 60, loss: 4.207345409668051e-05\n",
            "step: 70, loss: 2.936428973043803e-05\n",
            "step: 80, loss: 2.3486491045332514e-05\n",
            "step: 90, loss: 3.120469409623183e-05\n",
            "step: 100, loss: 6.04140805080533e-05\n",
            "step: 110, loss: 9.325100836576894e-05\n",
            "step: 120, loss: 4.4327531213639304e-05\n",
            "step: 130, loss: 6.001477231620811e-05\n",
            "step: 140, loss: 4.285995237296447e-05\n",
            "step: 150, loss: 1.9325570974615403e-05\n",
            "step: 160, loss: 0.025308439508080482\n",
            "step: 170, loss: 5.454795973491855e-05\n",
            "step: 180, loss: 0.0012570908293128014\n",
            "step: 190, loss: 5.323756340658292e-05\n",
            "step: 200, loss: 2.005932583415415e-05\n",
            "step: 210, loss: 5.486497320816852e-05\n",
            "step: 220, loss: 4.807109144167043e-05\n",
            "step: 230, loss: 0.00021399730758275837\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9899216125419933, f1=0.9864864864864865, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.6258807085687295e-05\n",
            "step: 10, loss: 2.9982558771735057e-05\n",
            "step: 20, loss: 0.0012262965319678187\n",
            "step: 30, loss: 7.212837226688862e-05\n",
            "step: 40, loss: 3.451210795901716e-05\n",
            "step: 50, loss: 3.858284981106408e-05\n",
            "step: 60, loss: 4.3309933971613646e-05\n",
            "step: 70, loss: 3.460914740571752e-05\n",
            "step: 80, loss: 4.305157926864922e-05\n",
            "step: 90, loss: 6.110136018833145e-05\n",
            "step: 100, loss: 4.1618652176111937e-05\n",
            "step: 110, loss: 4.993742913939059e-05\n",
            "step: 120, loss: 1.558601070428267e-05\n",
            "step: 130, loss: 5.3167244914220646e-05\n",
            "step: 140, loss: 3.3637305023148656e-05\n",
            "step: 150, loss: 4.06179933634121e-05\n",
            "step: 160, loss: 3.45052030752413e-05\n",
            "step: 170, loss: 3.9220802136696875e-05\n",
            "step: 180, loss: 4.0215694752987474e-05\n",
            "step: 190, loss: 3.7957379390718415e-05\n",
            "step: 200, loss: 4.600756074069068e-05\n",
            "step: 210, loss: 2.6232097297906876e-05\n",
            "step: 220, loss: 6.39580175629817e-05\n",
            "step: 230, loss: 2.092757677019108e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9899216125419933, f1=0.9887640449438202, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014097033999860287\n",
            "step: 10, loss: 3.769745308090933e-05\n",
            "step: 20, loss: 6.927783397259191e-05\n",
            "step: 30, loss: 3.6818473745370284e-05\n",
            "step: 40, loss: 2.2924265067558736e-05\n",
            "step: 50, loss: 3.361120616318658e-05\n",
            "step: 60, loss: 0.025953946635127068\n",
            "step: 70, loss: 2.914136894105468e-05\n",
            "step: 80, loss: 3.21400621032808e-05\n",
            "step: 90, loss: 2.669402238097973e-05\n",
            "step: 100, loss: 2.547949770814739e-05\n",
            "step: 110, loss: 4.7691628424217924e-05\n",
            "step: 120, loss: 0.0226425901055336\n",
            "step: 130, loss: 2.7230402338318527e-05\n",
            "step: 140, loss: 0.01934669353067875\n",
            "step: 150, loss: 3.641654257080518e-05\n",
            "step: 160, loss: 2.7964095352217555e-05\n",
            "step: 170, loss: 1.9750506908167154e-05\n",
            "step: 180, loss: 3.431538789300248e-05\n",
            "step: 190, loss: 9.872261580312625e-05\n",
            "step: 200, loss: 0.0017664864426478744\n",
            "step: 210, loss: 0.032858990132808685\n",
            "step: 220, loss: 3.7000765587436035e-05\n",
            "step: 230, loss: 4.208544487482868e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9899216125419933, f1=0.9887892376681614, best_f1=0.9854096520763187\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 210.16it/s]\n",
            "load_f1 = 0.9853768278965129\n",
            "real_f1 = 0.9842696629213483\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 194.17it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_G0OicNeCnd"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkIRgx40ezP8",
        "outputId": "010061ed-e0d2-4949-84ef-15d82f05f993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6428455114364624\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.436378538608551\n",
            "step: 20, loss: 0.3920472264289856\n",
            "step: 30, loss: 0.31671714782714844\n",
            "step: 40, loss: 0.2833322584629059\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 50, loss: 1.070077896118164\n",
            "step: 60, loss: 0.3247353732585907\n",
            "step: 70, loss: 0.2664162814617157\n",
            "step: 80, loss: 0.16604173183441162\n",
            "step: 90, loss: 0.17757917940616608\n",
            "step: 100, loss: 0.2391546070575714\n",
            "step: 110, loss: 0.1717437207698822\n",
            "step: 120, loss: 0.29878103733062744\n",
            "step: 130, loss: 0.0975213572382927\n",
            "step: 140, loss: 0.23760360479354858\n",
            "step: 150, loss: 0.2501985430717468\n",
            "step: 160, loss: 0.19869188964366913\n",
            "step: 170, loss: 0.13719572126865387\n",
            "step: 180, loss: 0.13052107393741608\n",
            "step: 190, loss: 0.10149785131216049\n",
            "step: 200, loss: 0.059414226561784744\n",
            "step: 210, loss: 0.11074522882699966\n",
            "step: 220, loss: 0.11725971102714539\n",
            "step: 230, loss: 0.1370001882314682\n",
            "step: 240, loss: 0.049905840307474136\n",
            "step: 250, loss: 0.053800422698259354\n",
            "step: 260, loss: 0.15877531468868256\n",
            "step: 270, loss: 0.32333290576934814\n",
            "step: 280, loss: 0.0854954719543457\n",
            "step: 290, loss: 0.1435670107603073\n",
            "step: 300, loss: 0.05103469640016556\n",
            "step: 310, loss: 0.12384134531021118\n",
            "step: 320, loss: 0.10911592096090317\n",
            "step: 330, loss: 0.18416261672973633\n",
            "step: 340, loss: 0.3610546588897705\n",
            "step: 350, loss: 0.1679777204990387\n",
            "step: 360, loss: 0.07055708765983582\n",
            "step: 370, loss: 0.036135826259851456\n",
            "step: 380, loss: 0.22095686197280884\n",
            "step: 390, loss: 0.025992706418037415\n",
            "step: 400, loss: 0.05191895365715027\n",
            "step: 410, loss: 0.2814394533634186\n",
            "step: 420, loss: 0.0466008298099041\n",
            "step: 430, loss: 0.019946126267313957\n",
            "step: 440, loss: 0.01708020456135273\n",
            "step: 450, loss: 0.04015304520726204\n",
            "step: 460, loss: 0.057416342198848724\n",
            "step: 470, loss: 0.027925455942749977\n",
            "step: 480, loss: 0.18375176191329956\n",
            "step: 490, loss: 0.014219951815903187\n",
            "step: 500, loss: 0.11895200610160828\n",
            "step: 510, loss: 0.06581354886293411\n",
            "step: 520, loss: 0.08467630296945572\n",
            "step: 530, loss: 0.05154057592153549\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.940795559666975, f1=0.9317972350230416, best_f1=0.9317972350230416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09071556478738785\n",
            "step: 10, loss: 0.048416778445243835\n",
            "step: 20, loss: 0.06987757235765457\n",
            "step: 30, loss: 0.013401794247329235\n",
            "step: 40, loss: 0.04430977627635002\n",
            "step: 50, loss: 0.06556609272956848\n",
            "step: 60, loss: 0.12296409159898758\n",
            "step: 70, loss: 0.027293719351291656\n",
            "step: 80, loss: 0.030915703624486923\n",
            "step: 90, loss: 0.006236235611140728\n",
            "step: 100, loss: 0.10217363387346268\n",
            "step: 110, loss: 0.008330713026225567\n",
            "step: 120, loss: 0.07289460301399231\n",
            "step: 130, loss: 0.0140118058770895\n",
            "step: 140, loss: 0.11198582500219345\n",
            "step: 150, loss: 0.04370497167110443\n",
            "step: 160, loss: 0.07711318880319595\n",
            "step: 170, loss: 0.08033488690853119\n",
            "step: 180, loss: 0.03491684049367905\n",
            "step: 190, loss: 0.016213424503803253\n",
            "step: 200, loss: 0.1649269014596939\n",
            "step: 210, loss: 0.015330142341554165\n",
            "step: 220, loss: 0.0009397593094035983\n",
            "step: 230, loss: 0.041741251945495605\n",
            "step: 240, loss: 0.04152462258934975\n",
            "step: 250, loss: 0.04931073635816574\n",
            "step: 260, loss: 0.05295509099960327\n",
            "step: 270, loss: 0.11096250265836716\n",
            "step: 280, loss: 0.04834398627281189\n",
            "step: 290, loss: 0.03520923852920532\n",
            "step: 300, loss: 0.01413422916084528\n",
            "step: 310, loss: 0.019924458116292953\n",
            "step: 320, loss: 0.1726522445678711\n",
            "step: 330, loss: 0.06882220506668091\n",
            "step: 340, loss: 0.03129159286618233\n",
            "step: 350, loss: 0.005053170491009951\n",
            "step: 360, loss: 0.11508022248744965\n",
            "step: 370, loss: 0.037617579102516174\n",
            "step: 380, loss: 0.15042924880981445\n",
            "step: 390, loss: 0.011171405203640461\n",
            "step: 400, loss: 0.04719995707273483\n",
            "step: 410, loss: 0.06794720888137817\n",
            "step: 420, loss: 0.046285342425107956\n",
            "step: 430, loss: 0.16357043385505676\n",
            "step: 440, loss: 0.012844924814999104\n",
            "step: 450, loss: 0.04039726406335831\n",
            "step: 460, loss: 0.04013751447200775\n",
            "step: 470, loss: 0.026865556836128235\n",
            "step: 480, loss: 0.018977178260684013\n",
            "step: 490, loss: 0.08422830700874329\n",
            "step: 500, loss: 0.0137609438970685\n",
            "step: 510, loss: 0.03554972633719444\n",
            "step: 520, loss: 0.4246916174888611\n",
            "step: 530, loss: 0.09603389352560043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9411764705882353, f1=0.9344490934449092, best_f1=0.9344490934449092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11084365099668503\n",
            "step: 10, loss: 0.022083908319473267\n",
            "step: 20, loss: 0.004749061074107885\n",
            "step: 30, loss: 0.046191927045583725\n",
            "step: 40, loss: 0.033986590802669525\n",
            "step: 50, loss: 0.11481466889381409\n",
            "step: 60, loss: 0.04661191254854202\n",
            "step: 70, loss: 0.005030884873121977\n",
            "step: 80, loss: 0.007007255684584379\n",
            "step: 90, loss: 0.020683934912085533\n",
            "step: 100, loss: 0.04876783490180969\n",
            "step: 110, loss: 0.0767252966761589\n",
            "step: 120, loss: 0.11470083892345428\n",
            "step: 130, loss: 0.037711694836616516\n",
            "step: 140, loss: 0.029119715094566345\n",
            "step: 150, loss: 0.06648741662502289\n",
            "step: 160, loss: 0.031190382316708565\n",
            "step: 170, loss: 0.012268709018826485\n",
            "step: 180, loss: 0.025930359959602356\n",
            "step: 190, loss: 0.004266597796231508\n",
            "step: 200, loss: 0.00931665301322937\n",
            "step: 210, loss: 0.03017275594174862\n",
            "step: 220, loss: 0.10154324024915695\n",
            "step: 230, loss: 0.02166687324643135\n",
            "step: 240, loss: 0.08882357180118561\n",
            "step: 250, loss: 0.2268189638853073\n",
            "step: 260, loss: 0.10052593052387238\n",
            "step: 270, loss: 0.0048355781473219395\n",
            "step: 280, loss: 0.0017934638308361173\n",
            "step: 290, loss: 0.05598932132124901\n",
            "step: 300, loss: 0.11402438580989838\n",
            "step: 310, loss: 0.08059557527303696\n",
            "step: 320, loss: 0.05328860506415367\n",
            "step: 330, loss: 0.00840666238218546\n",
            "step: 340, loss: 0.00949153769761324\n",
            "step: 350, loss: 0.08721388876438141\n",
            "step: 360, loss: 0.009627953171730042\n",
            "step: 370, loss: 0.022065885365009308\n",
            "step: 380, loss: 0.03848652169108391\n",
            "step: 390, loss: 0.00350208580493927\n",
            "step: 400, loss: 0.13361996412277222\n",
            "step: 410, loss: 0.038122981786727905\n",
            "step: 420, loss: 0.004489355254918337\n",
            "step: 430, loss: 0.018754683434963226\n",
            "step: 440, loss: 0.17703042924404144\n",
            "step: 450, loss: 0.008556765504181385\n",
            "step: 460, loss: 0.08866633474826813\n",
            "step: 470, loss: 0.03787568211555481\n",
            "step: 480, loss: 0.09636642783880234\n",
            "step: 490, loss: 0.008474914357066154\n",
            "step: 500, loss: 0.050067637115716934\n",
            "step: 510, loss: 0.06417733430862427\n",
            "step: 520, loss: 0.004100969526916742\n",
            "step: 530, loss: 0.018521996214985847\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9544186046511628, f1=0.9456928838951311, best_f1=0.9456928838951311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010784360580146313\n",
            "step: 10, loss: 0.0010160357924178243\n",
            "step: 20, loss: 0.0936414822936058\n",
            "step: 30, loss: 0.12846630811691284\n",
            "step: 40, loss: 0.011540671810507774\n",
            "step: 50, loss: 0.02767351269721985\n",
            "step: 60, loss: 0.0109174232929945\n",
            "step: 70, loss: 0.024306273087859154\n",
            "step: 80, loss: 0.05170964077115059\n",
            "step: 90, loss: 0.03406869247555733\n",
            "step: 100, loss: 0.000970583816524595\n",
            "step: 110, loss: 0.01708579994738102\n",
            "step: 120, loss: 0.003059742273762822\n",
            "step: 130, loss: 0.004116202238947153\n",
            "step: 140, loss: 0.041512053459882736\n",
            "step: 150, loss: 0.01617429405450821\n",
            "step: 160, loss: 0.0015984044875949621\n",
            "step: 170, loss: 0.01903085596859455\n",
            "step: 180, loss: 0.12501004338264465\n",
            "step: 190, loss: 0.04543760418891907\n",
            "step: 200, loss: 0.07252948731184006\n",
            "step: 210, loss: 0.004415198229253292\n",
            "step: 220, loss: 0.002898281440138817\n",
            "step: 230, loss: 0.029959101229906082\n",
            "step: 240, loss: 0.03182126581668854\n",
            "step: 250, loss: 0.1424063891172409\n",
            "step: 260, loss: 0.012250254862010479\n",
            "step: 270, loss: 0.06713180243968964\n",
            "step: 280, loss: 0.014357421547174454\n",
            "step: 290, loss: 0.07527118176221848\n",
            "step: 300, loss: 0.002183838514611125\n",
            "step: 310, loss: 0.016745280474424362\n",
            "step: 320, loss: 0.07723191380500793\n",
            "step: 330, loss: 0.015360258519649506\n",
            "step: 340, loss: 0.008497586473822594\n",
            "step: 350, loss: 0.04838789254426956\n",
            "step: 360, loss: 0.044763240963220596\n",
            "step: 370, loss: 0.003142973408102989\n",
            "step: 380, loss: 0.01898556388914585\n",
            "step: 390, loss: 0.0008148064953275025\n",
            "step: 400, loss: 0.010018294677138329\n",
            "step: 410, loss: 0.06671394407749176\n",
            "step: 420, loss: 0.021702965721488\n",
            "step: 430, loss: 0.03172372654080391\n",
            "step: 440, loss: 0.006550562102347612\n",
            "step: 450, loss: 0.006286136340349913\n",
            "step: 460, loss: 0.046895332634449005\n",
            "step: 470, loss: 0.0016749667702242732\n",
            "step: 480, loss: 0.002959619741886854\n",
            "step: 490, loss: 0.0005609566578641534\n",
            "step: 500, loss: 0.007173325400799513\n",
            "step: 510, loss: 0.03979984298348427\n",
            "step: 520, loss: 0.09126976132392883\n",
            "step: 530, loss: 0.1458308845758438\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9499072356215214, f1=0.9465437788018433, best_f1=0.9456928838951311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017911663744598627\n",
            "step: 10, loss: 0.006139229983091354\n",
            "step: 20, loss: 0.07069671154022217\n",
            "step: 30, loss: 0.002575530903413892\n",
            "step: 40, loss: 0.0005753772566094995\n",
            "step: 50, loss: 0.011764845810830593\n",
            "step: 60, loss: 0.003315706504508853\n",
            "step: 70, loss: 0.001595210051164031\n",
            "step: 80, loss: 0.0021653228905051947\n",
            "step: 90, loss: 0.027002889662981033\n",
            "step: 100, loss: 0.024415137246251106\n",
            "step: 110, loss: 0.001157106482423842\n",
            "step: 120, loss: 0.23258891701698303\n",
            "step: 130, loss: 0.006754620466381311\n",
            "step: 140, loss: 0.008406680077314377\n",
            "step: 150, loss: 0.004752673674374819\n",
            "step: 160, loss: 0.03133028373122215\n",
            "step: 170, loss: 0.016237430274486542\n",
            "step: 180, loss: 0.003946714103221893\n",
            "step: 190, loss: 0.0032966528087854385\n",
            "step: 200, loss: 0.009161331690847874\n",
            "step: 210, loss: 0.0007793824188411236\n",
            "step: 220, loss: 0.003276695031672716\n",
            "step: 230, loss: 0.003717076499015093\n",
            "step: 240, loss: 0.009633459150791168\n",
            "step: 250, loss: 0.05798187106847763\n",
            "step: 260, loss: 0.0016420497559010983\n",
            "step: 270, loss: 0.0021989960223436356\n",
            "step: 280, loss: 0.003125265473499894\n",
            "step: 290, loss: 0.03451366722583771\n",
            "step: 300, loss: 0.048961713910102844\n",
            "step: 310, loss: 0.05552603676915169\n",
            "step: 320, loss: 0.14459136128425598\n",
            "step: 330, loss: 0.02370275929570198\n",
            "step: 340, loss: 0.009390830993652344\n",
            "step: 350, loss: 0.0008715293370187283\n",
            "step: 360, loss: 0.0006227961275726557\n",
            "step: 370, loss: 0.0010357891442254186\n",
            "step: 380, loss: 0.009563571773469448\n",
            "step: 390, loss: 0.010115444660186768\n",
            "step: 400, loss: 0.0322122797369957\n",
            "step: 410, loss: 0.029726078733801842\n",
            "step: 420, loss: 0.23802100121974945\n",
            "step: 430, loss: 0.0131898894906044\n",
            "step: 440, loss: 0.0005177437560632825\n",
            "step: 450, loss: 0.048117607831954956\n",
            "step: 460, loss: 0.009546143002808094\n",
            "step: 470, loss: 0.020527135580778122\n",
            "step: 480, loss: 0.023882631212472916\n",
            "step: 490, loss: 0.058916255831718445\n",
            "step: 500, loss: 0.028023086488246918\n",
            "step: 510, loss: 0.0011265479261055589\n",
            "step: 520, loss: 0.1378631293773651\n",
            "step: 530, loss: 0.0040959990583360195\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.952513966480447, f1=0.9419475655430711, best_f1=0.9456928838951311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004036587662994862\n",
            "step: 10, loss: 0.0022519631311297417\n",
            "step: 20, loss: 0.01252794824540615\n",
            "step: 30, loss: 0.0016198130324482918\n",
            "step: 40, loss: 0.19010503590106964\n",
            "step: 50, loss: 0.0016875243745744228\n",
            "step: 60, loss: 0.0043105692602694035\n",
            "step: 70, loss: 0.11432459950447083\n",
            "step: 80, loss: 0.0006280098459683359\n",
            "step: 90, loss: 0.004539988934993744\n",
            "step: 100, loss: 0.005470810923725367\n",
            "step: 110, loss: 0.003898969618603587\n",
            "step: 120, loss: 0.17937034368515015\n",
            "step: 130, loss: 0.0017440203810110688\n",
            "step: 140, loss: 0.0008307466632686555\n",
            "step: 150, loss: 0.0010450895642861724\n",
            "step: 160, loss: 0.03562411665916443\n",
            "step: 170, loss: 0.001544514438137412\n",
            "step: 180, loss: 0.0027398974634706974\n",
            "step: 190, loss: 0.16481514275074005\n",
            "step: 200, loss: 0.008811675943434238\n",
            "step: 210, loss: 0.004349452909082174\n",
            "step: 220, loss: 0.004049328155815601\n",
            "step: 230, loss: 0.002960752695798874\n",
            "step: 240, loss: 0.04174908250570297\n",
            "step: 250, loss: 0.017001036554574966\n",
            "step: 260, loss: 0.0015109538799151778\n",
            "step: 270, loss: 0.0033993786200881004\n",
            "step: 280, loss: 0.0023763002827763557\n",
            "step: 290, loss: 0.0007855013245716691\n",
            "step: 300, loss: 0.008879845961928368\n",
            "step: 310, loss: 0.056657824665308\n",
            "step: 320, loss: 0.00026740459725260735\n",
            "step: 330, loss: 0.008953725919127464\n",
            "step: 340, loss: 0.04154288023710251\n",
            "step: 350, loss: 0.0028737178072333336\n",
            "step: 360, loss: 0.07264851778745651\n",
            "step: 370, loss: 0.0025664675049483776\n",
            "step: 380, loss: 0.00015689338033553213\n",
            "step: 390, loss: 0.004024897236377001\n",
            "step: 400, loss: 0.10850993543863297\n",
            "step: 410, loss: 0.005964592099189758\n",
            "step: 420, loss: 0.007522300351411104\n",
            "step: 430, loss: 0.001195669872686267\n",
            "step: 440, loss: 0.00023670891823712736\n",
            "step: 450, loss: 0.16730307042598724\n",
            "step: 460, loss: 0.003007170045748353\n",
            "step: 470, loss: 0.031240040436387062\n",
            "step: 480, loss: 0.006832359358668327\n",
            "step: 490, loss: 0.005226545035839081\n",
            "step: 500, loss: 0.0283577349036932\n",
            "step: 510, loss: 0.2188146412372589\n",
            "step: 520, loss: 0.0005636180867440999\n",
            "step: 530, loss: 0.050264038145542145\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9518015910154421, f1=0.9503280224929709, best_f1=0.9456928838951311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002421777928248048\n",
            "step: 10, loss: 0.0019747517071664333\n",
            "step: 20, loss: 0.003423137590289116\n",
            "step: 30, loss: 0.018827401101589203\n",
            "step: 40, loss: 0.001811276888474822\n",
            "step: 50, loss: 0.003653570543974638\n",
            "step: 60, loss: 0.005888179410248995\n",
            "step: 70, loss: 0.00023148265609052032\n",
            "step: 80, loss: 0.0011741736670956016\n",
            "step: 90, loss: 0.0002181615709559992\n",
            "step: 100, loss: 0.09282669425010681\n",
            "step: 110, loss: 0.004032832570374012\n",
            "step: 120, loss: 0.004019351210445166\n",
            "step: 130, loss: 0.0013629210880026221\n",
            "step: 140, loss: 0.0028727364260703325\n",
            "step: 150, loss: 0.002458631992340088\n",
            "step: 160, loss: 0.001399464439600706\n",
            "step: 170, loss: 0.0014634976396337152\n",
            "step: 180, loss: 0.013547569513320923\n",
            "step: 190, loss: 0.019488830119371414\n",
            "step: 200, loss: 0.000657523749396205\n",
            "step: 210, loss: 0.001693468657322228\n",
            "step: 220, loss: 0.009934922680258751\n",
            "step: 230, loss: 0.0017379655037075281\n",
            "step: 240, loss: 0.06260666251182556\n",
            "step: 250, loss: 0.02238328941166401\n",
            "step: 260, loss: 0.01334551814943552\n",
            "step: 270, loss: 0.0006763431592844427\n",
            "step: 280, loss: 0.002618331229314208\n",
            "step: 290, loss: 0.0012962794862687588\n",
            "step: 300, loss: 0.0004323706089053303\n",
            "step: 310, loss: 0.001352431601844728\n",
            "step: 320, loss: 0.0029666235204786062\n",
            "step: 330, loss: 0.025249386206269264\n",
            "step: 340, loss: 0.008331498131155968\n",
            "step: 350, loss: 0.0009129612590186298\n",
            "step: 360, loss: 0.022937975823879242\n",
            "step: 370, loss: 0.052108749747276306\n",
            "step: 380, loss: 0.003859141608700156\n",
            "step: 390, loss: 0.0032447988633066416\n",
            "step: 400, loss: 0.016184119507670403\n",
            "step: 410, loss: 0.003970160149037838\n",
            "step: 420, loss: 0.11359325051307678\n",
            "step: 430, loss: 0.002545698080211878\n",
            "step: 440, loss: 0.00380340707488358\n",
            "step: 450, loss: 0.01002802886068821\n",
            "step: 460, loss: 0.0018137098522856832\n",
            "step: 470, loss: 0.26111477613449097\n",
            "step: 480, loss: 0.006088997703045607\n",
            "step: 490, loss: 0.0075179473496973515\n",
            "step: 500, loss: 0.008029531687498093\n",
            "step: 510, loss: 0.0019462838536128402\n",
            "step: 520, loss: 0.0036613685078918934\n",
            "step: 530, loss: 0.002211366780102253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9520260829063809, f1=0.9478098788443615, best_f1=0.9456928838951311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017381382640451193\n",
            "step: 10, loss: 0.00890401005744934\n",
            "step: 20, loss: 0.002976402174681425\n",
            "step: 30, loss: 0.00040873041143640876\n",
            "step: 40, loss: 0.00021729108993895352\n",
            "step: 50, loss: 0.0016175889177247882\n",
            "step: 60, loss: 0.0005602026358246803\n",
            "step: 70, loss: 0.0011201928136870265\n",
            "step: 80, loss: 0.0033820911776274443\n",
            "step: 90, loss: 0.00027085162582807243\n",
            "step: 100, loss: 0.0003278615768067539\n",
            "step: 110, loss: 0.0003636517212726176\n",
            "step: 120, loss: 0.0008248809026554227\n",
            "step: 130, loss: 0.0023144155275076628\n",
            "step: 140, loss: 0.0006214894237928092\n",
            "step: 150, loss: 0.0013423544587567449\n",
            "step: 160, loss: 0.00010248873149976134\n",
            "step: 170, loss: 0.14241817593574524\n",
            "step: 180, loss: 0.00020864132966380566\n",
            "step: 190, loss: 0.0009483560570515692\n",
            "step: 200, loss: 0.0013836428988724947\n",
            "step: 210, loss: 0.09075827151536942\n",
            "step: 220, loss: 0.0013792234240099788\n",
            "step: 230, loss: 0.0024686388205736876\n",
            "step: 240, loss: 0.00044653660734184086\n",
            "step: 250, loss: 0.0016653335187584162\n",
            "step: 260, loss: 0.00011926217121072114\n",
            "step: 270, loss: 0.002888477174565196\n",
            "step: 280, loss: 0.00011448210716480389\n",
            "step: 290, loss: 0.00011296752927592024\n",
            "step: 300, loss: 5.6010692787822336e-05\n",
            "step: 310, loss: 0.0001470624702051282\n",
            "step: 320, loss: 0.005577407777309418\n",
            "step: 330, loss: 0.0003279618395026773\n",
            "step: 340, loss: 0.1317559778690338\n",
            "step: 350, loss: 0.0002288206887897104\n",
            "step: 360, loss: 0.035937946289777756\n",
            "step: 370, loss: 0.008243966847658157\n",
            "step: 380, loss: 0.00014235831622499973\n",
            "step: 390, loss: 0.004663634579628706\n",
            "step: 400, loss: 0.002200178802013397\n",
            "step: 410, loss: 0.0014313748106360435\n",
            "step: 420, loss: 0.004353879950940609\n",
            "step: 430, loss: 0.0034282496199011803\n",
            "step: 440, loss: 0.020583108067512512\n",
            "step: 450, loss: 0.0008856214699335396\n",
            "step: 460, loss: 0.004972873721271753\n",
            "step: 470, loss: 0.036747150123119354\n",
            "step: 480, loss: 0.04297645762562752\n",
            "step: 490, loss: 0.0019603674300014973\n",
            "step: 500, loss: 0.0009992329869419336\n",
            "step: 510, loss: 0.006462186574935913\n",
            "step: 520, loss: 0.0007404112257063389\n",
            "step: 530, loss: 0.0009190631681121886\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9489322191272052, f1=0.9438515081206497, best_f1=0.9456928838951311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012569117825478315\n",
            "step: 10, loss: 0.00041733362013474107\n",
            "step: 20, loss: 0.00023386803513858467\n",
            "step: 30, loss: 0.04134334996342659\n",
            "step: 40, loss: 0.0004958867793902755\n",
            "step: 50, loss: 0.017297988757491112\n",
            "step: 60, loss: 0.004336588084697723\n",
            "step: 70, loss: 0.019266342744231224\n",
            "step: 80, loss: 0.007960719056427479\n",
            "step: 90, loss: 0.0762588158249855\n",
            "step: 100, loss: 0.0006524701020680368\n",
            "step: 110, loss: 0.0021105215419083834\n",
            "step: 120, loss: 0.0018832116620615125\n",
            "step: 130, loss: 0.0013382574543356895\n",
            "step: 140, loss: 0.0003512138209771365\n",
            "step: 150, loss: 0.0018693411257117987\n",
            "step: 160, loss: 0.0009081231546588242\n",
            "step: 170, loss: 0.0016785371117293835\n",
            "step: 180, loss: 0.006161737721413374\n",
            "step: 190, loss: 8.482330304104835e-05\n",
            "step: 200, loss: 0.00011797394108725712\n",
            "step: 210, loss: 0.0004924405948258936\n",
            "step: 220, loss: 0.004519609268754721\n",
            "step: 230, loss: 0.0005320970085449517\n",
            "step: 240, loss: 0.00030526123009622097\n",
            "step: 250, loss: 0.0003463192260824144\n",
            "step: 260, loss: 0.0005903275450691581\n",
            "step: 270, loss: 0.00790527742356062\n",
            "step: 280, loss: 0.0018090368248522282\n",
            "step: 290, loss: 0.00016685047012288123\n",
            "step: 300, loss: 0.0004949549329467118\n",
            "step: 310, loss: 0.008904779329895973\n",
            "step: 320, loss: 0.0001108868164010346\n",
            "step: 330, loss: 0.0004949251306243241\n",
            "step: 340, loss: 0.0039734793826937675\n",
            "step: 350, loss: 0.03241670876741409\n",
            "step: 360, loss: 0.0007696668617427349\n",
            "step: 370, loss: 0.00036813667975366116\n",
            "step: 380, loss: 0.009839949198067188\n",
            "step: 390, loss: 5.406954369391315e-05\n",
            "step: 400, loss: 0.06194952130317688\n",
            "step: 410, loss: 0.001322534866631031\n",
            "step: 420, loss: 0.000358390825567767\n",
            "step: 430, loss: 0.0004928457201458514\n",
            "step: 440, loss: 0.01099467370659113\n",
            "step: 450, loss: 0.002859319793060422\n",
            "step: 460, loss: 0.000141443480970338\n",
            "step: 470, loss: 0.006587877869606018\n",
            "step: 480, loss: 0.0020389927085489035\n",
            "step: 490, loss: 0.0003114438150078058\n",
            "step: 500, loss: 0.00043844617903232574\n",
            "step: 510, loss: 0.0037657844368368387\n",
            "step: 520, loss: 0.002873774617910385\n",
            "step: 530, loss: 0.0020726658403873444\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9498580889309367, f1=0.9451795841209829, best_f1=0.9456928838951311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002512797713279724\n",
            "step: 10, loss: 0.0050746118649840355\n",
            "step: 20, loss: 0.0003205843095201999\n",
            "step: 30, loss: 0.004407827276736498\n",
            "step: 40, loss: 0.0011371553409844637\n",
            "step: 50, loss: 0.0005017908406443894\n",
            "step: 60, loss: 0.0011681924806907773\n",
            "step: 70, loss: 0.00024754501646384597\n",
            "step: 80, loss: 0.0001701819128356874\n",
            "step: 90, loss: 0.01624842919409275\n",
            "step: 100, loss: 0.0012162530329078436\n",
            "step: 110, loss: 0.010027850978076458\n",
            "step: 120, loss: 0.00024846848100423813\n",
            "step: 130, loss: 0.002712627872824669\n",
            "step: 140, loss: 0.010663512162864208\n",
            "step: 150, loss: 0.00018832214118447155\n",
            "step: 160, loss: 0.021931439638137817\n",
            "step: 170, loss: 0.002099870704114437\n",
            "step: 180, loss: 0.0002117332478519529\n",
            "step: 190, loss: 0.09235210716724396\n",
            "step: 200, loss: 0.015548406168818474\n",
            "step: 210, loss: 0.004683646839112043\n",
            "step: 220, loss: 0.002876052400097251\n",
            "step: 230, loss: 8.226809586631134e-05\n",
            "step: 240, loss: 0.00018444101442582905\n",
            "step: 250, loss: 0.001254342612810433\n",
            "step: 260, loss: 0.0016283350996673107\n",
            "step: 270, loss: 0.00013155445049051195\n",
            "step: 280, loss: 0.0237441286444664\n",
            "step: 290, loss: 0.00013938495249021798\n",
            "step: 300, loss: 0.0016226700972765684\n",
            "step: 310, loss: 0.02719958871603012\n",
            "step: 320, loss: 0.008181188255548477\n",
            "step: 330, loss: 0.0010512351291254163\n",
            "step: 340, loss: 0.00013139254588168114\n",
            "step: 350, loss: 0.00034708884777501225\n",
            "step: 360, loss: 0.00016036244051065296\n",
            "step: 370, loss: 0.0160478837788105\n",
            "step: 380, loss: 0.0009264542022719979\n",
            "step: 390, loss: 0.00032934240880422294\n",
            "step: 400, loss: 0.00026697281282395124\n",
            "step: 410, loss: 0.00024327379651367664\n",
            "step: 420, loss: 0.00014189760258886963\n",
            "step: 430, loss: 0.0002864408306777477\n",
            "step: 440, loss: 0.00015524108312092721\n",
            "step: 450, loss: 0.02152165211737156\n",
            "step: 460, loss: 0.00020606376347132027\n",
            "step: 470, loss: 0.003779681632295251\n",
            "step: 480, loss: 0.010588285513222218\n",
            "step: 490, loss: 0.0012584467185661197\n",
            "step: 500, loss: 0.000728511658962816\n",
            "step: 510, loss: 0.00011006057320628315\n",
            "step: 520, loss: 0.0019626093562692404\n",
            "step: 530, loss: 0.01786009781062603\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9575757575757574, f1=0.9491841491841492, best_f1=0.9491841491841492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005397719796746969\n",
            "step: 10, loss: 8.88911381480284e-05\n",
            "step: 20, loss: 0.0001277662959182635\n",
            "step: 30, loss: 0.00013788229261990637\n",
            "step: 40, loss: 0.0002384827093919739\n",
            "step: 50, loss: 0.00012959481682628393\n",
            "step: 60, loss: 0.0069680726155638695\n",
            "step: 70, loss: 9.036614937940612e-05\n",
            "step: 80, loss: 6.118091550888494e-05\n",
            "step: 90, loss: 0.00036563791218213737\n",
            "step: 100, loss: 0.000463494478026405\n",
            "step: 110, loss: 0.0023047581780701876\n",
            "step: 120, loss: 0.00048751302529126406\n",
            "step: 130, loss: 0.00018887191254179925\n",
            "step: 140, loss: 0.00016189724556170404\n",
            "step: 150, loss: 0.0011886986903846264\n",
            "step: 160, loss: 8.486188744427636e-05\n",
            "step: 170, loss: 2.9640250431839377e-05\n",
            "step: 180, loss: 8.72536184033379e-05\n",
            "step: 190, loss: 0.00020939834939781576\n",
            "step: 200, loss: 4.247929609846324e-05\n",
            "step: 210, loss: 0.00022466899827122688\n",
            "step: 220, loss: 0.003944115713238716\n",
            "step: 230, loss: 0.0001268559426534921\n",
            "step: 240, loss: 0.008065053261816502\n",
            "step: 250, loss: 0.003390273079276085\n",
            "step: 260, loss: 0.00626661442220211\n",
            "step: 270, loss: 0.0025281759444624186\n",
            "step: 280, loss: 0.005054634064435959\n",
            "step: 290, loss: 0.00035906920675188303\n",
            "step: 300, loss: 0.0006340932450257242\n",
            "step: 310, loss: 0.0003470387600827962\n",
            "step: 320, loss: 0.0005186771741136909\n",
            "step: 330, loss: 3.8922113162698224e-05\n",
            "step: 340, loss: 0.00020359450718387961\n",
            "step: 350, loss: 4.5195592974778265e-05\n",
            "step: 360, loss: 0.0004928760463371873\n",
            "step: 370, loss: 0.0016920168418437243\n",
            "step: 380, loss: 0.00021851247583981603\n",
            "step: 390, loss: 0.0002257609012303874\n",
            "step: 400, loss: 8.969460031948984e-05\n",
            "step: 410, loss: 0.0004179116222076118\n",
            "step: 420, loss: 6.0295118601061404e-05\n",
            "step: 430, loss: 0.0011126261670142412\n",
            "step: 440, loss: 0.00021294120233505964\n",
            "step: 450, loss: 0.0006380650447681546\n",
            "step: 460, loss: 0.0022664412390440702\n",
            "step: 470, loss: 0.00016209669411182404\n",
            "step: 480, loss: 0.0004901653155684471\n",
            "step: 490, loss: 0.005084388889372349\n",
            "step: 500, loss: 9.644406236475334e-05\n",
            "step: 510, loss: 0.0011123422300443053\n",
            "step: 520, loss: 0.0009632160072214901\n",
            "step: 530, loss: 0.009253681637346745\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9471733086190918, f1=0.9431192660550459, best_f1=0.9491841491841492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000436954025644809\n",
            "step: 10, loss: 0.0032310769893229008\n",
            "step: 20, loss: 0.0014593459200114012\n",
            "step: 30, loss: 0.00010166969150304794\n",
            "step: 40, loss: 0.0002329240378458053\n",
            "step: 50, loss: 0.00011604510655160993\n",
            "step: 60, loss: 8.004371920833364e-05\n",
            "step: 70, loss: 9.712615428725258e-05\n",
            "step: 80, loss: 0.2545364499092102\n",
            "step: 90, loss: 0.009676985442638397\n",
            "step: 100, loss: 0.0007802650798112154\n",
            "step: 110, loss: 0.00030074172536842525\n",
            "step: 120, loss: 0.0009135388536378741\n",
            "step: 130, loss: 0.0008159307762980461\n",
            "step: 140, loss: 0.012406392954289913\n",
            "step: 150, loss: 0.011240012012422085\n",
            "step: 160, loss: 0.040532831102609634\n",
            "step: 170, loss: 0.003386058611795306\n",
            "step: 180, loss: 0.00010036725871032104\n",
            "step: 190, loss: 0.00021220583585090935\n",
            "step: 200, loss: 0.016563810408115387\n",
            "step: 210, loss: 0.011333351023495197\n",
            "step: 220, loss: 9.139113535638899e-05\n",
            "step: 230, loss: 0.003144505899399519\n",
            "step: 240, loss: 0.0024494901299476624\n",
            "step: 250, loss: 9.850761853158474e-05\n",
            "step: 260, loss: 5.2740000683115795e-05\n",
            "step: 270, loss: 0.0064943330362439156\n",
            "step: 280, loss: 0.00018585183715913445\n",
            "step: 290, loss: 0.0002571379882283509\n",
            "step: 300, loss: 0.00016570965817663819\n",
            "step: 310, loss: 0.00010997473873430863\n",
            "step: 320, loss: 0.004354106727987528\n",
            "step: 330, loss: 0.011516304686665535\n",
            "step: 340, loss: 0.0019849108066409826\n",
            "step: 350, loss: 2.6614632588461973e-05\n",
            "step: 360, loss: 5.60809530725237e-05\n",
            "step: 370, loss: 7.180229295045137e-05\n",
            "step: 380, loss: 4.806019569514319e-05\n",
            "step: 390, loss: 0.0018885097233578563\n",
            "step: 400, loss: 0.0032492359168827534\n",
            "step: 410, loss: 0.0003626826801337302\n",
            "step: 420, loss: 3.78075274056755e-05\n",
            "step: 430, loss: 0.00021736552298534662\n",
            "step: 440, loss: 0.00294170412234962\n",
            "step: 450, loss: 0.009122745133936405\n",
            "step: 460, loss: 0.0004376620636321604\n",
            "step: 470, loss: 0.0033771393354982138\n",
            "step: 480, loss: 0.014114348217844963\n",
            "step: 490, loss: 0.00017638840654399246\n",
            "step: 500, loss: 6.06595967838075e-05\n",
            "step: 510, loss: 0.001766461762599647\n",
            "step: 520, loss: 0.0025681492406874895\n",
            "step: 530, loss: 0.000340884318575263\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9536607970342911, f1=0.9492151431209603, best_f1=0.9491841491841492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006127231754362583\n",
            "step: 10, loss: 6.188720726640895e-05\n",
            "step: 20, loss: 8.159514982253313e-05\n",
            "step: 30, loss: 2.7859625333803706e-05\n",
            "step: 40, loss: 6.083952757762745e-05\n",
            "step: 50, loss: 0.002229972742497921\n",
            "step: 60, loss: 9.655234316596761e-05\n",
            "step: 70, loss: 7.623298733960837e-05\n",
            "step: 80, loss: 8.824377437122166e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 90, loss: 0.0001349256344838068\n",
            "step: 100, loss: 0.003087046556174755\n",
            "step: 110, loss: 0.0022212283220142126\n",
            "step: 120, loss: 4.120251105632633e-05\n",
            "step: 130, loss: 2.9865699616493657e-05\n",
            "step: 140, loss: 0.00029001623624935746\n",
            "step: 150, loss: 0.0009241732186637819\n",
            "step: 160, loss: 9.497790597379208e-05\n",
            "step: 170, loss: 0.004301822278648615\n",
            "step: 180, loss: 0.0001349820231553167\n",
            "step: 190, loss: 7.781950262142345e-05\n",
            "step: 200, loss: 4.611936310539022e-05\n",
            "step: 210, loss: 7.899887714302167e-05\n",
            "step: 220, loss: 4.2180545278824866e-05\n",
            "step: 230, loss: 0.00029889147845096886\n",
            "step: 240, loss: 5.69321600778494e-05\n",
            "step: 250, loss: 7.13925328454934e-05\n",
            "step: 260, loss: 5.875614078831859e-05\n",
            "step: 270, loss: 0.08034560084342957\n",
            "step: 280, loss: 5.665449134539813e-05\n",
            "step: 290, loss: 0.00014566126628778875\n",
            "step: 300, loss: 8.871866157278419e-05\n",
            "step: 310, loss: 0.00019344172324053943\n",
            "step: 320, loss: 0.006830545142292976\n",
            "step: 330, loss: 0.000341722130542621\n",
            "step: 340, loss: 0.0012076868442818522\n",
            "step: 350, loss: 1.4196553820511326e-05\n",
            "step: 360, loss: 0.06549926847219467\n",
            "step: 370, loss: 0.00018709659343585372\n",
            "step: 380, loss: 0.0003149554831907153\n",
            "step: 390, loss: 0.0012655439786612988\n",
            "step: 400, loss: 0.00013861416664440185\n",
            "step: 410, loss: 0.005233116913586855\n",
            "step: 420, loss: 6.583402864634991e-05\n",
            "step: 430, loss: 0.00023693102411925793\n",
            "step: 440, loss: 4.2085415770998225e-05\n",
            "step: 450, loss: 0.0008323561050929129\n",
            "step: 460, loss: 0.000180178860318847\n",
            "step: 470, loss: 0.00012784967839252204\n",
            "step: 480, loss: 3.5180331906303763e-05\n",
            "step: 490, loss: 0.00010802598990267143\n",
            "step: 500, loss: 5.477893864735961e-05\n",
            "step: 510, loss: 0.00020858852076344192\n",
            "step: 520, loss: 0.0005415737978182733\n",
            "step: 530, loss: 0.004068127367645502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9496268656716418, f1=0.9462867818776274, best_f1=0.9491841491841492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002926560409832746\n",
            "step: 10, loss: 0.001163668348453939\n",
            "step: 20, loss: 0.00010603827831801027\n",
            "step: 30, loss: 0.004489606246352196\n",
            "step: 40, loss: 5.849526860401966e-05\n",
            "step: 50, loss: 0.024504993110895157\n",
            "step: 60, loss: 4.477081893128343e-05\n",
            "step: 70, loss: 4.594403435476124e-05\n",
            "step: 80, loss: 9.650111314840615e-05\n",
            "step: 90, loss: 3.9539405406685546e-05\n",
            "step: 100, loss: 4.8260400944855064e-05\n",
            "step: 110, loss: 0.00034840425360016525\n",
            "step: 120, loss: 4.4364594941725954e-05\n",
            "step: 130, loss: 6.124685023678467e-05\n",
            "step: 140, loss: 0.00039792899042367935\n",
            "step: 150, loss: 6.19704369455576e-05\n",
            "step: 160, loss: 6.469463551184162e-05\n",
            "step: 170, loss: 9.242711530532688e-05\n",
            "step: 180, loss: 5.7517590903444216e-05\n",
            "step: 190, loss: 7.23324847058393e-05\n",
            "step: 200, loss: 6.7705666879192e-05\n",
            "step: 210, loss: 0.024369604885578156\n",
            "step: 220, loss: 4.499250644585118e-05\n",
            "step: 230, loss: 4.846036608796567e-05\n",
            "step: 240, loss: 0.00039054802618920803\n",
            "step: 250, loss: 0.0008680908940732479\n",
            "step: 260, loss: 0.00014785610255785286\n",
            "step: 270, loss: 0.00030384177807718515\n",
            "step: 280, loss: 0.00014271136024035513\n",
            "step: 290, loss: 2.8335991373751312e-05\n",
            "step: 300, loss: 5.121417416376062e-05\n",
            "step: 310, loss: 0.00022462962078861892\n",
            "step: 320, loss: 5.119963316246867e-05\n",
            "step: 330, loss: 0.00012248128768987954\n",
            "step: 340, loss: 0.001845114747993648\n",
            "step: 350, loss: 6.963602208998054e-05\n",
            "step: 360, loss: 4.290796277928166e-05\n",
            "step: 370, loss: 0.004985298495739698\n",
            "step: 380, loss: 0.0001574742782395333\n",
            "step: 390, loss: 0.004008554387837648\n",
            "step: 400, loss: 0.009587435983121395\n",
            "step: 410, loss: 2.919603866757825e-05\n",
            "step: 420, loss: 0.0004766028723679483\n",
            "step: 430, loss: 0.00023445405531674623\n",
            "step: 440, loss: 0.00011939999967580661\n",
            "step: 450, loss: 6.358874816214666e-05\n",
            "step: 460, loss: 0.005650816950947046\n",
            "step: 470, loss: 0.000809514953289181\n",
            "step: 480, loss: 7.707967597525567e-05\n",
            "step: 490, loss: 0.0004800260649062693\n",
            "step: 500, loss: 0.0008371055009774864\n",
            "step: 510, loss: 0.0005712178535759449\n",
            "step: 520, loss: 0.00012584308569785208\n",
            "step: 530, loss: 0.0006434397655539215\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9524253731343284, f1=0.9467289719626168, best_f1=0.9491841491841492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013380103337112814\n",
            "step: 10, loss: 0.000519688124768436\n",
            "step: 20, loss: 0.00042838414083234966\n",
            "step: 30, loss: 0.0016926922835409641\n",
            "step: 40, loss: 0.00018183472275268286\n",
            "step: 50, loss: 0.0022841196041554213\n",
            "step: 60, loss: 0.0005679387832060456\n",
            "step: 70, loss: 7.547229324700311e-05\n",
            "step: 80, loss: 7.017006282694638e-05\n",
            "step: 90, loss: 5.862990292371251e-05\n",
            "step: 100, loss: 1.9086386600974947e-05\n",
            "step: 110, loss: 0.0005223239422775805\n",
            "step: 120, loss: 6.0112033679615706e-05\n",
            "step: 130, loss: 0.00017556818784214556\n",
            "step: 140, loss: 6.261104863369837e-05\n",
            "step: 150, loss: 0.0002712543064262718\n",
            "step: 160, loss: 7.718682172708213e-05\n",
            "step: 170, loss: 5.0641225243452936e-05\n",
            "step: 180, loss: 0.00011137066758237779\n",
            "step: 190, loss: 0.0004933031741529703\n",
            "step: 200, loss: 0.0008576521067880094\n",
            "step: 210, loss: 0.00023773554130457342\n",
            "step: 220, loss: 7.218424434540793e-05\n",
            "step: 230, loss: 0.24631720781326294\n",
            "step: 240, loss: 9.487134957453236e-05\n",
            "step: 250, loss: 5.172564851818606e-05\n",
            "step: 260, loss: 5.532243085326627e-05\n",
            "step: 270, loss: 9.726315329317003e-05\n",
            "step: 280, loss: 7.086002733558416e-05\n",
            "step: 290, loss: 5.147802221472375e-05\n",
            "step: 300, loss: 0.004609516356140375\n",
            "step: 310, loss: 0.04303043335676193\n",
            "step: 320, loss: 0.00011678897135425359\n",
            "step: 330, loss: 0.00012994305870961398\n",
            "step: 340, loss: 0.0005355658358894289\n",
            "step: 350, loss: 0.0007328983629122376\n",
            "step: 360, loss: 6.164415390230715e-05\n",
            "step: 370, loss: 0.00010000359907280654\n",
            "step: 380, loss: 7.216943049570546e-05\n",
            "step: 390, loss: 9.936308197211474e-05\n",
            "step: 400, loss: 0.01256515085697174\n",
            "step: 410, loss: 0.00045664928620681167\n",
            "step: 420, loss: 5.6530418078182265e-05\n",
            "step: 430, loss: 3.9666447264607996e-05\n",
            "step: 440, loss: 8.019357483135536e-05\n",
            "step: 450, loss: 0.03059791401028633\n",
            "step: 460, loss: 0.00019440740288700908\n",
            "step: 470, loss: 5.6340995797654614e-05\n",
            "step: 480, loss: 0.0014746689703315496\n",
            "step: 490, loss: 0.0011419735383242369\n",
            "step: 500, loss: 0.011261099949479103\n",
            "step: 510, loss: 7.670358172617853e-05\n",
            "step: 520, loss: 0.0017114623915404081\n",
            "step: 530, loss: 4.0167920815292746e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9519813519813519, f1=0.9472701819878675, best_f1=0.9491841491841492\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:20, 275.82it/s]\n",
            "load_f1 = 0.9544186046511628\n",
            "real_f1 = 0.9514925373134329\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 207.29it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdkSbOTQeC3W"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4XWqpo1e0O0",
        "outputId": "fcd3856a-07da-455a-cc91-0b05cdbf3200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.48703861236572266\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4372633099555969\n",
            "step: 20, loss: 0.4277274012565613\n",
            "step: 30, loss: 0.38341236114501953\n",
            "step: 40, loss: 0.28291016817092896\n",
            "step: 50, loss: 0.39987537264823914\n",
            "step: 60, loss: 0.48679688572883606\n",
            "step: 70, loss: 0.33430930972099304\n",
            "step: 80, loss: 0.372348427772522\n",
            "step: 90, loss: 0.27226659655570984\n",
            "step: 100, loss: 0.2314242571592331\n",
            "step: 110, loss: 0.3398303687572479\n",
            "step: 120, loss: 0.3918023109436035\n",
            "step: 130, loss: 0.2656960189342499\n",
            "step: 140, loss: 0.46980318427085876\n",
            "step: 150, loss: 0.37369003891944885\n",
            "step: 160, loss: 0.49410662055015564\n",
            "step: 170, loss: 0.24525731801986694\n",
            "step: 180, loss: 0.3771621584892273\n",
            "step: 190, loss: 0.5957652926445007\n",
            "step: 200, loss: 0.397040992975235\n",
            "step: 210, loss: 0.5420620441436768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3757440745830536\n",
            "step: 10, loss: 0.20241248607635498\n",
            "step: 20, loss: 0.5166983008384705\n",
            "step: 30, loss: 0.46990135312080383\n",
            "step: 40, loss: 0.44552719593048096\n",
            "step: 50, loss: 0.23045092821121216\n",
            "step: 60, loss: 0.3198981285095215\n",
            "step: 70, loss: 0.45119988918304443\n",
            "step: 80, loss: 0.3118818700313568\n",
            "step: 90, loss: 0.39277493953704834\n",
            "step: 100, loss: 0.5293835997581482\n",
            "step: 110, loss: 0.3757587969303131\n",
            "step: 120, loss: 0.23983502388000488\n",
            "step: 130, loss: 0.17310838401317596\n",
            "step: 140, loss: 0.2345464825630188\n",
            "step: 150, loss: 0.42417484521865845\n",
            "step: 160, loss: 0.15653164684772491\n",
            "step: 170, loss: 0.5923997759819031\n",
            "step: 180, loss: 0.3194909393787384\n",
            "step: 190, loss: 0.30947524309158325\n",
            "step: 200, loss: 0.14850708842277527\n",
            "step: 210, loss: 0.3077012598514557\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2451642006635666\n",
            "step: 10, loss: 0.2371755987405777\n",
            "step: 20, loss: 0.4841457009315491\n",
            "step: 30, loss: 0.251263827085495\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 40, loss: 0.4605080485343933\n",
            "step: 50, loss: 0.48426976799964905\n",
            "step: 60, loss: 0.4987736940383911\n",
            "step: 70, loss: 0.1991957575082779\n",
            "step: 80, loss: 0.46862974762916565\n",
            "step: 90, loss: 0.23879146575927734\n",
            "step: 100, loss: 0.389672189950943\n",
            "step: 110, loss: 0.23956641554832458\n",
            "step: 120, loss: 0.23186980187892914\n",
            "step: 130, loss: 0.15442048013210297\n",
            "step: 140, loss: 0.37767094373703003\n",
            "step: 150, loss: 0.3237311840057373\n",
            "step: 160, loss: 0.20295314490795135\n",
            "step: 170, loss: 0.37545251846313477\n",
            "step: 180, loss: 0.2391950637102127\n",
            "step: 190, loss: 0.16166311502456665\n",
            "step: 200, loss: 0.23304584622383118\n",
            "step: 210, loss: 0.2582980692386627\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3453706204891205\n",
            "step: 10, loss: 0.30933719873428345\n",
            "step: 20, loss: 0.2936190962791443\n",
            "step: 30, loss: 0.2416699379682541\n",
            "step: 40, loss: 0.23439329862594604\n",
            "step: 50, loss: 0.24960024654865265\n",
            "step: 60, loss: 0.49758973717689514\n",
            "step: 70, loss: 0.24814923107624054\n",
            "step: 80, loss: 0.24035976827144623\n",
            "step: 90, loss: 0.4496185779571533\n",
            "step: 100, loss: 0.38118618726730347\n",
            "step: 110, loss: 0.5867581367492676\n",
            "step: 120, loss: 0.3803413212299347\n",
            "step: 130, loss: 0.6726879477500916\n",
            "step: 140, loss: 0.5013930797576904\n",
            "step: 150, loss: 0.38548433780670166\n",
            "step: 160, loss: 0.31357815861701965\n",
            "step: 170, loss: 0.17513759434223175\n",
            "step: 180, loss: 0.08939892053604126\n",
            "step: 190, loss: 0.1659162938594818\n",
            "step: 200, loss: 0.30582523345947266\n",
            "step: 210, loss: 0.38481783866882324\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37944886088371277\n",
            "step: 10, loss: 0.31658992171287537\n",
            "step: 20, loss: 0.31824928522109985\n",
            "step: 30, loss: 0.23246335983276367\n",
            "step: 40, loss: 0.42316141724586487\n",
            "step: 50, loss: 0.3787674307823181\n",
            "step: 60, loss: 0.39026597142219543\n",
            "step: 70, loss: 0.2510892152786255\n",
            "step: 80, loss: 0.43217605352401733\n",
            "step: 90, loss: 0.42500582337379456\n",
            "step: 100, loss: 0.24510735273361206\n",
            "step: 110, loss: 0.1665959507226944\n",
            "step: 120, loss: 0.2404485046863556\n",
            "step: 130, loss: 0.3110901415348053\n",
            "step: 140, loss: 0.515785813331604\n",
            "step: 150, loss: 0.31370842456817627\n",
            "step: 160, loss: 0.2436876893043518\n",
            "step: 170, loss: 0.37529587745666504\n",
            "step: 180, loss: 0.24153423309326172\n",
            "step: 190, loss: 0.5439451336860657\n",
            "step: 200, loss: 0.36992964148521423\n",
            "step: 210, loss: 0.24611423909664154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16056574881076813\n",
            "step: 10, loss: 0.3828463852405548\n",
            "step: 20, loss: 0.3182421326637268\n",
            "step: 30, loss: 0.23413124680519104\n",
            "step: 40, loss: 0.30842965841293335\n",
            "step: 50, loss: 0.6150900721549988\n",
            "step: 60, loss: 0.31653720140457153\n",
            "step: 70, loss: 0.45154595375061035\n",
            "step: 80, loss: 0.3142932057380676\n",
            "step: 90, loss: 0.31131261587142944\n",
            "step: 100, loss: 0.31508126854896545\n",
            "step: 110, loss: 0.3779354393482208\n",
            "step: 120, loss: 0.24615079164505005\n",
            "step: 130, loss: 0.23990210890769958\n",
            "step: 140, loss: 0.37550732493400574\n",
            "step: 150, loss: 0.19073431193828583\n",
            "step: 160, loss: 0.16595245897769928\n",
            "step: 170, loss: 0.4510861933231354\n",
            "step: 180, loss: 0.3920387327671051\n",
            "step: 190, loss: 0.4506545066833496\n",
            "step: 200, loss: 0.31556251645088196\n",
            "step: 210, loss: 0.372784823179245\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.45664992928504944\n",
            "step: 10, loss: 0.3092542290687561\n",
            "step: 20, loss: 0.4545627534389496\n",
            "step: 30, loss: 0.24663172662258148\n",
            "step: 40, loss: 0.25348997116088867\n",
            "step: 50, loss: 0.3760402202606201\n",
            "step: 60, loss: 0.31277936697006226\n",
            "step: 70, loss: 0.2485278695821762\n",
            "step: 80, loss: 0.5063970685005188\n",
            "step: 90, loss: 0.37100091576576233\n",
            "step: 100, loss: 0.4335322380065918\n",
            "step: 110, loss: 0.3081360161304474\n",
            "step: 120, loss: 0.3018527626991272\n",
            "step: 130, loss: 0.4683201313018799\n",
            "step: 140, loss: 0.31496527791023254\n",
            "step: 150, loss: 0.31963032484054565\n",
            "step: 160, loss: 0.5652273893356323\n",
            "step: 170, loss: 0.6376721262931824\n",
            "step: 180, loss: 0.25031715631484985\n",
            "step: 190, loss: 0.2438494712114334\n",
            "step: 200, loss: 0.31476324796676636\n",
            "step: 210, loss: 0.3159103989601135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6082519292831421\n",
            "step: 10, loss: 0.3121248185634613\n",
            "step: 20, loss: 0.25338393449783325\n",
            "step: 30, loss: 0.25562286376953125\n",
            "step: 40, loss: 0.23968090116977692\n",
            "step: 50, loss: 0.16452111303806305\n",
            "step: 60, loss: 0.1640169769525528\n",
            "step: 70, loss: 0.4543125331401825\n",
            "step: 80, loss: 0.31236085295677185\n",
            "step: 90, loss: 0.3737366199493408\n",
            "step: 100, loss: 0.605002760887146\n",
            "step: 110, loss: 0.384113609790802\n",
            "step: 120, loss: 0.3101346492767334\n",
            "step: 130, loss: 0.17221282422542572\n",
            "step: 140, loss: 0.3124088943004608\n",
            "step: 150, loss: 0.45411455631256104\n",
            "step: 160, loss: 0.45082545280456543\n",
            "step: 170, loss: 0.5348711609840393\n",
            "step: 180, loss: 0.3143744468688965\n",
            "step: 190, loss: 0.23824520409107208\n",
            "step: 200, loss: 0.4617118239402771\n",
            "step: 210, loss: 0.37894222140312195\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5708286166191101\n",
            "step: 10, loss: 0.2537992298603058\n",
            "step: 20, loss: 0.3737708330154419\n",
            "step: 30, loss: 0.17656971514225006\n",
            "step: 40, loss: 0.16766107082366943\n",
            "step: 50, loss: 0.4536532461643219\n",
            "step: 60, loss: 0.17417222261428833\n",
            "step: 70, loss: 0.3811720311641693\n",
            "step: 80, loss: 0.3179512917995453\n",
            "step: 90, loss: 0.3083569407463074\n",
            "step: 100, loss: 0.4582410156726837\n",
            "step: 110, loss: 0.5393180847167969\n",
            "step: 120, loss: 0.37872856855392456\n",
            "step: 130, loss: 0.24186736345291138\n",
            "step: 140, loss: 0.5969129204750061\n",
            "step: 150, loss: 0.11203697323799133\n",
            "step: 160, loss: 0.3860814571380615\n",
            "step: 170, loss: 0.3143436312675476\n",
            "step: 180, loss: 0.4521755278110504\n",
            "step: 190, loss: 0.3139299154281616\n",
            "step: 200, loss: 0.3132971227169037\n",
            "step: 210, loss: 0.31449824571609497\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3113134503364563\n",
            "step: 10, loss: 0.3780207931995392\n",
            "step: 20, loss: 0.25283247232437134\n",
            "step: 30, loss: 0.1698472499847412\n",
            "step: 40, loss: 0.3071991503238678\n",
            "step: 50, loss: 0.5078248977661133\n",
            "step: 60, loss: 0.24526795744895935\n",
            "step: 70, loss: 0.37664496898651123\n",
            "step: 80, loss: 0.16624920070171356\n",
            "step: 90, loss: 0.3847285807132721\n",
            "step: 100, loss: 0.45269060134887695\n",
            "step: 110, loss: 0.1802794188261032\n",
            "step: 120, loss: 0.45618486404418945\n",
            "step: 130, loss: 0.24269329011440277\n",
            "step: 140, loss: 0.3752972185611725\n",
            "step: 150, loss: 0.24708817899227142\n",
            "step: 160, loss: 0.3765205144882202\n",
            "step: 170, loss: 0.2504688799381256\n",
            "step: 180, loss: 0.3186076283454895\n",
            "step: 190, loss: 0.24713164567947388\n",
            "step: 200, loss: 0.6515439748764038\n",
            "step: 210, loss: 0.24646729230880737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30904334783554077\n",
            "step: 10, loss: 0.3191092014312744\n",
            "step: 20, loss: 0.38280296325683594\n",
            "step: 30, loss: 0.16964685916900635\n",
            "step: 40, loss: 0.44382983446121216\n",
            "step: 50, loss: 0.43502628803253174\n",
            "step: 60, loss: 0.4389890730381012\n",
            "step: 70, loss: 0.17433233559131622\n",
            "step: 80, loss: 0.5194947719573975\n",
            "step: 90, loss: 0.3874870538711548\n",
            "step: 100, loss: 0.5251038074493408\n",
            "step: 110, loss: 0.44089335203170776\n",
            "step: 120, loss: 0.4462621510028839\n",
            "step: 130, loss: 0.23719938099384308\n",
            "step: 140, loss: 0.30763083696365356\n",
            "step: 150, loss: 0.3072744905948639\n",
            "step: 160, loss: 0.1648751050233841\n",
            "step: 170, loss: 0.31366047263145447\n",
            "step: 180, loss: 0.24323531985282898\n",
            "step: 190, loss: 0.4591929316520691\n",
            "step: 200, loss: 0.18135273456573486\n",
            "step: 210, loss: 0.3726663589477539\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24092510342597961\n",
            "step: 10, loss: 0.3092138171195984\n",
            "step: 20, loss: 0.45850569009780884\n",
            "step: 30, loss: 0.3118878901004791\n",
            "step: 40, loss: 0.1734471619129181\n",
            "step: 50, loss: 0.44855043292045593\n",
            "step: 60, loss: 0.10776621848344803\n",
            "step: 70, loss: 0.4430095851421356\n",
            "step: 80, loss: 0.3134494125843048\n",
            "step: 90, loss: 0.38666921854019165\n",
            "step: 100, loss: 0.12769252061843872\n",
            "step: 110, loss: 0.24606254696846008\n",
            "step: 120, loss: 0.2502076327800751\n",
            "step: 130, loss: 0.4443376660346985\n",
            "step: 140, loss: 0.38425305485725403\n",
            "step: 150, loss: 0.1021689772605896\n",
            "step: 160, loss: 0.24689967930316925\n",
            "step: 170, loss: 0.2410484105348587\n",
            "step: 180, loss: 0.31431689858436584\n",
            "step: 190, loss: 0.37501105666160583\n",
            "step: 200, loss: 0.1730794459581375\n",
            "step: 210, loss: 0.311300665140152\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31191906332969666\n",
            "step: 10, loss: 0.24180461466312408\n",
            "step: 20, loss: 0.3091887831687927\n",
            "step: 30, loss: 0.24509698152542114\n",
            "step: 40, loss: 0.24681290984153748\n",
            "step: 50, loss: 0.24699999392032623\n",
            "step: 60, loss: 0.5725386142730713\n",
            "step: 70, loss: 0.43967315554618835\n",
            "step: 80, loss: 0.37979400157928467\n",
            "step: 90, loss: 0.24581508338451385\n",
            "step: 100, loss: 0.38011476397514343\n",
            "step: 110, loss: 0.23763605952262878\n",
            "step: 120, loss: 0.3138800263404846\n",
            "step: 130, loss: 0.3129939138889313\n",
            "step: 140, loss: 0.16218361258506775\n",
            "step: 150, loss: 0.31212422251701355\n",
            "step: 160, loss: 0.5327454209327698\n",
            "step: 170, loss: 0.23767292499542236\n",
            "step: 180, loss: 0.24126584827899933\n",
            "step: 190, loss: 0.2411515861749649\n",
            "step: 200, loss: 0.37230947613716125\n",
            "step: 210, loss: 0.38392174243927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24454770982265472\n",
            "step: 10, loss: 0.38524767756462097\n",
            "step: 20, loss: 0.5075430274009705\n",
            "step: 30, loss: 0.2520592510700226\n",
            "step: 40, loss: 0.3117053508758545\n",
            "step: 50, loss: 0.24626126885414124\n",
            "step: 60, loss: 0.3813238739967346\n",
            "step: 70, loss: 0.2531658411026001\n",
            "step: 80, loss: 0.308134526014328\n",
            "step: 90, loss: 0.18042612075805664\n",
            "step: 100, loss: 0.244638592004776\n",
            "step: 110, loss: 0.45274582505226135\n",
            "step: 120, loss: 0.24609923362731934\n",
            "step: 130, loss: 0.31528493762016296\n",
            "step: 140, loss: 0.30777910351753235\n",
            "step: 150, loss: 0.3128507733345032\n",
            "step: 160, loss: 0.1724320352077484\n",
            "step: 170, loss: 0.45436733961105347\n",
            "step: 180, loss: 0.24278387427330017\n",
            "step: 190, loss: 0.38212040066719055\n",
            "step: 200, loss: 0.16864942014217377\n",
            "step: 210, loss: 0.4436681568622589\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24112240970134735\n",
            "step: 10, loss: 0.24367056787014008\n",
            "step: 20, loss: 0.5118926167488098\n",
            "step: 30, loss: 0.231472447514534\n",
            "step: 40, loss: 0.3856126070022583\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.17336484789848328\n",
            "step: 60, loss: 0.31686800718307495\n",
            "step: 70, loss: 0.44870591163635254\n",
            "step: 80, loss: 0.24088053405284882\n",
            "step: 90, loss: 0.4568709135055542\n",
            "step: 100, loss: 0.3105657994747162\n",
            "step: 110, loss: 0.31512197852134705\n",
            "step: 120, loss: 0.45711708068847656\n",
            "step: 130, loss: 0.171123206615448\n",
            "step: 140, loss: 0.316692054271698\n",
            "step: 150, loss: 0.5225810408592224\n",
            "step: 160, loss: 0.37596607208251953\n",
            "step: 170, loss: 0.37474966049194336\n",
            "step: 180, loss: 0.31099170446395874\n",
            "step: 190, loss: 0.305527001619339\n",
            "step: 200, loss: 0.3037177622318268\n",
            "step: 210, loss: 0.4471181035041809\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 516.91it/s]\n",
            "load_f1 = 0.18519984170953702\n",
            "real_f1 = 0.18519984170953702\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 215.80it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewoOK8t9eDFM"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJDquM2Oe05D",
        "outputId": "8894c49c-0761-4b62-e34b-0e4767493770"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.46749117970466614\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4711432158946991\n",
            "step: 20, loss: 0.25864213705062866\n",
            "step: 30, loss: 0.3859328031539917\n",
            "step: 40, loss: 0.23194637894630432\n",
            "step: 50, loss: 0.3062077462673187\n",
            "step: 60, loss: 0.44431716203689575\n",
            "step: 70, loss: 0.46816691756248474\n",
            "step: 80, loss: 0.19567257165908813\n",
            "step: 90, loss: 0.31527912616729736\n",
            "step: 100, loss: 0.43343082070350647\n",
            "step: 110, loss: 0.2556871771812439\n",
            "step: 120, loss: 0.33639729022979736\n",
            "step: 130, loss: 0.3177908658981323\n",
            "step: 140, loss: 0.1759427785873413\n",
            "step: 150, loss: 0.3341194689273834\n",
            "step: 160, loss: 0.23342496156692505\n",
            "step: 170, loss: 0.3845243453979492\n",
            "step: 180, loss: 0.15773622691631317\n",
            "step: 190, loss: 0.15927042067050934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3828509449958801\n",
            "step: 10, loss: 0.31999513506889343\n",
            "step: 20, loss: 0.660132110118866\n",
            "step: 30, loss: 0.2632036507129669\n",
            "step: 40, loss: 0.5645864605903625\n",
            "step: 50, loss: 0.32047346234321594\n",
            "step: 60, loss: 0.44959011673927307\n",
            "step: 70, loss: 0.3074430227279663\n",
            "step: 80, loss: 0.15015187859535217\n",
            "step: 90, loss: 0.32117509841918945\n",
            "step: 100, loss: 0.24748340249061584\n",
            "step: 110, loss: 0.38001346588134766\n",
            "step: 120, loss: 0.2435767948627472\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 130, loss: 0.4958072602748871\n",
            "step: 140, loss: 0.2995646595954895\n",
            "step: 150, loss: 0.28761813044548035\n",
            "step: 160, loss: 0.23232366144657135\n",
            "step: 170, loss: 0.2776481509208679\n",
            "step: 180, loss: 0.15177883207798004\n",
            "step: 190, loss: 0.20545470714569092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.40704500978473573, f1=0.4448051948051948, best_f1=0.4448051948051948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3693639636039734\n",
            "step: 10, loss: 0.27162814140319824\n",
            "step: 20, loss: 0.3473513126373291\n",
            "step: 30, loss: 0.2902079224586487\n",
            "step: 40, loss: 0.11084762215614319\n",
            "step: 50, loss: 0.17319047451019287\n",
            "step: 60, loss: 0.16656973958015442\n",
            "step: 70, loss: 0.19301128387451172\n",
            "step: 80, loss: 0.15352784097194672\n",
            "step: 90, loss: 0.13251237571239471\n",
            "step: 100, loss: 0.21030589938163757\n",
            "step: 110, loss: 0.4224573075771332\n",
            "step: 120, loss: 0.2866271734237671\n",
            "step: 130, loss: 0.04688937962055206\n",
            "step: 140, loss: 0.0455731600522995\n",
            "step: 150, loss: 0.3166724741458893\n",
            "step: 160, loss: 0.13323499262332916\n",
            "step: 170, loss: 0.22951585054397583\n",
            "step: 180, loss: 0.1956261396408081\n",
            "step: 190, loss: 0.010864981450140476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7934782608695652, f1=0.7859078590785908, best_f1=0.7859078590785908\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14519478380680084\n",
            "step: 10, loss: 0.11885262280702591\n",
            "step: 20, loss: 0.1601632982492447\n",
            "step: 30, loss: 0.026522550731897354\n",
            "step: 40, loss: 0.15313807129859924\n",
            "step: 50, loss: 0.05650584027171135\n",
            "step: 60, loss: 0.02916325442492962\n",
            "step: 70, loss: 0.03717274218797684\n",
            "step: 80, loss: 0.04149243235588074\n",
            "step: 90, loss: 0.045339375734329224\n",
            "step: 100, loss: 0.0752648189663887\n",
            "step: 110, loss: 0.19584617018699646\n",
            "step: 120, loss: 0.0851249247789383\n",
            "step: 130, loss: 0.22812460362911224\n",
            "step: 140, loss: 0.1447087973356247\n",
            "step: 150, loss: 0.15685966610908508\n",
            "step: 160, loss: 0.025841455906629562\n",
            "step: 170, loss: 0.07957150787115097\n",
            "step: 180, loss: 0.08842317759990692\n",
            "step: 190, loss: 0.06457951664924622\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8392370572207084, f1=0.8100558659217876, best_f1=0.8100558659217876\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022151481360197067\n",
            "step: 10, loss: 0.16518868505954742\n",
            "step: 20, loss: 0.04434782266616821\n",
            "step: 30, loss: 0.10920198261737823\n",
            "step: 40, loss: 0.11940715461969376\n",
            "step: 50, loss: 0.041834764182567596\n",
            "step: 60, loss: 0.0034494802821427584\n",
            "step: 70, loss: 0.07771698385477066\n",
            "step: 80, loss: 0.03419042378664017\n",
            "step: 90, loss: 0.047625284641981125\n",
            "step: 100, loss: 0.0622994489967823\n",
            "step: 110, loss: 0.1794556826353073\n",
            "step: 120, loss: 0.014058029279112816\n",
            "step: 130, loss: 0.22070378065109253\n",
            "step: 140, loss: 0.018850967288017273\n",
            "step: 150, loss: 0.0857955664396286\n",
            "step: 160, loss: 0.03788745775818825\n",
            "step: 170, loss: 0.02341519482433796\n",
            "step: 180, loss: 0.020264923572540283\n",
            "step: 190, loss: 0.04731239378452301\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8556701030927835, f1=0.8177083333333333, best_f1=0.8177083333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15774069726467133\n",
            "step: 10, loss: 0.058100562542676926\n",
            "step: 20, loss: 0.01300124078989029\n",
            "step: 30, loss: 0.06581227481365204\n",
            "step: 40, loss: 0.013703505508601665\n",
            "step: 50, loss: 0.022301677614450455\n",
            "step: 60, loss: 0.020022790879011154\n",
            "step: 70, loss: 0.01802227832376957\n",
            "step: 80, loss: 0.06287568062543869\n",
            "step: 90, loss: 0.07106364518404007\n",
            "step: 100, loss: 0.14054393768310547\n",
            "step: 110, loss: 0.005402375478297472\n",
            "step: 120, loss: 0.007644682191312313\n",
            "step: 130, loss: 0.008206469938158989\n",
            "step: 140, loss: 0.006654699333012104\n",
            "step: 150, loss: 0.005595089867711067\n",
            "step: 160, loss: 0.04601164162158966\n",
            "step: 170, loss: 0.12181323021650314\n",
            "step: 180, loss: 0.05153147503733635\n",
            "step: 190, loss: 0.0490235909819603\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8333333333333334, f1=0.8099173553719008, best_f1=0.8177083333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008545148186385632\n",
            "step: 10, loss: 0.00647529074922204\n",
            "step: 20, loss: 0.0007320214062929153\n",
            "step: 30, loss: 0.10598912835121155\n",
            "step: 40, loss: 0.13201899826526642\n",
            "step: 50, loss: 0.000900412502232939\n",
            "step: 60, loss: 0.058605264872312546\n",
            "step: 70, loss: 0.0026217824779450893\n",
            "step: 80, loss: 0.0011987551115453243\n",
            "step: 90, loss: 0.0008326248498633504\n",
            "step: 100, loss: 0.4419930577278137\n",
            "step: 110, loss: 0.008875110186636448\n",
            "step: 120, loss: 0.008125420659780502\n",
            "step: 130, loss: 0.012012076564133167\n",
            "step: 140, loss: 0.02315710298717022\n",
            "step: 150, loss: 0.08094563335180283\n",
            "step: 160, loss: 0.06643091142177582\n",
            "step: 170, loss: 0.013111195527017117\n",
            "step: 180, loss: 0.028076646849513054\n",
            "step: 190, loss: 0.10239938646554947\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8483290488431877, f1=0.8195876288659794, best_f1=0.8177083333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01644030772149563\n",
            "step: 10, loss: 0.12139217555522919\n",
            "step: 20, loss: 0.041170634329319\n",
            "step: 30, loss: 0.14714182913303375\n",
            "step: 40, loss: 0.05883647873997688\n",
            "step: 50, loss: 0.05291755124926567\n",
            "step: 60, loss: 0.0046848696656525135\n",
            "step: 70, loss: 0.001380103058181703\n",
            "step: 80, loss: 0.03199135512113571\n",
            "step: 90, loss: 0.009517517872154713\n",
            "step: 100, loss: 0.004738811869174242\n",
            "step: 110, loss: 0.17641063034534454\n",
            "step: 120, loss: 0.001972961239516735\n",
            "step: 130, loss: 0.004408976528793573\n",
            "step: 140, loss: 0.005972818937152624\n",
            "step: 150, loss: 0.020381009206175804\n",
            "step: 160, loss: 0.002167634665966034\n",
            "step: 170, loss: 0.028828062117099762\n",
            "step: 180, loss: 0.006580435670912266\n",
            "step: 190, loss: 0.011119825765490532\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.8571428571428572, f1=0.8195876288659794, best_f1=0.8195876288659794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009036602452397346\n",
            "step: 10, loss: 0.002360731363296509\n",
            "step: 20, loss: 0.00265683070756495\n",
            "step: 30, loss: 0.005194493569433689\n",
            "step: 40, loss: 0.019757147878408432\n",
            "step: 50, loss: 0.009029194712638855\n",
            "step: 60, loss: 0.003594169393181801\n",
            "step: 70, loss: 0.0006919695879332721\n",
            "step: 80, loss: 0.20240825414657593\n",
            "step: 90, loss: 0.0039252908900380135\n",
            "step: 100, loss: 0.001857337774708867\n",
            "step: 110, loss: 0.05620137229561806\n",
            "step: 120, loss: 0.0033865920267999172\n",
            "step: 130, loss: 0.03542625159025192\n",
            "step: 140, loss: 0.15549565851688385\n",
            "step: 150, loss: 0.00695383595302701\n",
            "step: 160, loss: 0.004621313419193029\n",
            "step: 170, loss: 0.14907410740852356\n",
            "step: 180, loss: 0.009653238579630852\n",
            "step: 190, loss: 0.0009720729431137443\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8391959798994976, f1=0.8241469816272966, best_f1=0.8195876288659794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038592941127717495\n",
            "step: 10, loss: 0.005835734773427248\n",
            "step: 20, loss: 0.07187049090862274\n",
            "step: 30, loss: 0.1486852616071701\n",
            "step: 40, loss: 0.006909720133990049\n",
            "step: 50, loss: 0.028631199151277542\n",
            "step: 60, loss: 0.0029131299816071987\n",
            "step: 70, loss: 0.0011824237881228328\n",
            "step: 80, loss: 0.003148280782625079\n",
            "step: 90, loss: 0.0005948477773927152\n",
            "step: 100, loss: 0.006860617082566023\n",
            "step: 110, loss: 0.0009797642705962062\n",
            "step: 120, loss: 0.002635700162500143\n",
            "step: 130, loss: 0.011404616758227348\n",
            "step: 140, loss: 0.0353541225194931\n",
            "step: 150, loss: 0.007648400031030178\n",
            "step: 160, loss: 0.0027500432915985584\n",
            "step: 170, loss: 0.004081349354237318\n",
            "step: 180, loss: 0.03755710646510124\n",
            "step: 190, loss: 0.0018307029968127608\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8350515463917526, f1=0.8144329896907216, best_f1=0.8195876288659794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011144669726490974\n",
            "step: 10, loss: 0.0007315626135095954\n",
            "step: 20, loss: 0.004725002218037844\n",
            "step: 30, loss: 0.04468480497598648\n",
            "step: 40, loss: 0.0013239897089079022\n",
            "step: 50, loss: 0.0013524852693080902\n",
            "step: 60, loss: 0.0019253477221354842\n",
            "step: 70, loss: 0.004928520414978266\n",
            "step: 80, loss: 0.007080019451677799\n",
            "step: 90, loss: 0.1534908264875412\n",
            "step: 100, loss: 0.023590339347720146\n",
            "step: 110, loss: 0.06556150317192078\n",
            "step: 120, loss: 0.0012982224579900503\n",
            "step: 130, loss: 0.009811780415475368\n",
            "step: 140, loss: 0.001826282124966383\n",
            "step: 150, loss: 0.0017780618509277701\n",
            "step: 160, loss: 0.006892452482134104\n",
            "step: 170, loss: 0.0010543158277869225\n",
            "step: 180, loss: 0.01195554994046688\n",
            "step: 190, loss: 0.03204677999019623\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8491048593350383, f1=0.8443271767810027, best_f1=0.8195876288659794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007693280465900898\n",
            "step: 10, loss: 0.012127459980547428\n",
            "step: 20, loss: 0.001607050304301083\n",
            "step: 30, loss: 0.0058102114126086235\n",
            "step: 40, loss: 0.003713061800226569\n",
            "step: 50, loss: 0.017068596556782722\n",
            "step: 60, loss: 0.0076842401176691055\n",
            "step: 70, loss: 0.0025716214440762997\n",
            "step: 80, loss: 0.003320883261039853\n",
            "step: 90, loss: 0.0034530670382082462\n",
            "step: 100, loss: 0.000645877153147012\n",
            "step: 110, loss: 0.0034353199880570173\n",
            "step: 120, loss: 0.0293634831905365\n",
            "step: 130, loss: 0.0004950065049342811\n",
            "step: 140, loss: 0.009122254326939583\n",
            "step: 150, loss: 0.0059776464477181435\n",
            "step: 160, loss: 0.0007011474808678031\n",
            "step: 170, loss: 0.002379931742325425\n",
            "step: 180, loss: 0.0002828651631716639\n",
            "step: 190, loss: 0.0007204539142549038\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8293963254593176, f1=0.8395721925133689, best_f1=0.8195876288659794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020473819226026535\n",
            "step: 10, loss: 0.0005445372080430388\n",
            "step: 20, loss: 0.00017846132686827332\n",
            "step: 30, loss: 0.007527850568294525\n",
            "step: 40, loss: 0.006762392818927765\n",
            "step: 50, loss: 0.0011851171730086207\n",
            "step: 60, loss: 0.024838166311383247\n",
            "step: 70, loss: 0.02038346230983734\n",
            "step: 80, loss: 0.00036977598210796714\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.10294406861066818\n",
            "step: 100, loss: 0.01967497169971466\n",
            "step: 110, loss: 0.02867473103106022\n",
            "step: 120, loss: 0.0016936148749664426\n",
            "step: 130, loss: 0.026112448424100876\n",
            "step: 140, loss: 0.003592359833419323\n",
            "step: 150, loss: 0.012214752845466137\n",
            "step: 160, loss: 0.002401254838332534\n",
            "step: 170, loss: 0.0009397217072546482\n",
            "step: 180, loss: 0.0008684486383572221\n",
            "step: 190, loss: 0.024700623005628586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8288770053475937, f1=0.8432432432432432, best_f1=0.8195876288659794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004586989991366863\n",
            "step: 10, loss: 0.006616570521146059\n",
            "step: 20, loss: 0.0030095167458057404\n",
            "step: 30, loss: 0.0010853464482352138\n",
            "step: 40, loss: 0.0008637820719741285\n",
            "step: 50, loss: 0.0228824932128191\n",
            "step: 60, loss: 0.0038400008343160152\n",
            "step: 70, loss: 0.0014139995910227299\n",
            "step: 80, loss: 0.0008298391476273537\n",
            "step: 90, loss: 0.0012053201207891107\n",
            "step: 100, loss: 0.0010097200283780694\n",
            "step: 110, loss: 0.25391143560409546\n",
            "step: 120, loss: 0.002590768039226532\n",
            "step: 130, loss: 0.0005509023903869092\n",
            "step: 140, loss: 0.0011075454531237483\n",
            "step: 150, loss: 0.004891573917120695\n",
            "step: 160, loss: 0.0018176232697442174\n",
            "step: 170, loss: 0.007557322271168232\n",
            "step: 180, loss: 0.00396156869828701\n",
            "step: 190, loss: 0.0020917938090860844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8232189973614776, f1=0.8409703504043127, best_f1=0.8195876288659794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006534077692776918\n",
            "step: 10, loss: 0.0009476529667153955\n",
            "step: 20, loss: 0.0555666983127594\n",
            "step: 30, loss: 0.0009252357413060963\n",
            "step: 40, loss: 0.033563461154699326\n",
            "step: 50, loss: 0.0007183132693171501\n",
            "step: 60, loss: 0.007287212647497654\n",
            "step: 70, loss: 0.0007314825197681785\n",
            "step: 80, loss: 0.005439380649477243\n",
            "step: 90, loss: 0.015212769620120525\n",
            "step: 100, loss: 0.0012133773416280746\n",
            "step: 110, loss: 0.02192508801817894\n",
            "step: 120, loss: 0.0007072443258948624\n",
            "step: 130, loss: 0.0008491299231536686\n",
            "step: 140, loss: 0.0020823220256716013\n",
            "step: 150, loss: 0.00040123151848092675\n",
            "step: 160, loss: 0.0842042863368988\n",
            "step: 170, loss: 0.05888392776250839\n",
            "step: 180, loss: 0.0006296406500041485\n",
            "step: 190, loss: 0.018684759736061096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.828125, f1=0.8457446808510638, best_f1=0.8195876288659794\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 236.72it/s]\n",
            "load_f1 = 0.8179419525065963\n",
            "real_f1 = 0.8188976377952756\n",
            "733it [00:00, 3208.84it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 135.66it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      },
      "source": [
        "## DA TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1O9a5RjeDtU"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2WpDwuee1mM",
        "outputId": "1f90778f-12ca-44f0-fc42-d96c68017022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.47827619314193726\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5314756631851196\n",
            "step: 20, loss: 0.3522363603115082\n",
            "step: 30, loss: 0.4422685503959656\n",
            "step: 40, loss: 0.5095581412315369\n",
            "step: 50, loss: 0.3370139002799988\n",
            "step: 60, loss: 0.5604312419891357\n",
            "step: 70, loss: 0.3249945342540741\n",
            "step: 80, loss: 0.2282475382089615\n",
            "step: 90, loss: 0.1921595335006714\n",
            "step: 100, loss: 0.15583176910877228\n",
            "step: 110, loss: 0.4070395231246948\n",
            "step: 120, loss: 0.30008015036582947\n",
            "step: 130, loss: 0.33538150787353516\n",
            "step: 140, loss: 0.38045117259025574\n",
            "step: 150, loss: 0.3034323453903198\n",
            "step: 160, loss: 0.401550829410553\n",
            "step: 170, loss: 0.33673977851867676\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31894782185554504\n",
            "step: 10, loss: 0.48923298716545105\n",
            "step: 20, loss: 0.35953840613365173\n",
            "step: 30, loss: 0.32935404777526855\n",
            "step: 40, loss: 0.08377423882484436\n",
            "step: 50, loss: 0.42443224787712097\n",
            "step: 60, loss: 0.1640920639038086\n",
            "step: 70, loss: 0.4931207001209259\n",
            "step: 80, loss: 0.24362021684646606\n",
            "step: 90, loss: 0.2443619966506958\n",
            "step: 100, loss: 0.5143795609474182\n",
            "step: 110, loss: 0.2664680778980255\n",
            "step: 120, loss: 0.25128597021102905\n",
            "step: 130, loss: 0.5853283405303955\n",
            "step: 140, loss: 0.5161676406860352\n",
            "step: 150, loss: 0.41437309980392456\n",
            "step: 160, loss: 0.4344285726547241\n",
            "step: 170, loss: 0.39917492866516113\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.20027434842249658, f1=0.23107569721115537, best_f1=0.23107569721115537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5806066393852234\n",
            "step: 10, loss: 0.3254907429218292\n",
            "step: 20, loss: 0.2275172770023346\n",
            "step: 30, loss: 0.2550395429134369\n",
            "step: 40, loss: 0.38357365131378174\n",
            "step: 50, loss: 0.5857028365135193\n",
            "step: 60, loss: 0.3027248978614807\n",
            "step: 70, loss: 0.24366852641105652\n",
            "step: 80, loss: 0.36870765686035156\n",
            "step: 90, loss: 0.5757909417152405\n",
            "step: 100, loss: 0.28955477476119995\n",
            "step: 110, loss: 0.13827279210090637\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.4503336548805237\n",
            "step: 130, loss: 0.4554498493671417\n",
            "step: 140, loss: 0.27873310446739197\n",
            "step: 150, loss: 0.150865375995636\n",
            "step: 160, loss: 0.17235535383224487\n",
            "step: 170, loss: 0.308504581451416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.23107569721115537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3695036470890045\n",
            "step: 10, loss: 0.5678054690361023\n",
            "step: 20, loss: 0.22469036281108856\n",
            "step: 30, loss: 0.452867716550827\n",
            "step: 40, loss: 0.24225199222564697\n",
            "step: 50, loss: 0.3449922502040863\n",
            "step: 60, loss: 0.5749025940895081\n",
            "step: 70, loss: 0.26910117268562317\n",
            "step: 80, loss: 0.620756208896637\n",
            "step: 90, loss: 0.3567522466182709\n",
            "step: 100, loss: 0.39666131138801575\n",
            "step: 110, loss: 0.3990367650985718\n",
            "step: 120, loss: 0.5104129314422607\n",
            "step: 130, loss: 0.23457255959510803\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 140, loss: 0.13119928538799286\n",
            "step: 150, loss: 0.6680266261100769\n",
            "step: 160, loss: 0.13574959337711334\n",
            "step: 170, loss: 0.16745103895664215\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6098901098901098, f1=0.5934065934065935, best_f1=0.5934065934065935\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12332476675510406\n",
            "step: 10, loss: 0.31126871705055237\n",
            "step: 20, loss: 0.33167994022369385\n",
            "step: 30, loss: 0.3390037715435028\n",
            "step: 40, loss: 0.25688889622688293\n",
            "step: 50, loss: 0.2371821105480194\n",
            "step: 60, loss: 0.3088320791721344\n",
            "step: 70, loss: 0.18402285873889923\n",
            "step: 80, loss: 0.35785990953445435\n",
            "step: 90, loss: 0.23837006092071533\n",
            "step: 100, loss: 0.13889193534851074\n",
            "step: 110, loss: 0.26145175099372864\n",
            "step: 120, loss: 0.11090783774852753\n",
            "step: 130, loss: 0.15593573451042175\n",
            "step: 140, loss: 0.13449867069721222\n",
            "step: 150, loss: 0.30357998609542847\n",
            "step: 160, loss: 0.21803201735019684\n",
            "step: 170, loss: 0.11412443220615387\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6571428571428571, f1=0.6652892561983471, best_f1=0.6652892561983471\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21608099341392517\n",
            "step: 10, loss: 0.1635228544473648\n",
            "step: 20, loss: 0.21922023594379425\n",
            "step: 30, loss: 0.1209331676363945\n",
            "step: 40, loss: 0.1160367876291275\n",
            "step: 50, loss: 0.0908762663602829\n",
            "step: 60, loss: 0.0990990698337555\n",
            "step: 70, loss: 0.112077996134758\n",
            "step: 80, loss: 0.10291191935539246\n",
            "step: 90, loss: 0.14791667461395264\n",
            "step: 100, loss: 0.104973703622818\n",
            "step: 110, loss: 0.17501668632030487\n",
            "step: 120, loss: 0.19500966370105743\n",
            "step: 130, loss: 0.2612784504890442\n",
            "step: 140, loss: 0.2406693398952484\n",
            "step: 150, loss: 0.18437650799751282\n",
            "step: 160, loss: 0.15403631329536438\n",
            "step: 170, loss: 0.18676403164863586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7180722891566266, f1=0.6812652068126521, best_f1=0.6812652068126521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.089011549949646\n",
            "step: 10, loss: 0.21340513229370117\n",
            "step: 20, loss: 0.3688625395298004\n",
            "step: 30, loss: 0.12701065838336945\n",
            "step: 40, loss: 0.03616338595747948\n",
            "step: 50, loss: 0.0340816006064415\n",
            "step: 60, loss: 0.18630240857601166\n",
            "step: 70, loss: 0.15743057429790497\n",
            "step: 80, loss: 0.08454042673110962\n",
            "step: 90, loss: 0.08800237625837326\n",
            "step: 100, loss: 0.04327787831425667\n",
            "step: 110, loss: 0.11167503893375397\n",
            "step: 120, loss: 0.11629495024681091\n",
            "step: 130, loss: 0.1404402107000351\n",
            "step: 140, loss: 0.06374004483222961\n",
            "step: 150, loss: 0.13207173347473145\n",
            "step: 160, loss: 0.06712555140256882\n",
            "step: 170, loss: 0.10185717791318893\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7647058823529412, f1=0.7553444180522566, best_f1=0.7553444180522566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12569178640842438\n",
            "step: 10, loss: 0.0869804248213768\n",
            "step: 20, loss: 0.18401655554771423\n",
            "step: 30, loss: 0.04145967587828636\n",
            "step: 40, loss: 0.10320966690778732\n",
            "step: 50, loss: 0.055644772946834564\n",
            "step: 60, loss: 0.06129584461450577\n",
            "step: 70, loss: 0.07895231992006302\n",
            "step: 80, loss: 0.02334880270063877\n",
            "step: 90, loss: 0.1991572380065918\n",
            "step: 100, loss: 0.03192467242479324\n",
            "step: 110, loss: 0.12435464560985565\n",
            "step: 120, loss: 0.19227324426174164\n",
            "step: 130, loss: 0.06494805216789246\n",
            "step: 140, loss: 0.14227242767810822\n",
            "step: 150, loss: 0.04171639308333397\n",
            "step: 160, loss: 0.035567641258239746\n",
            "step: 170, loss: 0.10224423557519913\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7634660421545668, f1=0.7727272727272726, best_f1=0.7553444180522566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022838084027171135\n",
            "step: 10, loss: 0.1939198225736618\n",
            "step: 20, loss: 0.04374432936310768\n",
            "step: 30, loss: 0.11958733946084976\n",
            "step: 40, loss: 0.2073901742696762\n",
            "step: 50, loss: 0.028205858543515205\n",
            "step: 60, loss: 0.11524546146392822\n",
            "step: 70, loss: 0.17850278317928314\n",
            "step: 80, loss: 0.022625582292675972\n",
            "step: 90, loss: 0.08268392831087112\n",
            "step: 100, loss: 0.11156272888183594\n",
            "step: 110, loss: 0.13765934109687805\n",
            "step: 120, loss: 0.2609314024448395\n",
            "step: 130, loss: 0.1968134045600891\n",
            "step: 140, loss: 0.06684735417366028\n",
            "step: 150, loss: 0.2897653579711914\n",
            "step: 160, loss: 0.14016152918338776\n",
            "step: 170, loss: 0.16680867969989777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.7751196172248804, f1=0.787037037037037, best_f1=0.787037037037037\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04746940732002258\n",
            "step: 10, loss: 0.019333142787218094\n",
            "step: 20, loss: 0.21901434659957886\n",
            "step: 30, loss: 0.09393689781427383\n",
            "step: 40, loss: 0.12524546682834625\n",
            "step: 50, loss: 0.11747822910547256\n",
            "step: 60, loss: 0.29316210746765137\n",
            "step: 70, loss: 0.07393492013216019\n",
            "step: 80, loss: 0.05287580192089081\n",
            "step: 90, loss: 0.010139680467545986\n",
            "step: 100, loss: 0.05798114836215973\n",
            "step: 110, loss: 0.01110400352627039\n",
            "step: 120, loss: 0.018172651529312134\n",
            "step: 130, loss: 0.052939288318157196\n",
            "step: 140, loss: 0.015682391822338104\n",
            "step: 150, loss: 0.08170858770608902\n",
            "step: 160, loss: 0.004297483712434769\n",
            "step: 170, loss: 0.023834850639104843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7792207792207791, f1=0.7928388746803069, best_f1=0.7928388746803069\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14287957549095154\n",
            "step: 10, loss: 0.0061471667140722275\n",
            "step: 20, loss: 0.022377341985702515\n",
            "step: 30, loss: 0.09065014868974686\n",
            "step: 40, loss: 0.00330843566916883\n",
            "step: 50, loss: 0.07896408438682556\n",
            "step: 60, loss: 0.14415177702903748\n",
            "step: 70, loss: 0.05130711942911148\n",
            "step: 80, loss: 0.011442191898822784\n",
            "step: 90, loss: 0.02443922869861126\n",
            "step: 100, loss: 0.2521412670612335\n",
            "step: 110, loss: 0.024996599182486534\n",
            "step: 120, loss: 0.012422353960573673\n",
            "step: 130, loss: 0.0245506688952446\n",
            "step: 140, loss: 0.3253364861011505\n",
            "step: 150, loss: 0.006720529403537512\n",
            "step: 160, loss: 0.07517996430397034\n",
            "step: 170, loss: 0.1262059211730957\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7755102040816327, f1=0.8068459657701711, best_f1=0.7928388746803069\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017930487170815468\n",
            "step: 10, loss: 0.06707582622766495\n",
            "step: 20, loss: 0.00553295249119401\n",
            "step: 30, loss: 0.17692570388317108\n",
            "step: 40, loss: 0.013212349265813828\n",
            "step: 50, loss: 0.050483133643865585\n",
            "step: 60, loss: 0.2547139823436737\n",
            "step: 70, loss: 0.015907229855656624\n",
            "step: 80, loss: 0.021158814430236816\n",
            "step: 90, loss: 0.07624957710504532\n",
            "step: 100, loss: 0.016965191811323166\n",
            "step: 110, loss: 0.056681692600250244\n",
            "step: 120, loss: 0.03226936608552933\n",
            "step: 130, loss: 0.20409893989562988\n",
            "step: 140, loss: 0.018588732928037643\n",
            "step: 150, loss: 0.008948949165642262\n",
            "step: 160, loss: 0.09137038141489029\n",
            "step: 170, loss: 0.20010028779506683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.765375854214123, f1=0.7822222222222223, best_f1=0.7928388746803069\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011117218062281609\n",
            "step: 10, loss: 0.0038813319988548756\n",
            "step: 20, loss: 0.004559213761240244\n",
            "step: 30, loss: 0.034104958176612854\n",
            "step: 40, loss: 0.017574692144989967\n",
            "step: 50, loss: 0.11353220045566559\n",
            "step: 60, loss: 0.029691345989704132\n",
            "step: 70, loss: 0.1609361618757248\n",
            "step: 80, loss: 0.005618160590529442\n",
            "step: 90, loss: 0.02807820960879326\n",
            "step: 100, loss: 0.02173713967204094\n",
            "step: 110, loss: 0.025663206353783607\n",
            "step: 120, loss: 0.011625200510025024\n",
            "step: 130, loss: 0.019836759194731712\n",
            "step: 140, loss: 0.06486961245536804\n",
            "step: 150, loss: 0.014872798696160316\n",
            "step: 160, loss: 0.0222477987408638\n",
            "step: 170, loss: 0.005383268464356661\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.7815533980582524, f1=0.8085106382978723, best_f1=0.8085106382978723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04149670526385307\n",
            "step: 10, loss: 0.012806976214051247\n",
            "step: 20, loss: 0.07675842195749283\n",
            "step: 30, loss: 0.03661806508898735\n",
            "step: 40, loss: 0.027883507311344147\n",
            "step: 50, loss: 0.007277539931237698\n",
            "step: 60, loss: 0.030208101496100426\n",
            "step: 70, loss: 0.02536429464817047\n",
            "step: 80, loss: 0.01311145443469286\n",
            "step: 90, loss: 0.004774438217282295\n",
            "step: 100, loss: 0.1235990822315216\n",
            "step: 110, loss: 0.05728035792708397\n",
            "step: 120, loss: 0.06273464858531952\n",
            "step: 130, loss: 0.17152981460094452\n",
            "step: 140, loss: 0.07673206180334091\n",
            "step: 150, loss: 0.025732586160302162\n",
            "step: 160, loss: 0.06582163274288177\n",
            "step: 170, loss: 0.047611918300390244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.792079207920792, f1=0.8132387706855791, best_f1=0.8132387706855791\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003140874905511737\n",
            "step: 10, loss: 0.01416783407330513\n",
            "step: 20, loss: 0.0032979988027364016\n",
            "step: 30, loss: 0.04981815069913864\n",
            "step: 40, loss: 0.007203984074294567\n",
            "step: 50, loss: 0.02398689277470112\n",
            "step: 60, loss: 0.009063017554581165\n",
            "step: 70, loss: 0.1788255125284195\n",
            "step: 80, loss: 0.004884145222604275\n",
            "step: 90, loss: 0.006311760749667883\n",
            "step: 100, loss: 0.06453820317983627\n",
            "step: 110, loss: 0.012452853843569756\n",
            "step: 120, loss: 0.2151188850402832\n",
            "step: 130, loss: 0.18229109048843384\n",
            "step: 140, loss: 0.04466485232114792\n",
            "step: 150, loss: 0.012777073308825493\n",
            "step: 160, loss: 0.01050348486751318\n",
            "step: 170, loss: 0.008974668569862843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7869674185463658, f1=0.8123515439429927, best_f1=0.8132387706855791\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:05, 329.72it/s]\n",
            "load_f1 = 0.6094890510948906\n",
            "real_f1 = 0.6148282097649186\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 202.05it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pmKonkXeD7k"
      },
      "source": [
        "## DA DIRTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRxHd3j2eEH8"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lnLoRSEe2fE",
        "outputId": "78b3ba4a-04e5-4703-f7bc-009d48bd0678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 431kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 744kB/s] \n",
            "Downloading: 100% 456k/456k [00:01<00:00, 400kB/s]\n",
            "Downloading: 100% 501M/501M [00:10<00:00, 49.8MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.564807653427124\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4297533631324768\n",
            "step: 20, loss: 0.5516487956047058\n",
            "step: 30, loss: 0.28701257705688477\n",
            "step: 40, loss: 0.18429380655288696\n",
            "step: 50, loss: 0.2449190467596054\n",
            "step: 60, loss: 0.09337695688009262\n",
            "step: 70, loss: 0.08734926581382751\n",
            "step: 80, loss: 0.19211949408054352\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.2074487805366516\n",
            "step: 100, loss: 0.17801129817962646\n",
            "step: 110, loss: 0.06487247347831726\n",
            "step: 120, loss: 0.0618782564997673\n",
            "step: 130, loss: 0.08677094429731369\n",
            "step: 140, loss: 0.015024441294372082\n",
            "step: 150, loss: 0.12715472280979156\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 160, loss: 0.003943971823900938\n",
            "step: 170, loss: 0.26892757415771484\n",
            "step: 180, loss: 0.039002206176519394\n",
            "step: 190, loss: 0.01385321095585823\n",
            "step: 200, loss: 0.042406171560287476\n",
            "step: 210, loss: 0.05627509951591492\n",
            "step: 220, loss: 0.19037365913391113\n",
            "step: 230, loss: 0.01412101462483406\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9508571428571428, f1=0.9541284403669725, best_f1=0.9541284403669725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025163467973470688\n",
            "step: 10, loss: 0.165103942155838\n",
            "step: 20, loss: 0.02260114625096321\n",
            "step: 30, loss: 0.021092917770147324\n",
            "step: 40, loss: 0.019963832572102547\n",
            "step: 50, loss: 0.010240490548312664\n",
            "step: 60, loss: 0.012220365926623344\n",
            "step: 70, loss: 0.034861549735069275\n",
            "step: 80, loss: 0.002331983530893922\n",
            "step: 90, loss: 0.04429050534963608\n",
            "step: 100, loss: 0.005002077668905258\n",
            "step: 110, loss: 0.031325533986091614\n",
            "step: 120, loss: 0.04094771668314934\n",
            "step: 130, loss: 0.065371073782444\n",
            "step: 140, loss: 0.010063597932457924\n",
            "step: 150, loss: 0.10513950884342194\n",
            "step: 160, loss: 0.016361655667424202\n",
            "step: 170, loss: 0.012331683188676834\n",
            "step: 180, loss: 0.02041245624423027\n",
            "step: 190, loss: 0.08484108000993729\n",
            "step: 200, loss: 0.0391845740377903\n",
            "step: 210, loss: 0.021506156772375107\n",
            "step: 220, loss: 0.01843593269586563\n",
            "step: 230, loss: 0.0025693541392683983\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9696969696969697, f1=0.9636363636363636, best_f1=0.9636363636363636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023531407117843628\n",
            "step: 10, loss: 0.00762079656124115\n",
            "step: 20, loss: 0.01758972369134426\n",
            "step: 30, loss: 0.003950200974941254\n",
            "step: 40, loss: 0.03516264259815216\n",
            "step: 50, loss: 0.08332644402980804\n",
            "step: 60, loss: 0.02813202701508999\n",
            "step: 70, loss: 0.018717166036367416\n",
            "step: 80, loss: 0.10557029396295547\n",
            "step: 90, loss: 0.03813569247722626\n",
            "step: 100, loss: 0.005061613395810127\n",
            "step: 110, loss: 0.008535685017704964\n",
            "step: 120, loss: 0.0027831948827952147\n",
            "step: 130, loss: 0.004577309358865023\n",
            "step: 140, loss: 0.004638336598873138\n",
            "step: 150, loss: 0.098726287484169\n",
            "step: 160, loss: 0.017785558477044106\n",
            "step: 170, loss: 0.0028247677255421877\n",
            "step: 180, loss: 0.026518139988183975\n",
            "step: 190, loss: 0.11079105734825134\n",
            "step: 200, loss: 0.008576392196118832\n",
            "step: 210, loss: 0.002120482036843896\n",
            "step: 220, loss: 0.036752328276634216\n",
            "step: 230, loss: 0.013797011226415634\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9659863945578231, f1=0.9737742303306728, best_f1=0.9636363636363636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.041070323437452316\n",
            "step: 10, loss: 0.006484880577772856\n",
            "step: 20, loss: 0.0055948542430996895\n",
            "step: 30, loss: 0.010280396789312363\n",
            "step: 40, loss: 0.022333214059472084\n",
            "step: 50, loss: 0.008418474346399307\n",
            "step: 60, loss: 0.011684635654091835\n",
            "step: 70, loss: 0.02109358087182045\n",
            "step: 80, loss: 0.022980058565735817\n",
            "step: 90, loss: 0.005322948098182678\n",
            "step: 100, loss: 0.0033201107289642096\n",
            "step: 110, loss: 0.002198008121922612\n",
            "step: 120, loss: 0.00890409667044878\n",
            "step: 130, loss: 0.007903559133410454\n",
            "step: 140, loss: 0.0010652905330061913\n",
            "step: 150, loss: 0.0012248086277395487\n",
            "step: 160, loss: 0.0019624752458184958\n",
            "step: 170, loss: 0.0009577552555128932\n",
            "step: 180, loss: 0.06365686655044556\n",
            "step: 190, loss: 0.0027821743860840797\n",
            "step: 200, loss: 0.006922677159309387\n",
            "step: 210, loss: 0.07813326269388199\n",
            "step: 220, loss: 0.0011374797904863954\n",
            "step: 230, loss: 0.04019227996468544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9738933030646991, f1=0.971297359357061, best_f1=0.971297359357061\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002504336182028055\n",
            "step: 10, loss: 0.003056056797504425\n",
            "step: 20, loss: 0.004320717882364988\n",
            "step: 30, loss: 0.0021697706542909145\n",
            "step: 40, loss: 0.0036884527653455734\n",
            "step: 50, loss: 0.0017627314664423466\n",
            "step: 60, loss: 0.047404881566762924\n",
            "step: 70, loss: 0.004833359736949205\n",
            "step: 80, loss: 0.11772596836090088\n",
            "step: 90, loss: 0.04739479348063469\n",
            "step: 100, loss: 0.0007015002775005996\n",
            "step: 110, loss: 0.0006441136356443167\n",
            "step: 120, loss: 0.0014097357634454966\n",
            "step: 130, loss: 0.0005535815143957734\n",
            "step: 140, loss: 0.0014926670119166374\n",
            "step: 150, loss: 0.01321756187826395\n",
            "step: 160, loss: 0.018672820180654526\n",
            "step: 170, loss: 0.002490613842383027\n",
            "step: 180, loss: 0.0017940745456144214\n",
            "step: 190, loss: 0.00775150815024972\n",
            "step: 200, loss: 0.003136423183605075\n",
            "step: 210, loss: 0.03737763315439224\n",
            "step: 220, loss: 0.001589434570632875\n",
            "step: 230, loss: 0.14169064164161682\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9797752808988766, f1=0.9715585893060296, best_f1=0.9715585893060296\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00035216426476836205\n",
            "step: 10, loss: 0.0019380302401259542\n",
            "step: 20, loss: 0.0019281900022178888\n",
            "step: 30, loss: 0.0006112235714681447\n",
            "step: 40, loss: 0.00046860743896104395\n",
            "step: 50, loss: 0.0002831749734468758\n",
            "step: 60, loss: 0.0012402479769662023\n",
            "step: 70, loss: 0.13195671141147614\n",
            "step: 80, loss: 0.002074070740491152\n",
            "step: 90, loss: 0.004643748514354229\n",
            "step: 100, loss: 0.0017221702728420496\n",
            "step: 110, loss: 0.013963887467980385\n",
            "step: 120, loss: 0.0007448501419275999\n",
            "step: 130, loss: 0.06179412081837654\n",
            "step: 140, loss: 0.0004965451662428677\n",
            "step: 150, loss: 0.002072138013318181\n",
            "step: 160, loss: 0.009962515905499458\n",
            "step: 170, loss: 0.0010956812184304\n",
            "step: 180, loss: 0.00040342361899092793\n",
            "step: 190, loss: 0.0014037879882380366\n",
            "step: 200, loss: 0.0064537543803453445\n",
            "step: 210, loss: 0.006307034753262997\n",
            "step: 220, loss: 0.0138021195307374\n",
            "step: 230, loss: 0.017443224787712097\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9831271091113611, f1=0.9719416386083053, best_f1=0.9719416386083053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015658042393624783\n",
            "step: 10, loss: 0.0020954585634171963\n",
            "step: 20, loss: 0.0008476717630401254\n",
            "step: 30, loss: 0.0007848941022530198\n",
            "step: 40, loss: 0.005675529129803181\n",
            "step: 50, loss: 0.012529068626463413\n",
            "step: 60, loss: 0.00042337950435467064\n",
            "step: 70, loss: 0.0007942054653540254\n",
            "step: 80, loss: 0.0007307685445994139\n",
            "step: 90, loss: 0.0009928514482453465\n",
            "step: 100, loss: 0.0012089596129953861\n",
            "step: 110, loss: 0.0011727388482540846\n",
            "step: 120, loss: 0.0008690867689438164\n",
            "step: 130, loss: 0.0019767466001212597\n",
            "step: 140, loss: 0.0002678185992408544\n",
            "step: 150, loss: 0.005438077729195356\n",
            "step: 160, loss: 0.0009559248574078083\n",
            "step: 170, loss: 0.006117900367826223\n",
            "step: 180, loss: 0.0023720362223684788\n",
            "step: 190, loss: 0.005966739729046822\n",
            "step: 200, loss: 0.000903647392988205\n",
            "step: 210, loss: 0.0007868417305871844\n",
            "step: 220, loss: 0.0013901495840400457\n",
            "step: 230, loss: 0.004406806081533432\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9785794813979707, f1=0.9761092150170648, best_f1=0.9719416386083053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002818965818732977\n",
            "step: 10, loss: 0.01506307814270258\n",
            "step: 20, loss: 0.08916109800338745\n",
            "step: 30, loss: 0.07433689385652542\n",
            "step: 40, loss: 0.0027059258427470922\n",
            "step: 50, loss: 0.0010526090627536178\n",
            "step: 60, loss: 0.001860043965280056\n",
            "step: 70, loss: 0.011505392380058765\n",
            "step: 80, loss: 0.004398331977427006\n",
            "step: 90, loss: 0.0005495777586475015\n",
            "step: 100, loss: 0.0007606637082062662\n",
            "step: 110, loss: 0.0012862824369221926\n",
            "step: 120, loss: 0.0009630063432268798\n",
            "step: 130, loss: 0.0011091248597949743\n",
            "step: 140, loss: 0.006585684604942799\n",
            "step: 150, loss: 0.2742387652397156\n",
            "step: 160, loss: 0.0022014500573277473\n",
            "step: 170, loss: 0.009584433399140835\n",
            "step: 180, loss: 0.025723934173583984\n",
            "step: 190, loss: 0.0009995547588914633\n",
            "step: 200, loss: 0.0327884815633297\n",
            "step: 210, loss: 0.004645452369004488\n",
            "step: 220, loss: 0.0014207020867615938\n",
            "step: 230, loss: 0.000854150508530438\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9807037457434733, f1=0.9713631156930126, best_f1=0.9719416386083053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019601471722126007\n",
            "step: 10, loss: 0.0008025029092095792\n",
            "step: 20, loss: 0.0009756376384757459\n",
            "step: 30, loss: 0.005121797788888216\n",
            "step: 40, loss: 0.001956250751391053\n",
            "step: 50, loss: 0.0014685624046251178\n",
            "step: 60, loss: 0.0007464203517884016\n",
            "step: 70, loss: 0.03822178393602371\n",
            "step: 80, loss: 0.00020848905842285603\n",
            "step: 90, loss: 0.08581572026014328\n",
            "step: 100, loss: 0.00038485968252643943\n",
            "step: 110, loss: 0.0002947227912954986\n",
            "step: 120, loss: 0.025172313675284386\n",
            "step: 130, loss: 0.001154389581643045\n",
            "step: 140, loss: 0.005972547456622124\n",
            "step: 150, loss: 0.0010747804772108793\n",
            "step: 160, loss: 0.070456862449646\n",
            "step: 170, loss: 0.0005606524064205587\n",
            "step: 180, loss: 0.0007580043748021126\n",
            "step: 190, loss: 0.0635673776268959\n",
            "step: 200, loss: 0.011444580741226673\n",
            "step: 210, loss: 0.03308788314461708\n",
            "step: 220, loss: 0.0009469327633269131\n",
            "step: 230, loss: 0.0005362754454836249\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9775784753363228, f1=0.9739524348810873, best_f1=0.9719416386083053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010215967195108533\n",
            "step: 10, loss: 0.0008151897927746177\n",
            "step: 20, loss: 0.001282989513128996\n",
            "step: 30, loss: 0.00029959692619740963\n",
            "step: 40, loss: 0.0014232629910111427\n",
            "step: 50, loss: 0.001286880113184452\n",
            "step: 60, loss: 0.0006464797770604491\n",
            "step: 70, loss: 0.004603639245033264\n",
            "step: 80, loss: 0.0011260568862780929\n",
            "step: 90, loss: 0.00018950965022668242\n",
            "step: 100, loss: 0.0002290646079927683\n",
            "step: 110, loss: 0.002061137929558754\n",
            "step: 120, loss: 0.0002531330392230302\n",
            "step: 130, loss: 0.00038846596726216376\n",
            "step: 140, loss: 0.00022661649563815445\n",
            "step: 150, loss: 0.000284312991425395\n",
            "step: 160, loss: 8.649995288578793e-05\n",
            "step: 170, loss: 0.0002630456583574414\n",
            "step: 180, loss: 0.0005906244041398168\n",
            "step: 190, loss: 0.0005099994014017284\n",
            "step: 200, loss: 0.001117880456149578\n",
            "step: 210, loss: 0.0009629758424125612\n",
            "step: 220, loss: 0.04909569397568703\n",
            "step: 230, loss: 0.00046504332567565143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9829351535836178, f1=0.9726651480637813, best_f1=0.9719416386083053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006194582674652338\n",
            "step: 10, loss: 0.001030131708830595\n",
            "step: 20, loss: 0.0010360486339777708\n",
            "step: 30, loss: 0.0006004552124068141\n",
            "step: 40, loss: 9.383722499478608e-05\n",
            "step: 50, loss: 0.0004508741549216211\n",
            "step: 60, loss: 0.00922320131212473\n",
            "step: 70, loss: 0.0016479501500725746\n",
            "step: 80, loss: 0.0005920950789004564\n",
            "step: 90, loss: 0.16390947997570038\n",
            "step: 100, loss: 0.0005805248511023819\n",
            "step: 110, loss: 0.0010821628384292126\n",
            "step: 120, loss: 0.0003421300498303026\n",
            "step: 130, loss: 0.00022977974731475115\n",
            "step: 140, loss: 0.01005602814257145\n",
            "step: 150, loss: 0.0004406201187521219\n",
            "step: 160, loss: 0.0018194284057244658\n",
            "step: 170, loss: 0.0013173818588256836\n",
            "step: 180, loss: 0.001858262112364173\n",
            "step: 190, loss: 0.0005619923467747867\n",
            "step: 200, loss: 0.0013149534352123737\n",
            "step: 210, loss: 0.0009859215933829546\n",
            "step: 220, loss: 0.0006354115321300924\n",
            "step: 230, loss: 0.0005688787787221372\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9841986455981941, f1=0.9727891156462585, best_f1=0.9727891156462585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000608264293987304\n",
            "step: 10, loss: 0.0001611778570804745\n",
            "step: 20, loss: 0.011668316088616848\n",
            "step: 30, loss: 0.0026943201664835215\n",
            "step: 40, loss: 0.00042024313006550074\n",
            "step: 50, loss: 0.00044814785360358655\n",
            "step: 60, loss: 0.0005842758691869676\n",
            "step: 70, loss: 0.0002570826036389917\n",
            "step: 80, loss: 0.0002353664895053953\n",
            "step: 90, loss: 0.0005431402241811156\n",
            "step: 100, loss: 0.0002453806810081005\n",
            "step: 110, loss: 0.00040391285438090563\n",
            "step: 120, loss: 0.0007227911846712232\n",
            "step: 130, loss: 0.0004383932682685554\n",
            "step: 140, loss: 0.0011441153474152088\n",
            "step: 150, loss: 0.0004767244099639356\n",
            "step: 160, loss: 0.0018998971208930016\n",
            "step: 170, loss: 0.00170269759837538\n",
            "step: 180, loss: 0.0017236738931387663\n",
            "step: 190, loss: 0.0020621786825358868\n",
            "step: 200, loss: 0.0003198266203980893\n",
            "step: 210, loss: 0.060666799545288086\n",
            "step: 220, loss: 0.0017970781773328781\n",
            "step: 230, loss: 0.004527405370026827\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9831271091113611, f1=0.9740112994350283, best_f1=0.9727891156462585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008832024759612978\n",
            "step: 10, loss: 0.0009736130014061928\n",
            "step: 20, loss: 0.0023959174286574125\n",
            "step: 30, loss: 9.26295542740263e-05\n",
            "step: 40, loss: 0.0008596126572228968\n",
            "step: 50, loss: 0.0016392734833061695\n",
            "step: 60, loss: 0.000722908356692642\n",
            "step: 70, loss: 0.005636018700897694\n",
            "step: 80, loss: 0.0006410395726561546\n",
            "step: 90, loss: 0.0005871693720109761\n",
            "step: 100, loss: 0.0009259807993657887\n",
            "step: 110, loss: 0.0025258525274693966\n",
            "step: 120, loss: 0.0011920843971893191\n",
            "step: 130, loss: 0.0015509374206885695\n",
            "step: 140, loss: 0.0004262958827894181\n",
            "step: 150, loss: 0.00024223688524216413\n",
            "step: 160, loss: 0.0037554092705249786\n",
            "step: 170, loss: 0.00031139777274802327\n",
            "step: 180, loss: 0.005453118123114109\n",
            "step: 190, loss: 0.0002419169177301228\n",
            "step: 200, loss: 7.047780673019588e-05\n",
            "step: 210, loss: 0.0008976193494163454\n",
            "step: 220, loss: 0.0001864700752776116\n",
            "step: 230, loss: 0.0004554332699626684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9842696629213483, f1=0.9764309764309763, best_f1=0.9764309764309763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011942796845687553\n",
            "step: 10, loss: 0.00032877232297323644\n",
            "step: 20, loss: 0.00017926222062669694\n",
            "step: 30, loss: 0.000349063629982993\n",
            "step: 40, loss: 0.00024264983949251473\n",
            "step: 50, loss: 0.000185574492206797\n",
            "step: 60, loss: 0.00020097869855817407\n",
            "step: 70, loss: 0.0003933871630579233\n",
            "step: 80, loss: 0.0003563673817552626\n",
            "step: 90, loss: 0.0019739691633731127\n",
            "step: 100, loss: 0.0004608622402884066\n",
            "step: 110, loss: 0.0010405495995655656\n",
            "step: 120, loss: 4.047348920721561e-05\n",
            "step: 130, loss: 0.002681075595319271\n",
            "step: 140, loss: 0.0004592329205479473\n",
            "step: 150, loss: 0.0007139876252040267\n",
            "step: 160, loss: 0.00020260378369130194\n",
            "step: 170, loss: 0.0010316353291273117\n",
            "step: 180, loss: 0.0005823852261528373\n",
            "step: 190, loss: 0.00010948726412607357\n",
            "step: 200, loss: 0.0003562843776308\n",
            "step: 210, loss: 0.0002791859151329845\n",
            "step: 220, loss: 0.00023553581559099257\n",
            "step: 230, loss: 0.0016701024724170566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.984304932735426, f1=0.9764837625979844, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010422329418361187\n",
            "step: 10, loss: 0.0001536373601993546\n",
            "step: 20, loss: 0.001170448842458427\n",
            "step: 30, loss: 0.00026711318059824407\n",
            "step: 40, loss: 0.0001197649835376069\n",
            "step: 50, loss: 7.959020877024159e-05\n",
            "step: 60, loss: 0.010459567420184612\n",
            "step: 70, loss: 0.0016753117088228464\n",
            "step: 80, loss: 0.00026206718757748604\n",
            "step: 90, loss: 0.00015918178542051464\n",
            "step: 100, loss: 7.157595246098936e-05\n",
            "step: 110, loss: 0.00015424737648572773\n",
            "step: 120, loss: 0.033568523824214935\n",
            "step: 130, loss: 0.0005937325768172741\n",
            "step: 140, loss: 0.0018968931399285793\n",
            "step: 150, loss: 0.00017192687664646655\n",
            "step: 160, loss: 0.012257004156708717\n",
            "step: 170, loss: 5.33180900674779e-05\n",
            "step: 180, loss: 0.00020473050244618207\n",
            "step: 190, loss: 0.0035951808094978333\n",
            "step: 200, loss: 0.0017232151003554463\n",
            "step: 210, loss: 0.009973814710974693\n",
            "step: 220, loss: 0.0004325972986407578\n",
            "step: 230, loss: 0.00011987981997663155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9832026875699889, f1=0.9764837625979844, best_f1=0.9764837625979844\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:17, 142.44it/s]\n",
            "load_f1 = 0.9854748603351955\n",
            "real_f1 = 0.9821029082774049\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 136.61it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW78AaaneEUs"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hwn5WkZe3Kb",
        "outputId": "d7b9e3c4-adba-451e-fdf5-360a26090b9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7208261489868164\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.41731059551239014\n",
            "step: 20, loss: 0.3506248891353607\n",
            "step: 30, loss: 0.3948271572589874\n",
            "step: 40, loss: 0.19617338478565216\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.5094064474105835\n",
            "step: 60, loss: 0.1623222678899765\n",
            "step: 70, loss: 0.18014772236347198\n",
            "step: 80, loss: 0.11208117753267288\n",
            "step: 90, loss: 0.13906855881214142\n",
            "step: 100, loss: 0.47078293561935425\n",
            "step: 110, loss: 0.11808696389198303\n",
            "step: 120, loss: 0.23722636699676514\n",
            "step: 130, loss: 0.16298383474349976\n",
            "step: 140, loss: 0.32329073548316956\n",
            "step: 150, loss: 0.10458291321992874\n",
            "step: 160, loss: 0.09984090924263\n",
            "step: 170, loss: 0.04655781388282776\n",
            "step: 180, loss: 0.08482000976800919\n",
            "step: 190, loss: 0.041054390370845795\n",
            "step: 200, loss: 0.1743331104516983\n",
            "step: 210, loss: 0.1490347534418106\n",
            "step: 220, loss: 0.04531518369913101\n",
            "step: 230, loss: 0.08847472816705704\n",
            "step: 240, loss: 0.03171071037650108\n",
            "step: 250, loss: 0.06659876555204391\n",
            "step: 260, loss: 0.16257883608341217\n",
            "step: 270, loss: 0.37339621782302856\n",
            "step: 280, loss: 0.06739874184131622\n",
            "step: 290, loss: 0.10938166081905365\n",
            "step: 300, loss: 0.05446028709411621\n",
            "step: 310, loss: 0.1398921012878418\n",
            "step: 320, loss: 0.10324323177337646\n",
            "step: 330, loss: 0.07535836845636368\n",
            "step: 340, loss: 0.3066200017929077\n",
            "step: 350, loss: 0.07252293825149536\n",
            "step: 360, loss: 0.03223886713385582\n",
            "step: 370, loss: 0.07101593911647797\n",
            "step: 380, loss: 0.1638464331626892\n",
            "step: 390, loss: 0.015040051192045212\n",
            "step: 400, loss: 0.08513890951871872\n",
            "step: 410, loss: 0.2349909543991089\n",
            "step: 420, loss: 0.08424166589975357\n",
            "step: 430, loss: 0.0690818727016449\n",
            "step: 440, loss: 0.049499280750751495\n",
            "step: 450, loss: 0.040630124509334564\n",
            "step: 460, loss: 0.016700247302651405\n",
            "step: 470, loss: 0.060196541249752045\n",
            "step: 480, loss: 0.100895456969738\n",
            "step: 490, loss: 0.15076297521591187\n",
            "step: 500, loss: 0.09051138907670975\n",
            "step: 510, loss: 0.10176962614059448\n",
            "step: 520, loss: 0.11244010925292969\n",
            "step: 530, loss: 0.06710643321275711\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9298245614035088, f1=0.9297597042513862, best_f1=0.9297597042513862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14394813776016235\n",
            "step: 10, loss: 0.04738503694534302\n",
            "step: 20, loss: 0.018451634794473648\n",
            "step: 30, loss: 0.01640971750020981\n",
            "step: 40, loss: 0.12205512821674347\n",
            "step: 50, loss: 0.18248726427555084\n",
            "step: 60, loss: 0.01786869764328003\n",
            "step: 70, loss: 0.01244441233575344\n",
            "step: 80, loss: 0.022209780290722847\n",
            "step: 90, loss: 0.022022048011422157\n",
            "step: 100, loss: 0.17162545025348663\n",
            "step: 110, loss: 0.01043783314526081\n",
            "step: 120, loss: 0.01848393864929676\n",
            "step: 130, loss: 0.005996183957904577\n",
            "step: 140, loss: 0.15478961169719696\n",
            "step: 150, loss: 0.019294172525405884\n",
            "step: 160, loss: 0.035602349787950516\n",
            "step: 170, loss: 0.0525505468249321\n",
            "step: 180, loss: 0.008947753347456455\n",
            "step: 190, loss: 0.0031506691593676805\n",
            "step: 200, loss: 0.12027031183242798\n",
            "step: 210, loss: 0.034887656569480896\n",
            "step: 220, loss: 0.006670203525573015\n",
            "step: 230, loss: 0.06820300966501236\n",
            "step: 240, loss: 0.002699154894798994\n",
            "step: 250, loss: 0.054259661585092545\n",
            "step: 260, loss: 0.027138935402035713\n",
            "step: 270, loss: 0.050852108746767044\n",
            "step: 280, loss: 0.06645364314317703\n",
            "step: 290, loss: 0.04249089956283569\n",
            "step: 300, loss: 0.027695786207914352\n",
            "step: 310, loss: 0.05553487315773964\n",
            "step: 320, loss: 0.04170946031808853\n",
            "step: 330, loss: 0.17423301935195923\n",
            "step: 340, loss: 0.21799908578395844\n",
            "step: 350, loss: 0.0019406683277338743\n",
            "step: 360, loss: 0.12220408022403717\n",
            "step: 370, loss: 0.027679244056344032\n",
            "step: 380, loss: 0.11994452029466629\n",
            "step: 390, loss: 0.004657236859202385\n",
            "step: 400, loss: 0.0544724203646183\n",
            "step: 410, loss: 0.02133912965655327\n",
            "step: 420, loss: 0.06719934940338135\n",
            "step: 430, loss: 0.11130338162183762\n",
            "step: 440, loss: 0.021950673311948776\n",
            "step: 450, loss: 0.05013292655348778\n",
            "step: 460, loss: 0.047421567142009735\n",
            "step: 470, loss: 0.06730484217405319\n",
            "step: 480, loss: 0.05901051312685013\n",
            "step: 490, loss: 0.05787115544080734\n",
            "step: 500, loss: 0.0036645501386374235\n",
            "step: 510, loss: 0.049524009227752686\n",
            "step: 520, loss: 0.30406710505485535\n",
            "step: 530, loss: 0.06250070780515671\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9371584699453551, f1=0.9380127620783956, best_f1=0.9380127620783956\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14033947885036469\n",
            "step: 10, loss: 0.2750968933105469\n",
            "step: 20, loss: 0.06849890202283859\n",
            "step: 30, loss: 0.0605732686817646\n",
            "step: 40, loss: 0.10058233141899109\n",
            "step: 50, loss: 0.0027491494547575712\n",
            "step: 60, loss: 0.00934015866369009\n",
            "step: 70, loss: 0.014359411783516407\n",
            "step: 80, loss: 0.03576921671628952\n",
            "step: 90, loss: 0.03545499965548515\n",
            "step: 100, loss: 0.05348643660545349\n",
            "step: 110, loss: 0.03287656977772713\n",
            "step: 120, loss: 0.14694203436374664\n",
            "step: 130, loss: 0.12623344361782074\n",
            "step: 140, loss: 0.02656704932451248\n",
            "step: 150, loss: 0.020757514983415604\n",
            "step: 160, loss: 0.0023890468291938305\n",
            "step: 170, loss: 0.01026911474764347\n",
            "step: 180, loss: 0.0034937048330903053\n",
            "step: 190, loss: 0.017087379470467567\n",
            "step: 200, loss: 0.09982658922672272\n",
            "step: 210, loss: 0.023267868906259537\n",
            "step: 220, loss: 0.12220954895019531\n",
            "step: 230, loss: 0.04161111265420914\n",
            "step: 240, loss: 0.03047804720699787\n",
            "step: 250, loss: 0.02804487757384777\n",
            "step: 260, loss: 0.02608213573694229\n",
            "step: 270, loss: 0.004021290689706802\n",
            "step: 280, loss: 0.0009764556307345629\n",
            "step: 290, loss: 0.0034305446315556765\n",
            "step: 300, loss: 0.10821763426065445\n",
            "step: 310, loss: 0.07563499361276627\n",
            "step: 320, loss: 0.0441729910671711\n",
            "step: 330, loss: 0.003701006295159459\n",
            "step: 340, loss: 0.0022928190883249044\n",
            "step: 350, loss: 0.12326404452323914\n",
            "step: 360, loss: 0.02312406152486801\n",
            "step: 370, loss: 0.07803698629140854\n",
            "step: 380, loss: 0.0799250602722168\n",
            "step: 390, loss: 0.021550772711634636\n",
            "step: 400, loss: 0.16554629802703857\n",
            "step: 410, loss: 0.07270729541778564\n",
            "step: 420, loss: 0.010040998458862305\n",
            "step: 430, loss: 0.01887296326458454\n",
            "step: 440, loss: 0.2091255486011505\n",
            "step: 450, loss: 0.012951460666954517\n",
            "step: 460, loss: 0.11784528940916061\n",
            "step: 470, loss: 0.21105599403381348\n",
            "step: 480, loss: 0.2985934615135193\n",
            "step: 490, loss: 0.02723541110754013\n",
            "step: 500, loss: 0.03266235813498497\n",
            "step: 510, loss: 0.008421999402344227\n",
            "step: 520, loss: 0.06277871131896973\n",
            "step: 530, loss: 0.00641339598223567\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9421028253821214, f1=0.9255663430420712, best_f1=0.9255663430420712\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007226517889648676\n",
            "step: 10, loss: 0.004156900104135275\n",
            "step: 20, loss: 0.007309845648705959\n",
            "step: 30, loss: 0.040406350046396255\n",
            "step: 40, loss: 0.005104545503854752\n",
            "step: 50, loss: 0.07995985448360443\n",
            "step: 60, loss: 0.004202755633741617\n",
            "step: 70, loss: 0.030630668625235558\n",
            "step: 80, loss: 0.1514783650636673\n",
            "step: 90, loss: 0.04815511777997017\n",
            "step: 100, loss: 0.0020245041232556105\n",
            "step: 110, loss: 0.08905773609876633\n",
            "step: 120, loss: 0.03515656664967537\n",
            "step: 130, loss: 0.05000946670770645\n",
            "step: 140, loss: 0.0894552618265152\n",
            "step: 150, loss: 0.0030990445520728827\n",
            "step: 160, loss: 0.04989860579371452\n",
            "step: 170, loss: 0.01856495626270771\n",
            "step: 180, loss: 0.005737274885177612\n",
            "step: 190, loss: 0.06337582319974899\n",
            "step: 200, loss: 0.07872481644153595\n",
            "step: 210, loss: 0.0016393272671848536\n",
            "step: 220, loss: 0.005218889564275742\n",
            "step: 230, loss: 0.07662505656480789\n",
            "step: 240, loss: 0.019782476127147675\n",
            "step: 250, loss: 0.08243149518966675\n",
            "step: 260, loss: 0.0014077534433454275\n",
            "step: 270, loss: 0.07619298994541168\n",
            "step: 280, loss: 0.005486281588673592\n",
            "step: 290, loss: 0.005566947627812624\n",
            "step: 300, loss: 0.003043434116989374\n",
            "step: 310, loss: 0.0037165842950344086\n",
            "step: 320, loss: 0.05392272397875786\n",
            "step: 330, loss: 0.042679060250520706\n",
            "step: 340, loss: 0.01390956062823534\n",
            "step: 350, loss: 0.04958617314696312\n",
            "step: 360, loss: 0.15008564293384552\n",
            "step: 370, loss: 0.005249158013612032\n",
            "step: 380, loss: 0.0036097464617341757\n",
            "step: 390, loss: 0.00022295974486041814\n",
            "step: 400, loss: 0.016498425975441933\n",
            "step: 410, loss: 0.0018439082195982337\n",
            "step: 420, loss: 0.024395478889346123\n",
            "step: 430, loss: 0.00728612719103694\n",
            "step: 440, loss: 0.0026121041737496853\n",
            "step: 450, loss: 0.013016235083341599\n",
            "step: 460, loss: 0.05622696131467819\n",
            "step: 470, loss: 0.0014320939080789685\n",
            "step: 480, loss: 0.05490565672516823\n",
            "step: 490, loss: 0.01720457524061203\n",
            "step: 500, loss: 0.09866756200790405\n",
            "step: 510, loss: 0.2005043476819992\n",
            "step: 520, loss: 0.029065165668725967\n",
            "step: 530, loss: 0.13264881074428558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9474671669793621, f1=0.9400278940027894, best_f1=0.9400278940027894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013754062820225954\n",
            "step: 10, loss: 0.0033352875616401434\n",
            "step: 20, loss: 0.009858457371592522\n",
            "step: 30, loss: 0.06311589479446411\n",
            "step: 40, loss: 0.003096937667578459\n",
            "step: 50, loss: 0.002464870922267437\n",
            "step: 60, loss: 0.04456410929560661\n",
            "step: 70, loss: 0.0006936682621017098\n",
            "step: 80, loss: 0.0037226404529064894\n",
            "step: 90, loss: 0.0028076793532818556\n",
            "step: 100, loss: 0.014474115334451199\n",
            "step: 110, loss: 0.006425668951123953\n",
            "step: 120, loss: 0.06517960876226425\n",
            "step: 130, loss: 0.007255617529153824\n",
            "step: 140, loss: 0.0029722116887569427\n",
            "step: 150, loss: 0.07925143837928772\n",
            "step: 160, loss: 0.01961151324212551\n",
            "step: 170, loss: 0.0753280371427536\n",
            "step: 180, loss: 0.008197293616831303\n",
            "step: 190, loss: 0.114530049264431\n",
            "step: 200, loss: 0.009241798892617226\n",
            "step: 210, loss: 0.0009448539931327105\n",
            "step: 220, loss: 0.0627857968211174\n",
            "step: 230, loss: 0.0034928449895232916\n",
            "step: 240, loss: 0.014313950203359127\n",
            "step: 250, loss: 0.10606885701417923\n",
            "step: 260, loss: 0.015109394676983356\n",
            "step: 270, loss: 0.020629480481147766\n",
            "step: 280, loss: 0.007969274185597897\n",
            "step: 290, loss: 0.002716844668611884\n",
            "step: 300, loss: 0.01900378428399563\n",
            "step: 310, loss: 0.004397440701723099\n",
            "step: 320, loss: 0.01616804488003254\n",
            "step: 330, loss: 0.020200073719024658\n",
            "step: 340, loss: 0.004001704044640064\n",
            "step: 350, loss: 0.007328803185373545\n",
            "step: 360, loss: 0.0008651597891002893\n",
            "step: 370, loss: 0.00368562713265419\n",
            "step: 380, loss: 0.0018244166858494282\n",
            "step: 390, loss: 0.001634144107811153\n",
            "step: 400, loss: 0.07369483262300491\n",
            "step: 410, loss: 0.017978554591536522\n",
            "step: 420, loss: 0.10687561333179474\n",
            "step: 430, loss: 0.11533123254776001\n",
            "step: 440, loss: 0.004594144411385059\n",
            "step: 450, loss: 0.15777041018009186\n",
            "step: 460, loss: 0.01463061198592186\n",
            "step: 470, loss: 0.02411031723022461\n",
            "step: 480, loss: 0.022779151797294617\n",
            "step: 490, loss: 0.0374239943921566\n",
            "step: 500, loss: 0.016060572117567062\n",
            "step: 510, loss: 0.0465354360640049\n",
            "step: 520, loss: 0.0535159595310688\n",
            "step: 530, loss: 0.03471904620528221\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.944704779756326, f1=0.9320754716981132, best_f1=0.9400278940027894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0393865592777729\n",
            "step: 10, loss: 0.013587665744125843\n",
            "step: 20, loss: 0.014565971679985523\n",
            "step: 30, loss: 0.001538545242510736\n",
            "step: 40, loss: 0.01110862661153078\n",
            "step: 50, loss: 0.0009594039293006063\n",
            "step: 60, loss: 0.002552961464971304\n",
            "step: 70, loss: 0.004559722729027271\n",
            "step: 80, loss: 6.457351992139593e-05\n",
            "step: 90, loss: 0.004331469070166349\n",
            "step: 100, loss: 0.02410808578133583\n",
            "step: 110, loss: 0.030129902064800262\n",
            "step: 120, loss: 0.04276247322559357\n",
            "step: 130, loss: 0.024874577298760414\n",
            "step: 140, loss: 0.0013140342198312283\n",
            "step: 150, loss: 0.00030014297226443887\n",
            "step: 160, loss: 0.003322798293083906\n",
            "step: 170, loss: 0.0018159158062189817\n",
            "step: 180, loss: 0.005836889613419771\n",
            "step: 190, loss: 0.13357438147068024\n",
            "step: 200, loss: 0.007295504678040743\n",
            "step: 210, loss: 0.00150210689753294\n",
            "step: 220, loss: 0.07401008158922195\n",
            "step: 230, loss: 0.006890340242534876\n",
            "step: 240, loss: 0.016233572736382484\n",
            "step: 250, loss: 0.0718943327665329\n",
            "step: 260, loss: 0.013742375187575817\n",
            "step: 270, loss: 0.008799106813967228\n",
            "step: 280, loss: 0.08524863421916962\n",
            "step: 290, loss: 0.01959274709224701\n",
            "step: 300, loss: 0.007681285496801138\n",
            "step: 310, loss: 0.10221588611602783\n",
            "step: 320, loss: 0.0012568809324875474\n",
            "step: 330, loss: 0.0031296799425035715\n",
            "step: 340, loss: 0.0029651192016899586\n",
            "step: 350, loss: 0.026075217872858047\n",
            "step: 360, loss: 0.025676734745502472\n",
            "step: 370, loss: 0.0010926842223852873\n",
            "step: 380, loss: 0.00011530805932125077\n",
            "step: 390, loss: 0.002187141450121999\n",
            "step: 400, loss: 0.03437718376517296\n",
            "step: 410, loss: 6.916873098816723e-05\n",
            "step: 420, loss: 0.12723681330680847\n",
            "step: 430, loss: 0.0066863554529845715\n",
            "step: 440, loss: 0.0028910706751048565\n",
            "step: 450, loss: 0.050317294895648956\n",
            "step: 460, loss: 0.0015198104083538055\n",
            "step: 470, loss: 0.001197111327201128\n",
            "step: 480, loss: 0.0008292206330224872\n",
            "step: 490, loss: 0.009356536902487278\n",
            "step: 500, loss: 0.013378445990383625\n",
            "step: 510, loss: 0.05670247599482536\n",
            "step: 520, loss: 0.007293465081602335\n",
            "step: 530, loss: 0.0222573671489954\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9500924214417744, f1=0.9390018484288354, best_f1=0.9390018484288354\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005579730030149221\n",
            "step: 10, loss: 0.0005409774603322148\n",
            "step: 20, loss: 0.0018320393282920122\n",
            "step: 30, loss: 0.01558644138276577\n",
            "step: 40, loss: 0.0003778492391575128\n",
            "step: 50, loss: 0.022419700399041176\n",
            "step: 60, loss: 0.0018114240374416113\n",
            "step: 70, loss: 0.002037315396592021\n",
            "step: 80, loss: 0.0030162313487380743\n",
            "step: 90, loss: 0.0007006244268268347\n",
            "step: 100, loss: 0.013811304233968258\n",
            "step: 110, loss: 0.00016059317567851394\n",
            "step: 120, loss: 0.00023018325737211853\n",
            "step: 130, loss: 0.00032236508559435606\n",
            "step: 140, loss: 0.0017035611672326922\n",
            "step: 150, loss: 0.0012629976263269782\n",
            "step: 160, loss: 0.00018433154036756605\n",
            "step: 170, loss: 0.05402996763586998\n",
            "step: 180, loss: 0.0467509999871254\n",
            "step: 190, loss: 0.0038089591544121504\n",
            "step: 200, loss: 0.00037447645445354283\n",
            "step: 210, loss: 0.0012316345237195492\n",
            "step: 220, loss: 3.7698180676670745e-05\n",
            "step: 230, loss: 0.00020946857694070786\n",
            "step: 240, loss: 0.00016330975631717592\n",
            "step: 250, loss: 0.00454458175227046\n",
            "step: 260, loss: 0.010027243755757809\n",
            "step: 270, loss: 0.0021431308705359697\n",
            "step: 280, loss: 0.03013310208916664\n",
            "step: 290, loss: 0.007711514830589294\n",
            "step: 300, loss: 7.256653043441474e-05\n",
            "step: 310, loss: 0.00017249178199563175\n",
            "step: 320, loss: 0.018479540944099426\n",
            "step: 330, loss: 0.003022597637027502\n",
            "step: 340, loss: 0.00045875334762968123\n",
            "step: 350, loss: 0.0019877790473401546\n",
            "step: 360, loss: 0.034809648990631104\n",
            "step: 370, loss: 0.015375983901321888\n",
            "step: 380, loss: 0.008303419686853886\n",
            "step: 390, loss: 0.0283404178917408\n",
            "step: 400, loss: 0.01181887835264206\n",
            "step: 410, loss: 0.13296343386173248\n",
            "step: 420, loss: 0.005948410835117102\n",
            "step: 430, loss: 0.007763804867863655\n",
            "step: 440, loss: 0.049911487847566605\n",
            "step: 450, loss: 0.007467741146683693\n",
            "step: 460, loss: 0.0016000756295397878\n",
            "step: 470, loss: 0.19112299382686615\n",
            "step: 480, loss: 0.0021029645577073097\n",
            "step: 490, loss: 0.026155643165111542\n",
            "step: 500, loss: 0.0017544450238347054\n",
            "step: 510, loss: 0.000314609264023602\n",
            "step: 520, loss: 0.0010884609073400497\n",
            "step: 530, loss: 0.0042085954919457436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9417840375586854, f1=0.9296037296037295, best_f1=0.9390018484288354\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004388396628201008\n",
            "step: 10, loss: 0.003141577821224928\n",
            "step: 20, loss: 0.0037167815025895834\n",
            "step: 30, loss: 0.0026437253691256046\n",
            "step: 40, loss: 0.002092626178637147\n",
            "step: 50, loss: 0.0012820007978007197\n",
            "step: 60, loss: 0.0008753429283387959\n",
            "step: 70, loss: 0.04465221241116524\n",
            "step: 80, loss: 0.000862916640471667\n",
            "step: 90, loss: 0.01816745474934578\n",
            "step: 100, loss: 0.012518583796918392\n",
            "step: 110, loss: 0.0006112545961514115\n",
            "step: 120, loss: 0.004962841048836708\n",
            "step: 130, loss: 0.0019459922332316637\n",
            "step: 140, loss: 0.04140333831310272\n",
            "step: 150, loss: 0.001040967763401568\n",
            "step: 160, loss: 0.007733880542218685\n",
            "step: 170, loss: 0.28221267461776733\n",
            "step: 180, loss: 0.006067198235541582\n",
            "step: 190, loss: 0.004270344972610474\n",
            "step: 200, loss: 0.039042290300130844\n",
            "step: 210, loss: 0.12464630603790283\n",
            "step: 220, loss: 0.027059121057391167\n",
            "step: 230, loss: 0.04475095495581627\n",
            "step: 240, loss: 0.009691379964351654\n",
            "step: 250, loss: 0.011198081076145172\n",
            "step: 260, loss: 0.0022747262846678495\n",
            "step: 270, loss: 0.009748916141688824\n",
            "step: 280, loss: 0.002491454128175974\n",
            "step: 290, loss: 0.04669272527098656\n",
            "step: 300, loss: 0.0014232132816687226\n",
            "step: 310, loss: 0.0012874230742454529\n",
            "step: 320, loss: 0.02429487742483616\n",
            "step: 330, loss: 0.00024648697581142187\n",
            "step: 340, loss: 0.006578438449651003\n",
            "step: 350, loss: 0.00348775926977396\n",
            "step: 360, loss: 0.009664276614785194\n",
            "step: 370, loss: 0.07319650053977966\n",
            "step: 380, loss: 0.0006762562552466989\n",
            "step: 390, loss: 0.0480726957321167\n",
            "step: 400, loss: 0.03292554244399071\n",
            "step: 410, loss: 0.08295812457799911\n",
            "step: 420, loss: 0.00014206110790837556\n",
            "step: 430, loss: 0.02668185718357563\n",
            "step: 440, loss: 0.003138133557513356\n",
            "step: 450, loss: 0.0032398905605077744\n",
            "step: 460, loss: 0.047794077545404434\n",
            "step: 470, loss: 0.22053347527980804\n",
            "step: 480, loss: 0.00161934329662472\n",
            "step: 490, loss: 0.006064313463866711\n",
            "step: 500, loss: 0.005178837571293116\n",
            "step: 510, loss: 0.006676522549241781\n",
            "step: 520, loss: 0.005488567985594273\n",
            "step: 530, loss: 0.00031088723335415125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9505316689782709, f1=0.9362092703074805, best_f1=0.9362092703074805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007449167314916849\n",
            "step: 10, loss: 0.003372818697243929\n",
            "step: 20, loss: 0.001305879675783217\n",
            "step: 30, loss: 0.02887612208724022\n",
            "step: 40, loss: 0.040868427604436874\n",
            "step: 50, loss: 0.0011604288592934608\n",
            "step: 60, loss: 0.009473457932472229\n",
            "step: 70, loss: 0.0008031547185964882\n",
            "step: 80, loss: 0.12628445029258728\n",
            "step: 90, loss: 0.09673059731721878\n",
            "step: 100, loss: 0.001978003652766347\n",
            "step: 110, loss: 0.010408051311969757\n",
            "step: 120, loss: 0.001772148534655571\n",
            "step: 130, loss: 0.0005342814256437123\n",
            "step: 140, loss: 0.002296506194397807\n",
            "step: 150, loss: 0.006226508412510157\n",
            "step: 160, loss: 0.0037704790011048317\n",
            "step: 170, loss: 0.00043913419358432293\n",
            "step: 180, loss: 0.022861963137984276\n",
            "step: 190, loss: 0.0028275379445403814\n",
            "step: 200, loss: 8.447823347523808e-05\n",
            "step: 210, loss: 0.0005230656242929399\n",
            "step: 220, loss: 0.000497045402880758\n",
            "step: 230, loss: 0.004423648584634066\n",
            "step: 240, loss: 0.0055889650247991085\n",
            "step: 250, loss: 0.001241447520442307\n",
            "step: 260, loss: 0.0024954488035291433\n",
            "step: 270, loss: 0.02276911586523056\n",
            "step: 280, loss: 0.010722186416387558\n",
            "step: 290, loss: 0.0009675096953287721\n",
            "step: 300, loss: 0.0004382047918625176\n",
            "step: 310, loss: 0.14642341434955597\n",
            "step: 320, loss: 0.15087954699993134\n",
            "step: 330, loss: 0.0006147018284536898\n",
            "step: 340, loss: 0.0934896394610405\n",
            "step: 350, loss: 0.17930035293102264\n",
            "step: 360, loss: 0.0012294645421206951\n",
            "step: 370, loss: 0.003584421705454588\n",
            "step: 380, loss: 0.011428819969296455\n",
            "step: 390, loss: 0.001555754104629159\n",
            "step: 400, loss: 0.004130814224481583\n",
            "step: 410, loss: 0.0005765542155131698\n",
            "step: 420, loss: 0.0011611268855631351\n",
            "step: 430, loss: 0.03741292282938957\n",
            "step: 440, loss: 0.005262494552880526\n",
            "step: 450, loss: 0.028235413134098053\n",
            "step: 460, loss: 0.00036313384771347046\n",
            "step: 470, loss: 0.0006265139672905207\n",
            "step: 480, loss: 0.0015486221527680755\n",
            "step: 490, loss: 0.018901150673627853\n",
            "step: 500, loss: 0.00677532609552145\n",
            "step: 510, loss: 0.05661391094326973\n",
            "step: 520, loss: 0.004965729545801878\n",
            "step: 530, loss: 0.006924069952219725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9479981592268752, f1=0.9371847776249428, best_f1=0.9362092703074805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004704564809799194\n",
            "step: 10, loss: 0.0004879040934611112\n",
            "step: 20, loss: 0.0013632085174322128\n",
            "step: 30, loss: 0.0007825755164958537\n",
            "step: 40, loss: 0.0013658016687259078\n",
            "step: 50, loss: 0.0012252095621079206\n",
            "step: 60, loss: 0.025947099551558495\n",
            "step: 70, loss: 0.0006137277232483029\n",
            "step: 80, loss: 0.002292510587722063\n",
            "step: 90, loss: 0.0002593040117062628\n",
            "step: 100, loss: 0.003682107664644718\n",
            "step: 110, loss: 0.023849599063396454\n",
            "step: 120, loss: 0.0008683076594024897\n",
            "step: 130, loss: 0.00017448890139348805\n",
            "step: 140, loss: 0.00014863621618133038\n",
            "step: 150, loss: 0.006293043959885836\n",
            "step: 160, loss: 0.0054657235741615295\n",
            "step: 170, loss: 0.011356120929121971\n",
            "step: 180, loss: 0.0071091726422309875\n",
            "step: 190, loss: 0.002203435404226184\n",
            "step: 200, loss: 0.0016988886054605246\n",
            "step: 210, loss: 0.021792501211166382\n",
            "step: 220, loss: 0.0018952953396365047\n",
            "step: 230, loss: 0.0008552017970941961\n",
            "step: 240, loss: 0.0008940143161453307\n",
            "step: 250, loss: 0.0004619368992280215\n",
            "step: 260, loss: 0.08864610642194748\n",
            "step: 270, loss: 0.00027946149930357933\n",
            "step: 280, loss: 0.008158288896083832\n",
            "step: 290, loss: 0.0032208114862442017\n",
            "step: 300, loss: 0.002687284490093589\n",
            "step: 310, loss: 0.004007870331406593\n",
            "step: 320, loss: 0.0016701847780495882\n",
            "step: 330, loss: 0.012498252093791962\n",
            "step: 340, loss: 0.0011204518377780914\n",
            "step: 350, loss: 0.19554857909679413\n",
            "step: 360, loss: 0.0005315595772117376\n",
            "step: 370, loss: 0.014721297658979893\n",
            "step: 380, loss: 0.005018214695155621\n",
            "step: 390, loss: 0.0002803526585921645\n",
            "step: 400, loss: 0.0016903612995520234\n",
            "step: 410, loss: 0.0018310000887140632\n",
            "step: 420, loss: 0.0012355716899037361\n",
            "step: 430, loss: 0.0010239268885925412\n",
            "step: 440, loss: 0.0001121004024753347\n",
            "step: 450, loss: 0.0002918616228271276\n",
            "step: 460, loss: 0.0010045368690043688\n",
            "step: 470, loss: 0.0013465478550642729\n",
            "step: 480, loss: 0.0005206966889090836\n",
            "step: 490, loss: 0.0010589949088171124\n",
            "step: 500, loss: 0.00704084150493145\n",
            "step: 510, loss: 0.000405426457291469\n",
            "step: 520, loss: 0.0731925442814827\n",
            "step: 530, loss: 0.0012123910710215569\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9486348912540491, f1=0.9363086936308694, best_f1=0.9362092703074805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003322848060633987\n",
            "step: 10, loss: 6.186791870277375e-05\n",
            "step: 20, loss: 0.002459537470713258\n",
            "step: 30, loss: 0.003253024537116289\n",
            "step: 40, loss: 0.00024765764828771353\n",
            "step: 50, loss: 0.0006245410768315196\n",
            "step: 60, loss: 0.0020902405958622694\n",
            "step: 70, loss: 0.0005046877195127308\n",
            "step: 80, loss: 0.001182050909847021\n",
            "step: 90, loss: 0.0046755229122936726\n",
            "step: 100, loss: 0.0036526776384562254\n",
            "step: 110, loss: 0.0011826095869764686\n",
            "step: 120, loss: 0.0005640657036565244\n",
            "step: 130, loss: 0.0010164575651288033\n",
            "step: 140, loss: 0.00013457766908686608\n",
            "step: 150, loss: 0.0022431646939367056\n",
            "step: 160, loss: 0.001478402642533183\n",
            "step: 170, loss: 0.025632472708821297\n",
            "step: 180, loss: 4.249401536071673e-05\n",
            "step: 190, loss: 0.0007028415566310287\n",
            "step: 200, loss: 0.0016323686577379704\n",
            "step: 210, loss: 0.00041493753087706864\n",
            "step: 220, loss: 0.000784456089604646\n",
            "step: 230, loss: 0.0003903216274920851\n",
            "step: 240, loss: 0.0023136772215366364\n",
            "step: 250, loss: 1.0736123840615619e-05\n",
            "step: 260, loss: 0.0015577003359794617\n",
            "step: 270, loss: 0.003952522296458483\n",
            "step: 280, loss: 0.029112134128808975\n",
            "step: 290, loss: 0.002114493167027831\n",
            "step: 300, loss: 0.00014559787814505398\n",
            "step: 310, loss: 0.0026044233236461878\n",
            "step: 320, loss: 0.0004557106294669211\n",
            "step: 330, loss: 1.0393474440206774e-05\n",
            "step: 340, loss: 0.0029786594677716494\n",
            "step: 350, loss: 0.0027704332023859024\n",
            "step: 360, loss: 0.06981941312551498\n",
            "step: 370, loss: 0.00033153328695334494\n",
            "step: 380, loss: 0.0014335284940898418\n",
            "step: 390, loss: 0.003069564001634717\n",
            "step: 400, loss: 2.5773586457944475e-05\n",
            "step: 410, loss: 0.0006621085340157151\n",
            "step: 420, loss: 5.079658512840979e-05\n",
            "step: 430, loss: 0.007336802314966917\n",
            "step: 440, loss: 7.120834197849035e-05\n",
            "step: 450, loss: 0.0007781851454637945\n",
            "step: 460, loss: 0.01638297736644745\n",
            "step: 470, loss: 0.0007420505280606449\n",
            "step: 480, loss: 0.00021772057516500354\n",
            "step: 490, loss: 0.00011347110557835549\n",
            "step: 500, loss: 0.0007420906331390142\n",
            "step: 510, loss: 0.0012199210468679667\n",
            "step: 520, loss: 0.00013503464288078249\n",
            "step: 530, loss: 0.12568077445030212\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.942095588235294, f1=0.9308924485125859, best_f1=0.9362092703074805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004281776200514287\n",
            "step: 10, loss: 0.0012871181825175881\n",
            "step: 20, loss: 0.002552968682721257\n",
            "step: 30, loss: 0.00013289068010635674\n",
            "step: 40, loss: 0.0016085106180980802\n",
            "step: 50, loss: 0.02164877951145172\n",
            "step: 60, loss: 0.0004868795513175428\n",
            "step: 70, loss: 0.0024702518712729216\n",
            "step: 80, loss: 0.00020770962873939425\n",
            "step: 90, loss: 0.0058720228262245655\n",
            "step: 100, loss: 0.005314957350492477\n",
            "step: 110, loss: 0.00021866150200366974\n",
            "step: 120, loss: 0.00015647034160792828\n",
            "step: 130, loss: 0.002647171728312969\n",
            "step: 140, loss: 0.09045674651861191\n",
            "step: 150, loss: 3.391972495592199e-05\n",
            "step: 160, loss: 0.0006803278811275959\n",
            "step: 170, loss: 4.878800245933235e-05\n",
            "step: 180, loss: 0.0035886107943952084\n",
            "step: 190, loss: 0.0005172966048121452\n",
            "step: 200, loss: 0.000495883054099977\n",
            "step: 210, loss: 0.00046491759712807834\n",
            "step: 220, loss: 0.0004337886057328433\n",
            "step: 230, loss: 0.006979544181376696\n",
            "step: 240, loss: 0.0008853835752233863\n",
            "step: 250, loss: 0.11993085592985153\n",
            "step: 260, loss: 0.0008535758242942393\n",
            "step: 270, loss: 0.0013048246037214994\n",
            "step: 280, loss: 0.001876886817626655\n",
            "step: 290, loss: 0.0036172522231936455\n",
            "step: 300, loss: 0.0017959829419851303\n",
            "step: 310, loss: 0.0069994074292480946\n",
            "step: 320, loss: 0.03297688439488411\n",
            "step: 330, loss: 0.0031590168364346027\n",
            "step: 340, loss: 0.02382401004433632\n",
            "step: 350, loss: 0.00011157608969369903\n",
            "step: 360, loss: 0.001401923829689622\n",
            "step: 370, loss: 0.0009576893644407392\n",
            "step: 380, loss: 0.00027028092881664634\n",
            "step: 390, loss: 0.009178532287478447\n",
            "step: 400, loss: 0.0001351269893348217\n",
            "step: 410, loss: 0.0004968939465470612\n",
            "step: 420, loss: 0.003189110429957509\n",
            "step: 430, loss: 0.0020538067910820246\n",
            "step: 440, loss: 0.003033652435988188\n",
            "step: 450, loss: 0.0010519670322537422\n",
            "step: 460, loss: 0.0008796390029601753\n",
            "step: 470, loss: 0.0034704485442489386\n",
            "step: 480, loss: 0.013085935264825821\n",
            "step: 490, loss: 0.0008689548121765256\n",
            "step: 500, loss: 0.00033526282641105354\n",
            "step: 510, loss: 0.003132549347355962\n",
            "step: 520, loss: 0.000151230618939735\n",
            "step: 530, loss: 0.00018578083836473525\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9486461251167133, f1=0.9377323420074348, best_f1=0.9362092703074805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001133263693191111\n",
            "step: 10, loss: 0.0006768024177290499\n",
            "step: 20, loss: 0.0014872490428388119\n",
            "step: 30, loss: 1.152961249317741e-05\n",
            "step: 40, loss: 0.037663884460926056\n",
            "step: 50, loss: 0.00127887143753469\n",
            "step: 60, loss: 0.00041768778464756906\n",
            "step: 70, loss: 0.054923467338085175\n",
            "step: 80, loss: 0.003084995551034808\n",
            "step: 90, loss: 0.00036112265661358833\n",
            "step: 100, loss: 0.02092961221933365\n",
            "step: 110, loss: 0.0009852222865447402\n",
            "step: 120, loss: 0.00014164713502395898\n",
            "step: 130, loss: 0.0012103498447686434\n",
            "step: 140, loss: 0.0008583373273722827\n",
            "step: 150, loss: 0.01828853040933609\n",
            "step: 160, loss: 0.002564270282164216\n",
            "step: 170, loss: 0.1384160965681076\n",
            "step: 180, loss: 0.0023322284687310457\n",
            "step: 190, loss: 0.00010678703984012827\n",
            "step: 200, loss: 0.0051544844172894955\n",
            "step: 210, loss: 3.652410305221565e-05\n",
            "step: 220, loss: 0.0008095118100754917\n",
            "step: 230, loss: 0.002048541558906436\n",
            "step: 240, loss: 0.0024416563101112843\n",
            "step: 250, loss: 0.0012787358136847615\n",
            "step: 260, loss: 0.001039651921018958\n",
            "step: 270, loss: 0.11217743903398514\n",
            "step: 280, loss: 0.000779110356234014\n",
            "step: 290, loss: 0.0007774113910272717\n",
            "step: 300, loss: 0.0012248557759448886\n",
            "step: 310, loss: 0.0008499794639647007\n",
            "step: 320, loss: 0.00274123577401042\n",
            "step: 330, loss: 8.228297519963235e-05\n",
            "step: 340, loss: 0.0022049483377486467\n",
            "step: 350, loss: 0.0004926439141854644\n",
            "step: 360, loss: 0.09814946353435516\n",
            "step: 370, loss: 0.007040213327854872\n",
            "step: 380, loss: 0.0005395157495513558\n",
            "step: 390, loss: 0.006503280717879534\n",
            "step: 400, loss: 0.0013353020185604692\n",
            "step: 410, loss: 0.00017438259965274483\n",
            "step: 420, loss: 0.0011005975538864732\n",
            "step: 430, loss: 0.0007130011217668653\n",
            "step: 440, loss: 0.00016306013276334852\n",
            "step: 450, loss: 0.000571548705920577\n",
            "step: 460, loss: 0.0009994667489081621\n",
            "step: 470, loss: 0.004137189593166113\n",
            "step: 480, loss: 0.0003222355153411627\n",
            "step: 490, loss: 0.00011902500409632921\n",
            "step: 500, loss: 0.0001747835922287777\n",
            "step: 510, loss: 0.000722161668818444\n",
            "step: 520, loss: 0.0001924919633893296\n",
            "step: 530, loss: 0.0002198194561060518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9528698086794214, f1=0.9402501157943491, best_f1=0.9402501157943491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007204197463579476\n",
            "step: 10, loss: 0.0016325905453413725\n",
            "step: 20, loss: 0.00047163182171061635\n",
            "step: 30, loss: 0.006388308014720678\n",
            "step: 40, loss: 0.00018245608953293413\n",
            "step: 50, loss: 0.00518425926566124\n",
            "step: 60, loss: 0.0008734029834158719\n",
            "step: 70, loss: 0.0005567888729274273\n",
            "step: 80, loss: 0.0004138282092753798\n",
            "step: 90, loss: 0.00012637345935218036\n",
            "step: 100, loss: 0.00037040296592749655\n",
            "step: 110, loss: 0.0009235473116859794\n",
            "step: 120, loss: 7.362765609286726e-05\n",
            "step: 130, loss: 7.627743616467342e-05\n",
            "step: 140, loss: 0.0297746192663908\n",
            "step: 150, loss: 0.00017317431047558784\n",
            "step: 160, loss: 0.00014273193664848804\n",
            "step: 170, loss: 0.0003037089772988111\n",
            "step: 180, loss: 0.000250600918661803\n",
            "step: 190, loss: 0.002222080249339342\n",
            "step: 200, loss: 6.129437679192051e-05\n",
            "step: 210, loss: 0.00044026263640262187\n",
            "step: 220, loss: 5.467875234899111e-05\n",
            "step: 230, loss: 0.00037191997398622334\n",
            "step: 240, loss: 0.0010174415074288845\n",
            "step: 250, loss: 0.05283748358488083\n",
            "step: 260, loss: 0.16557785868644714\n",
            "step: 270, loss: 0.0002899138198699802\n",
            "step: 280, loss: 0.0003960324975196272\n",
            "step: 290, loss: 0.000303042063023895\n",
            "step: 300, loss: 0.0003493853728286922\n",
            "step: 310, loss: 0.020659197121858597\n",
            "step: 320, loss: 0.00018148224626202136\n",
            "step: 330, loss: 0.0012499872827902436\n",
            "step: 340, loss: 0.00039357724017463624\n",
            "step: 350, loss: 6.707790453219786e-05\n",
            "step: 360, loss: 7.31443942640908e-05\n",
            "step: 370, loss: 0.009429420344531536\n",
            "step: 380, loss: 0.0022511433344334364\n",
            "step: 390, loss: 0.0011115969391539693\n",
            "step: 400, loss: 0.004819415509700775\n",
            "step: 410, loss: 0.00020915466302540153\n",
            "step: 420, loss: 0.04605463147163391\n",
            "step: 430, loss: 0.000832880032248795\n",
            "step: 440, loss: 0.006620470900088549\n",
            "step: 450, loss: 0.0013910734560340643\n",
            "step: 460, loss: 0.005519534461200237\n",
            "step: 470, loss: 7.854776049498469e-05\n",
            "step: 480, loss: 0.0015158277237787843\n",
            "step: 490, loss: 0.0029906879644840956\n",
            "step: 500, loss: 0.00870178360491991\n",
            "step: 510, loss: 0.0011000687954947352\n",
            "step: 520, loss: 0.00027010260964743793\n",
            "step: 530, loss: 0.001754925586283207\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9523368810735771, f1=0.9405803777061263, best_f1=0.9402501157943491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019053411961067468\n",
            "step: 10, loss: 0.0013015023432672024\n",
            "step: 20, loss: 0.002361125312745571\n",
            "step: 30, loss: 0.004930063616484404\n",
            "step: 40, loss: 0.00021033968369010836\n",
            "step: 50, loss: 0.0015587455127388239\n",
            "step: 60, loss: 0.0005365770775824785\n",
            "step: 70, loss: 0.0001267286716029048\n",
            "step: 80, loss: 0.00026221247389912605\n",
            "step: 90, loss: 0.0010297077242285013\n",
            "step: 100, loss: 0.00012599927140399814\n",
            "step: 110, loss: 0.0002062011626549065\n",
            "step: 120, loss: 4.820443791686557e-05\n",
            "step: 130, loss: 0.0002242157352156937\n",
            "step: 140, loss: 0.00012093201075913385\n",
            "step: 150, loss: 0.0001078929562936537\n",
            "step: 160, loss: 0.00017071448382921517\n",
            "step: 170, loss: 7.801932224538177e-05\n",
            "step: 180, loss: 0.00015679745411034673\n",
            "step: 190, loss: 0.0021559300366789103\n",
            "step: 200, loss: 0.0021023331210017204\n",
            "step: 210, loss: 0.00013819652667734772\n",
            "step: 220, loss: 2.7226609745412134e-05\n",
            "step: 230, loss: 0.01881333813071251\n",
            "step: 240, loss: 9.927619248628616e-05\n",
            "step: 250, loss: 0.0030895124655216932\n",
            "step: 260, loss: 4.08029263780918e-05\n",
            "step: 270, loss: 0.0011033324990421534\n",
            "step: 280, loss: 3.366647069924511e-05\n",
            "step: 290, loss: 0.0007265859167091548\n",
            "step: 300, loss: 0.00016691013297531754\n",
            "step: 310, loss: 0.04905768483877182\n",
            "step: 320, loss: 0.00011304783402010798\n",
            "step: 330, loss: 0.0002214450214523822\n",
            "step: 340, loss: 0.00022872912813909352\n",
            "step: 350, loss: 0.0044379751197993755\n",
            "step: 360, loss: 0.00020460969244595617\n",
            "step: 370, loss: 0.015705741941928864\n",
            "step: 380, loss: 0.0004068720154464245\n",
            "step: 390, loss: 4.06275357818231e-05\n",
            "step: 400, loss: 0.004381339997053146\n",
            "step: 410, loss: 0.0003863669408019632\n",
            "step: 420, loss: 0.0013437814777716994\n",
            "step: 430, loss: 0.0001170868199551478\n",
            "step: 440, loss: 0.001018883311189711\n",
            "step: 450, loss: 0.001118990476243198\n",
            "step: 460, loss: 0.0008643961627967656\n",
            "step: 470, loss: 6.778885290259495e-05\n",
            "step: 480, loss: 0.0014984471490606666\n",
            "step: 490, loss: 0.0014247136423364282\n",
            "step: 500, loss: 0.007464051712304354\n",
            "step: 510, loss: 0.00028430449310690165\n",
            "step: 520, loss: 1.6763458916102536e-05\n",
            "step: 530, loss: 0.00024087345809675753\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9506976744186046, f1=0.9397031539888682, best_f1=0.9402501157943491\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:34, 167.61it/s]\n",
            "load_f1 = 0.9534450651769087\n",
            "real_f1 = 0.9496738117427772\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 142.82it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6rVRw-HgNFH"
      },
      "source": [
        "# BASELINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ifpsOJMgNFH"
      },
      "source": [
        "## BASELINE STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE7bDM3EgNFI",
        "outputId": "413bef6a-42ce-44ed-ebe7-2fb014b3f651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.43292754888534546\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3218390804597701, f1=0.29268292682926833, best_f1=0.29268292682926833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.39040783047676086\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.29268292682926833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.410506933927536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.2745098039215686, f1=0.3023255813953489, best_f1=0.29268292682926833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31688961386680603\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.30434782608695654, f1=0.2857142857142857, best_f1=0.29268292682926833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3122955858707428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.34210526315789475, f1=0.3, best_f1=0.3\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24305710196495056\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.2745098039215686, f1=0.35135135135135137, best_f1=0.3\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5130338668823242\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.4, f1=0.48888888888888893, best_f1=0.48888888888888893\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31974244117736816\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.46428571428571436, f1=0.5000000000000001, best_f1=0.5000000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25407281517982483\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.4905660377358491, f1=0.45161290322580644, best_f1=0.45161290322580644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23102864623069763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.4444444444444445, f1=0.5531914893617021, best_f1=0.45161290322580644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24230292439460754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.48148148148148157, f1=0.5945945945945946, best_f1=0.45161290322580644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16386152803897858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.5581395348837208, f1=0.5925925925925927, best_f1=0.5925925925925927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09454485028982162\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.49122807017543857, f1=0.5945945945945946, best_f1=0.5925925925925927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10643554478883743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.49122807017543857, f1=0.5945945945945946, best_f1=0.5925925925925927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16743804514408112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.49122807017543857, f1=0.5945945945945946, best_f1=0.5925925925925927\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 102300.10it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.5306122448979592\n",
            "real_f1 = 0.49122807017543857\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 198.03it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zyjgIIwdgNFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d83b2c92-08b4-4d5c-f437-fe7f7b8281f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.586753249168396\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.44319602847099304\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.47736337780952454\n",
            "step: 30, loss: 0.28039032220840454\n",
            "step: 40, loss: 0.308490514755249\n",
            "step: 50, loss: 0.6247422695159912\n",
            "step: 60, loss: 0.4759730100631714\n",
            "step: 70, loss: 0.4285849928855896\n",
            "step: 80, loss: 0.6099539995193481\n",
            "step: 90, loss: 0.4629555344581604\n",
            "step: 100, loss: 0.4950568675994873\n",
            "step: 110, loss: 0.5852223038673401\n",
            "step: 120, loss: 0.43616223335266113\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 130, loss: 0.1552935540676117\n",
            "step: 140, loss: 0.12030753493309021\n",
            "step: 150, loss: 0.15223829448223114\n",
            "step: 160, loss: 0.047423433512449265\n",
            "step: 170, loss: 0.22106605768203735\n",
            "step: 180, loss: 0.18525108695030212\n",
            "step: 190, loss: 0.2663538157939911\n",
            "step: 200, loss: 0.06445156037807465\n",
            "step: 210, loss: 0.0856584683060646\n",
            "step: 220, loss: 0.15595529973506927\n",
            "step: 230, loss: 0.016394294798374176\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9599999999999999, f1=0.9516483516483516, best_f1=0.9516483516483516\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11499536782503128\n",
            "step: 10, loss: 0.15516774356365204\n",
            "step: 20, loss: 0.08532114326953888\n",
            "step: 30, loss: 0.07934780418872833\n",
            "step: 40, loss: 0.07924312353134155\n",
            "step: 50, loss: 0.021161789074540138\n",
            "step: 60, loss: 0.03149382397532463\n",
            "step: 70, loss: 0.05812068283557892\n",
            "step: 80, loss: 0.01916339062154293\n",
            "step: 90, loss: 0.005978086963295937\n",
            "step: 100, loss: 0.10731802880764008\n",
            "step: 110, loss: 0.02839544042944908\n",
            "step: 120, loss: 0.03897407278418541\n",
            "step: 130, loss: 0.007195227313786745\n",
            "step: 140, loss: 0.0024208843242377043\n",
            "step: 150, loss: 0.06974537670612335\n",
            "step: 160, loss: 0.014492515474557877\n",
            "step: 170, loss: 0.007408624514937401\n",
            "step: 180, loss: 0.011229554191231728\n",
            "step: 190, loss: 0.040388599038124084\n",
            "step: 200, loss: 0.047627631574869156\n",
            "step: 210, loss: 0.017091281712055206\n",
            "step: 220, loss: 0.027370022609829903\n",
            "step: 230, loss: 0.039587512612342834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9819819819819819, f1=0.9832026875699889, best_f1=0.9832026875699889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006346203852444887\n",
            "step: 10, loss: 0.004259107634425163\n",
            "step: 20, loss: 0.0006831747014075518\n",
            "step: 30, loss: 0.0028292040806263685\n",
            "step: 40, loss: 0.0349222794175148\n",
            "step: 50, loss: 0.05848206579685211\n",
            "step: 60, loss: 0.07375676929950714\n",
            "step: 70, loss: 0.016064418479800224\n",
            "step: 80, loss: 0.07032843679189682\n",
            "step: 90, loss: 0.049949366599321365\n",
            "step: 100, loss: 0.0018331978935748339\n",
            "step: 110, loss: 0.0034391360823065042\n",
            "step: 120, loss: 0.003972066566348076\n",
            "step: 130, loss: 0.003287253202870488\n",
            "step: 140, loss: 0.10911202430725098\n",
            "step: 150, loss: 0.045642200857400894\n",
            "step: 160, loss: 0.008168739266693592\n",
            "step: 170, loss: 0.01612565852701664\n",
            "step: 180, loss: 0.01691019907593727\n",
            "step: 190, loss: 0.005774686112999916\n",
            "step: 200, loss: 0.03520599752664566\n",
            "step: 210, loss: 0.004900924861431122\n",
            "step: 220, loss: 0.002684788778424263\n",
            "step: 230, loss: 0.019797202199697495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9819413092550789, f1=0.9820224719101124, best_f1=0.9832026875699889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006988632492721081\n",
            "step: 10, loss: 0.002059153513982892\n",
            "step: 20, loss: 0.0027130786329507828\n",
            "step: 30, loss: 0.0012119847815483809\n",
            "step: 40, loss: 0.041948746889829636\n",
            "step: 50, loss: 0.007926111109554768\n",
            "step: 60, loss: 0.0030039455741643906\n",
            "step: 70, loss: 0.0005891930195502937\n",
            "step: 80, loss: 0.0005448558367788792\n",
            "step: 90, loss: 0.0046300096437335014\n",
            "step: 100, loss: 0.0012843949953094125\n",
            "step: 110, loss: 0.0018033263040706515\n",
            "step: 120, loss: 0.0016789407236501575\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 130, loss: 0.0067617096938192844\n",
            "step: 140, loss: 0.0013453603023663163\n",
            "step: 150, loss: 0.0044273198582232\n",
            "step: 160, loss: 0.0022786634508520365\n",
            "step: 170, loss: 0.023197844624519348\n",
            "step: 180, loss: 0.04959931969642639\n",
            "step: 190, loss: 0.001867721788585186\n",
            "step: 200, loss: 0.009077069349586964\n",
            "step: 210, loss: 0.0006450144574046135\n",
            "step: 220, loss: 0.0007381623727269471\n",
            "step: 230, loss: 0.0036790312733501196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9729119638826186, f1=0.9819819819819819, best_f1=0.9832026875699889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014556723181158304\n",
            "step: 10, loss: 0.005559121258556843\n",
            "step: 20, loss: 0.002768662292510271\n",
            "step: 30, loss: 0.0014734533615410328\n",
            "step: 40, loss: 0.006787394173443317\n",
            "step: 50, loss: 0.0009193720761686563\n",
            "step: 60, loss: 0.0019744650926440954\n",
            "step: 70, loss: 0.001909014885313809\n",
            "step: 80, loss: 0.0253570806235075\n",
            "step: 90, loss: 0.045144639909267426\n",
            "step: 100, loss: 0.000397503754356876\n",
            "step: 110, loss: 0.04591011255979538\n",
            "step: 120, loss: 0.02525591105222702\n",
            "step: 130, loss: 0.0056300382129848\n",
            "step: 140, loss: 0.08927767723798752\n",
            "step: 150, loss: 0.017303461208939552\n",
            "step: 160, loss: 0.06010754778981209\n",
            "step: 170, loss: 0.030545223504304886\n",
            "step: 180, loss: 0.0033828658051788807\n",
            "step: 190, loss: 0.020684154704213142\n",
            "step: 200, loss: 0.14879456162452698\n",
            "step: 210, loss: 0.0040958113968372345\n",
            "step: 220, loss: 0.005162620451301336\n",
            "step: 230, loss: 0.00996147096157074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9887892376681614, f1=0.9843400447427293, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002097065793350339\n",
            "step: 10, loss: 0.011362612247467041\n",
            "step: 20, loss: 0.002638099482282996\n",
            "step: 30, loss: 0.0008520493283867836\n",
            "step: 40, loss: 0.0005083063733763993\n",
            "step: 50, loss: 0.0010841891635209322\n",
            "step: 60, loss: 0.009406808763742447\n",
            "step: 70, loss: 0.0033048910554498434\n",
            "step: 80, loss: 0.001934692612849176\n",
            "step: 90, loss: 0.003365958109498024\n",
            "step: 100, loss: 0.005016183480620384\n",
            "step: 110, loss: 0.0125547731295228\n",
            "step: 120, loss: 0.0021253428421914577\n",
            "step: 130, loss: 0.0014497041702270508\n",
            "step: 140, loss: 0.017377736046910286\n",
            "step: 150, loss: 0.0016359015135094523\n",
            "step: 160, loss: 0.005137543193995953\n",
            "step: 170, loss: 0.0005750560667365789\n",
            "step: 180, loss: 0.004481646232306957\n",
            "step: 190, loss: 0.000363118335371837\n",
            "step: 200, loss: 0.006982982624322176\n",
            "step: 210, loss: 0.0013391831889748573\n",
            "step: 220, loss: 0.02661122940480709\n",
            "step: 230, loss: 0.003904881188645959\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9809203142536477, f1=0.987598647125141, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009939017705619335\n",
            "step: 10, loss: 0.0012901787413284183\n",
            "step: 20, loss: 0.003179608378559351\n",
            "step: 30, loss: 0.0009612789144739509\n",
            "step: 40, loss: 0.003866110695526004\n",
            "step: 50, loss: 0.001911164028570056\n",
            "step: 60, loss: 0.00089325598673895\n",
            "step: 70, loss: 0.00081369758117944\n",
            "step: 80, loss: 0.0009424533927813172\n",
            "step: 90, loss: 0.0017639750149101019\n",
            "step: 100, loss: 0.0035256901755928993\n",
            "step: 110, loss: 0.0010560165392234921\n",
            "step: 120, loss: 0.0009137073066085577\n",
            "step: 130, loss: 0.004562385380268097\n",
            "step: 140, loss: 0.0003913016407750547\n",
            "step: 150, loss: 0.0358000174164772\n",
            "step: 160, loss: 0.0004916333127766848\n",
            "step: 170, loss: 0.020525867119431496\n",
            "step: 180, loss: 0.0005214893026277423\n",
            "step: 190, loss: 0.0006687234272249043\n",
            "step: 200, loss: 0.00039127585478127003\n",
            "step: 210, loss: 0.0894169807434082\n",
            "step: 220, loss: 0.00030757213244214654\n",
            "step: 230, loss: 0.0008944208384491503\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9843400447427293, f1=0.978865406006674, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005870667286217213\n",
            "step: 10, loss: 0.007749693468213081\n",
            "step: 20, loss: 0.001820818753913045\n",
            "step: 30, loss: 0.0009675953770056367\n",
            "step: 40, loss: 0.0027165410574525595\n",
            "step: 50, loss: 0.003432591911405325\n",
            "step: 60, loss: 0.0006794885848648846\n",
            "step: 70, loss: 0.00034601613879203796\n",
            "step: 80, loss: 0.03366189822554588\n",
            "step: 90, loss: 0.002779319416731596\n",
            "step: 100, loss: 0.0004904197994619608\n",
            "step: 110, loss: 0.01410544291138649\n",
            "step: 120, loss: 0.029457291588187218\n",
            "step: 130, loss: 0.0007295217947103083\n",
            "step: 140, loss: 0.0005736316670663655\n",
            "step: 150, loss: 0.04085591807961464\n",
            "step: 160, loss: 0.017355602234601974\n",
            "step: 170, loss: 0.09608807414770126\n",
            "step: 180, loss: 0.0007760972948744893\n",
            "step: 190, loss: 0.0014197430573403835\n",
            "step: 200, loss: 0.016928106546401978\n",
            "step: 210, loss: 0.00574982026591897\n",
            "step: 220, loss: 0.0009611360728740692\n",
            "step: 230, loss: 0.000676620053127408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9854096520763187, f1=0.9821428571428571, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003916827554348856\n",
            "step: 10, loss: 0.0006232610903680325\n",
            "step: 20, loss: 0.00032789798569865525\n",
            "step: 30, loss: 0.00030138788861222565\n",
            "step: 40, loss: 0.00022340496070683002\n",
            "step: 50, loss: 0.0018965299241244793\n",
            "step: 60, loss: 0.00115133635699749\n",
            "step: 70, loss: 0.04241817444562912\n",
            "step: 80, loss: 0.0004625098081305623\n",
            "step: 90, loss: 0.021484248340129852\n",
            "step: 100, loss: 0.0006838358240202069\n",
            "step: 110, loss: 0.0005913063650950789\n",
            "step: 120, loss: 0.007588914595544338\n",
            "step: 130, loss: 0.0010434980504214764\n",
            "step: 140, loss: 0.0006778504466637969\n",
            "step: 150, loss: 0.001448752242140472\n",
            "step: 160, loss: 0.0017855934565886855\n",
            "step: 170, loss: 0.0006351299816742539\n",
            "step: 180, loss: 0.0005479294341057539\n",
            "step: 190, loss: 0.00016115298785734922\n",
            "step: 200, loss: 0.0002491832128725946\n",
            "step: 210, loss: 0.004214686807245016\n",
            "step: 220, loss: 0.0003805299929808825\n",
            "step: 230, loss: 0.00023782291100360453\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9853438556933484, f1=0.9864559819413092, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00038579668034799397\n",
            "step: 10, loss: 0.00045957168913446367\n",
            "step: 20, loss: 0.0003673759929370135\n",
            "step: 30, loss: 0.00021108087094035\n",
            "step: 40, loss: 0.001405875664204359\n",
            "step: 50, loss: 0.0002762764343060553\n",
            "step: 60, loss: 0.0010898851323872805\n",
            "step: 70, loss: 0.013478667475283146\n",
            "step: 80, loss: 0.0010926120448857546\n",
            "step: 90, loss: 0.0004077372723259032\n",
            "step: 100, loss: 0.0002763225056696683\n",
            "step: 110, loss: 0.011329395696520805\n",
            "step: 120, loss: 0.0005162183078937232\n",
            "step: 130, loss: 0.0017602540319785476\n",
            "step: 140, loss: 0.000525005511008203\n",
            "step: 150, loss: 0.035295311361551285\n",
            "step: 160, loss: 0.00015136392903514206\n",
            "step: 170, loss: 0.00021126255160197616\n",
            "step: 180, loss: 0.0006990727270022035\n",
            "step: 190, loss: 0.0003620207717176527\n",
            "step: 200, loss: 0.0009202365763485432\n",
            "step: 210, loss: 0.001093795639462769\n",
            "step: 220, loss: 0.007019623648375273\n",
            "step: 230, loss: 0.0007562923710793257\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9865470852017937, f1=0.9843749999999999, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019646566943265498\n",
            "step: 10, loss: 0.0004531860467977822\n",
            "step: 20, loss: 0.0002882422413676977\n",
            "step: 30, loss: 0.00023266897187568247\n",
            "step: 40, loss: 9.47764128795825e-05\n",
            "step: 50, loss: 0.0002289195981575176\n",
            "step: 60, loss: 0.024686569347977638\n",
            "step: 70, loss: 0.00021411773923318833\n",
            "step: 80, loss: 0.010455723851919174\n",
            "step: 90, loss: 0.16172270476818085\n",
            "step: 100, loss: 0.0007990108570083976\n",
            "step: 110, loss: 0.0016444886568933725\n",
            "step: 120, loss: 0.000561414344701916\n",
            "step: 130, loss: 0.0004192673950456083\n",
            "step: 140, loss: 0.0017787368269637227\n",
            "step: 150, loss: 0.000621231272816658\n",
            "step: 160, loss: 0.002357185585424304\n",
            "step: 170, loss: 0.0009776551742106676\n",
            "step: 180, loss: 0.0012429089983925223\n",
            "step: 190, loss: 0.0005383066600188613\n",
            "step: 200, loss: 0.011234004981815815\n",
            "step: 210, loss: 0.0006148756947368383\n",
            "step: 220, loss: 0.003260847646743059\n",
            "step: 230, loss: 0.0006255318876355886\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9865168539325843, f1=0.9842696629213483, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008709730464033782\n",
            "step: 10, loss: 0.00032719189766794443\n",
            "step: 20, loss: 0.05691900476813316\n",
            "step: 30, loss: 0.032291341572999954\n",
            "step: 40, loss: 0.0010055832099169493\n",
            "step: 50, loss: 0.0016080053756013513\n",
            "step: 60, loss: 0.0025849754456430674\n",
            "step: 70, loss: 0.0004785394121427089\n",
            "step: 80, loss: 0.00018593313870951533\n",
            "step: 90, loss: 0.0009176636813208461\n",
            "step: 100, loss: 0.0002874645870178938\n",
            "step: 110, loss: 0.00026063420227728784\n",
            "step: 120, loss: 0.0003939716552849859\n",
            "step: 130, loss: 0.0019523499067872763\n",
            "step: 140, loss: 0.0013473144499585032\n",
            "step: 150, loss: 0.0005724021466448903\n",
            "step: 160, loss: 0.006254751235246658\n",
            "step: 170, loss: 0.0037308968603610992\n",
            "step: 180, loss: 0.0003917767317034304\n",
            "step: 190, loss: 0.001025454606860876\n",
            "step: 200, loss: 0.0003472002281341702\n",
            "step: 210, loss: 0.0005158110288903117\n",
            "step: 220, loss: 0.017606940120458603\n",
            "step: 230, loss: 0.0008720502373762429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9843400447427293, f1=0.9799554565701558, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000701434793882072\n",
            "step: 10, loss: 0.0004186210862826556\n",
            "step: 20, loss: 0.0006523177726194263\n",
            "step: 30, loss: 0.0006763769197277725\n",
            "step: 40, loss: 0.0008963198633864522\n",
            "step: 50, loss: 0.0049469382502138615\n",
            "step: 60, loss: 0.0007401625625789165\n",
            "step: 70, loss: 0.0042539979331195354\n",
            "step: 80, loss: 0.0004345816560089588\n",
            "step: 90, loss: 0.0006378066609613597\n",
            "step: 100, loss: 0.0010614031925797462\n",
            "step: 110, loss: 0.004266309551894665\n",
            "step: 120, loss: 0.0013865578221157193\n",
            "step: 130, loss: 0.0010971418814733624\n",
            "step: 140, loss: 0.0005031466716900468\n",
            "step: 150, loss: 0.000245417671976611\n",
            "step: 160, loss: 0.01782395876944065\n",
            "step: 170, loss: 0.0007763077737763524\n",
            "step: 180, loss: 0.037020500749349594\n",
            "step: 190, loss: 0.0006963036139495671\n",
            "step: 200, loss: 0.00011961498239543289\n",
            "step: 210, loss: 0.0018034297972917557\n",
            "step: 220, loss: 0.0006948407390154898\n",
            "step: 230, loss: 0.0011198497377336025\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9841628959276018, f1=0.9841269841269841, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004209191829431802\n",
            "step: 10, loss: 0.0003107877855654806\n",
            "step: 20, loss: 0.0005365196266211569\n",
            "step: 30, loss: 0.0008255590219050646\n",
            "step: 40, loss: 0.0007253524381667376\n",
            "step: 50, loss: 0.0003061715979129076\n",
            "step: 60, loss: 0.0005073355278000236\n",
            "step: 70, loss: 0.0003672015736810863\n",
            "step: 80, loss: 0.00026968243764713407\n",
            "step: 90, loss: 0.0007956717163324356\n",
            "step: 100, loss: 0.00046911006211303174\n",
            "step: 110, loss: 0.0006662526866421103\n",
            "step: 120, loss: 0.00010424415813758969\n",
            "step: 130, loss: 0.003038732334971428\n",
            "step: 140, loss: 0.0003161401255056262\n",
            "step: 150, loss: 0.0012472722446545959\n",
            "step: 160, loss: 0.0004336550773587078\n",
            "step: 170, loss: 0.0004572780744638294\n",
            "step: 180, loss: 0.00036981733865104616\n",
            "step: 190, loss: 0.0004762596799992025\n",
            "step: 200, loss: 0.0007330402149818838\n",
            "step: 210, loss: 0.0003146314120385796\n",
            "step: 220, loss: 0.00039002197445370257\n",
            "step: 230, loss: 0.00020191411022096872\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9853438556933484, f1=0.9864559819413092, best_f1=0.9843400447427293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009853074327111244\n",
            "step: 10, loss: 0.00035203713923692703\n",
            "step: 20, loss: 0.0006280466332100332\n",
            "step: 30, loss: 0.0004113032773602754\n",
            "step: 40, loss: 0.00018311066378373653\n",
            "step: 50, loss: 0.0002810250734910369\n",
            "step: 60, loss: 0.04588646814227104\n",
            "step: 70, loss: 0.0003477951686363667\n",
            "step: 80, loss: 0.0002642220351845026\n",
            "step: 90, loss: 0.0002297472528880462\n",
            "step: 100, loss: 0.00018833251670002937\n",
            "step: 110, loss: 0.0021046858746558428\n",
            "step: 120, loss: 0.010620050132274628\n",
            "step: 130, loss: 0.0002602076274342835\n",
            "step: 140, loss: 0.022067654877901077\n",
            "step: 150, loss: 0.0004991025198251009\n",
            "step: 160, loss: 0.0020684755872935057\n",
            "step: 170, loss: 0.0001805843785405159\n",
            "step: 180, loss: 0.00037725025322288275\n",
            "step: 190, loss: 0.0006360136321745813\n",
            "step: 200, loss: 0.003776922356337309\n",
            "step: 210, loss: 0.0580689013004303\n",
            "step: 220, loss: 0.00033917438122443855\n",
            "step: 230, loss: 0.0006079875165596604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9842696629213483, f1=0.9842696629213483, best_f1=0.9843400447427293\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:13, 181.05it/s]\n",
            "load_f1 = 0.9898989898989898\n",
            "real_f1 = 0.9876819708846584\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:25, 169.31it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck7uL6uPgNFK"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyyxG2qpgNFL",
        "outputId": "04090fbc-ab8b-4634-f00d-87af6a194a96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6410531997680664\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4776136577129364\n",
            "step: 20, loss: 0.26390767097473145\n",
            "step: 30, loss: 0.29800736904144287\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 40, loss: 0.39198365807533264\n",
            "step: 50, loss: 0.6672455668449402\n",
            "step: 60, loss: 0.2911683917045593\n",
            "step: 70, loss: 0.2229628562927246\n",
            "step: 80, loss: 0.2172715961933136\n",
            "step: 90, loss: 0.16999918222427368\n",
            "step: 100, loss: 0.2055845409631729\n",
            "step: 110, loss: 0.1924009472131729\n",
            "step: 120, loss: 0.21217626333236694\n",
            "step: 130, loss: 0.15577779710292816\n",
            "step: 140, loss: 0.25622624158859253\n",
            "step: 150, loss: 0.11807439476251602\n",
            "step: 160, loss: 0.4009971022605896\n",
            "step: 170, loss: 0.12830616533756256\n",
            "step: 180, loss: 0.1280282437801361\n",
            "step: 190, loss: 0.13601218163967133\n",
            "step: 200, loss: 0.06224867329001427\n",
            "step: 210, loss: 0.0461321659386158\n",
            "step: 220, loss: 0.17034733295440674\n",
            "step: 230, loss: 0.23639024794101715\n",
            "step: 240, loss: 0.055169977247714996\n",
            "step: 250, loss: 0.02804027684032917\n",
            "step: 260, loss: 0.22923457622528076\n",
            "step: 270, loss: 0.2821701467037201\n",
            "step: 280, loss: 0.08747202157974243\n",
            "step: 290, loss: 0.11123954504728317\n",
            "step: 300, loss: 0.12010218948125839\n",
            "step: 310, loss: 0.19079291820526123\n",
            "step: 320, loss: 0.08394522219896317\n",
            "step: 330, loss: 0.07274119555950165\n",
            "step: 340, loss: 0.3063580393791199\n",
            "step: 350, loss: 0.16999219357967377\n",
            "step: 360, loss: 0.06835293024778366\n",
            "step: 370, loss: 0.05375988036394119\n",
            "step: 380, loss: 0.1635858714580536\n",
            "step: 390, loss: 0.01929938793182373\n",
            "step: 400, loss: 0.03317749500274658\n",
            "step: 410, loss: 0.2749432921409607\n",
            "step: 420, loss: 0.0164693184196949\n",
            "step: 430, loss: 0.10367023199796677\n",
            "step: 440, loss: 0.019558696076273918\n",
            "step: 450, loss: 0.020161224529147148\n",
            "step: 460, loss: 0.0588124580681324\n",
            "step: 470, loss: 0.030861668288707733\n",
            "step: 480, loss: 0.06872591376304626\n",
            "step: 490, loss: 0.20415374636650085\n",
            "step: 500, loss: 0.08535058796405792\n",
            "step: 510, loss: 0.10379473865032196\n",
            "step: 520, loss: 0.29112255573272705\n",
            "step: 530, loss: 0.11866450309753418\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9235697940503432, f1=0.9297052154195011, best_f1=0.9297052154195011\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08339633792638779\n",
            "step: 10, loss: 0.05968375876545906\n",
            "step: 20, loss: 0.10230959951877594\n",
            "step: 30, loss: 0.13660265505313873\n",
            "step: 40, loss: 0.07264135032892227\n",
            "step: 50, loss: 0.04662466421723366\n",
            "step: 60, loss: 0.02835194021463394\n",
            "step: 70, loss: 0.014513533562421799\n",
            "step: 80, loss: 0.09690044075250626\n",
            "step: 90, loss: 0.03962469473481178\n",
            "step: 100, loss: 0.23560646176338196\n",
            "step: 110, loss: 0.010598687455058098\n",
            "step: 120, loss: 0.12671972811222076\n",
            "step: 130, loss: 0.007932194508612156\n",
            "step: 140, loss: 0.08065558224916458\n",
            "step: 150, loss: 0.03310929983854294\n",
            "step: 160, loss: 0.05847505107522011\n",
            "step: 170, loss: 0.02529807947576046\n",
            "step: 180, loss: 0.019852224737405777\n",
            "step: 190, loss: 0.03477676212787628\n",
            "step: 200, loss: 0.2749656140804291\n",
            "step: 210, loss: 0.05413079634308815\n",
            "step: 220, loss: 0.001040064264088869\n",
            "step: 230, loss: 0.05928930640220642\n",
            "step: 240, loss: 0.10719120502471924\n",
            "step: 250, loss: 0.017486874014139175\n",
            "step: 260, loss: 0.06602494418621063\n",
            "step: 270, loss: 0.10236301273107529\n",
            "step: 280, loss: 0.1085863932967186\n",
            "step: 290, loss: 0.03261319175362587\n",
            "step: 300, loss: 0.046039335429668427\n",
            "step: 310, loss: 0.016269929707050323\n",
            "step: 320, loss: 0.12079708278179169\n",
            "step: 330, loss: 0.11168783158063889\n",
            "step: 340, loss: 0.07715918868780136\n",
            "step: 350, loss: 0.012792833149433136\n",
            "step: 360, loss: 0.1990956813097\n",
            "step: 370, loss: 0.05133889243006706\n",
            "step: 380, loss: 0.1117672398686409\n",
            "step: 390, loss: 0.009233994409441948\n",
            "step: 400, loss: 0.016474947333335876\n",
            "step: 410, loss: 0.06790303438901901\n",
            "step: 420, loss: 0.13102051615715027\n",
            "step: 430, loss: 0.1559874713420868\n",
            "step: 440, loss: 0.007072414271533489\n",
            "step: 450, loss: 0.022861870005726814\n",
            "step: 460, loss: 0.0318131186068058\n",
            "step: 470, loss: 0.010816162452101707\n",
            "step: 480, loss: 0.010625351220369339\n",
            "step: 490, loss: 0.03774571418762207\n",
            "step: 500, loss: 0.008836745284497738\n",
            "step: 510, loss: 0.03440868854522705\n",
            "step: 520, loss: 0.33735471963882446\n",
            "step: 530, loss: 0.08717866241931915\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9462465245597776, f1=0.9353187529083293, best_f1=0.9353187529083293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09209319204092026\n",
            "step: 10, loss: 0.017274197190999985\n",
            "step: 20, loss: 0.01068224385380745\n",
            "step: 30, loss: 0.06235218793153763\n",
            "step: 40, loss: 0.18094991147518158\n",
            "step: 50, loss: 0.02022162638604641\n",
            "step: 60, loss: 0.04241672530770302\n",
            "step: 70, loss: 0.007163624744862318\n",
            "step: 80, loss: 0.021646317094564438\n",
            "step: 90, loss: 0.015574599616229534\n",
            "step: 100, loss: 0.060107726603746414\n",
            "step: 110, loss: 0.06294839084148407\n",
            "step: 120, loss: 0.0859876498579979\n",
            "step: 130, loss: 0.06020328029990196\n",
            "step: 140, loss: 0.03717082366347313\n",
            "step: 150, loss: 0.04842095077037811\n",
            "step: 160, loss: 0.01715218462049961\n",
            "step: 170, loss: 0.01352704782038927\n",
            "step: 180, loss: 0.019000427797436714\n",
            "step: 190, loss: 0.004521268419921398\n",
            "step: 200, loss: 0.016655217856168747\n",
            "step: 210, loss: 0.023529469966888428\n",
            "step: 220, loss: 0.12713059782981873\n",
            "step: 230, loss: 0.02072206884622574\n",
            "step: 240, loss: 0.028373094275593758\n",
            "step: 250, loss: 0.025600697845220566\n",
            "step: 260, loss: 0.026022138074040413\n",
            "step: 270, loss: 0.006161956116557121\n",
            "step: 280, loss: 0.012848054990172386\n",
            "step: 290, loss: 0.02092590741813183\n",
            "step: 300, loss: 0.0907859206199646\n",
            "step: 310, loss: 0.09211745113134384\n",
            "step: 320, loss: 0.10318563878536224\n",
            "step: 330, loss: 0.027570553123950958\n",
            "step: 340, loss: 0.013185176998376846\n",
            "step: 350, loss: 0.2533947825431824\n",
            "step: 360, loss: 0.008996625430881977\n",
            "step: 370, loss: 0.05375028774142265\n",
            "step: 380, loss: 0.004175447393208742\n",
            "step: 390, loss: 0.008263993076980114\n",
            "step: 400, loss: 0.1646547019481659\n",
            "step: 410, loss: 0.06676676869392395\n",
            "step: 420, loss: 0.00578319514170289\n",
            "step: 430, loss: 0.024474119767546654\n",
            "step: 440, loss: 0.19315940141677856\n",
            "step: 450, loss: 0.03740796819329262\n",
            "step: 460, loss: 0.06500599533319473\n",
            "step: 470, loss: 0.0501403771340847\n",
            "step: 480, loss: 0.12704530358314514\n",
            "step: 490, loss: 0.007535808254033327\n",
            "step: 500, loss: 0.031483087688684464\n",
            "step: 510, loss: 0.05542471259832382\n",
            "step: 520, loss: 0.005671173334121704\n",
            "step: 530, loss: 0.004341010935604572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9391385767790261, f1=0.9302325581395349, best_f1=0.9353187529083293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03144469112157822\n",
            "step: 10, loss: 0.014738958328962326\n",
            "step: 20, loss: 0.015202676877379417\n",
            "step: 30, loss: 0.08687100559473038\n",
            "step: 40, loss: 0.12113774567842484\n",
            "step: 50, loss: 0.019389964640140533\n",
            "step: 60, loss: 0.09045295417308807\n",
            "step: 70, loss: 0.011065327562391758\n",
            "step: 80, loss: 0.014755699783563614\n",
            "step: 90, loss: 0.03244214132428169\n",
            "step: 100, loss: 0.0018442010041326284\n",
            "step: 110, loss: 0.13697443902492523\n",
            "step: 120, loss: 0.012218422256410122\n",
            "step: 130, loss: 0.04366119205951691\n",
            "step: 140, loss: 0.07468177378177643\n",
            "step: 150, loss: 0.053900569677352905\n",
            "step: 160, loss: 0.023902764543890953\n",
            "step: 170, loss: 0.015669802203774452\n",
            "step: 180, loss: 0.07956714928150177\n",
            "step: 190, loss: 0.04144616797566414\n",
            "step: 200, loss: 0.1299707144498825\n",
            "step: 210, loss: 0.007065576501190662\n",
            "step: 220, loss: 0.005938849877566099\n",
            "step: 230, loss: 0.020952114835381508\n",
            "step: 240, loss: 0.024373751133680344\n",
            "step: 250, loss: 0.15107274055480957\n",
            "step: 260, loss: 0.002606517868116498\n",
            "step: 270, loss: 0.016365524381399155\n",
            "step: 280, loss: 0.012526712380349636\n",
            "step: 290, loss: 0.008402934297919273\n",
            "step: 300, loss: 0.006442543584853411\n",
            "step: 310, loss: 0.008234122768044472\n",
            "step: 320, loss: 0.07863326370716095\n",
            "step: 330, loss: 0.0028229288291186094\n",
            "step: 340, loss: 0.021118782460689545\n",
            "step: 350, loss: 0.0640597864985466\n",
            "step: 360, loss: 0.10615239292383194\n",
            "step: 370, loss: 0.019666511565446854\n",
            "step: 380, loss: 0.0038948776200413704\n",
            "step: 390, loss: 0.0004325365589465946\n",
            "step: 400, loss: 0.04218219220638275\n",
            "step: 410, loss: 0.09551180899143219\n",
            "step: 420, loss: 0.02762690745294094\n",
            "step: 430, loss: 0.004266723524779081\n",
            "step: 440, loss: 0.005499198101460934\n",
            "step: 450, loss: 0.03278771787881851\n",
            "step: 460, loss: 0.07155811041593552\n",
            "step: 470, loss: 0.007631118409335613\n",
            "step: 480, loss: 0.007592648733407259\n",
            "step: 490, loss: 0.0041082194074988365\n",
            "step: 500, loss: 0.09917983412742615\n",
            "step: 510, loss: 0.14943791925907135\n",
            "step: 520, loss: 0.01117826346307993\n",
            "step: 530, loss: 0.04531993716955185\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9535315985130112, f1=0.946931241347485, best_f1=0.946931241347485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013292822986841202\n",
            "step: 10, loss: 0.004019508603960276\n",
            "step: 20, loss: 0.020382627844810486\n",
            "step: 30, loss: 0.0010612716432660818\n",
            "step: 40, loss: 0.00033001016709022224\n",
            "step: 50, loss: 0.006524748634546995\n",
            "step: 60, loss: 0.07414278388023376\n",
            "step: 70, loss: 0.001719295629300177\n",
            "step: 80, loss: 0.0013729048660025\n",
            "step: 90, loss: 0.07866004854440689\n",
            "step: 100, loss: 0.01667735166847706\n",
            "step: 110, loss: 0.002428903942927718\n",
            "step: 120, loss: 0.11834133416414261\n",
            "step: 130, loss: 0.017191771417856216\n",
            "step: 140, loss: 0.05734727904200554\n",
            "step: 150, loss: 0.006868541706353426\n",
            "step: 160, loss: 0.0508195199072361\n",
            "step: 170, loss: 0.12445145845413208\n",
            "step: 180, loss: 0.0012108412338420749\n",
            "step: 190, loss: 0.0017741962801665068\n",
            "step: 200, loss: 0.03840727359056473\n",
            "step: 210, loss: 0.008858778513967991\n",
            "step: 220, loss: 0.001235826057381928\n",
            "step: 230, loss: 0.0003294306807219982\n",
            "step: 240, loss: 0.00180122721940279\n",
            "step: 250, loss: 0.12311425060033798\n",
            "step: 260, loss: 0.004449455067515373\n",
            "step: 270, loss: 0.002796979621052742\n",
            "step: 280, loss: 0.002003470668569207\n",
            "step: 290, loss: 0.005191014613956213\n",
            "step: 300, loss: 0.04837912693619728\n",
            "step: 310, loss: 0.05785493925213814\n",
            "step: 320, loss: 0.04632418230175972\n",
            "step: 330, loss: 0.0034395663533359766\n",
            "step: 340, loss: 0.01834956742823124\n",
            "step: 350, loss: 0.0019098955672234297\n",
            "step: 360, loss: 0.0007922331569716334\n",
            "step: 370, loss: 0.0005338834598660469\n",
            "step: 380, loss: 0.0005993717932142317\n",
            "step: 390, loss: 0.0015810249606147408\n",
            "step: 400, loss: 0.009507509879767895\n",
            "step: 410, loss: 0.07603033632040024\n",
            "step: 420, loss: 0.18041299283504486\n",
            "step: 430, loss: 0.01426493190228939\n",
            "step: 440, loss: 0.0006612301804125309\n",
            "step: 450, loss: 0.04647579789161682\n",
            "step: 460, loss: 0.006308317184448242\n",
            "step: 470, loss: 0.015966424718499184\n",
            "step: 480, loss: 0.015175262466073036\n",
            "step: 490, loss: 0.007749305572360754\n",
            "step: 500, loss: 0.038333021104335785\n",
            "step: 510, loss: 0.000994976027868688\n",
            "step: 520, loss: 0.06855059415102005\n",
            "step: 530, loss: 0.010545575991272926\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9514837494112105, f1=0.9418989135569201, best_f1=0.946931241347485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005135306157171726\n",
            "step: 10, loss: 0.0056302654556930065\n",
            "step: 20, loss: 0.0005994649836793542\n",
            "step: 30, loss: 0.007434887811541557\n",
            "step: 40, loss: 0.0012814842630177736\n",
            "step: 50, loss: 0.0017401383956894279\n",
            "step: 60, loss: 0.016465837135910988\n",
            "step: 70, loss: 0.00022496251040138304\n",
            "step: 80, loss: 0.000768747937399894\n",
            "step: 90, loss: 0.001991430763155222\n",
            "step: 100, loss: 0.02064567618072033\n",
            "step: 110, loss: 0.0005356360925361514\n",
            "step: 120, loss: 0.03412289172410965\n",
            "step: 130, loss: 0.004415005445480347\n",
            "step: 140, loss: 0.002313935663551092\n",
            "step: 150, loss: 0.0033353015314787626\n",
            "step: 160, loss: 0.023719586431980133\n",
            "step: 170, loss: 0.012046893127262592\n",
            "step: 180, loss: 0.0006750017055310309\n",
            "step: 190, loss: 0.23575294017791748\n",
            "step: 200, loss: 0.009136577136814594\n",
            "step: 210, loss: 0.0036794908810406923\n",
            "step: 220, loss: 0.012153894640505314\n",
            "step: 230, loss: 0.01661282777786255\n",
            "step: 240, loss: 0.010497933253645897\n",
            "step: 250, loss: 0.01566224731504917\n",
            "step: 260, loss: 0.0005125055904500186\n",
            "step: 270, loss: 0.0004848510434385389\n",
            "step: 280, loss: 0.010877853259444237\n",
            "step: 290, loss: 0.0005404396797530353\n",
            "step: 300, loss: 0.0022665916476398706\n",
            "step: 310, loss: 0.01813535764813423\n",
            "step: 320, loss: 0.00350522855296731\n",
            "step: 330, loss: 0.0012603738578036427\n",
            "step: 340, loss: 0.0019954333547502756\n",
            "step: 350, loss: 0.0022316258400678635\n",
            "step: 360, loss: 0.07763303816318512\n",
            "step: 370, loss: 0.0023217611014842987\n",
            "step: 380, loss: 0.0004379400343168527\n",
            "step: 390, loss: 0.0007171115721575916\n",
            "step: 400, loss: 0.012197955511510372\n",
            "step: 410, loss: 0.002341781510040164\n",
            "step: 420, loss: 0.06434961408376694\n",
            "step: 430, loss: 0.00015295537014026195\n",
            "step: 440, loss: 0.00016237373347394168\n",
            "step: 450, loss: 0.24293455481529236\n",
            "step: 460, loss: 0.003432776080444455\n",
            "step: 470, loss: 0.00219158036634326\n",
            "step: 480, loss: 0.005284764338284731\n",
            "step: 490, loss: 0.002762220334261656\n",
            "step: 500, loss: 0.0018490712391212583\n",
            "step: 510, loss: 0.21303203701972961\n",
            "step: 520, loss: 0.013591142371296883\n",
            "step: 530, loss: 0.007676883600652218\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9509116409537166, f1=0.9412861136999067, best_f1=0.946931241347485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019901860505342484\n",
            "step: 10, loss: 0.006411497015506029\n",
            "step: 20, loss: 0.005853429902344942\n",
            "step: 30, loss: 0.011592775583267212\n",
            "step: 40, loss: 0.019356105476617813\n",
            "step: 50, loss: 0.004209741950035095\n",
            "step: 60, loss: 0.005332069005817175\n",
            "step: 70, loss: 0.032540738582611084\n",
            "step: 80, loss: 0.04464134946465492\n",
            "step: 90, loss: 0.0004333044635131955\n",
            "step: 100, loss: 0.003059792798012495\n",
            "step: 110, loss: 0.001482343184761703\n",
            "step: 120, loss: 0.00957420188933611\n",
            "step: 130, loss: 0.0005752926808781922\n",
            "step: 140, loss: 0.002906227018684149\n",
            "step: 150, loss: 0.0006406580796465278\n",
            "step: 160, loss: 0.000488532125018537\n",
            "step: 170, loss: 0.017994891852140427\n",
            "step: 180, loss: 0.15078668296337128\n",
            "step: 190, loss: 0.01471562497317791\n",
            "step: 200, loss: 0.0005837198114022613\n",
            "step: 210, loss: 0.01808890886604786\n",
            "step: 220, loss: 0.002081881044432521\n",
            "step: 230, loss: 0.00048213565605692565\n",
            "step: 240, loss: 0.001974639017134905\n",
            "step: 250, loss: 0.0036655976437032223\n",
            "step: 260, loss: 0.002360182348638773\n",
            "step: 270, loss: 0.0012268224963918328\n",
            "step: 280, loss: 0.042175132781267166\n",
            "step: 290, loss: 0.0016237694071605802\n",
            "step: 300, loss: 0.0015770068857818842\n",
            "step: 310, loss: 0.0014791139401495457\n",
            "step: 320, loss: 0.0011369731510058045\n",
            "step: 330, loss: 0.000554859871044755\n",
            "step: 340, loss: 0.0010538449278101325\n",
            "step: 350, loss: 0.0008231992833316326\n",
            "step: 360, loss: 0.0032341359183192253\n",
            "step: 370, loss: 0.021210666745901108\n",
            "step: 380, loss: 0.01747935079038143\n",
            "step: 390, loss: 0.0030093032401055098\n",
            "step: 400, loss: 0.009743595495820045\n",
            "step: 410, loss: 0.000273136276518926\n",
            "step: 420, loss: 0.04291822761297226\n",
            "step: 430, loss: 0.11012278497219086\n",
            "step: 440, loss: 0.001818464370444417\n",
            "step: 450, loss: 0.017149023711681366\n",
            "step: 460, loss: 0.0003973868733737618\n",
            "step: 470, loss: 0.11629337817430496\n",
            "step: 480, loss: 0.0027118006255477667\n",
            "step: 490, loss: 0.003441434819251299\n",
            "step: 500, loss: 0.0021513598039746284\n",
            "step: 510, loss: 0.004330481868237257\n",
            "step: 520, loss: 0.01386323757469654\n",
            "step: 530, loss: 0.010337181389331818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9371752479924421, f1=0.932573599240266, best_f1=0.946931241347485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006824991432949901\n",
            "step: 10, loss: 0.007453846279531717\n",
            "step: 20, loss: 0.07877836376428604\n",
            "step: 30, loss: 0.0001848009560490027\n",
            "step: 40, loss: 0.0001575796923134476\n",
            "step: 50, loss: 0.002170566702261567\n",
            "step: 60, loss: 0.000535998959094286\n",
            "step: 70, loss: 0.0016256365925073624\n",
            "step: 80, loss: 0.015002897009253502\n",
            "step: 90, loss: 0.0003653528983704746\n",
            "step: 100, loss: 0.0004135190974920988\n",
            "step: 110, loss: 0.0003920221934095025\n",
            "step: 120, loss: 0.01179435197263956\n",
            "step: 130, loss: 0.005441519897431135\n",
            "step: 140, loss: 0.0012088791700080037\n",
            "step: 150, loss: 0.00019241489644628018\n",
            "step: 160, loss: 0.0008812538580968976\n",
            "step: 170, loss: 0.012838183902204037\n",
            "step: 180, loss: 0.0007953245658427477\n",
            "step: 190, loss: 0.06018028408288956\n",
            "step: 200, loss: 0.004343485459685326\n",
            "step: 210, loss: 0.20456616580486298\n",
            "step: 220, loss: 0.02343924157321453\n",
            "step: 230, loss: 0.020892184227705002\n",
            "step: 240, loss: 0.0016280650161206722\n",
            "step: 250, loss: 0.00027184057398699224\n",
            "step: 260, loss: 0.00011797421757364646\n",
            "step: 270, loss: 0.0025547214318066835\n",
            "step: 280, loss: 0.00013774952094536275\n",
            "step: 290, loss: 0.002493631560355425\n",
            "step: 300, loss: 4.4885931856697425e-05\n",
            "step: 310, loss: 6.46000771666877e-05\n",
            "step: 320, loss: 0.00015001444262452424\n",
            "step: 330, loss: 8.7968677689787e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 340, loss: 0.0017271587857976556\n",
            "step: 350, loss: 5.1775445172097534e-05\n",
            "step: 360, loss: 0.00023397790209855884\n",
            "step: 370, loss: 0.0623953640460968\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 380, loss: 8.175874245353043e-05\n",
            "step: 390, loss: 0.024227173998951912\n",
            "step: 400, loss: 0.0043763574212789536\n",
            "step: 410, loss: 0.0002625702181831002\n",
            "step: 420, loss: 0.00564921647310257\n",
            "step: 430, loss: 0.1347322016954422\n",
            "step: 440, loss: 0.021653473377227783\n",
            "step: 450, loss: 0.002104413229972124\n",
            "step: 460, loss: 0.00561919528990984\n",
            "step: 470, loss: 0.03726893663406372\n",
            "step: 480, loss: 0.0075118825770914555\n",
            "step: 490, loss: 0.005678033456206322\n",
            "step: 500, loss: 0.0005349920829758048\n",
            "step: 510, loss: 0.010972846299409866\n",
            "step: 520, loss: 0.0005169669166207314\n",
            "step: 530, loss: 0.00023228759528137743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9508041627246925, f1=0.9373219373219372, best_f1=0.946931241347485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001918480993481353\n",
            "step: 10, loss: 0.000995173817500472\n",
            "step: 20, loss: 0.0002767806581687182\n",
            "step: 30, loss: 0.14297422766685486\n",
            "step: 40, loss: 0.007656001951545477\n",
            "step: 50, loss: 0.0008385457331314683\n",
            "step: 60, loss: 0.0037914463318884373\n",
            "step: 70, loss: 0.004920303355902433\n",
            "step: 80, loss: 0.0054296753369271755\n",
            "step: 90, loss: 0.06724919378757477\n",
            "step: 100, loss: 0.000635709788184613\n",
            "step: 110, loss: 0.01710263453423977\n",
            "step: 120, loss: 0.0003052288666367531\n",
            "step: 130, loss: 0.0006049457006156445\n",
            "step: 140, loss: 0.004862209316343069\n",
            "step: 150, loss: 0.00020409298304002732\n",
            "step: 160, loss: 0.0013285847380757332\n",
            "step: 170, loss: 0.002484863856807351\n",
            "step: 180, loss: 0.0033367841970175505\n",
            "step: 190, loss: 0.00033461919520050287\n",
            "step: 200, loss: 0.00012142085324740037\n",
            "step: 210, loss: 0.0001404979993822053\n",
            "step: 220, loss: 0.00025002207257784903\n",
            "step: 230, loss: 0.000205142394406721\n",
            "step: 240, loss: 0.00017518512322567403\n",
            "step: 250, loss: 0.000297560851322487\n",
            "step: 260, loss: 0.00025696822558529675\n",
            "step: 270, loss: 0.031059565022587776\n",
            "step: 280, loss: 0.027783995494246483\n",
            "step: 290, loss: 0.0004545636475086212\n",
            "step: 300, loss: 0.061028871685266495\n",
            "step: 310, loss: 0.027928227558732033\n",
            "step: 320, loss: 0.0002544069429859519\n",
            "step: 330, loss: 0.0007104690303094685\n",
            "step: 340, loss: 0.003733817022293806\n",
            "step: 350, loss: 0.08402007073163986\n",
            "step: 360, loss: 0.15976877510547638\n",
            "step: 370, loss: 9.189817501464859e-05\n",
            "step: 380, loss: 0.019770674407482147\n",
            "step: 390, loss: 0.002348288195207715\n",
            "step: 400, loss: 0.002145562320947647\n",
            "step: 410, loss: 0.0005337715265341103\n",
            "step: 420, loss: 0.00044772092951461673\n",
            "step: 430, loss: 0.007099812850356102\n",
            "step: 440, loss: 0.00026378934853710234\n",
            "step: 450, loss: 0.005567966960370541\n",
            "step: 460, loss: 0.004972935654222965\n",
            "step: 470, loss: 0.00013644766295328736\n",
            "step: 480, loss: 0.002339705592021346\n",
            "step: 490, loss: 0.014126960188150406\n",
            "step: 500, loss: 0.0013589112786576152\n",
            "step: 510, loss: 0.009225253947079182\n",
            "step: 520, loss: 0.03765714541077614\n",
            "step: 530, loss: 0.11144043505191803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9512195121951219, f1=0.9464788732394367, best_f1=0.946931241347485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004117493983358145\n",
            "step: 10, loss: 0.0018803507555276155\n",
            "step: 20, loss: 0.00022740697022527456\n",
            "step: 30, loss: 0.0007961642113514245\n",
            "step: 40, loss: 0.0011548396432772279\n",
            "step: 50, loss: 0.00042967492481693625\n",
            "step: 60, loss: 0.007263274397701025\n",
            "step: 70, loss: 0.00010462365025887266\n",
            "step: 80, loss: 0.00013034055882599205\n",
            "step: 90, loss: 0.03851543739438057\n",
            "step: 100, loss: 0.0004788614169228822\n",
            "step: 110, loss: 0.011787819676101208\n",
            "step: 120, loss: 0.00040586222894489765\n",
            "step: 130, loss: 0.0019350942457094789\n",
            "step: 140, loss: 0.0005238619050942361\n",
            "step: 150, loss: 0.00020911124011036009\n",
            "step: 160, loss: 0.034128475934267044\n",
            "step: 170, loss: 9.948184015229344e-05\n",
            "step: 180, loss: 0.0026419467758387327\n",
            "step: 190, loss: 8.844317198963836e-05\n",
            "step: 200, loss: 0.00024420255795121193\n",
            "step: 210, loss: 0.00636669946834445\n",
            "step: 220, loss: 0.001049902057275176\n",
            "step: 230, loss: 0.00023422703088726848\n",
            "step: 240, loss: 0.0007958869100548327\n",
            "step: 250, loss: 0.01701231487095356\n",
            "step: 260, loss: 0.0014821621589362621\n",
            "step: 270, loss: 0.006672195624560118\n",
            "step: 280, loss: 0.02763824537396431\n",
            "step: 290, loss: 0.0004873854049947113\n",
            "step: 300, loss: 0.0002098380064126104\n",
            "step: 310, loss: 0.0038442290388047695\n",
            "step: 320, loss: 0.0019788166973739862\n",
            "step: 330, loss: 0.0007960383663885295\n",
            "step: 340, loss: 6.181655044201761e-05\n",
            "step: 350, loss: 0.0016919933259487152\n",
            "step: 360, loss: 9.742812108015642e-05\n",
            "step: 370, loss: 0.019792301580309868\n",
            "step: 380, loss: 0.170232892036438\n",
            "step: 390, loss: 0.0002900479012168944\n",
            "step: 400, loss: 0.0004987327847629786\n",
            "step: 410, loss: 0.00014941388508304954\n",
            "step: 420, loss: 0.00022099337365943938\n",
            "step: 430, loss: 6.799896800657734e-05\n",
            "step: 440, loss: 9.26678185351193e-05\n",
            "step: 450, loss: 0.1457778513431549\n",
            "step: 460, loss: 0.0005431086174212396\n",
            "step: 470, loss: 0.019435562193393707\n",
            "step: 480, loss: 0.0012884734896942973\n",
            "step: 490, loss: 0.0110927177593112\n",
            "step: 500, loss: 0.001992122968658805\n",
            "step: 510, loss: 0.0002590535150375217\n",
            "step: 520, loss: 0.018251169472932816\n",
            "step: 530, loss: 0.002539378125220537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9521126760563381, f1=0.9442622950819672, best_f1=0.946931241347485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001683546812273562\n",
            "step: 10, loss: 0.00037622166564688087\n",
            "step: 20, loss: 0.0023055693600326777\n",
            "step: 30, loss: 0.00042757581104524434\n",
            "step: 40, loss: 0.0005266951047815382\n",
            "step: 50, loss: 0.00036478479159995914\n",
            "step: 60, loss: 0.00024413850042037666\n",
            "step: 70, loss: 0.0002278315951116383\n",
            "step: 80, loss: 0.00018632160208653659\n",
            "step: 90, loss: 0.0005237172008492053\n",
            "step: 100, loss: 0.0021760037634521723\n",
            "step: 110, loss: 0.0001298098941333592\n",
            "step: 120, loss: 0.0006072514224797487\n",
            "step: 130, loss: 0.00011806405382230878\n",
            "step: 140, loss: 0.000765450531616807\n",
            "step: 150, loss: 0.0003211452276445925\n",
            "step: 160, loss: 0.0030695530585944653\n",
            "step: 170, loss: 0.00024382521223742515\n",
            "step: 180, loss: 0.0004463930381461978\n",
            "step: 190, loss: 0.0018547597574070096\n",
            "step: 200, loss: 0.00045503725414164364\n",
            "step: 210, loss: 0.0018520097946748137\n",
            "step: 220, loss: 0.005836819764226675\n",
            "step: 230, loss: 0.0003590166161302477\n",
            "step: 240, loss: 0.001680357032455504\n",
            "step: 250, loss: 0.053840767592191696\n",
            "step: 260, loss: 0.005625756923109293\n",
            "step: 270, loss: 0.004438641015440226\n",
            "step: 280, loss: 0.0056360336020588875\n",
            "step: 290, loss: 0.0003139501204714179\n",
            "step: 300, loss: 0.0003241421945858747\n",
            "step: 310, loss: 0.00014616815315093845\n",
            "step: 320, loss: 0.0008450138266198337\n",
            "step: 330, loss: 0.00010781494347611442\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 340, loss: 0.0004692107322625816\n",
            "step: 350, loss: 8.603402238804847e-05\n",
            "step: 360, loss: 0.00019791873637586832\n",
            "step: 370, loss: 0.0013208382297307253\n",
            "step: 380, loss: 0.0014485373394563794\n",
            "step: 390, loss: 0.0017986168386414647\n",
            "step: 400, loss: 0.0001923427334986627\n",
            "step: 410, loss: 0.0012156175216659904\n",
            "step: 420, loss: 0.00013887867680750787\n",
            "step: 430, loss: 0.000761836941819638\n",
            "step: 440, loss: 0.00013491167919710279\n",
            "step: 450, loss: 0.001631437917239964\n",
            "step: 460, loss: 0.00023692265676800162\n",
            "step: 470, loss: 0.0004324372857809067\n",
            "step: 480, loss: 0.0005123370792716742\n",
            "step: 490, loss: 5.677516310242936e-05\n",
            "step: 500, loss: 7.319574797293171e-05\n",
            "step: 510, loss: 0.0002015354548348114\n",
            "step: 520, loss: 0.00024759789812378585\n",
            "step: 530, loss: 0.00036870475742034614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9496738117427772, f1=0.9416666666666667, best_f1=0.946931241347485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.005889008520171e-05\n",
            "step: 10, loss: 0.0005214160773903131\n",
            "step: 20, loss: 0.001975269988179207\n",
            "step: 30, loss: 0.000102351994428318\n",
            "step: 40, loss: 8.621605957159773e-05\n",
            "step: 50, loss: 6.388716428773478e-05\n",
            "step: 60, loss: 6.779443356208503e-05\n",
            "step: 70, loss: 0.00010803664190461859\n",
            "step: 80, loss: 6.717917858622968e-05\n",
            "step: 90, loss: 0.00660286657512188\n",
            "step: 100, loss: 0.2561822533607483\n",
            "step: 110, loss: 0.00010010385449277237\n",
            "step: 120, loss: 0.00021462592121679336\n",
            "step: 130, loss: 0.00042976593249477446\n",
            "step: 140, loss: 0.00017556396778672934\n",
            "step: 150, loss: 0.00012350206088740379\n",
            "step: 160, loss: 0.0001618512033019215\n",
            "step: 170, loss: 0.00016209603927563876\n",
            "step: 180, loss: 0.0002506253367755562\n",
            "step: 190, loss: 0.0001813020498957485\n",
            "step: 200, loss: 0.018594805151224136\n",
            "step: 210, loss: 0.00014236883725970984\n",
            "step: 220, loss: 0.00011449780140537769\n",
            "step: 230, loss: 0.0002250366669613868\n",
            "step: 240, loss: 0.00034851516829803586\n",
            "step: 250, loss: 0.00010254004155285656\n",
            "step: 260, loss: 6.15555327385664e-05\n",
            "step: 270, loss: 0.00015107890067156404\n",
            "step: 280, loss: 0.00023211754160001874\n",
            "step: 290, loss: 0.001239077653735876\n",
            "step: 300, loss: 0.001174261444248259\n",
            "step: 310, loss: 0.03223929926753044\n",
            "step: 320, loss: 0.015517709776759148\n",
            "step: 330, loss: 0.0338471457362175\n",
            "step: 340, loss: 0.00024141703033819795\n",
            "step: 350, loss: 7.341952004935592e-05\n",
            "step: 360, loss: 0.00024921895237639546\n",
            "step: 370, loss: 0.0001694559323368594\n",
            "step: 380, loss: 0.00015576438454445451\n",
            "step: 390, loss: 0.0023423200473189354\n",
            "step: 400, loss: 8.653195982333273e-05\n",
            "step: 410, loss: 0.004325937479734421\n",
            "step: 420, loss: 0.0001115535051212646\n",
            "step: 430, loss: 0.0006396675016731024\n",
            "step: 440, loss: 0.005087577272206545\n",
            "step: 450, loss: 0.0017057337099686265\n",
            "step: 460, loss: 8.754240843700245e-05\n",
            "step: 470, loss: 0.010404725559055805\n",
            "step: 480, loss: 0.000697667826898396\n",
            "step: 490, loss: 9.039512951858342e-05\n",
            "step: 500, loss: 0.0001472529984312132\n",
            "step: 510, loss: 0.004551767837256193\n",
            "step: 520, loss: 0.0006510468665510416\n",
            "step: 530, loss: 0.000985126942396164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9527340129749768, f1=0.9488243430152145, best_f1=0.946931241347485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.21905637672171e-05\n",
            "step: 10, loss: 7.049249688861892e-05\n",
            "step: 20, loss: 0.00017754109285306185\n",
            "step: 30, loss: 7.756602281006053e-05\n",
            "step: 40, loss: 0.0007191848126240075\n",
            "step: 50, loss: 0.0015588193200528622\n",
            "step: 60, loss: 0.00015224944218061864\n",
            "step: 70, loss: 0.0004596611834131181\n",
            "step: 80, loss: 0.00026777994935400784\n",
            "step: 90, loss: 0.00011140805145259947\n",
            "step: 100, loss: 6.1001999711152166e-05\n",
            "step: 110, loss: 7.761794404359534e-05\n",
            "step: 120, loss: 5.633191540255211e-05\n",
            "step: 130, loss: 4.466369500732981e-05\n",
            "step: 140, loss: 0.00029535588691942394\n",
            "step: 150, loss: 0.0013717911206185818\n",
            "step: 160, loss: 0.0007898105541244149\n",
            "step: 170, loss: 2.0573481378960423e-05\n",
            "step: 180, loss: 0.00016983880777843297\n",
            "step: 190, loss: 5.9675759985111654e-05\n",
            "step: 200, loss: 6.287715950747952e-05\n",
            "step: 210, loss: 5.065560617367737e-05\n",
            "step: 220, loss: 5.933025022386573e-05\n",
            "step: 230, loss: 0.00029566840385086834\n",
            "step: 240, loss: 0.00017544825095683336\n",
            "step: 250, loss: 7.199942047009245e-05\n",
            "step: 260, loss: 0.00013358907017391175\n",
            "step: 270, loss: 0.0009494540863670409\n",
            "step: 280, loss: 9.736057108966634e-05\n",
            "step: 290, loss: 7.07576036802493e-05\n",
            "step: 300, loss: 5.49878750462085e-05\n",
            "step: 310, loss: 9.007701009977609e-05\n",
            "step: 320, loss: 5.3633742936654016e-05\n",
            "step: 330, loss: 0.00015731582243461162\n",
            "step: 340, loss: 0.0011339125921949744\n",
            "step: 350, loss: 4.783388430951163e-05\n",
            "step: 360, loss: 0.09634476155042648\n",
            "step: 370, loss: 0.00036495571839623153\n",
            "step: 380, loss: 0.00016244682774413377\n",
            "step: 390, loss: 0.00022246439766604453\n",
            "step: 400, loss: 0.00015338334196712822\n",
            "step: 410, loss: 0.00012679783685598522\n",
            "step: 420, loss: 8.681986946612597e-05\n",
            "step: 430, loss: 0.0003103717463091016\n",
            "step: 440, loss: 5.137754851602949e-05\n",
            "step: 450, loss: 0.1552739292383194\n",
            "step: 460, loss: 0.00014449261652771384\n",
            "step: 470, loss: 0.0012218368938192725\n",
            "step: 480, loss: 3.546154039213434e-05\n",
            "step: 490, loss: 0.00022348822676576674\n",
            "step: 500, loss: 0.00022111760335974395\n",
            "step: 510, loss: 0.0002137538103852421\n",
            "step: 520, loss: 0.0009026016923598945\n",
            "step: 530, loss: 9.278654033550993e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9557021677662583, f1=0.947022972339428, best_f1=0.947022972339428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001486199558712542\n",
            "step: 10, loss: 0.0014747236855328083\n",
            "step: 20, loss: 0.00010483126970939338\n",
            "step: 30, loss: 0.002086395863443613\n",
            "step: 40, loss: 0.0002686189254745841\n",
            "step: 50, loss: 0.0053170956671237946\n",
            "step: 60, loss: 0.00010783245670609176\n",
            "step: 70, loss: 0.00020396552281454206\n",
            "step: 80, loss: 0.00023301907640416175\n",
            "step: 90, loss: 0.00014527879829984158\n",
            "step: 100, loss: 0.00012511180830188096\n",
            "step: 110, loss: 0.0001747684000292793\n",
            "step: 120, loss: 8.164123573806137e-05\n",
            "step: 130, loss: 8.737476309761405e-05\n",
            "step: 140, loss: 0.00023134369985200465\n",
            "step: 150, loss: 0.0008367938571609557\n",
            "step: 160, loss: 0.00015361575060524046\n",
            "step: 170, loss: 0.00010593170736683533\n",
            "step: 180, loss: 4.806220385944471e-05\n",
            "step: 190, loss: 7.622939301654696e-05\n",
            "step: 200, loss: 0.00011504354188218713\n",
            "step: 210, loss: 0.0015388766769319773\n",
            "step: 220, loss: 6.0404683608794585e-05\n",
            "step: 230, loss: 0.0001119101871154271\n",
            "step: 240, loss: 0.0013761507580056787\n",
            "step: 250, loss: 0.0076738581992685795\n",
            "step: 260, loss: 0.0003641262883320451\n",
            "step: 270, loss: 0.0004154751368332654\n",
            "step: 280, loss: 0.00041344837518408895\n",
            "step: 290, loss: 4.685363455791958e-05\n",
            "step: 300, loss: 0.00019133530440740287\n",
            "step: 310, loss: 9.749074524734169e-05\n",
            "step: 320, loss: 0.0004463551740627736\n",
            "step: 330, loss: 0.000622231571469456\n",
            "step: 340, loss: 0.00010783046309370548\n",
            "step: 350, loss: 5.98948827246204e-05\n",
            "step: 360, loss: 7.273277878994122e-05\n",
            "step: 370, loss: 0.004743644967675209\n",
            "step: 380, loss: 0.00026204853202216327\n",
            "step: 390, loss: 0.0028471876867115498\n",
            "step: 400, loss: 0.010798463597893715\n",
            "step: 410, loss: 3.617561742430553e-05\n",
            "step: 420, loss: 4.9116279114969075e-05\n",
            "step: 430, loss: 0.0009381826384924352\n",
            "step: 440, loss: 0.006453900132328272\n",
            "step: 450, loss: 0.0009340202668681741\n",
            "step: 460, loss: 0.002077521523460746\n",
            "step: 470, loss: 9.535733988741413e-05\n",
            "step: 480, loss: 0.00015929003711789846\n",
            "step: 490, loss: 0.00030972884269431233\n",
            "step: 500, loss: 0.0015257400227710605\n",
            "step: 510, loss: 0.0020420453511178493\n",
            "step: 520, loss: 4.6390552597586066e-05\n",
            "step: 530, loss: 0.0019224146381020546\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9536178107606679, f1=0.9497464269248502, best_f1=0.947022972339428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011918208474526182\n",
            "step: 10, loss: 0.001743054948747158\n",
            "step: 20, loss: 0.0008600398432463408\n",
            "step: 30, loss: 0.002416069619357586\n",
            "step: 40, loss: 7.526092667831108e-05\n",
            "step: 50, loss: 0.0005097496323287487\n",
            "step: 60, loss: 0.0004279833228792995\n",
            "step: 70, loss: 0.00018638464098330587\n",
            "step: 80, loss: 5.978584522381425e-05\n",
            "step: 90, loss: 5.861672980245203e-05\n",
            "step: 100, loss: 2.0633198801078834e-05\n",
            "step: 110, loss: 0.0003887033089995384\n",
            "step: 120, loss: 4.627781891031191e-05\n",
            "step: 130, loss: 0.00021683478553313762\n",
            "step: 140, loss: 5.572885129367933e-05\n",
            "step: 150, loss: 9.805030276766047e-05\n",
            "step: 160, loss: 9.226910333381966e-05\n",
            "step: 170, loss: 3.238841600250453e-05\n",
            "step: 180, loss: 0.00010940904030576348\n",
            "step: 190, loss: 0.0003923483891412616\n",
            "step: 200, loss: 0.0011509613832458854\n",
            "step: 210, loss: 4.071359217050485e-05\n",
            "step: 220, loss: 8.317790343426168e-05\n",
            "step: 230, loss: 0.00011136670946143568\n",
            "step: 240, loss: 5.446936120279133e-05\n",
            "step: 250, loss: 5.6975499319378287e-05\n",
            "step: 260, loss: 4.888115290668793e-05\n",
            "step: 270, loss: 9.838271944317967e-05\n",
            "step: 280, loss: 0.00017268213559873402\n",
            "step: 290, loss: 5.105424497742206e-05\n",
            "step: 300, loss: 6.42876693746075e-05\n",
            "step: 310, loss: 0.08290035277605057\n",
            "step: 320, loss: 0.000240478795603849\n",
            "step: 330, loss: 8.667857036925852e-05\n",
            "step: 340, loss: 8.564317249692976e-05\n",
            "step: 350, loss: 0.0014614008832722902\n",
            "step: 360, loss: 7.591769826831296e-05\n",
            "step: 370, loss: 0.0002920891856774688\n",
            "step: 380, loss: 7.230541086755693e-05\n",
            "step: 390, loss: 9.87105377134867e-05\n",
            "step: 400, loss: 0.00017712742555886507\n",
            "step: 410, loss: 9.378221147926524e-05\n",
            "step: 420, loss: 9.578339086147025e-05\n",
            "step: 430, loss: 3.118891254416667e-05\n",
            "step: 440, loss: 0.15355022251605988\n",
            "step: 450, loss: 0.003787447465583682\n",
            "step: 460, loss: 0.0014857081696391106\n",
            "step: 470, loss: 5.3064621170051396e-05\n",
            "step: 480, loss: 0.001669514225795865\n",
            "step: 490, loss: 0.0009526055655442178\n",
            "step: 500, loss: 0.028679771348834038\n",
            "step: 510, loss: 5.5673997849226e-05\n",
            "step: 520, loss: 7.055460446281359e-05\n",
            "step: 530, loss: 3.567776730051264e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9541804440245631, f1=0.9481132075471699, best_f1=0.947022972339428\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:22, 258.80it/s]\n",
            "load_f1 = 0.9546940681924334\n",
            "real_f1 = 0.9532710280373832\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 199.24it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb_EWW7DgNFL"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oQ7ANLogNFM",
        "outputId": "cc010512-09c6-496a-b241-7a849b2ce7d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 365kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 1.73MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.31MB/s]\n",
            "Downloading: 100% 501M/501M [00:13<00:00, 37.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5475060343742371\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4192194938659668\n",
            "step: 20, loss: 0.4085792005062103\n",
            "step: 30, loss: 0.4120912551879883\n",
            "step: 40, loss: 0.27877315878868103\n",
            "step: 50, loss: 0.41970428824424744\n",
            "step: 60, loss: 0.5299904346466064\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 70, loss: 0.3458731472492218\n",
            "step: 80, loss: 0.3744012713432312\n",
            "step: 90, loss: 0.24934378266334534\n",
            "step: 100, loss: 0.29081565141677856\n",
            "step: 110, loss: 0.3148813545703888\n",
            "step: 120, loss: 0.3791716694831848\n",
            "step: 130, loss: 0.24214166402816772\n",
            "step: 140, loss: 0.4741656184196472\n",
            "step: 150, loss: 0.36170297861099243\n",
            "step: 160, loss: 0.5172631144523621\n",
            "step: 170, loss: 0.25953999161720276\n",
            "step: 180, loss: 0.39638352394104004\n",
            "step: 190, loss: 0.6977786421775818\n",
            "step: 200, loss: 0.35931891202926636\n",
            "step: 210, loss: 0.5044553875923157\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3669426441192627\n",
            "step: 10, loss: 0.16370432078838348\n",
            "step: 20, loss: 0.47080937027931213\n",
            "step: 30, loss: 0.5000967979431152\n",
            "step: 40, loss: 0.4513693153858185\n",
            "step: 50, loss: 0.32134485244750977\n",
            "step: 60, loss: 0.33093634247779846\n",
            "step: 70, loss: 0.3589194416999817\n",
            "step: 80, loss: 0.32452288269996643\n",
            "step: 90, loss: 0.20680733025074005\n",
            "step: 100, loss: 0.4579811990261078\n",
            "step: 110, loss: 0.41764628887176514\n",
            "step: 120, loss: 0.13118450343608856\n",
            "step: 130, loss: 0.24865727126598358\n",
            "step: 140, loss: 0.1894187182188034\n",
            "step: 150, loss: 0.3384535312652588\n",
            "step: 160, loss: 0.09390611201524734\n",
            "step: 170, loss: 0.37323155999183655\n",
            "step: 180, loss: 0.305441677570343\n",
            "step: 190, loss: 0.3252173066139221\n",
            "step: 200, loss: 0.14767004549503326\n",
            "step: 210, loss: 0.16888615489006042\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.537117903930131, f1=0.55, best_f1=0.55\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11848083883523941\n",
            "step: 10, loss: 0.07563940435647964\n",
            "step: 20, loss: 0.2848118245601654\n",
            "step: 30, loss: 0.0550009161233902\n",
            "step: 40, loss: 0.3412099778652191\n",
            "step: 50, loss: 0.49531233310699463\n",
            "step: 60, loss: 0.3219683766365051\n",
            "step: 70, loss: 0.13644656538963318\n",
            "step: 80, loss: 0.2576616406440735\n",
            "step: 90, loss: 0.11002644151449203\n",
            "step: 100, loss: 0.2379886507987976\n",
            "step: 110, loss: 0.10986697673797607\n",
            "step: 120, loss: 0.11080457270145416\n",
            "step: 130, loss: 0.3294955790042877\n",
            "step: 140, loss: 0.13280777633190155\n",
            "step: 150, loss: 0.2936226725578308\n",
            "step: 160, loss: 0.20672933757305145\n",
            "step: 170, loss: 0.2796919643878937\n",
            "step: 180, loss: 0.1381911337375641\n",
            "step: 190, loss: 0.15973185002803802\n",
            "step: 200, loss: 0.13680224120616913\n",
            "step: 210, loss: 0.2463354468345642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6145038167938932, f1=0.6323809523809524, best_f1=0.6323809523809524\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1050807535648346\n",
            "step: 10, loss: 0.14240096509456635\n",
            "step: 20, loss: 0.27712908387184143\n",
            "step: 30, loss: 0.140095055103302\n",
            "step: 40, loss: 0.06049547716975212\n",
            "step: 50, loss: 0.1473277509212494\n",
            "step: 60, loss: 0.09543930739164352\n",
            "step: 70, loss: 0.08570719510316849\n",
            "step: 80, loss: 0.07212355732917786\n",
            "step: 90, loss: 0.03858532756567001\n",
            "step: 100, loss: 0.15774621069431305\n",
            "step: 110, loss: 0.38067758083343506\n",
            "step: 120, loss: 0.3064025044441223\n",
            "step: 130, loss: 0.38939186930656433\n",
            "step: 140, loss: 0.2594287097454071\n",
            "step: 150, loss: 0.14066417515277863\n",
            "step: 160, loss: 0.21146202087402344\n",
            "step: 170, loss: 0.10424946248531342\n",
            "step: 180, loss: 0.06889484077692032\n",
            "step: 190, loss: 0.13183414936065674\n",
            "step: 200, loss: 0.039157260209321976\n",
            "step: 210, loss: 0.43662598729133606\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6352941176470588, f1=0.6506024096385543, best_f1=0.6506024096385543\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1514880359172821\n",
            "step: 10, loss: 0.0368863046169281\n",
            "step: 20, loss: 0.12477368116378784\n",
            "step: 30, loss: 0.13918671011924744\n",
            "step: 40, loss: 0.07001124322414398\n",
            "step: 50, loss: 0.1265668123960495\n",
            "step: 60, loss: 0.15781691670417786\n",
            "step: 70, loss: 0.2097218632698059\n",
            "step: 80, loss: 0.06964045763015747\n",
            "step: 90, loss: 0.08147967606782913\n",
            "step: 100, loss: 0.03214964643120766\n",
            "step: 110, loss: 0.03358667343854904\n",
            "step: 120, loss: 0.1011008620262146\n",
            "step: 130, loss: 0.12304659932851791\n",
            "step: 140, loss: 0.09411761909723282\n",
            "step: 150, loss: 0.09220247715711594\n",
            "step: 160, loss: 0.04133979231119156\n",
            "step: 170, loss: 0.11267562955617905\n",
            "step: 180, loss: 0.21081693470478058\n",
            "step: 190, loss: 0.1671629697084427\n",
            "step: 200, loss: 0.09891501814126968\n",
            "step: 210, loss: 0.07011056691408157\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6415094339622641, f1=0.6388308977035491, best_f1=0.6388308977035491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08642378449440002\n",
            "step: 10, loss: 0.09568468481302261\n",
            "step: 20, loss: 0.01536202896386385\n",
            "step: 30, loss: 0.16768454015254974\n",
            "step: 40, loss: 0.0978696271777153\n",
            "step: 50, loss: 0.09837418794631958\n",
            "step: 60, loss: 0.19898946583271027\n",
            "step: 70, loss: 0.14835263788700104\n",
            "step: 80, loss: 0.11966212093830109\n",
            "step: 90, loss: 0.04959942400455475\n",
            "step: 100, loss: 0.16231673955917358\n",
            "step: 110, loss: 0.1056709736585617\n",
            "step: 120, loss: 0.011260390281677246\n",
            "step: 130, loss: 0.043461594730615616\n",
            "step: 140, loss: 0.08612941205501556\n",
            "step: 150, loss: 0.025658197700977325\n",
            "step: 160, loss: 0.023125076666474342\n",
            "step: 170, loss: 0.14514102041721344\n",
            "step: 180, loss: 0.022906163707375526\n",
            "step: 190, loss: 0.03300909698009491\n",
            "step: 200, loss: 0.3254722058773041\n",
            "step: 210, loss: 0.1330813318490982\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6129753914988815, f1=0.6405228758169934, best_f1=0.6388308977035491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08630023151636124\n",
            "step: 10, loss: 0.015479018911719322\n",
            "step: 20, loss: 0.023380370810627937\n",
            "step: 30, loss: 0.08632446825504303\n",
            "step: 40, loss: 0.0028717753011733294\n",
            "step: 50, loss: 0.026789570227265358\n",
            "step: 60, loss: 0.03489065170288086\n",
            "step: 70, loss: 0.09738609194755554\n",
            "step: 80, loss: 0.04438593238592148\n",
            "step: 90, loss: 0.034641895443201065\n",
            "step: 100, loss: 0.07006800174713135\n",
            "step: 110, loss: 0.034115102142095566\n",
            "step: 120, loss: 0.07424519211053848\n",
            "step: 130, loss: 0.2832280695438385\n",
            "step: 140, loss: 0.10854081064462662\n",
            "step: 150, loss: 0.15445534884929657\n",
            "step: 160, loss: 0.19733567535877228\n",
            "step: 170, loss: 0.08219332993030548\n",
            "step: 180, loss: 0.05779499188065529\n",
            "step: 190, loss: 0.009514587931334972\n",
            "step: 200, loss: 0.11713308840990067\n",
            "step: 210, loss: 0.24671967327594757\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6223021582733813, f1=0.6272401433691756, best_f1=0.6388308977035491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03846431523561478\n",
            "step: 10, loss: 0.017206337302923203\n",
            "step: 20, loss: 0.008121682330965996\n",
            "step: 30, loss: 0.025744084268808365\n",
            "step: 40, loss: 0.001285431208088994\n",
            "step: 50, loss: 0.006567764095962048\n",
            "step: 60, loss: 0.004820812493562698\n",
            "step: 70, loss: 0.17255575954914093\n",
            "step: 80, loss: 0.17193365097045898\n",
            "step: 90, loss: 0.035664018243551254\n",
            "step: 100, loss: 0.09307321161031723\n",
            "step: 110, loss: 0.04783233255147934\n",
            "step: 120, loss: 0.038215164095163345\n",
            "step: 130, loss: 0.00659555708989501\n",
            "step: 140, loss: 0.11684661358594894\n",
            "step: 150, loss: 0.13144662976264954\n",
            "step: 160, loss: 0.0774916335940361\n",
            "step: 170, loss: 0.03972387686371803\n",
            "step: 180, loss: 0.07231878489255905\n",
            "step: 190, loss: 0.05069033429026604\n",
            "step: 200, loss: 0.027724670246243477\n",
            "step: 210, loss: 0.05122580751776695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6310344827586207, f1=0.6112054329371815, best_f1=0.6388308977035491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1544766128063202\n",
            "step: 10, loss: 0.022094199433922768\n",
            "step: 20, loss: 0.046243757009506226\n",
            "step: 30, loss: 0.006351597607135773\n",
            "step: 40, loss: 0.0008551526698283851\n",
            "step: 50, loss: 0.12003976106643677\n",
            "step: 60, loss: 0.0015548373339697719\n",
            "step: 70, loss: 0.051153041422367096\n",
            "step: 80, loss: 0.09023191034793854\n",
            "step: 90, loss: 0.06390698254108429\n",
            "step: 100, loss: 0.029284559190273285\n",
            "step: 110, loss: 0.05525222420692444\n",
            "step: 120, loss: 0.013723446056246758\n",
            "step: 130, loss: 0.1144665852189064\n",
            "step: 140, loss: 0.015274116769433022\n",
            "step: 150, loss: 0.048033613711595535\n",
            "step: 160, loss: 0.33123552799224854\n",
            "step: 170, loss: 0.040807586163282394\n",
            "step: 180, loss: 0.04859264940023422\n",
            "step: 190, loss: 0.02064920775592327\n",
            "step: 200, loss: 0.004408596083521843\n",
            "step: 210, loss: 0.1617612987756729\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6170678336980306, f1=0.6439909297052154, best_f1=0.6388308977035491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006089388392865658\n",
            "step: 10, loss: 0.007215231657028198\n",
            "step: 20, loss: 0.007881268858909607\n",
            "step: 30, loss: 0.0007054340676404536\n",
            "step: 40, loss: 0.0035177255049347878\n",
            "step: 50, loss: 0.05839994177222252\n",
            "step: 60, loss: 0.00949932262301445\n",
            "step: 70, loss: 0.01977979578077793\n",
            "step: 80, loss: 0.024303385987877846\n",
            "step: 90, loss: 0.03175632655620575\n",
            "step: 100, loss: 0.16611774265766144\n",
            "step: 110, loss: 0.009186332114040852\n",
            "step: 120, loss: 0.049222975969314575\n",
            "step: 130, loss: 0.05073372274637222\n",
            "step: 140, loss: 0.0033486851025372744\n",
            "step: 150, loss: 0.00962571520358324\n",
            "step: 160, loss: 0.059716928750276566\n",
            "step: 170, loss: 0.0506901778280735\n",
            "step: 180, loss: 0.014460490085184574\n",
            "step: 190, loss: 0.02819492481648922\n",
            "step: 200, loss: 0.051947180181741714\n",
            "step: 210, loss: 0.0016626986907795072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6322067594433399, f1=0.6666666666666666, best_f1=0.6388308977035491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06879120320081711\n",
            "step: 10, loss: 0.005334039684385061\n",
            "step: 20, loss: 0.004534280858933926\n",
            "step: 30, loss: 0.004460558760911226\n",
            "step: 40, loss: 0.006208166480064392\n",
            "step: 50, loss: 0.09847467392683029\n",
            "step: 60, loss: 0.03378371149301529\n",
            "step: 70, loss: 0.020532850176095963\n",
            "step: 80, loss: 0.006071555428206921\n",
            "step: 90, loss: 0.1929388791322708\n",
            "step: 100, loss: 0.023307427763938904\n",
            "step: 110, loss: 0.02897661365568638\n",
            "step: 120, loss: 0.04546008259057999\n",
            "step: 130, loss: 0.004255687817931175\n",
            "step: 140, loss: 0.019502053037285805\n",
            "step: 150, loss: 0.04282231256365776\n",
            "step: 160, loss: 0.0008216026471927762\n",
            "step: 170, loss: 0.0110095189884305\n",
            "step: 180, loss: 0.009565891698002815\n",
            "step: 190, loss: 0.03788873553276062\n",
            "step: 200, loss: 0.013511833734810352\n",
            "step: 210, loss: 0.03959844633936882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6311637080867851, f1=0.6516393442622951, best_f1=0.6388308977035491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017229719087481499\n",
            "step: 10, loss: 0.0151385348290205\n",
            "step: 20, loss: 0.01785723865032196\n",
            "step: 30, loss: 0.008707272820174694\n",
            "step: 40, loss: 0.0003498812729958445\n",
            "step: 50, loss: 0.0015356853837147355\n",
            "step: 60, loss: 0.0037837871350347996\n",
            "step: 70, loss: 0.009081773459911346\n",
            "step: 80, loss: 0.0339091531932354\n",
            "step: 90, loss: 0.01029975526034832\n",
            "step: 100, loss: 0.0022925708908587694\n",
            "step: 110, loss: 0.1370517909526825\n",
            "step: 120, loss: 0.0022741775028407574\n",
            "step: 130, loss: 0.03369038552045822\n",
            "step: 140, loss: 0.07021086663007736\n",
            "step: 150, loss: 0.000163894597790204\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 160, loss: 0.0006040356238372624\n",
            "step: 170, loss: 0.1524653285741806\n",
            "step: 180, loss: 0.002118697389960289\n",
            "step: 190, loss: 0.03485334292054176\n",
            "step: 200, loss: 0.0028631456661969423\n",
            "step: 210, loss: 0.04052410647273064\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6307385229540918, f1=0.6448979591836735, best_f1=0.6388308977035491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028284990694373846\n",
            "step: 10, loss: 0.025921232998371124\n",
            "step: 20, loss: 0.005719548091292381\n",
            "step: 30, loss: 0.014841757714748383\n",
            "step: 40, loss: 0.0010234410874545574\n",
            "step: 50, loss: 0.17381179332733154\n",
            "step: 60, loss: 0.01583748497068882\n",
            "step: 70, loss: 0.005410260055214167\n",
            "step: 80, loss: 0.17559699714183807\n",
            "step: 90, loss: 0.027602486312389374\n",
            "step: 100, loss: 0.0020028813742101192\n",
            "step: 110, loss: 0.04581542685627937\n",
            "step: 120, loss: 0.0021634099539369345\n",
            "step: 130, loss: 0.005050220992416143\n",
            "step: 140, loss: 0.006664101034402847\n",
            "step: 150, loss: 0.002333735814318061\n",
            "step: 160, loss: 0.03534796088933945\n",
            "step: 170, loss: 0.0013205045834183693\n",
            "step: 180, loss: 0.002332722768187523\n",
            "step: 190, loss: 0.02463538572192192\n",
            "step: 200, loss: 0.005082267336547375\n",
            "step: 210, loss: 0.14925217628479004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6367041198501873, f1=0.6425855513307984, best_f1=0.6388308977035491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002356564626097679\n",
            "step: 10, loss: 0.09091349691152573\n",
            "step: 20, loss: 0.002264933427795768\n",
            "step: 30, loss: 0.0014448290457949042\n",
            "step: 40, loss: 0.016473736613988876\n",
            "step: 50, loss: 0.0051567391492426395\n",
            "step: 60, loss: 0.12036645412445068\n",
            "step: 70, loss: 0.005105873569846153\n",
            "step: 80, loss: 0.017929673194885254\n",
            "step: 90, loss: 0.0005132087389938533\n",
            "step: 100, loss: 0.014441807754337788\n",
            "step: 110, loss: 0.002449811203405261\n",
            "step: 120, loss: 0.001108461758121848\n",
            "step: 130, loss: 0.016711294651031494\n",
            "step: 140, loss: 0.0011954933870583773\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 150, loss: 0.035586025565862656\n",
            "step: 160, loss: 0.013954708352684975\n",
            "step: 170, loss: 0.03810713812708855\n",
            "step: 180, loss: 0.0007504393579438329\n",
            "step: 190, loss: 0.05636712163686752\n",
            "step: 200, loss: 0.0007891888380981982\n",
            "step: 210, loss: 0.0016901949420571327\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.617283950617284, f1=0.6515463917525772, best_f1=0.6388308977035491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026621201541274786\n",
            "step: 10, loss: 0.002973872469738126\n",
            "step: 20, loss: 0.05286013334989548\n",
            "step: 30, loss: 0.010553041473031044\n",
            "step: 40, loss: 0.005619758740067482\n",
            "step: 50, loss: 0.0006612472352571785\n",
            "step: 60, loss: 0.013018688187003136\n",
            "step: 70, loss: 0.0026698322035372257\n",
            "step: 80, loss: 0.06327628344297409\n",
            "step: 90, loss: 0.005336458794772625\n",
            "step: 100, loss: 0.0024227285757660866\n",
            "step: 110, loss: 0.004374115727841854\n",
            "step: 120, loss: 0.00422790739685297\n",
            "step: 130, loss: 0.001439272309653461\n",
            "step: 140, loss: 0.010767695493996143\n",
            "step: 150, loss: 0.02774989604949951\n",
            "step: 160, loss: 0.003342424286529422\n",
            "step: 170, loss: 0.055437274277210236\n",
            "step: 180, loss: 0.002881844062358141\n",
            "step: 190, loss: 0.0012344305869191885\n",
            "step: 200, loss: 0.0008217753493227065\n",
            "step: 210, loss: 0.028741173446178436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6086956521739131, f1=0.650103519668737, best_f1=0.6388308977035491\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:06, 339.52it/s]\n",
            "load_f1 = 0.6335403726708074\n",
            "real_f1 = 0.6359832635983262\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:25, 171.41it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIIoASlugNFN",
        "outputId": "18066f24-f7ca-4583-8cb3-e412c0b4bfb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4288548231124878\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4566175043582916\n",
            "step: 20, loss: 0.26247233152389526\n",
            "step: 30, loss: 0.3509836792945862\n",
            "step: 40, loss: 0.26394426822662354\n",
            "step: 50, loss: 0.32558155059814453\n",
            "step: 60, loss: 0.44326525926589966\n",
            "step: 70, loss: 0.4025137424468994\n",
            "step: 80, loss: 0.17092333734035492\n",
            "step: 90, loss: 0.29263654351234436\n",
            "step: 100, loss: 0.4466719627380371\n",
            "step: 110, loss: 0.19748635590076447\n",
            "step: 120, loss: 0.32798469066619873\n",
            "step: 130, loss: 0.341731995344162\n",
            "step: 140, loss: 0.18872126936912537\n",
            "step: 150, loss: 0.27670714259147644\n",
            "step: 160, loss: 0.19449450075626373\n",
            "step: 170, loss: 0.3873932361602783\n",
            "step: 180, loss: 0.22969332337379456\n",
            "step: 190, loss: 0.12492495775222778\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4526558891454966, f1=0.5188470066518847, best_f1=0.5188470066518847\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.27371078729629517\n",
            "step: 10, loss: 0.2380666583776474\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.713784396648407\n",
            "step: 30, loss: 0.19283238053321838\n",
            "step: 40, loss: 0.23626410961151123\n",
            "step: 50, loss: 0.34026387333869934\n",
            "step: 60, loss: 0.2060551941394806\n",
            "step: 70, loss: 0.2313794493675232\n",
            "step: 80, loss: 0.06335984915494919\n",
            "step: 90, loss: 0.1144426167011261\n",
            "step: 100, loss: 0.11302945762872696\n",
            "step: 110, loss: 0.05478953942656517\n",
            "step: 120, loss: 0.15811878442764282\n",
            "step: 130, loss: 0.11300838738679886\n",
            "step: 140, loss: 0.19309131801128387\n",
            "step: 150, loss: 0.23621486127376556\n",
            "step: 160, loss: 0.08736107498407364\n",
            "step: 170, loss: 0.029644062742590904\n",
            "step: 180, loss: 0.06463753432035446\n",
            "step: 190, loss: 0.11520742624998093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.8053333333333333, f1=0.7978436657681941, best_f1=0.7978436657681941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10430026799440384\n",
            "step: 10, loss: 0.0819181427359581\n",
            "step: 20, loss: 0.01186447124928236\n",
            "step: 30, loss: 0.02777501568198204\n",
            "step: 40, loss: 0.015544845722615719\n",
            "step: 50, loss: 0.1386820375919342\n",
            "step: 60, loss: 0.05362938344478607\n",
            "step: 70, loss: 0.1509748101234436\n",
            "step: 80, loss: 0.3041498363018036\n",
            "step: 90, loss: 0.06299461424350739\n",
            "step: 100, loss: 0.09403858333826065\n",
            "step: 110, loss: 0.27506664395332336\n",
            "step: 120, loss: 0.4146132171154022\n",
            "step: 130, loss: 0.10566169023513794\n",
            "step: 140, loss: 0.015924841165542603\n",
            "step: 150, loss: 0.251449316740036\n",
            "step: 160, loss: 0.05025660619139671\n",
            "step: 170, loss: 0.18330854177474976\n",
            "step: 180, loss: 0.16574588418006897\n",
            "step: 190, loss: 0.026820501312613487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8440860215053764, f1=0.8412256267409471, best_f1=0.8412256267409471\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012196077965199947\n",
            "step: 10, loss: 0.041403643786907196\n",
            "step: 20, loss: 0.008654399774968624\n",
            "step: 30, loss: 0.005533084738999605\n",
            "step: 40, loss: 0.07496985793113708\n",
            "step: 50, loss: 0.0618736557662487\n",
            "step: 60, loss: 0.08670750260353088\n",
            "step: 70, loss: 0.02920600026845932\n",
            "step: 80, loss: 0.009136198088526726\n",
            "step: 90, loss: 0.04643981531262398\n",
            "step: 100, loss: 0.12603391706943512\n",
            "step: 110, loss: 0.13550350069999695\n",
            "step: 120, loss: 0.015139274299144745\n",
            "step: 130, loss: 0.03908596187829971\n",
            "step: 140, loss: 0.1528458148241043\n",
            "step: 150, loss: 0.1456831842660904\n",
            "step: 160, loss: 0.02810279093682766\n",
            "step: 170, loss: 0.010540150105953217\n",
            "step: 180, loss: 0.08933249115943909\n",
            "step: 190, loss: 0.007950286380946636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8587570621468926, f1=0.8387096774193548, best_f1=0.8387096774193548\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06571350246667862\n",
            "step: 10, loss: 0.08819311857223511\n",
            "step: 20, loss: 0.1013399139046669\n",
            "step: 30, loss: 0.029008865356445312\n",
            "step: 40, loss: 0.05145685747265816\n",
            "step: 50, loss: 0.0390777625143528\n",
            "step: 60, loss: 0.0038084520492702723\n",
            "step: 70, loss: 0.02018168568611145\n",
            "step: 80, loss: 0.006997869350016117\n",
            "step: 90, loss: 0.11177463829517365\n",
            "step: 100, loss: 0.1216685026884079\n",
            "step: 110, loss: 0.048943184316158295\n",
            "step: 120, loss: 0.01852243021130562\n",
            "step: 130, loss: 0.26327428221702576\n",
            "step: 140, loss: 0.016824128106236458\n",
            "step: 150, loss: 0.014861969277262688\n",
            "step: 160, loss: 0.004745223093777895\n",
            "step: 170, loss: 0.0016256296075880527\n",
            "step: 180, loss: 0.005026835482567549\n",
            "step: 190, loss: 0.002178947441279888\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8726287262872628, f1=0.8435754189944134, best_f1=0.8435754189944134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18163377046585083\n",
            "step: 10, loss: 0.03053869865834713\n",
            "step: 20, loss: 0.0023372843861579895\n",
            "step: 30, loss: 0.08204340934753418\n",
            "step: 40, loss: 0.02993318811058998\n",
            "step: 50, loss: 0.05664385110139847\n",
            "step: 60, loss: 0.021462345495820045\n",
            "step: 70, loss: 0.10706944018602371\n",
            "step: 80, loss: 0.04957476258277893\n",
            "step: 90, loss: 0.009258344769477844\n",
            "step: 100, loss: 0.09724009037017822\n",
            "step: 110, loss: 0.0032394311856478453\n",
            "step: 120, loss: 0.007947885431349277\n",
            "step: 130, loss: 0.08003837615251541\n",
            "step: 140, loss: 0.007267233915627003\n",
            "step: 150, loss: 0.0032208398915827274\n",
            "step: 160, loss: 0.04210694134235382\n",
            "step: 170, loss: 0.10150128602981567\n",
            "step: 180, loss: 0.008111303672194481\n",
            "step: 190, loss: 0.003746042726561427\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8595041322314049, f1=0.8491620111731845, best_f1=0.8435754189944134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007070232182741165\n",
            "step: 10, loss: 0.0457807257771492\n",
            "step: 20, loss: 0.0025475851725786924\n",
            "step: 30, loss: 0.00276504666544497\n",
            "step: 40, loss: 0.004332822747528553\n",
            "step: 50, loss: 0.0022017110604792833\n",
            "step: 60, loss: 0.03666781261563301\n",
            "step: 70, loss: 0.0023923651315271854\n",
            "step: 80, loss: 0.004204300232231617\n",
            "step: 90, loss: 0.0013648758176714182\n",
            "step: 100, loss: 0.3872354328632355\n",
            "step: 110, loss: 0.08815612643957138\n",
            "step: 120, loss: 0.17974452674388885\n",
            "step: 130, loss: 0.013019341044127941\n",
            "step: 140, loss: 0.03554774075746536\n",
            "step: 150, loss: 0.01948402263224125\n",
            "step: 160, loss: 0.013302190229296684\n",
            "step: 170, loss: 0.008031359873712063\n",
            "step: 180, loss: 0.014486439526081085\n",
            "step: 190, loss: 0.012200187891721725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8438356164383561, f1=0.8547008547008547, best_f1=0.8435754189944134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003078218549489975\n",
            "step: 10, loss: 0.023428218439221382\n",
            "step: 20, loss: 0.002928889822214842\n",
            "step: 30, loss: 0.20733323693275452\n",
            "step: 40, loss: 0.012424931861460209\n",
            "step: 50, loss: 0.00643658172339201\n",
            "step: 60, loss: 0.030478404834866524\n",
            "step: 70, loss: 0.004563593305647373\n",
            "step: 80, loss: 0.014528967440128326\n",
            "step: 90, loss: 0.0022047569509595633\n",
            "step: 100, loss: 0.03031698428094387\n",
            "step: 110, loss: 0.037798065692186356\n",
            "step: 120, loss: 0.0012034053215757012\n",
            "step: 130, loss: 0.04326840490102768\n",
            "step: 140, loss: 0.003189951414242387\n",
            "step: 150, loss: 0.010504703037440777\n",
            "step: 160, loss: 0.001041662646457553\n",
            "step: 170, loss: 0.001498694997280836\n",
            "step: 180, loss: 0.02902979962527752\n",
            "step: 190, loss: 0.002303674817085266\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8346883468834688, f1=0.8435754189944134, best_f1=0.8435754189944134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.035174690186977386\n",
            "step: 10, loss: 0.000307630019960925\n",
            "step: 20, loss: 0.0018272591987624764\n",
            "step: 30, loss: 0.0021156154107302427\n",
            "step: 40, loss: 0.04111682251095772\n",
            "step: 50, loss: 0.002634285483509302\n",
            "step: 60, loss: 0.004903859458863735\n",
            "step: 70, loss: 0.0017676305724307895\n",
            "step: 80, loss: 0.003615631489083171\n",
            "step: 90, loss: 0.05995938926935196\n",
            "step: 100, loss: 0.0008515265653841197\n",
            "step: 110, loss: 0.0012091518146917224\n",
            "step: 120, loss: 0.0019227721495553851\n",
            "step: 130, loss: 0.00037686442374251783\n",
            "step: 140, loss: 0.07749998569488525\n",
            "step: 150, loss: 0.0004198602691758424\n",
            "step: 160, loss: 0.00019794091349467635\n",
            "step: 170, loss: 0.0023243073374032974\n",
            "step: 180, loss: 0.0029000621289014816\n",
            "step: 190, loss: 0.0004041357315145433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8484848484848485, f1=0.8350515463917526, best_f1=0.8435754189944134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000441745767602697\n",
            "step: 10, loss: 0.0009777627419680357\n",
            "step: 20, loss: 0.0004329691582825035\n",
            "step: 30, loss: 0.002772530307993293\n",
            "step: 40, loss: 0.004293725360184908\n",
            "step: 50, loss: 0.000415921735111624\n",
            "step: 60, loss: 0.0012058160500600934\n",
            "step: 70, loss: 0.0002399195946054533\n",
            "step: 80, loss: 0.0009769065072759986\n",
            "step: 90, loss: 0.0001507240376668051\n",
            "step: 100, loss: 0.0003100273315794766\n",
            "step: 110, loss: 0.00043363080476410687\n",
            "step: 120, loss: 0.003960315603762865\n",
            "step: 130, loss: 0.00023523395066149533\n",
            "step: 140, loss: 0.004414531402289867\n",
            "step: 150, loss: 0.0002859215310309082\n",
            "step: 160, loss: 0.00043288161396048963\n",
            "step: 170, loss: 0.0004488361009862274\n",
            "step: 180, loss: 0.0036339040379971266\n",
            "step: 190, loss: 0.000283699861029163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8431876606683805, f1=0.8602150537634409, best_f1=0.8435754189944134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028503856156021357\n",
            "step: 10, loss: 0.006059486418962479\n",
            "step: 20, loss: 0.00013576375204138458\n",
            "step: 30, loss: 0.02117305062711239\n",
            "step: 40, loss: 0.000304176879581064\n",
            "step: 50, loss: 0.004768006037920713\n",
            "step: 60, loss: 0.02300337329506874\n",
            "step: 70, loss: 0.0002822637907229364\n",
            "step: 80, loss: 0.016132649034261703\n",
            "step: 90, loss: 0.0001798134035198018\n",
            "step: 100, loss: 0.00047653206274844706\n",
            "step: 110, loss: 0.026944240555167198\n",
            "step: 120, loss: 0.0005653008120134473\n",
            "step: 130, loss: 0.0013309090863913298\n",
            "step: 140, loss: 0.00031751723145134747\n",
            "step: 150, loss: 0.012552346102893353\n",
            "step: 160, loss: 0.00032633685623295605\n",
            "step: 170, loss: 0.011996421962976456\n",
            "step: 180, loss: 0.0007712949300184846\n",
            "step: 190, loss: 0.0001713693782221526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8296296296296296, f1=0.8491048593350383, best_f1=0.8435754189944134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011317159980535507\n",
            "step: 10, loss: 0.0019701814744621515\n",
            "step: 20, loss: 0.0007854156428948045\n",
            "step: 30, loss: 0.003056248649954796\n",
            "step: 40, loss: 0.0012115953722968698\n",
            "step: 50, loss: 0.030312126502394676\n",
            "step: 60, loss: 0.00267288857139647\n",
            "step: 70, loss: 0.00040095497388392687\n",
            "step: 80, loss: 0.0014374115271493793\n",
            "step: 90, loss: 0.00153733033221215\n",
            "step: 100, loss: 0.00022790675575379282\n",
            "step: 110, loss: 0.005210140720009804\n",
            "step: 120, loss: 0.0007748683565296233\n",
            "step: 130, loss: 0.00014939196989871562\n",
            "step: 140, loss: 0.00038172377389855683\n",
            "step: 150, loss: 0.0020291877444833517\n",
            "step: 160, loss: 0.00015173810243140906\n",
            "step: 170, loss: 0.018045969307422638\n",
            "step: 180, loss: 0.0002315307647222653\n",
            "step: 190, loss: 0.0005516273668035865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8304668304668306, f1=0.8337595907928389, best_f1=0.8435754189944134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0072253113612532616\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 10, loss: 0.00018825425649993122\n",
            "step: 20, loss: 0.0003494689881335944\n",
            "step: 30, loss: 0.0014583944575861096\n",
            "step: 40, loss: 0.001912241568788886\n",
            "step: 50, loss: 0.002253051148727536\n",
            "step: 60, loss: 0.001476187608204782\n",
            "step: 70, loss: 0.0009532948606647551\n",
            "step: 80, loss: 7.796590216457844e-05\n",
            "step: 90, loss: 0.0013773003593087196\n",
            "step: 100, loss: 0.00021450901112984866\n",
            "step: 110, loss: 0.00304462225176394\n",
            "step: 120, loss: 0.00014472039765678346\n",
            "step: 130, loss: 0.0036153574474155903\n",
            "step: 140, loss: 0.018948310986161232\n",
            "step: 150, loss: 0.00026657592388801277\n",
            "step: 160, loss: 0.0002128937339875847\n",
            "step: 170, loss: 7.832504343241453e-05\n",
            "step: 180, loss: 5.87660979363136e-05\n",
            "step: 190, loss: 0.0255234744399786\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8235294117647058, f1=0.8455284552845528, best_f1=0.8435754189944134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00026374959270469844\n",
            "step: 10, loss: 0.00022875827562529594\n",
            "step: 20, loss: 0.0003964862262364477\n",
            "step: 30, loss: 0.00021668709814548492\n",
            "step: 40, loss: 0.0002521632704883814\n",
            "step: 50, loss: 0.003840785939246416\n",
            "step: 60, loss: 0.00232509383931756\n",
            "step: 70, loss: 0.00016281232819892466\n",
            "step: 80, loss: 0.00011142776202177629\n",
            "step: 90, loss: 6.492544343927875e-05\n",
            "step: 100, loss: 0.0003178496262989938\n",
            "step: 110, loss: 0.00011529131734278053\n",
            "step: 120, loss: 0.0010709440102800727\n",
            "step: 130, loss: 0.0003099295718129724\n",
            "step: 140, loss: 0.008497812785208225\n",
            "step: 150, loss: 0.0003062024188693613\n",
            "step: 160, loss: 0.0001283810706809163\n",
            "step: 170, loss: 0.0004020064661744982\n",
            "step: 180, loss: 0.00023850180150475353\n",
            "step: 190, loss: 0.0006263049435801804\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8333333333333334, f1=0.8496042216358839, best_f1=0.8435754189944134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005821309750899673\n",
            "step: 10, loss: 0.0004559689259622246\n",
            "step: 20, loss: 0.0008135998505167663\n",
            "step: 30, loss: 0.011919530108571053\n",
            "step: 40, loss: 0.0016494556330144405\n",
            "step: 50, loss: 8.044744026847184e-05\n",
            "step: 60, loss: 7.13382542016916e-05\n",
            "step: 70, loss: 0.00012505028280429542\n",
            "step: 80, loss: 0.00039057235699146986\n",
            "step: 90, loss: 0.003450939664617181\n",
            "step: 100, loss: 0.0017709850799292326\n",
            "step: 110, loss: 0.003291927045211196\n",
            "step: 120, loss: 0.0013241184642538428\n",
            "step: 130, loss: 0.0001482185034547001\n",
            "step: 140, loss: 0.0037165724206715822\n",
            "step: 150, loss: 7.465408998541534e-05\n",
            "step: 160, loss: 0.13664965331554413\n",
            "step: 170, loss: 0.0004993080510757864\n",
            "step: 180, loss: 9.233065793523565e-05\n",
            "step: 190, loss: 0.00015885428001638502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8346055979643766, f1=0.851851851851852, best_f1=0.8435754189944134\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:10, 189.66it/s]\n",
            "load_f1 = 0.8695652173913043\n",
            "real_f1 = 0.875\n",
            "733it [00:00, 3284.74it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:26, 167.30it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWkqC6MWgNFO"
      },
      "source": [
        "## BASELINE TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPR9KRSgNFO"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62jt5GiEgNFO",
        "outputId": "58c13f16-8f4d-4147-b408-d9d1b451bb49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4927980899810791\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.506746768951416\n",
            "step: 20, loss: 0.2683795988559723\n",
            "step: 30, loss: 0.4324023127555847\n",
            "step: 40, loss: 0.5834211111068726\n",
            "step: 50, loss: 0.3139124810695648\n",
            "step: 60, loss: 0.5098637342453003\n",
            "step: 70, loss: 0.2941877543926239\n",
            "step: 80, loss: 0.23739448189735413\n",
            "step: 90, loss: 0.24043980240821838\n",
            "step: 100, loss: 0.15081319212913513\n",
            "step: 110, loss: 0.39856258034706116\n",
            "step: 120, loss: 0.31101495027542114\n",
            "step: 130, loss: 0.31016021966934204\n",
            "step: 140, loss: 0.3728073835372925\n",
            "step: 150, loss: 0.32457566261291504\n",
            "step: 160, loss: 0.4091988503932953\n",
            "step: 170, loss: 0.29086926579475403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.20689655172413793, f1=0.21552205471803462, best_f1=0.21552205471803462\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3098354637622833\n",
            "step: 10, loss: 0.502065896987915\n",
            "step: 20, loss: 0.31471091508865356\n",
            "step: 30, loss: 0.30217939615249634\n",
            "step: 40, loss: 0.06906390935182571\n",
            "step: 50, loss: 0.38798823952674866\n",
            "step: 60, loss: 0.2103300541639328\n",
            "step: 70, loss: 0.4627819061279297\n",
            "step: 80, loss: 0.2075711488723755\n",
            "step: 90, loss: 0.250030517578125\n",
            "step: 100, loss: 0.5040295720100403\n",
            "step: 110, loss: 0.2607443034648895\n",
            "step: 120, loss: 0.12326718121767044\n",
            "step: 130, loss: 0.4006756544113159\n",
            "step: 140, loss: 0.20099148154258728\n",
            "step: 150, loss: 0.1876859813928604\n",
            "step: 160, loss: 0.1403547078371048\n",
            "step: 170, loss: 0.13615286350250244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7226107226107227, f1=0.7710843373493976, best_f1=0.7710843373493976\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2151722013950348\n",
            "step: 10, loss: 0.07751200348138809\n",
            "step: 20, loss: 0.08528495579957962\n",
            "step: 30, loss: 0.04514985904097557\n",
            "step: 40, loss: 0.12010633945465088\n",
            "step: 50, loss: 0.3254382610321045\n",
            "step: 60, loss: 0.15003792941570282\n",
            "step: 70, loss: 0.11698716878890991\n",
            "step: 80, loss: 0.22861778736114502\n",
            "step: 90, loss: 0.21312935650348663\n",
            "step: 100, loss: 0.034943122416734695\n",
            "step: 110, loss: 0.08378417044878006\n",
            "step: 120, loss: 0.12836842238903046\n",
            "step: 130, loss: 0.28926199674606323\n",
            "step: 140, loss: 0.13006530702114105\n",
            "step: 150, loss: 0.098650261759758\n",
            "step: 160, loss: 0.09669459611177444\n",
            "step: 170, loss: 0.11167342960834503\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8100686498855835, f1=0.8258928571428572, best_f1=0.8258928571428572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0765453353524208\n",
            "step: 10, loss: 0.05433601513504982\n",
            "step: 20, loss: 0.06906924396753311\n",
            "step: 30, loss: 0.11065566539764404\n",
            "step: 40, loss: 0.040510278195142746\n",
            "step: 50, loss: 0.024550259113311768\n",
            "step: 60, loss: 0.25847327709198\n",
            "step: 70, loss: 0.008706070482730865\n",
            "step: 80, loss: 0.33313602209091187\n",
            "step: 90, loss: 0.18621058762073517\n",
            "step: 100, loss: 0.31286197900772095\n",
            "step: 110, loss: 0.2126898467540741\n",
            "step: 120, loss: 0.2427627444267273\n",
            "step: 130, loss: 0.058609992265701294\n",
            "step: 140, loss: 0.07541526108980179\n",
            "step: 150, loss: 0.14573635160923004\n",
            "step: 160, loss: 0.00519581139087677\n",
            "step: 170, loss: 0.11885125935077667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.8019559902200489, f1=0.828235294117647, best_f1=0.8258928571428572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030220042914152145\n",
            "step: 10, loss: 0.04993230849504471\n",
            "step: 20, loss: 0.14744240045547485\n",
            "step: 30, loss: 0.005016608163714409\n",
            "step: 40, loss: 0.06700095534324646\n",
            "step: 50, loss: 0.016223788261413574\n",
            "step: 60, loss: 0.05584670975804329\n",
            "step: 70, loss: 0.016683947294950485\n",
            "step: 80, loss: 0.0008620894514024258\n",
            "step: 90, loss: 0.014981183223426342\n",
            "step: 100, loss: 0.02048044465482235\n",
            "step: 110, loss: 0.138109490275383\n",
            "step: 120, loss: 0.07183363288640976\n",
            "step: 130, loss: 0.002650853479281068\n",
            "step: 140, loss: 0.1531028151512146\n",
            "step: 150, loss: 0.06517212837934494\n",
            "step: 160, loss: 0.11224613338708878\n",
            "step: 170, loss: 0.007258345372974873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8431876606683805, f1=0.865, best_f1=0.865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15723751485347748\n",
            "step: 10, loss: 0.006759663112461567\n",
            "step: 20, loss: 0.08517920970916748\n",
            "step: 30, loss: 0.009142708033323288\n",
            "step: 40, loss: 0.0014894322957843542\n",
            "step: 50, loss: 0.0026381092611700296\n",
            "step: 60, loss: 0.038307517766952515\n",
            "step: 70, loss: 0.02016925998032093\n",
            "step: 80, loss: 0.025154974311590195\n",
            "step: 90, loss: 0.07124143093824387\n",
            "step: 100, loss: 0.01437375321984291\n",
            "step: 110, loss: 0.044591426849365234\n",
            "step: 120, loss: 0.01812789961695671\n",
            "step: 130, loss: 0.016258087009191513\n",
            "step: 140, loss: 0.05924302339553833\n",
            "step: 150, loss: 0.012889509089291096\n",
            "step: 160, loss: 0.07298707962036133\n",
            "step: 170, loss: 0.09422429651021957\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8346456692913385, f1=0.8678304239401496, best_f1=0.865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024134287610650063\n",
            "step: 10, loss: 0.04112136363983154\n",
            "step: 20, loss: 0.06649848818778992\n",
            "step: 30, loss: 0.1459527164697647\n",
            "step: 40, loss: 0.0006739392993040383\n",
            "step: 50, loss: 0.0024040043354034424\n",
            "step: 60, loss: 0.02265765704214573\n",
            "step: 70, loss: 0.006506824400275946\n",
            "step: 80, loss: 0.09132082015275955\n",
            "step: 90, loss: 0.03471163660287857\n",
            "step: 100, loss: 0.0007333848625421524\n",
            "step: 110, loss: 0.0052372668869793415\n",
            "step: 120, loss: 0.004505984950810671\n",
            "step: 130, loss: 0.003068433376029134\n",
            "step: 140, loss: 0.02546403557062149\n",
            "step: 150, loss: 0.2513065040111542\n",
            "step: 160, loss: 0.012910578399896622\n",
            "step: 170, loss: 0.03928692638874054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8457446808510638, f1=0.863157894736842, best_f1=0.863157894736842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031876340508461\n",
            "step: 10, loss: 0.014349891804158688\n",
            "step: 20, loss: 0.015205735340714455\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 30, loss: 0.0005048703169450164\n",
            "step: 40, loss: 0.0016221872065216303\n",
            "step: 50, loss: 0.0009536448633298278\n",
            "step: 60, loss: 0.001513222698122263\n",
            "step: 70, loss: 0.055499959737062454\n",
            "step: 80, loss: 0.0015073855174705386\n",
            "step: 90, loss: 0.07435086369514465\n",
            "step: 100, loss: 0.001854547648690641\n",
            "step: 110, loss: 0.039785340428352356\n",
            "step: 120, loss: 0.16412346065044403\n",
            "step: 130, loss: 0.015198457054793835\n",
            "step: 140, loss: 0.07512449473142624\n",
            "step: 150, loss: 0.007908460684120655\n",
            "step: 160, loss: 0.007866433821618557\n",
            "step: 170, loss: 0.2661641240119934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.837092731829574, f1=0.8656716417910448, best_f1=0.863157894736842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031331513077020645\n",
            "step: 10, loss: 0.0026842367369681597\n",
            "step: 20, loss: 0.005249055102467537\n",
            "step: 30, loss: 0.13650935888290405\n",
            "step: 40, loss: 0.01827574335038662\n",
            "step: 50, loss: 0.000511219201143831\n",
            "step: 60, loss: 0.015105051919817924\n",
            "step: 70, loss: 0.022621184587478638\n",
            "step: 80, loss: 0.0010884527582675219\n",
            "step: 90, loss: 0.0628746822476387\n",
            "step: 100, loss: 0.0028970325365662575\n",
            "step: 110, loss: 0.018901729956269264\n",
            "step: 120, loss: 0.035477980971336365\n",
            "step: 130, loss: 0.10677433758974075\n",
            "step: 140, loss: 0.010059287771582603\n",
            "step: 150, loss: 0.1481744945049286\n",
            "step: 160, loss: 0.0012282804818823934\n",
            "step: 170, loss: 0.012609321624040604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8614609571788413, f1=0.8480392156862744, best_f1=0.8480392156862744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001732461852952838\n",
            "step: 10, loss: 0.0007770452066324651\n",
            "step: 20, loss: 0.007123639807105064\n",
            "step: 30, loss: 0.052801914513111115\n",
            "step: 40, loss: 0.00835336372256279\n",
            "step: 50, loss: 0.004934818483889103\n",
            "step: 60, loss: 0.11990700662136078\n",
            "step: 70, loss: 0.0014050822937861085\n",
            "step: 80, loss: 0.0025253077037632465\n",
            "step: 90, loss: 0.0020732220727950335\n",
            "step: 100, loss: 0.08864671736955643\n",
            "step: 110, loss: 0.007185428403317928\n",
            "step: 120, loss: 0.0004529857251327485\n",
            "step: 130, loss: 0.005438453517854214\n",
            "step: 140, loss: 0.0006887318450026214\n",
            "step: 150, loss: 0.0007454646402038634\n",
            "step: 160, loss: 0.000451472878921777\n",
            "step: 170, loss: 0.041370779275894165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8396946564885497, f1=0.8457711442786069, best_f1=0.8480392156862744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02477400377392769\n",
            "step: 10, loss: 0.00026119500398635864\n",
            "step: 20, loss: 0.0005259381723590195\n",
            "step: 30, loss: 0.024911176413297653\n",
            "step: 40, loss: 0.00019356599659658968\n",
            "step: 50, loss: 0.003404487855732441\n",
            "step: 60, loss: 0.006508774124085903\n",
            "step: 70, loss: 0.014744502492249012\n",
            "step: 80, loss: 0.010667501017451286\n",
            "step: 90, loss: 0.0015929752262309194\n",
            "step: 100, loss: 0.021667638793587685\n",
            "step: 110, loss: 0.03843063488602638\n",
            "step: 120, loss: 0.000342807819833979\n",
            "step: 130, loss: 0.0037507587112486362\n",
            "step: 140, loss: 0.0021164279896765947\n",
            "step: 150, loss: 0.002210471546277404\n",
            "step: 160, loss: 0.004888392053544521\n",
            "step: 170, loss: 0.004244806244969368\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8563968668407311, f1=0.8571428571428572, best_f1=0.8480392156862744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002088752808049321\n",
            "step: 10, loss: 0.002662366023287177\n",
            "step: 20, loss: 0.00025564388488419354\n",
            "step: 30, loss: 0.04060085862874985\n",
            "step: 40, loss: 0.001857794588431716\n",
            "step: 50, loss: 0.021384939551353455\n",
            "step: 60, loss: 0.0013295153621584177\n",
            "step: 70, loss: 0.00012589061225298792\n",
            "step: 80, loss: 0.00011943963181693107\n",
            "step: 90, loss: 0.0023518933448940516\n",
            "step: 100, loss: 0.00024096027482300997\n",
            "step: 110, loss: 0.0008559920242987573\n",
            "step: 120, loss: 0.02842424437403679\n",
            "step: 130, loss: 0.003981643822044134\n",
            "step: 140, loss: 0.0008789505809545517\n",
            "step: 150, loss: 0.00023917178623378277\n",
            "step: 160, loss: 0.002178216353058815\n",
            "step: 170, loss: 0.00022056377201806754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8439897698209718, f1=0.8542713567839195, best_f1=0.8480392156862744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00027993781259283423\n",
            "step: 10, loss: 0.00010470645065652207\n",
            "step: 20, loss: 0.0005281917983666062\n",
            "step: 30, loss: 0.08223573863506317\n",
            "step: 40, loss: 0.011291075497865677\n",
            "step: 50, loss: 0.007273172494024038\n",
            "step: 60, loss: 0.0018048674101009965\n",
            "step: 70, loss: 0.16961129009723663\n",
            "step: 80, loss: 0.0038053905591368675\n",
            "step: 90, loss: 0.0005355463363230228\n",
            "step: 100, loss: 0.011047613807022572\n",
            "step: 110, loss: 0.0010836946312338114\n",
            "step: 120, loss: 0.024123454466462135\n",
            "step: 130, loss: 8.826397970551625e-05\n",
            "step: 140, loss: 0.007824594154953957\n",
            "step: 150, loss: 0.0008675375720486045\n",
            "step: 160, loss: 9.459521243115887e-05\n",
            "step: 170, loss: 0.00012105962377972901\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8431876606683805, f1=0.8673469387755103, best_f1=0.8480392156862744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010429563699290156\n",
            "step: 10, loss: 0.00016762386076152325\n",
            "step: 20, loss: 0.0019091746071353555\n",
            "step: 30, loss: 0.00016114673053380102\n",
            "step: 40, loss: 0.00010565127740846947\n",
            "step: 50, loss: 0.0008915156358852983\n",
            "step: 60, loss: 0.04592052847146988\n",
            "step: 70, loss: 0.00022650834580417722\n",
            "step: 80, loss: 0.00015852306387387216\n",
            "step: 90, loss: 0.00015015651297289878\n",
            "step: 100, loss: 0.009401382878422737\n",
            "step: 110, loss: 0.017207255586981773\n",
            "step: 120, loss: 8.41872242745012e-05\n",
            "step: 130, loss: 0.020978428423404694\n",
            "step: 140, loss: 0.01890026591718197\n",
            "step: 150, loss: 0.01644166372716427\n",
            "step: 160, loss: 0.0004512538725975901\n",
            "step: 170, loss: 0.034649647772312164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8496042216358839, f1=0.8578811369509044, best_f1=0.8480392156862744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.058020178694278e-05\n",
            "step: 10, loss: 0.000382648897357285\n",
            "step: 20, loss: 0.00010371991083957255\n",
            "step: 30, loss: 0.0004079201607964933\n",
            "step: 40, loss: 8.79575964063406e-05\n",
            "step: 50, loss: 0.0002533577790018171\n",
            "step: 60, loss: 0.00011132531653856859\n",
            "step: 70, loss: 0.07320334017276764\n",
            "step: 80, loss: 0.0001092962411348708\n",
            "step: 90, loss: 0.002780991606414318\n",
            "step: 100, loss: 0.014539874158799648\n",
            "step: 110, loss: 0.0011272523552179337\n",
            "step: 120, loss: 0.00022086936223786324\n",
            "step: 130, loss: 0.00850137323141098\n",
            "step: 140, loss: 0.003338359296321869\n",
            "step: 150, loss: 8.536434324923903e-05\n",
            "step: 160, loss: 0.00010452017158968374\n",
            "step: 170, loss: 8.21919966256246e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8534031413612565, f1=0.8637532133676092, best_f1=0.8480392156862744\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 220.19it/s]\n",
            "load_f1 = 0.8585858585858587\n",
            "real_f1 = 0.8614609571788413\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:33, 132.94it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djX3yHRNgNFP"
      },
      "source": [
        "## BASELINE DIRTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b011EMgogNFP"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_ai4a3YgNFQ",
        "outputId": "80625f81-ab58-4b15-8b09-b1ddc15a5216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5871580243110657\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.5137525796890259\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.4891977310180664\n",
            "step: 30, loss: 0.2751196622848511\n",
            "step: 40, loss: 0.35520192980766296\n",
            "step: 50, loss: 0.5955165028572083\n",
            "step: 60, loss: 0.46071359515190125\n",
            "step: 70, loss: 0.24552886188030243\n",
            "step: 80, loss: 0.25314459204673767\n",
            "step: 90, loss: 0.27322879433631897\n",
            "step: 100, loss: 0.08137717843055725\n",
            "step: 110, loss: 0.15810725092887878\n",
            "step: 120, loss: 0.10213150084018707\n",
            "step: 130, loss: 0.007346839644014835\n",
            "step: 140, loss: 0.039301976561546326\n",
            "step: 150, loss: 0.18074433505535126\n",
            "step: 160, loss: 0.034220825880765915\n",
            "step: 170, loss: 0.45716696977615356\n",
            "step: 180, loss: 0.07460378110408783\n",
            "step: 190, loss: 0.12976565957069397\n",
            "step: 200, loss: 0.0470656156539917\n",
            "step: 210, loss: 0.030541911721229553\n",
            "step: 220, loss: 0.06526480615139008\n",
            "step: 230, loss: 0.006570532917976379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9406494960806272, f1=0.9479638009049774, best_f1=0.9479638009049774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008920794352889061\n",
            "step: 10, loss: 0.08796384185552597\n",
            "step: 20, loss: 0.0260833278298378\n",
            "step: 30, loss: 0.02411065623164177\n",
            "step: 40, loss: 0.11939213424921036\n",
            "step: 50, loss: 0.012849947437644005\n",
            "step: 60, loss: 0.20185662806034088\n",
            "step: 70, loss: 0.0223771333694458\n",
            "step: 80, loss: 0.0026332526467740536\n",
            "step: 90, loss: 0.0440317802131176\n",
            "step: 100, loss: 0.006831542123109102\n",
            "step: 110, loss: 0.038581475615501404\n",
            "step: 120, loss: 0.042563870549201965\n",
            "step: 130, loss: 0.034924183040857315\n",
            "step: 140, loss: 0.006286506541073322\n",
            "step: 150, loss: 0.11001506447792053\n",
            "step: 160, loss: 0.034149229526519775\n",
            "step: 170, loss: 0.004080904182046652\n",
            "step: 180, loss: 0.030607689172029495\n",
            "step: 190, loss: 0.11431781202554703\n",
            "step: 200, loss: 0.021384285762906075\n",
            "step: 210, loss: 0.03007636032998562\n",
            "step: 220, loss: 0.0022699208930134773\n",
            "step: 230, loss: 0.0018830448389053345\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9580973952434882, f1=0.9635535307517085, best_f1=0.9635535307517085\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0070838541723787785\n",
            "step: 10, loss: 0.009043838828802109\n",
            "step: 20, loss: 0.026878954842686653\n",
            "step: 30, loss: 0.01494133286178112\n",
            "step: 40, loss: 0.0059510208666324615\n",
            "step: 50, loss: 0.03934796527028084\n",
            "step: 60, loss: 0.050901491194963455\n",
            "step: 70, loss: 0.013216632418334484\n",
            "step: 80, loss: 0.055474162101745605\n",
            "step: 90, loss: 0.013955998234450817\n",
            "step: 100, loss: 0.01365667860955\n",
            "step: 110, loss: 0.02340252511203289\n",
            "step: 120, loss: 0.0023278738372027874\n",
            "step: 130, loss: 0.024031780660152435\n",
            "step: 140, loss: 0.0017260591266676784\n",
            "step: 150, loss: 0.2440161406993866\n",
            "step: 160, loss: 0.011384967714548111\n",
            "step: 170, loss: 0.0022052687127143145\n",
            "step: 180, loss: 0.030777446925640106\n",
            "step: 190, loss: 0.03370775282382965\n",
            "step: 200, loss: 0.040587931871414185\n",
            "step: 210, loss: 0.004919370170682669\n",
            "step: 220, loss: 0.06871994584798813\n",
            "step: 230, loss: 0.013214179314672947\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9594594594594594, f1=0.963963963963964, best_f1=0.963963963963964\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014786751940846443\n",
            "step: 10, loss: 0.005281464662402868\n",
            "step: 20, loss: 0.004254079423844814\n",
            "step: 30, loss: 0.008529792539775372\n",
            "step: 40, loss: 0.12511146068572998\n",
            "step: 50, loss: 0.024047890678048134\n",
            "step: 60, loss: 0.009002722799777985\n",
            "step: 70, loss: 0.02278275415301323\n",
            "step: 80, loss: 0.10580209642648697\n",
            "step: 90, loss: 0.04931539297103882\n",
            "step: 100, loss: 0.020857729017734528\n",
            "step: 110, loss: 0.001642333809286356\n",
            "step: 120, loss: 0.006347838789224625\n",
            "step: 130, loss: 0.0072830067947506905\n",
            "step: 140, loss: 0.010602159425616264\n",
            "step: 150, loss: 0.002740278374403715\n",
            "step: 160, loss: 0.007970079779624939\n",
            "step: 170, loss: 0.0035098684020340443\n",
            "step: 180, loss: 0.0995093509554863\n",
            "step: 190, loss: 0.004638928920030594\n",
            "step: 200, loss: 0.047722455114126205\n",
            "step: 210, loss: 0.07164303213357925\n",
            "step: 220, loss: 0.0014948055613785982\n",
            "step: 230, loss: 0.006435055751353502\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9715585893060296, f1=0.9679633867276889, best_f1=0.9679633867276889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0030482877045869827\n",
            "step: 10, loss: 0.004116538446396589\n",
            "step: 20, loss: 0.01380497869104147\n",
            "step: 30, loss: 0.010803724639117718\n",
            "step: 40, loss: 0.005996274296194315\n",
            "step: 50, loss: 0.004333447199314833\n",
            "step: 60, loss: 0.001763079664669931\n",
            "step: 70, loss: 0.0020985575392842293\n",
            "step: 80, loss: 0.22681088745594025\n",
            "step: 90, loss: 0.10256848484277725\n",
            "step: 100, loss: 0.0006381003186106682\n",
            "step: 110, loss: 0.0024785767309367657\n",
            "step: 120, loss: 0.0037858341820538044\n",
            "step: 130, loss: 0.0005777380429208279\n",
            "step: 140, loss: 0.004508154932409525\n",
            "step: 150, loss: 0.011732647195458412\n",
            "step: 160, loss: 0.00540114613249898\n",
            "step: 170, loss: 0.0034251476172357798\n",
            "step: 180, loss: 0.0034638661891222\n",
            "step: 190, loss: 0.01849030703306198\n",
            "step: 200, loss: 0.003394614439457655\n",
            "step: 210, loss: 0.002361403312534094\n",
            "step: 220, loss: 0.0019934538286179304\n",
            "step: 230, loss: 0.017496280372142792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9718785151856018, f1=0.9784335981838819, best_f1=0.9784335981838819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009893302340060472\n",
            "step: 10, loss: 0.0035584927536547184\n",
            "step: 20, loss: 0.015392660163342953\n",
            "step: 30, loss: 0.0008991702925413847\n",
            "step: 40, loss: 0.00044784456258639693\n",
            "step: 50, loss: 0.0013815519632771611\n",
            "step: 60, loss: 0.005987759679555893\n",
            "step: 70, loss: 0.0014661593595519662\n",
            "step: 80, loss: 0.002065319800749421\n",
            "step: 90, loss: 0.004806894343346357\n",
            "step: 100, loss: 0.0021124309860169888\n",
            "step: 110, loss: 0.005960024893283844\n",
            "step: 120, loss: 0.0006490078521892428\n",
            "step: 130, loss: 0.0010664473520591855\n",
            "step: 140, loss: 0.033939577639102936\n",
            "step: 150, loss: 0.00042402197141200304\n",
            "step: 160, loss: 0.003231536829844117\n",
            "step: 170, loss: 0.0013806025963276625\n",
            "step: 180, loss: 0.001987106166779995\n",
            "step: 190, loss: 0.00282041123136878\n",
            "step: 200, loss: 0.0028496619779616594\n",
            "step: 210, loss: 0.0007488343981094658\n",
            "step: 220, loss: 0.09240757673978806\n",
            "step: 230, loss: 0.00474504753947258\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9807474518686297, f1=0.9750566893424036, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00398636981844902\n",
            "step: 10, loss: 0.0019331941148266196\n",
            "step: 20, loss: 0.0007581542595289648\n",
            "step: 30, loss: 0.0004980515805073082\n",
            "step: 40, loss: 0.001130406279116869\n",
            "step: 50, loss: 0.0012665078975260258\n",
            "step: 60, loss: 0.001318678492680192\n",
            "step: 70, loss: 0.0005855453200638294\n",
            "step: 80, loss: 0.0009164768853224814\n",
            "step: 90, loss: 0.00041508369031362236\n",
            "step: 100, loss: 0.0003521550679579377\n",
            "step: 110, loss: 0.0005000985111109912\n",
            "step: 120, loss: 0.0006148521788418293\n",
            "step: 130, loss: 0.12910473346710205\n",
            "step: 140, loss: 0.0006902384920977056\n",
            "step: 150, loss: 0.21706551313400269\n",
            "step: 160, loss: 0.021126531064510345\n",
            "step: 170, loss: 0.00489415368065238\n",
            "step: 180, loss: 0.0023095114156603813\n",
            "step: 190, loss: 0.018550600856542587\n",
            "step: 200, loss: 0.0007655165391042829\n",
            "step: 210, loss: 0.06597058475017548\n",
            "step: 220, loss: 0.0007042507641017437\n",
            "step: 230, loss: 0.001331223757006228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9766407119021134, f1=0.967525195968645, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010274696396663785\n",
            "step: 10, loss: 0.0034720548428595066\n",
            "step: 20, loss: 0.003302976954728365\n",
            "step: 30, loss: 0.0013923302758485079\n",
            "step: 40, loss: 0.002282612957060337\n",
            "step: 50, loss: 0.0010143170366063714\n",
            "step: 60, loss: 0.0015508889919146895\n",
            "step: 70, loss: 0.00041776237776502967\n",
            "step: 80, loss: 0.022057926282286644\n",
            "step: 90, loss: 0.0008892167243175209\n",
            "step: 100, loss: 0.0012389144394546747\n",
            "step: 110, loss: 0.0006190964486449957\n",
            "step: 120, loss: 0.0005163397872820497\n",
            "step: 130, loss: 0.0009948507649824023\n",
            "step: 140, loss: 0.0005248537636362016\n",
            "step: 150, loss: 0.16841110587120056\n",
            "step: 160, loss: 0.000882919819559902\n",
            "step: 170, loss: 0.005936041008681059\n",
            "step: 180, loss: 0.0005744318477809429\n",
            "step: 190, loss: 0.002584804780781269\n",
            "step: 200, loss: 0.006113177631050348\n",
            "step: 210, loss: 0.016543185338377953\n",
            "step: 220, loss: 0.003713525366038084\n",
            "step: 230, loss: 0.0007449750555679202\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9787234042553192, f1=0.976324689966178, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012046161573380232\n",
            "step: 10, loss: 0.07617294043302536\n",
            "step: 20, loss: 0.0017328757094219327\n",
            "step: 30, loss: 0.002088341396301985\n",
            "step: 40, loss: 0.001452173339203\n",
            "step: 50, loss: 0.0012985066277906299\n",
            "step: 60, loss: 0.0014718278544023633\n",
            "step: 70, loss: 0.015908606350421906\n",
            "step: 80, loss: 0.0006997567252255976\n",
            "step: 90, loss: 0.024734269827604294\n",
            "step: 100, loss: 0.0010178226511925459\n",
            "step: 110, loss: 0.00021951855160295963\n",
            "step: 120, loss: 0.0008242623880505562\n",
            "step: 130, loss: 0.0010992089519277215\n",
            "step: 140, loss: 0.001246394356712699\n",
            "step: 150, loss: 0.07859258353710175\n",
            "step: 160, loss: 0.012821673415601254\n",
            "step: 170, loss: 0.0006191351567395031\n",
            "step: 180, loss: 0.02071942202746868\n",
            "step: 190, loss: 0.00039985880721360445\n",
            "step: 200, loss: 0.0011494260979816318\n",
            "step: 210, loss: 0.036492686718702316\n",
            "step: 220, loss: 0.0006233828607946634\n",
            "step: 230, loss: 0.0004282299196347594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9784335981838819, f1=0.9739524348810873, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006717591895721853\n",
            "step: 10, loss: 0.0015180568443611264\n",
            "step: 20, loss: 0.0009404206648468971\n",
            "step: 30, loss: 0.00028674150235019624\n",
            "step: 40, loss: 0.0012667254777625203\n",
            "step: 50, loss: 0.00028033062699250877\n",
            "step: 60, loss: 0.00051696109585464\n",
            "step: 70, loss: 0.0048556639812886715\n",
            "step: 80, loss: 0.01605396158993244\n",
            "step: 90, loss: 0.0006221953080967069\n",
            "step: 100, loss: 0.0012267997954040766\n",
            "step: 110, loss: 0.004075950477272272\n",
            "step: 120, loss: 0.00041695949039421976\n",
            "step: 130, loss: 0.00225185533054173\n",
            "step: 140, loss: 0.0005793216405436397\n",
            "step: 150, loss: 0.0016701502026990056\n",
            "step: 160, loss: 0.00027928908821195364\n",
            "step: 170, loss: 0.0004114377370569855\n",
            "step: 180, loss: 0.0011615591356530786\n",
            "step: 190, loss: 0.0006311158649623394\n",
            "step: 200, loss: 0.012393683195114136\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 210, loss: 0.0014624539762735367\n",
            "step: 220, loss: 0.0008750060806050897\n",
            "step: 230, loss: 0.000366578227840364\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9773242630385486, f1=0.9751131221719457, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00046319447574205697\n",
            "step: 10, loss: 0.002425061073154211\n",
            "step: 20, loss: 0.0004389541572891176\n",
            "step: 30, loss: 0.0008519343100488186\n",
            "step: 40, loss: 0.00011687118239933625\n",
            "step: 50, loss: 0.00019929588597733527\n",
            "step: 60, loss: 0.018143180757761\n",
            "step: 70, loss: 0.0004620740655809641\n",
            "step: 80, loss: 0.006663662847131491\n",
            "step: 90, loss: 0.20631712675094604\n",
            "step: 100, loss: 0.00130231655202806\n",
            "step: 110, loss: 0.0011630502995103598\n",
            "step: 120, loss: 0.0009255344630219042\n",
            "step: 130, loss: 0.0006694981129840016\n",
            "step: 140, loss: 0.009498958475887775\n",
            "step: 150, loss: 0.003711393568664789\n",
            "step: 160, loss: 0.008531942032277584\n",
            "step: 170, loss: 0.0009946717182174325\n",
            "step: 180, loss: 0.0008585212053731084\n",
            "step: 190, loss: 0.0005529208574444056\n",
            "step: 200, loss: 0.0012050268705934286\n",
            "step: 210, loss: 0.0007207072922028601\n",
            "step: 220, loss: 0.0011683411430567503\n",
            "step: 230, loss: 0.0008193366229534149\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.980963045912654, f1=0.9684684684684683, best_f1=0.9684684684684683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004547573626041412\n",
            "step: 10, loss: 0.0036095979157835245\n",
            "step: 20, loss: 0.004503483418375254\n",
            "step: 30, loss: 0.13923247158527374\n",
            "step: 40, loss: 0.0010060588829219341\n",
            "step: 50, loss: 0.002389013534411788\n",
            "step: 60, loss: 0.0011616303818300366\n",
            "step: 70, loss: 0.0009989994578063488\n",
            "step: 80, loss: 0.00019388018699828535\n",
            "step: 90, loss: 0.0008884332492016256\n",
            "step: 100, loss: 0.00023743242491036654\n",
            "step: 110, loss: 0.00021577210281975567\n",
            "step: 120, loss: 0.0004494485619943589\n",
            "step: 130, loss: 0.00039662656490691006\n",
            "step: 140, loss: 0.00094194570556283\n",
            "step: 150, loss: 0.0007643340504728258\n",
            "step: 160, loss: 0.002842253539711237\n",
            "step: 170, loss: 0.0007476579048670828\n",
            "step: 180, loss: 0.0007625713478773832\n",
            "step: 190, loss: 0.001397319370880723\n",
            "step: 200, loss: 0.0006303559057414532\n",
            "step: 210, loss: 0.0014670079108327627\n",
            "step: 220, loss: 0.0003020043659489602\n",
            "step: 230, loss: 0.0009369557956233621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9841628959276018, f1=0.971815107102593, best_f1=0.971815107102593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015509293880313635\n",
            "step: 10, loss: 0.0006351436022669077\n",
            "step: 20, loss: 0.0015771619509905577\n",
            "step: 30, loss: 0.00030298996716737747\n",
            "step: 40, loss: 0.0017216593259945512\n",
            "step: 50, loss: 0.004516018554568291\n",
            "step: 60, loss: 0.0006756550865247846\n",
            "step: 70, loss: 0.0005544899613596499\n",
            "step: 80, loss: 0.00037972588324919343\n",
            "step: 90, loss: 0.0007459760527126491\n",
            "step: 100, loss: 0.001282707555219531\n",
            "step: 110, loss: 0.002046257024630904\n",
            "step: 120, loss: 0.0010034984443336725\n",
            "step: 130, loss: 0.0009079648298211396\n",
            "step: 140, loss: 0.00027704695821739733\n",
            "step: 150, loss: 0.0002696705050766468\n",
            "step: 160, loss: 0.0016784326871857047\n",
            "step: 170, loss: 0.0014357691397890449\n",
            "step: 180, loss: 0.06384280323982239\n",
            "step: 190, loss: 0.0011806460097432137\n",
            "step: 200, loss: 0.00011258203448960558\n",
            "step: 210, loss: 0.004909297451376915\n",
            "step: 220, loss: 0.0006586180534213781\n",
            "step: 230, loss: 0.0013112318702042103\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9771689497716894, f1=0.9715585893060296, best_f1=0.971815107102593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004732392553705722\n",
            "step: 10, loss: 0.0003285110287833959\n",
            "step: 20, loss: 0.0004778598085977137\n",
            "step: 30, loss: 0.0007353800465352833\n",
            "step: 40, loss: 0.0007251208880916238\n",
            "step: 50, loss: 0.00028160272631794214\n",
            "step: 60, loss: 0.0006174191366881132\n",
            "step: 70, loss: 0.000519675319083035\n",
            "step: 80, loss: 0.00035361392656341195\n",
            "step: 90, loss: 0.0012431818759068847\n",
            "step: 100, loss: 0.0013918336480855942\n",
            "step: 110, loss: 0.007636246737092733\n",
            "step: 120, loss: 0.00011211410310352221\n",
            "step: 130, loss: 0.0016210338799282908\n",
            "step: 140, loss: 0.0006332962075248361\n",
            "step: 150, loss: 0.0002940078265964985\n",
            "step: 160, loss: 0.00194832356646657\n",
            "step: 170, loss: 0.0027006168384104967\n",
            "step: 180, loss: 0.0005213369731791317\n",
            "step: 190, loss: 0.0014479877427220345\n",
            "step: 200, loss: 0.01675317995250225\n",
            "step: 210, loss: 0.0005290696863085032\n",
            "step: 220, loss: 0.0010552993044257164\n",
            "step: 230, loss: 0.001694210572168231\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.984304932735426, f1=0.9698324022346367, best_f1=0.9698324022346367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006960004102438688\n",
            "step: 10, loss: 0.0005601196899078786\n",
            "step: 20, loss: 0.001298147370107472\n",
            "step: 30, loss: 0.0008208613726310432\n",
            "step: 40, loss: 0.0004048601258546114\n",
            "step: 50, loss: 0.000653019524179399\n",
            "step: 60, loss: 0.03142153471708298\n",
            "step: 70, loss: 0.00034919779864139855\n",
            "step: 80, loss: 0.0007173779886215925\n",
            "step: 90, loss: 0.0003009735664818436\n",
            "step: 100, loss: 0.0002733810106292367\n",
            "step: 110, loss: 0.00046177173499017954\n",
            "step: 120, loss: 0.044298261404037476\n",
            "step: 130, loss: 0.0009306760039180517\n",
            "step: 140, loss: 0.010914687998592854\n",
            "step: 150, loss: 0.0007622306584380567\n",
            "step: 160, loss: 0.021417811512947083\n",
            "step: 170, loss: 0.00024064292665570974\n",
            "step: 180, loss: 0.0005873176851309836\n",
            "step: 190, loss: 0.0005602984456345439\n",
            "step: 200, loss: 0.0021216229069978\n",
            "step: 210, loss: 0.02283608168363571\n",
            "step: 220, loss: 0.0006829475751146674\n",
            "step: 230, loss: 0.0003482132451608777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9819819819819819, f1=0.9696969696969697, best_f1=0.9698324022346367\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:17, 143.03it/s]\n",
            "load_f1 = 0.9830890642615557\n",
            "real_f1 = 0.9831649831649831\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 134.22it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h62Yut_pgNFQ"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGEElkeagNFR",
        "outputId": "84f24930-4d59-447e-949f-4db35c2efbce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 497kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.40MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.79MB/s]\n",
            "Downloading: 100% 501M/501M [00:16<00:00, 30.2MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6532546281814575\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.47824540734291077\n",
            "step: 20, loss: 0.2524379789829254\n",
            "step: 30, loss: 0.3680313229560852\n",
            "step: 40, loss: 0.3169473707675934\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.15404440462589264\n",
            "step: 60, loss: 0.18225400149822235\n",
            "step: 70, loss: 0.21142041683197021\n",
            "step: 80, loss: 0.0923062190413475\n",
            "step: 90, loss: 0.17734487354755402\n",
            "step: 100, loss: 0.4316587746143341\n",
            "step: 110, loss: 0.10905889421701431\n",
            "step: 120, loss: 0.10502088069915771\n",
            "step: 130, loss: 0.1339646577835083\n",
            "step: 140, loss: 0.2741522490978241\n",
            "step: 150, loss: 0.12413064390420914\n",
            "step: 160, loss: 0.15160778164863586\n",
            "step: 170, loss: 0.08497345447540283\n",
            "step: 180, loss: 0.06989573687314987\n",
            "step: 190, loss: 0.08201098442077637\n",
            "step: 200, loss: 0.07049285620450974\n",
            "step: 210, loss: 0.05291526019573212\n",
            "step: 220, loss: 0.02953203022480011\n",
            "step: 230, loss: 0.19211028516292572\n",
            "step: 240, loss: 0.06543515622615814\n",
            "step: 250, loss: 0.09677764773368835\n",
            "step: 260, loss: 0.1388375163078308\n",
            "step: 270, loss: 0.6789646744728088\n",
            "step: 280, loss: 0.08033513277769089\n",
            "step: 290, loss: 0.20176155865192413\n",
            "step: 300, loss: 0.02639298513531685\n",
            "step: 310, loss: 0.09051183611154556\n",
            "step: 320, loss: 0.014086684212088585\n",
            "step: 330, loss: 0.0860525444149971\n",
            "step: 340, loss: 0.293308287858963\n",
            "step: 350, loss: 0.056265994906425476\n",
            "step: 360, loss: 0.025184454396367073\n",
            "step: 370, loss: 0.03325507044792175\n",
            "step: 380, loss: 0.13695374131202698\n",
            "step: 390, loss: 0.03258060663938522\n",
            "step: 400, loss: 0.029831692576408386\n",
            "step: 410, loss: 0.20999041199684143\n",
            "step: 420, loss: 0.07050420343875885\n",
            "step: 430, loss: 0.026992250233888626\n",
            "step: 440, loss: 0.12033317238092422\n",
            "step: 450, loss: 0.02119775116443634\n",
            "step: 460, loss: 0.05620047077536583\n",
            "step: 470, loss: 0.042656175792217255\n",
            "step: 480, loss: 0.1879993975162506\n",
            "step: 490, loss: 0.07329455763101578\n",
            "step: 500, loss: 0.01000463031232357\n",
            "step: 510, loss: 0.0817284807562828\n",
            "step: 520, loss: 0.05483602359890938\n",
            "step: 530, loss: 0.005617056041955948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9404096834264433, f1=0.9356943150046597, best_f1=0.9356943150046597\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06734523922204971\n",
            "step: 10, loss: 0.06944850087165833\n",
            "step: 20, loss: 0.01362349558621645\n",
            "step: 30, loss: 0.07850877195596695\n",
            "step: 40, loss: 0.03618954122066498\n",
            "step: 50, loss: 0.2100202590227127\n",
            "step: 60, loss: 0.017648985609412193\n",
            "step: 70, loss: 0.022478733211755753\n",
            "step: 80, loss: 0.014390720054507256\n",
            "step: 90, loss: 0.03749530762434006\n",
            "step: 100, loss: 0.17699532210826874\n",
            "step: 110, loss: 0.020124349743127823\n",
            "step: 120, loss: 0.02086193487048149\n",
            "step: 130, loss: 0.010215030051767826\n",
            "step: 140, loss: 0.009296794421970844\n",
            "step: 150, loss: 0.006571830250322819\n",
            "step: 160, loss: 0.05508023872971535\n",
            "step: 170, loss: 0.062434785068035126\n",
            "step: 180, loss: 0.013390779495239258\n",
            "step: 190, loss: 0.010442554950714111\n",
            "step: 200, loss: 0.09740457683801651\n",
            "step: 210, loss: 0.046637482941150665\n",
            "step: 220, loss: 0.001732812961563468\n",
            "step: 230, loss: 0.049055177718400955\n",
            "step: 240, loss: 0.004089142195880413\n",
            "step: 250, loss: 0.09795358777046204\n",
            "step: 260, loss: 0.08662408590316772\n",
            "step: 270, loss: 0.01583450473845005\n",
            "step: 280, loss: 0.054341014474630356\n",
            "step: 290, loss: 0.03470909968018532\n",
            "step: 300, loss: 0.032874707132577896\n",
            "step: 310, loss: 0.033955249935388565\n",
            "step: 320, loss: 0.05089721828699112\n",
            "step: 330, loss: 0.08009182661771774\n",
            "step: 340, loss: 0.12209313362836838\n",
            "step: 350, loss: 0.0018624719232320786\n",
            "step: 360, loss: 0.19213466346263885\n",
            "step: 370, loss: 0.01780455932021141\n",
            "step: 380, loss: 0.06833265721797943\n",
            "step: 390, loss: 0.003815979929640889\n",
            "step: 400, loss: 0.07900500297546387\n",
            "step: 410, loss: 0.09932415932416916\n",
            "step: 420, loss: 0.05981418490409851\n",
            "step: 430, loss: 0.17841637134552002\n",
            "step: 440, loss: 0.04170027747750282\n",
            "step: 450, loss: 0.031586553901433945\n",
            "step: 460, loss: 0.0232792180031538\n",
            "step: 470, loss: 0.09915716201066971\n",
            "step: 480, loss: 0.1396629810333252\n",
            "step: 490, loss: 0.03206980600953102\n",
            "step: 500, loss: 0.009015994146466255\n",
            "step: 510, loss: 0.008247480727732182\n",
            "step: 520, loss: 0.29032430052757263\n",
            "step: 530, loss: 0.1676531732082367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9480093676814988, f1=0.9449112978524743, best_f1=0.9449112978524743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0883052870631218\n",
            "step: 10, loss: 0.04604554548859596\n",
            "step: 20, loss: 0.0058432212099432945\n",
            "step: 30, loss: 0.027056924998760223\n",
            "step: 40, loss: 0.05964023619890213\n",
            "step: 50, loss: 0.011109530925750732\n",
            "step: 60, loss: 0.010546381585299969\n",
            "step: 70, loss: 0.031130356714129448\n",
            "step: 80, loss: 0.023050358518958092\n",
            "step: 90, loss: 0.15482570230960846\n",
            "step: 100, loss: 0.05834965780377388\n",
            "step: 110, loss: 0.09169831871986389\n",
            "step: 120, loss: 0.09687283635139465\n",
            "step: 130, loss: 0.04602190479636192\n",
            "step: 140, loss: 0.0051700640469789505\n",
            "step: 150, loss: 0.02024921216070652\n",
            "step: 160, loss: 0.025809505954384804\n",
            "step: 170, loss: 0.008748043328523636\n",
            "step: 180, loss: 0.011726034805178642\n",
            "step: 190, loss: 0.0044944072142243385\n",
            "step: 200, loss: 0.08966061472892761\n",
            "step: 210, loss: 0.02815387025475502\n",
            "step: 220, loss: 0.06527848541736603\n",
            "step: 230, loss: 0.10657674074172974\n",
            "step: 240, loss: 0.07207324355840683\n",
            "step: 250, loss: 0.03586428239941597\n",
            "step: 260, loss: 0.02261986769735813\n",
            "step: 270, loss: 0.005980911664664745\n",
            "step: 280, loss: 0.00023538109962828457\n",
            "step: 290, loss: 0.007817242294549942\n",
            "step: 300, loss: 0.088372603058815\n",
            "step: 310, loss: 0.12347527593374252\n",
            "step: 320, loss: 0.10007532685995102\n",
            "step: 330, loss: 0.04566723480820656\n",
            "step: 340, loss: 0.0043404302559792995\n",
            "step: 350, loss: 0.042401861399412155\n",
            "step: 360, loss: 0.011329410597682\n",
            "step: 370, loss: 0.026797812432050705\n",
            "step: 380, loss: 0.13879474997520447\n",
            "step: 390, loss: 0.011184110306203365\n",
            "step: 400, loss: 0.11748795956373215\n",
            "step: 410, loss: 0.03660549968481064\n",
            "step: 420, loss: 0.003108459524810314\n",
            "step: 430, loss: 0.03153286129236221\n",
            "step: 440, loss: 0.20164120197296143\n",
            "step: 450, loss: 0.05019167810678482\n",
            "step: 460, loss: 0.14581626653671265\n",
            "step: 470, loss: 0.07109794020652771\n",
            "step: 480, loss: 0.2241692692041397\n",
            "step: 490, loss: 0.06597208231687546\n",
            "step: 500, loss: 0.07282087206840515\n",
            "step: 510, loss: 0.006146302446722984\n",
            "step: 520, loss: 0.10133993625640869\n",
            "step: 530, loss: 0.011293089017271996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9449626865671642, f1=0.927468413664015, best_f1=0.9449112978524743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02576279453933239\n",
            "step: 10, loss: 0.0015649001579731703\n",
            "step: 20, loss: 0.10998547077178955\n",
            "step: 30, loss: 0.013971655629575253\n",
            "step: 40, loss: 0.005784517154097557\n",
            "step: 50, loss: 0.009484986774623394\n",
            "step: 60, loss: 0.020684074610471725\n",
            "step: 70, loss: 0.04722961038351059\n",
            "step: 80, loss: 0.2706667482852936\n",
            "step: 90, loss: 0.007970619946718216\n",
            "step: 100, loss: 0.011373440735042095\n",
            "step: 110, loss: 0.11805833131074905\n",
            "step: 120, loss: 0.002184451324865222\n",
            "step: 130, loss: 0.011055530980229378\n",
            "step: 140, loss: 0.016601532697677612\n",
            "step: 150, loss: 0.009311995469033718\n",
            "step: 160, loss: 0.03170617297291756\n",
            "step: 170, loss: 0.007080815266817808\n",
            "step: 180, loss: 0.01963677816092968\n",
            "step: 190, loss: 0.021039247512817383\n",
            "step: 200, loss: 0.1182284876704216\n",
            "step: 210, loss: 0.0013173534534871578\n",
            "step: 220, loss: 0.004711833782494068\n",
            "step: 230, loss: 0.003787215333431959\n",
            "step: 240, loss: 0.029755622148513794\n",
            "step: 250, loss: 0.03570142015814781\n",
            "step: 260, loss: 0.0008566994220018387\n",
            "step: 270, loss: 0.05531091243028641\n",
            "step: 280, loss: 0.0011357659241184592\n",
            "step: 290, loss: 0.011754270642995834\n",
            "step: 300, loss: 0.013950970023870468\n",
            "step: 310, loss: 0.0054148719646036625\n",
            "step: 320, loss: 0.04207371175289154\n",
            "step: 330, loss: 0.049757130444049835\n",
            "step: 340, loss: 0.012554898858070374\n",
            "step: 350, loss: 0.05497339367866516\n",
            "step: 360, loss: 0.03812260180711746\n",
            "step: 370, loss: 0.00836033746600151\n",
            "step: 380, loss: 0.007937065325677395\n",
            "step: 390, loss: 0.001970376819372177\n",
            "step: 400, loss: 0.09692850708961487\n",
            "step: 410, loss: 0.0012660013744607568\n",
            "step: 420, loss: 0.026671336963772774\n",
            "step: 430, loss: 0.0037119623739272356\n",
            "step: 440, loss: 0.0019150603329762816\n",
            "step: 450, loss: 0.014145140536129475\n",
            "step: 460, loss: 0.012215581722557545\n",
            "step: 470, loss: 0.002825212897732854\n",
            "step: 480, loss: 0.0019384390907362103\n",
            "step: 490, loss: 0.0018622392090037465\n",
            "step: 500, loss: 0.07079384475946426\n",
            "step: 510, loss: 0.10192796587944031\n",
            "step: 520, loss: 0.03667086735367775\n",
            "step: 530, loss: 0.10950618982315063\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9478098788443615, f1=0.9364269141531322, best_f1=0.9449112978524743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002934279851615429\n",
            "step: 10, loss: 0.024892445653676987\n",
            "step: 20, loss: 0.0080583356320858\n",
            "step: 30, loss: 0.002573499921709299\n",
            "step: 40, loss: 0.02644769288599491\n",
            "step: 50, loss: 0.004040106665343046\n",
            "step: 60, loss: 0.018426625058054924\n",
            "step: 70, loss: 0.030998343601822853\n",
            "step: 80, loss: 0.00030314933974295855\n",
            "step: 90, loss: 0.026889581233263016\n",
            "step: 100, loss: 0.02116321213543415\n",
            "step: 110, loss: 0.0006783325807191432\n",
            "step: 120, loss: 0.043043553829193115\n",
            "step: 130, loss: 0.004043241497129202\n",
            "step: 140, loss: 0.0020993186626583338\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 150, loss: 0.018275970593094826\n",
            "step: 160, loss: 0.0015142710180953145\n",
            "step: 170, loss: 0.044713180512189865\n",
            "step: 180, loss: 0.005108017940074205\n",
            "step: 190, loss: 0.0059374249540269375\n",
            "step: 200, loss: 0.004166995640844107\n",
            "step: 210, loss: 0.000625689048320055\n",
            "step: 220, loss: 0.005350057035684586\n",
            "step: 230, loss: 0.0017139199189841747\n",
            "step: 240, loss: 0.02534278854727745\n",
            "step: 250, loss: 0.17447295784950256\n",
            "step: 260, loss: 0.0011129140621051192\n",
            "step: 270, loss: 0.017017552629113197\n",
            "step: 280, loss: 0.00623171916231513\n",
            "step: 290, loss: 0.0009003037703223526\n",
            "step: 300, loss: 0.10096976161003113\n",
            "step: 310, loss: 0.06043640524148941\n",
            "step: 320, loss: 0.05122688040137291\n",
            "step: 330, loss: 0.004172894172370434\n",
            "step: 340, loss: 0.0008124663727357984\n",
            "step: 350, loss: 0.0007996626081876457\n",
            "step: 360, loss: 0.0001376587460981682\n",
            "step: 370, loss: 0.000287405593553558\n",
            "step: 380, loss: 0.0001258361153304577\n",
            "step: 390, loss: 0.0013938165502622724\n",
            "step: 400, loss: 0.11884628981351852\n",
            "step: 410, loss: 0.08342193067073822\n",
            "step: 420, loss: 0.10634318739175797\n",
            "step: 430, loss: 0.003078061854466796\n",
            "step: 440, loss: 0.0032855828758329153\n",
            "step: 450, loss: 0.03467727452516556\n",
            "step: 460, loss: 0.01564655266702175\n",
            "step: 470, loss: 0.04946082457900047\n",
            "step: 480, loss: 0.008940781466662884\n",
            "step: 490, loss: 0.01535582635551691\n",
            "step: 500, loss: 0.09345578402280807\n",
            "step: 510, loss: 0.0007502530352212489\n",
            "step: 520, loss: 0.10918381810188293\n",
            "step: 530, loss: 0.02906683459877968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9491525423728814, f1=0.9383280036546369, best_f1=0.9383280036546369\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00979416910558939\n",
            "step: 10, loss: 0.0007662491989322007\n",
            "step: 20, loss: 0.016550308093428612\n",
            "step: 30, loss: 0.0009387903846800327\n",
            "step: 40, loss: 0.001012665219604969\n",
            "step: 50, loss: 0.00029806740349158645\n",
            "step: 60, loss: 0.0010948039125651121\n",
            "step: 70, loss: 0.00017072477203328162\n",
            "step: 80, loss: 0.00179264135658741\n",
            "step: 90, loss: 0.05069095641374588\n",
            "step: 100, loss: 0.007713434286415577\n",
            "step: 110, loss: 0.005837816745042801\n",
            "step: 120, loss: 0.01054073590785265\n",
            "step: 130, loss: 0.001562771969474852\n",
            "step: 140, loss: 0.0013491937424987555\n",
            "step: 150, loss: 0.05149867385625839\n",
            "step: 160, loss: 0.026731085032224655\n",
            "step: 170, loss: 0.008410760201513767\n",
            "step: 180, loss: 0.0018638247856870294\n",
            "step: 190, loss: 0.23485396802425385\n",
            "step: 200, loss: 0.00251529342494905\n",
            "step: 210, loss: 0.01966635137796402\n",
            "step: 220, loss: 0.003461054991930723\n",
            "step: 230, loss: 0.014838453382253647\n",
            "step: 240, loss: 0.008815902285277843\n",
            "step: 250, loss: 0.053515657782554626\n",
            "step: 260, loss: 0.0015350221656262875\n",
            "step: 270, loss: 0.008494212292134762\n",
            "step: 280, loss: 0.0029202818404883146\n",
            "step: 290, loss: 0.0004882964421994984\n",
            "step: 300, loss: 0.06626340746879578\n",
            "step: 310, loss: 0.03641403093934059\n",
            "step: 320, loss: 0.0002054188516922295\n",
            "step: 330, loss: 0.004445885308086872\n",
            "step: 340, loss: 0.00015817908570170403\n",
            "step: 350, loss: 0.0035279039293527603\n",
            "step: 360, loss: 0.18121758103370667\n",
            "step: 370, loss: 0.01610967516899109\n",
            "step: 380, loss: 0.0013016827870160341\n",
            "step: 390, loss: 0.0030486066825687885\n",
            "step: 400, loss: 0.039905909448862076\n",
            "step: 410, loss: 0.006648289039731026\n",
            "step: 420, loss: 0.001974803628399968\n",
            "step: 430, loss: 0.0008344577508978546\n",
            "step: 440, loss: 0.0006745218997821212\n",
            "step: 450, loss: 0.01447316911071539\n",
            "step: 460, loss: 0.0022127265110611916\n",
            "step: 470, loss: 0.001261041616089642\n",
            "step: 480, loss: 0.005463081412017345\n",
            "step: 490, loss: 0.050794005393981934\n",
            "step: 500, loss: 0.0529417023062706\n",
            "step: 510, loss: 0.13139697909355164\n",
            "step: 520, loss: 0.03135494142770767\n",
            "step: 530, loss: 0.0009134431602433324\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9460227272727273, f1=0.936470588235294, best_f1=0.9383280036546369\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02484842762351036\n",
            "step: 10, loss: 0.0016410844400525093\n",
            "step: 20, loss: 0.013581291772425175\n",
            "step: 30, loss: 0.007544277235865593\n",
            "step: 40, loss: 0.002971825422719121\n",
            "step: 50, loss: 0.012031758204102516\n",
            "step: 60, loss: 0.010240491479635239\n",
            "step: 70, loss: 0.003555386560037732\n",
            "step: 80, loss: 0.001005023019388318\n",
            "step: 90, loss: 0.00021950469817966223\n",
            "step: 100, loss: 0.0011805961839854717\n",
            "step: 110, loss: 0.000429431238444522\n",
            "step: 120, loss: 0.008810224942862988\n",
            "step: 130, loss: 0.00039104610914364457\n",
            "step: 140, loss: 0.001651870901696384\n",
            "step: 150, loss: 0.006835335399955511\n",
            "step: 160, loss: 0.00020102763664908707\n",
            "step: 170, loss: 0.009639469906687737\n",
            "step: 180, loss: 0.15726731717586517\n",
            "step: 190, loss: 0.043224431574344635\n",
            "step: 200, loss: 0.0005773542216047645\n",
            "step: 210, loss: 0.0006361650303006172\n",
            "step: 220, loss: 0.00020692500402219594\n",
            "step: 230, loss: 0.0001417861203663051\n",
            "step: 240, loss: 0.005632961168885231\n",
            "step: 250, loss: 0.0010664894944056869\n",
            "step: 260, loss: 0.02173565700650215\n",
            "step: 270, loss: 0.00023436194169335067\n",
            "step: 280, loss: 0.021015798673033714\n",
            "step: 290, loss: 0.00021953447139821947\n",
            "step: 300, loss: 0.00016758286801632494\n",
            "step: 310, loss: 0.00011741340858861804\n",
            "step: 320, loss: 0.007384579628705978\n",
            "step: 330, loss: 5.4784686653874815e-05\n",
            "step: 340, loss: 0.022548343986272812\n",
            "step: 350, loss: 0.0016656587831676006\n",
            "step: 360, loss: 0.0006491955136880279\n",
            "step: 370, loss: 0.0021966390777379274\n",
            "step: 380, loss: 0.015134756453335285\n",
            "step: 390, loss: 0.00840156339108944\n",
            "step: 400, loss: 0.07320155203342438\n",
            "step: 410, loss: 0.02411503717303276\n",
            "step: 420, loss: 0.006741268560290337\n",
            "step: 430, loss: 0.0004737121344078332\n",
            "step: 440, loss: 0.0011635632254183292\n",
            "step: 450, loss: 0.08029985427856445\n",
            "step: 460, loss: 0.0466441810131073\n",
            "step: 470, loss: 0.18141774833202362\n",
            "step: 480, loss: 0.004636579193174839\n",
            "step: 490, loss: 0.0031595099717378616\n",
            "step: 500, loss: 0.00106896017678082\n",
            "step: 510, loss: 0.0003491169190965593\n",
            "step: 520, loss: 0.004766672849655151\n",
            "step: 530, loss: 0.004069885704666376\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9476609541454377, f1=0.9405255878284925, best_f1=0.9383280036546369\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000767586927395314\n",
            "step: 10, loss: 0.0014422731474041939\n",
            "step: 20, loss: 0.00022568779240828007\n",
            "step: 30, loss: 0.03943060338497162\n",
            "step: 40, loss: 4.731707667815499e-05\n",
            "step: 50, loss: 0.001296322443522513\n",
            "step: 60, loss: 0.00010718701378209516\n",
            "step: 70, loss: 0.000308704242343083\n",
            "step: 80, loss: 0.0002886533038690686\n",
            "step: 90, loss: 0.0004181099066045135\n",
            "step: 100, loss: 0.0003143748617731035\n",
            "step: 110, loss: 5.376310582505539e-05\n",
            "step: 120, loss: 5.0178419769508764e-05\n",
            "step: 130, loss: 0.00043709605233743787\n",
            "step: 140, loss: 7.418434688588604e-05\n",
            "step: 150, loss: 5.562582373386249e-05\n",
            "step: 160, loss: 2.8068678147974424e-05\n",
            "step: 170, loss: 0.0039024604484438896\n",
            "step: 180, loss: 0.0015010031638666987\n",
            "step: 190, loss: 0.0021858015097677708\n",
            "step: 200, loss: 0.009349007159471512\n",
            "step: 210, loss: 0.11579433083534241\n",
            "step: 220, loss: 0.000857498322147876\n",
            "step: 230, loss: 0.07458625733852386\n",
            "step: 240, loss: 0.00986485742032528\n",
            "step: 250, loss: 0.000977838528342545\n",
            "step: 260, loss: 0.0004401802143547684\n",
            "step: 270, loss: 0.0017235854174941778\n",
            "step: 280, loss: 0.0008204776095226407\n",
            "step: 290, loss: 0.00028716641827486455\n",
            "step: 300, loss: 0.00017344419029541314\n",
            "step: 310, loss: 0.0008413958130404353\n",
            "step: 320, loss: 0.009455080144107342\n",
            "step: 330, loss: 0.000175258974195458\n",
            "step: 340, loss: 0.01905435509979725\n",
            "step: 350, loss: 0.0010461027268320322\n",
            "step: 360, loss: 0.10364925861358643\n",
            "step: 370, loss: 0.1861714869737625\n",
            "step: 380, loss: 0.0005848844884894788\n",
            "step: 390, loss: 0.002244841307401657\n",
            "step: 400, loss: 0.00039949669735506177\n",
            "step: 410, loss: 0.0017902359832078218\n",
            "step: 420, loss: 0.00010757723794085905\n",
            "step: 430, loss: 7.976838969625533e-05\n",
            "step: 440, loss: 0.00013792562822345644\n",
            "step: 450, loss: 8.943086140789092e-05\n",
            "step: 460, loss: 0.0004424174258019775\n",
            "step: 470, loss: 0.02708972431719303\n",
            "step: 480, loss: 0.010070468299090862\n",
            "step: 490, loss: 0.08720697462558746\n",
            "step: 500, loss: 0.00169320625718683\n",
            "step: 510, loss: 0.005030814558267593\n",
            "step: 520, loss: 0.0008133291848935187\n",
            "step: 530, loss: 0.0016135042533278465\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9491211840888066, f1=0.9398806792106471, best_f1=0.9383280036546369\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016376236453652382\n",
            "step: 10, loss: 0.0006459199939854443\n",
            "step: 20, loss: 0.0005534177762456238\n",
            "step: 30, loss: 0.04792246222496033\n",
            "step: 40, loss: 0.00016842079639900476\n",
            "step: 50, loss: 0.00035528390435501933\n",
            "step: 60, loss: 0.0009704524418339133\n",
            "step: 70, loss: 0.0002797002671286464\n",
            "step: 80, loss: 0.0016353853279724717\n",
            "step: 90, loss: 0.05666283518075943\n",
            "step: 100, loss: 0.0007751267403364182\n",
            "step: 110, loss: 0.004409410525113344\n",
            "step: 120, loss: 0.0001195806689793244\n",
            "step: 130, loss: 0.0011967496247962117\n",
            "step: 140, loss: 0.05886802449822426\n",
            "step: 150, loss: 0.00048146297922357917\n",
            "step: 160, loss: 4.3602725781966e-05\n",
            "step: 170, loss: 0.0003179931081831455\n",
            "step: 180, loss: 0.019961126148700714\n",
            "step: 190, loss: 0.0001586825674166903\n",
            "step: 200, loss: 0.0018723881803452969\n",
            "step: 210, loss: 0.0005263371858745813\n",
            "step: 220, loss: 4.803571937372908e-05\n",
            "step: 230, loss: 7.236217061290517e-05\n",
            "step: 240, loss: 7.317541167140007e-05\n",
            "step: 250, loss: 3.206156543456018e-05\n",
            "step: 260, loss: 5.698244058294222e-05\n",
            "step: 270, loss: 0.0005706415395252407\n",
            "step: 280, loss: 0.0001897416659630835\n",
            "step: 290, loss: 2.3416088879457675e-05\n",
            "step: 300, loss: 0.08377489447593689\n",
            "step: 310, loss: 0.05314505100250244\n",
            "step: 320, loss: 0.0004595532373059541\n",
            "step: 330, loss: 0.000978741911239922\n",
            "step: 340, loss: 0.020320050418376923\n",
            "step: 350, loss: 0.014773596078157425\n",
            "step: 360, loss: 0.04884704574942589\n",
            "step: 370, loss: 0.0006795090739615262\n",
            "step: 380, loss: 0.0014035619096830487\n",
            "step: 390, loss: 0.0010920347413048148\n",
            "step: 400, loss: 0.031159337610006332\n",
            "step: 410, loss: 0.03025370091199875\n",
            "step: 420, loss: 0.01773151010274887\n",
            "step: 430, loss: 0.013810890726745129\n",
            "step: 440, loss: 4.025211455882527e-05\n",
            "step: 450, loss: 0.0020123689901083708\n",
            "step: 460, loss: 0.00013361798482947052\n",
            "step: 470, loss: 0.017741119489073753\n",
            "step: 480, loss: 4.9615526222623885e-05\n",
            "step: 490, loss: 0.0009374269284307957\n",
            "step: 500, loss: 5.183240500628017e-05\n",
            "step: 510, loss: 0.022974418476223946\n",
            "step: 520, loss: 0.002424411941319704\n",
            "step: 530, loss: 0.019108261913061142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9541627689429373, f1=0.9424326833797586, best_f1=0.9424326833797586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003758902312256396\n",
            "step: 10, loss: 4.718622221844271e-05\n",
            "step: 20, loss: 0.0016835584538057446\n",
            "step: 30, loss: 0.00022987548436503857\n",
            "step: 40, loss: 5.17990774824284e-05\n",
            "step: 50, loss: 0.0001019513001665473\n",
            "step: 60, loss: 6.72384921927005e-05\n",
            "step: 70, loss: 2.4194701836677268e-05\n",
            "step: 80, loss: 0.004498879425227642\n",
            "step: 90, loss: 5.260364559944719e-05\n",
            "step: 100, loss: 4.5012784539721906e-05\n",
            "step: 110, loss: 0.0015269003342837095\n",
            "step: 120, loss: 3.7507445085793734e-05\n",
            "step: 130, loss: 2.8428996301954612e-05\n",
            "step: 140, loss: 0.0003082027833443135\n",
            "step: 150, loss: 0.0006562185590155423\n",
            "step: 160, loss: 0.06210098788142204\n",
            "step: 170, loss: 0.0073824478313326836\n",
            "step: 180, loss: 0.0001251927751582116\n",
            "step: 190, loss: 0.00015977023576851934\n",
            "step: 200, loss: 0.0037479354068636894\n",
            "step: 210, loss: 0.0013562508393079042\n",
            "step: 220, loss: 0.00038754226989112794\n",
            "step: 230, loss: 0.0002702068886719644\n",
            "step: 240, loss: 9.359812247566879e-05\n",
            "step: 250, loss: 4.0693212213227525e-05\n",
            "step: 260, loss: 6.36223703622818e-05\n",
            "step: 270, loss: 1.4114910300122574e-05\n",
            "step: 280, loss: 8.423141116509214e-05\n",
            "step: 290, loss: 1.6692465578671545e-05\n",
            "step: 300, loss: 0.004016796126961708\n",
            "step: 310, loss: 0.038503121584653854\n",
            "step: 320, loss: 0.00524506438523531\n",
            "step: 330, loss: 6.214376480784267e-05\n",
            "step: 340, loss: 0.0005933194770477712\n",
            "step: 350, loss: 0.00012064059410477057\n",
            "step: 360, loss: 1.7433530956623144e-05\n",
            "step: 370, loss: 1.869994775915984e-05\n",
            "step: 380, loss: 0.0004731090448331088\n",
            "step: 390, loss: 1.770171547832433e-05\n",
            "step: 400, loss: 0.002418083604425192\n",
            "step: 410, loss: 1.5340376194217242e-05\n",
            "step: 420, loss: 3.259041113778949e-05\n",
            "step: 430, loss: 8.702175364305731e-06\n",
            "step: 440, loss: 1.4725616892974358e-05\n",
            "step: 450, loss: 0.006922766100615263\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 460, loss: 0.00013307614426594228\n",
            "step: 470, loss: 0.0016001507174223661\n",
            "step: 480, loss: 0.00301870284602046\n",
            "step: 490, loss: 0.00012905194307677448\n",
            "step: 500, loss: 0.032723285257816315\n",
            "step: 510, loss: 1.0166184438276105e-05\n",
            "step: 520, loss: 0.00023470804444514215\n",
            "step: 530, loss: 0.08995148539543152\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9475164011246484, f1=0.9393090569561158, best_f1=0.9424326833797586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.794383519561961e-05\n",
            "step: 10, loss: 0.004440520890057087\n",
            "step: 20, loss: 4.2193529225187376e-05\n",
            "step: 30, loss: 5.519598926184699e-05\n",
            "step: 40, loss: 0.00010854657739400864\n",
            "step: 50, loss: 6.200841744430363e-05\n",
            "step: 60, loss: 6.132567068561912e-05\n",
            "step: 70, loss: 2.8754540835507214e-05\n",
            "step: 80, loss: 6.007185220369138e-05\n",
            "step: 90, loss: 0.001194715965539217\n",
            "step: 100, loss: 0.0002920800179708749\n",
            "step: 110, loss: 0.0005902857519686222\n",
            "step: 120, loss: 2.6723777409642935e-05\n",
            "step: 130, loss: 0.00026661239098757505\n",
            "step: 140, loss: 5.142902955412865e-05\n",
            "step: 150, loss: 0.0007430792902596295\n",
            "step: 160, loss: 0.0005051809130236506\n",
            "step: 170, loss: 0.0030917644035071135\n",
            "step: 180, loss: 7.90422927821055e-05\n",
            "step: 190, loss: 9.709729056339711e-05\n",
            "step: 200, loss: 1.559008887852542e-05\n",
            "step: 210, loss: 0.0003184875240549445\n",
            "step: 220, loss: 0.006941680330783129\n",
            "step: 230, loss: 3.016948357981164e-05\n",
            "step: 240, loss: 0.00826103426516056\n",
            "step: 250, loss: 0.00020329182734712958\n",
            "step: 260, loss: 6.476557609857991e-05\n",
            "step: 270, loss: 0.003703460330143571\n",
            "step: 280, loss: 0.00016622785187792033\n",
            "step: 290, loss: 1.3414536624622997e-05\n",
            "step: 300, loss: 1.2226222679601051e-05\n",
            "step: 310, loss: 0.00034295287332497537\n",
            "step: 320, loss: 6.75428964314051e-05\n",
            "step: 330, loss: 3.399378329049796e-05\n",
            "step: 340, loss: 0.0041109537705779076\n",
            "step: 350, loss: 2.978320662805345e-05\n",
            "step: 360, loss: 0.00011553426884347573\n",
            "step: 370, loss: 1.7824741007643752e-05\n",
            "step: 380, loss: 8.845917182043195e-05\n",
            "step: 390, loss: 3.7580146454274654e-05\n",
            "step: 400, loss: 1.5530336895608343e-05\n",
            "step: 410, loss: 0.0003705544222611934\n",
            "step: 420, loss: 2.187629070249386e-05\n",
            "step: 430, loss: 0.0047829775139689445\n",
            "step: 440, loss: 2.2423795599024743e-05\n",
            "step: 450, loss: 1.5071968846314121e-05\n",
            "step: 460, loss: 1.8182521671405993e-05\n",
            "step: 470, loss: 0.014541452750563622\n",
            "step: 480, loss: 0.0001397078885929659\n",
            "step: 490, loss: 0.0001096410778700374\n",
            "step: 500, loss: 0.0012418117839843035\n",
            "step: 510, loss: 3.326290607219562e-05\n",
            "step: 520, loss: 2.464580575178843e-05\n",
            "step: 530, loss: 0.00426693307235837\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9480037140204272, f1=0.9382488479262673, best_f1=0.9424326833797586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005078744143247604\n",
            "step: 10, loss: 0.011167554184794426\n",
            "step: 20, loss: 0.0007136341882869601\n",
            "step: 30, loss: 2.3135051378631033e-05\n",
            "step: 40, loss: 3.95783172280062e-05\n",
            "step: 50, loss: 4.094407267984934e-05\n",
            "step: 60, loss: 6.230139842955396e-05\n",
            "step: 70, loss: 0.0005102158756926656\n",
            "step: 80, loss: 0.0002881104010157287\n",
            "step: 90, loss: 0.0003828128392342478\n",
            "step: 100, loss: 0.03341246768832207\n",
            "step: 110, loss: 3.787444802583195e-05\n",
            "step: 120, loss: 9.491165110375732e-05\n",
            "step: 130, loss: 0.00018896414258051664\n",
            "step: 140, loss: 2.2991343939793296e-05\n",
            "step: 150, loss: 0.0009021624573506415\n",
            "step: 160, loss: 8.092899224720895e-05\n",
            "step: 170, loss: 0.0002171232335967943\n",
            "step: 180, loss: 8.481623808620498e-05\n",
            "step: 190, loss: 0.0001360871974611655\n",
            "step: 200, loss: 0.0013373655965551734\n",
            "step: 210, loss: 2.8240105166332796e-05\n",
            "step: 220, loss: 6.846587348263711e-05\n",
            "step: 230, loss: 3.942261537304148e-05\n",
            "step: 240, loss: 0.00011705914948834106\n",
            "step: 250, loss: 2.6850233552977443e-05\n",
            "step: 260, loss: 1.9310673451400362e-05\n",
            "step: 270, loss: 0.0002956461685243994\n",
            "step: 280, loss: 0.00011099260882474482\n",
            "step: 290, loss: 3.727906005224213e-05\n",
            "step: 300, loss: 0.0007492162985727191\n",
            "step: 310, loss: 3.333613130962476e-05\n",
            "step: 320, loss: 0.0013450148981064558\n",
            "step: 330, loss: 0.0028469297103583813\n",
            "step: 340, loss: 0.002188236452639103\n",
            "step: 350, loss: 1.677355066931341e-05\n",
            "step: 360, loss: 4.540313602774404e-05\n",
            "step: 370, loss: 0.045717865228652954\n",
            "step: 380, loss: 4.3655822082655504e-05\n",
            "step: 390, loss: 0.005853912327438593\n",
            "step: 400, loss: 5.218467049417086e-05\n",
            "step: 410, loss: 0.0002781694056466222\n",
            "step: 420, loss: 5.620465526590124e-05\n",
            "step: 430, loss: 1.955692277988419e-05\n",
            "step: 440, loss: 3.6129251384409145e-05\n",
            "step: 450, loss: 0.00012322486145421863\n",
            "step: 460, loss: 1.98696980078239e-05\n",
            "step: 470, loss: 0.003183545544743538\n",
            "step: 480, loss: 7.797698344802484e-05\n",
            "step: 490, loss: 2.89690142381005e-05\n",
            "step: 500, loss: 6.539450259879231e-05\n",
            "step: 510, loss: 0.0007016057497821748\n",
            "step: 520, loss: 3.554693830665201e-05\n",
            "step: 530, loss: 8.347250695805997e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9484440315838365, f1=0.9400369003690037, best_f1=0.9424326833797586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022411514073610306\n",
            "step: 10, loss: 2.0253715774742886e-05\n",
            "step: 20, loss: 0.0034920659381896257\n",
            "step: 30, loss: 8.884740054782014e-06\n",
            "step: 40, loss: 0.0001009205006994307\n",
            "step: 50, loss: 0.0014828558778390288\n",
            "step: 60, loss: 0.0029231372755020857\n",
            "step: 70, loss: 3.521850157994777e-05\n",
            "step: 80, loss: 0.00534677691757679\n",
            "step: 90, loss: 6.439466960728168e-05\n",
            "step: 100, loss: 1.939683897944633e-05\n",
            "step: 110, loss: 0.0008841506787575781\n",
            "step: 120, loss: 0.0003226099943276495\n",
            "step: 130, loss: 1.5086502571648452e-05\n",
            "step: 140, loss: 3.1250998290488496e-05\n",
            "step: 150, loss: 1.2781320037902333e-05\n",
            "step: 160, loss: 0.0001359682355541736\n",
            "step: 170, loss: 2.5052790078916587e-05\n",
            "step: 180, loss: 1.7068892702809535e-05\n",
            "step: 190, loss: 2.4036995455389842e-05\n",
            "step: 200, loss: 1.3298948942974675e-05\n",
            "step: 210, loss: 1.7917875084094703e-05\n",
            "step: 220, loss: 1.7046351786120795e-05\n",
            "step: 230, loss: 0.001022382639348507\n",
            "step: 240, loss: 8.811176667222753e-05\n",
            "step: 250, loss: 2.3374799638986588e-05\n",
            "step: 260, loss: 6.893922545714304e-05\n",
            "step: 270, loss: 3.8675036194035783e-05\n",
            "step: 280, loss: 5.2998024330008775e-05\n",
            "step: 290, loss: 4.254471787135117e-05\n",
            "step: 300, loss: 1.5269139112206176e-05\n",
            "step: 310, loss: 0.0003072945983149111\n",
            "step: 320, loss: 2.705209408304654e-05\n",
            "step: 330, loss: 1.1470041499705985e-05\n",
            "step: 340, loss: 4.4565764255821705e-05\n",
            "step: 350, loss: 6.888001735205762e-06\n",
            "step: 360, loss: 0.08770023286342621\n",
            "step: 370, loss: 0.0001784299238352105\n",
            "step: 380, loss: 1.5146734767768066e-05\n",
            "step: 390, loss: 0.00015523449110332876\n",
            "step: 400, loss: 2.276775376230944e-05\n",
            "step: 410, loss: 1.2427381989255082e-05\n",
            "step: 420, loss: 0.00020722548651974648\n",
            "step: 430, loss: 3.797906538238749e-05\n",
            "step: 440, loss: 2.609243347251322e-05\n",
            "step: 450, loss: 1.4751620255992748e-05\n",
            "step: 460, loss: 0.00027946539921686053\n",
            "step: 470, loss: 0.0006826035096310079\n",
            "step: 480, loss: 7.532466952397954e-06\n",
            "step: 490, loss: 6.744228448951617e-05\n",
            "step: 500, loss: 6.269154255278409e-05\n",
            "step: 510, loss: 3.3709344279486686e-05\n",
            "step: 520, loss: 1.5224301932903472e-05\n",
            "step: 530, loss: 4.015819649794139e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9487895716945998, f1=0.9399260628465804, best_f1=0.9424326833797586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.43114181002602e-05\n",
            "step: 10, loss: 5.5410822824342176e-05\n",
            "step: 20, loss: 3.34122269123327e-05\n",
            "step: 30, loss: 0.00014603001181967556\n",
            "step: 40, loss: 1.24866419355385e-05\n",
            "step: 50, loss: 1.785478525562212e-05\n",
            "step: 60, loss: 0.00015226777759380639\n",
            "step: 70, loss: 5.619073999696411e-05\n",
            "step: 80, loss: 0.0046277521178126335\n",
            "step: 90, loss: 7.405820724670775e-06\n",
            "step: 100, loss: 9.111980034504086e-06\n",
            "step: 110, loss: 3.260956873418763e-05\n",
            "step: 120, loss: 8.188118954421952e-06\n",
            "step: 130, loss: 1.1495947546791285e-05\n",
            "step: 140, loss: 6.000838038744405e-05\n",
            "step: 150, loss: 1.2933870493725408e-05\n",
            "step: 160, loss: 1.2688055903709028e-05\n",
            "step: 170, loss: 1.392458852933487e-05\n",
            "step: 180, loss: 0.00013371568638831377\n",
            "step: 190, loss: 2.2327349142869934e-05\n",
            "step: 200, loss: 1.9522494767443277e-05\n",
            "step: 210, loss: 9.146754746325314e-05\n",
            "step: 220, loss: 6.813505478930892e-06\n",
            "step: 230, loss: 9.551503353577573e-06\n",
            "step: 240, loss: 2.975189454446081e-05\n",
            "step: 250, loss: 0.007727376185357571\n",
            "step: 260, loss: 7.111534159776056e-06\n",
            "step: 270, loss: 3.414889579289593e-05\n",
            "step: 280, loss: 2.686602783796843e-05\n",
            "step: 290, loss: 8.873434126144275e-06\n",
            "step: 300, loss: 8.05401214165613e-06\n",
            "step: 310, loss: 1.1980172530456912e-05\n",
            "step: 320, loss: 7.4505232987576164e-06\n",
            "step: 330, loss: 5.3967058192938566e-05\n",
            "step: 340, loss: 3.0089075153227895e-05\n",
            "step: 350, loss: 5.856130883330479e-06\n",
            "step: 360, loss: 8.985288332041819e-06\n",
            "step: 370, loss: 0.0002770027203951031\n",
            "step: 380, loss: 0.000904237327631563\n",
            "step: 390, loss: 0.0011260772589594126\n",
            "step: 400, loss: 0.01694120652973652\n",
            "step: 410, loss: 2.6330268156016245e-05\n",
            "step: 420, loss: 6.761353233741829e-06\n",
            "step: 430, loss: 3.640558134065941e-05\n",
            "step: 440, loss: 0.0002144140744348988\n",
            "step: 450, loss: 1.1659959454846103e-05\n",
            "step: 460, loss: 9.624647645978257e-05\n",
            "step: 470, loss: 6.40746247881907e-06\n",
            "step: 480, loss: 1.0814254892466124e-05\n",
            "step: 490, loss: 9.324304301117081e-06\n",
            "step: 500, loss: 6.835861768195173e-06\n",
            "step: 510, loss: 0.02831573598086834\n",
            "step: 520, loss: 6.347825546981767e-06\n",
            "step: 530, loss: 0.0019455758156254888\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9507593189139439, f1=0.9397260273972603, best_f1=0.9424326833797586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.372289474005811e-06\n",
            "step: 10, loss: 1.1652303328446578e-05\n",
            "step: 20, loss: 1.7868911527330056e-05\n",
            "step: 30, loss: 0.0002689447719603777\n",
            "step: 40, loss: 3.5080734960502014e-05\n",
            "step: 50, loss: 0.005279161036014557\n",
            "step: 60, loss: 1.2557607988128439e-05\n",
            "step: 70, loss: 9.10807284526527e-06\n",
            "step: 80, loss: 5.632615284412168e-06\n",
            "step: 90, loss: 6.273348844842985e-06\n",
            "step: 100, loss: 4.328772774897516e-06\n",
            "step: 110, loss: 9.884744213195518e-05\n",
            "step: 120, loss: 8.571774742449634e-06\n",
            "step: 130, loss: 1.1715608707163483e-05\n",
            "step: 140, loss: 8.292385246022604e-06\n",
            "step: 150, loss: 7.610709417349426e-06\n",
            "step: 160, loss: 6.373934866132913e-06\n",
            "step: 170, loss: 5.785334451502422e-06\n",
            "step: 180, loss: 4.903627632302232e-05\n",
            "step: 190, loss: 9.273404430132359e-05\n",
            "step: 200, loss: 1.8810103938449174e-05\n",
            "step: 210, loss: 4.958337285643211e-06\n",
            "step: 220, loss: 6.500587915070355e-06\n",
            "step: 230, loss: 0.0001654895895626396\n",
            "step: 240, loss: 2.9730479582212865e-05\n",
            "step: 250, loss: 6.347849648591364e-06\n",
            "step: 260, loss: 5.1297051868459675e-06\n",
            "step: 270, loss: 9.350297659693751e-06\n",
            "step: 280, loss: 7.469148386007873e-06\n",
            "step: 290, loss: 0.0030256547033786774\n",
            "step: 300, loss: 1.1104636541858781e-05\n",
            "step: 310, loss: 0.014642040245234966\n",
            "step: 320, loss: 8.575518222642131e-06\n",
            "step: 330, loss: 7.357398772001034e-06\n",
            "step: 340, loss: 3.409303826629184e-05\n",
            "step: 350, loss: 0.0021838892716914415\n",
            "step: 360, loss: 1.0710043170547578e-05\n",
            "step: 370, loss: 9.450895959162153e-06\n",
            "step: 380, loss: 6.45216414341121e-06\n",
            "step: 390, loss: 1.5641460777260363e-05\n",
            "step: 400, loss: 0.00034690325264818966\n",
            "step: 410, loss: 1.1652224202407524e-05\n",
            "step: 420, loss: 7.238184025482042e-06\n",
            "step: 430, loss: 6.169022071844665e-06\n",
            "step: 440, loss: 6.884295999043388e-06\n",
            "step: 450, loss: 0.0003819829726126045\n",
            "step: 460, loss: 8.928960596676916e-05\n",
            "step: 470, loss: 5.822595539939357e-06\n",
            "step: 480, loss: 0.0004500889335758984\n",
            "step: 490, loss: 3.103598646703176e-05\n",
            "step: 500, loss: 1.1052486115659121e-05\n",
            "step: 510, loss: 5.8784717111848295e-06\n",
            "step: 520, loss: 0.0024164849892258644\n",
            "step: 530, loss: 7.882574209361337e-06\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9526871841984381, f1=0.9390354868061875, best_f1=0.9424326833797586\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:26, 214.57it/s]\n",
            "load_f1 = 0.9548627268496975\n",
            "real_f1 = 0.9512652296157451\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 178.94it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10svv34hgw7-"
      },
      "source": [
        "# DK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWmGijt_svF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f3e0b4f-69fc-48ea-86f5-e3b85a1b9ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=4baf6012189777b40f11f4eda0a453fd1c900e4ca526e51a88791abf61eaaad0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5dghoraw/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdNk8ikFgw7-"
      },
      "source": [
        "## DK STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCvdP9vMgw7_",
        "outputId": "f2d1d078-7537-4d9f-ed9e-61fb068a1ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4455261826515198\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.27184466019417475, f1=0.32876712328767127, best_f1=0.32876712328767127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.44729769229888916\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.32876712328767127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.41643595695495605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.29629629629629634, f1=0.13333333333333333, best_f1=0.13333333333333333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23967649042606354\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.34285714285714286, f1=0.24390243902439024, best_f1=0.24390243902439024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2773329019546509\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.34285714285714286, f1=0.2, best_f1=0.24390243902439024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20244988799095154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.3636363636363636, f1=0.34374999999999994, best_f1=0.34374999999999994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4755539298057556\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.47058823529411764, f1=0.4444444444444445, best_f1=0.4444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.48466625809669495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.6153846153846153, f1=0.56, best_f1=0.56\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1930168867111206\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6111111111111112, f1=0.6060606060606061, best_f1=0.56\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.274048388004303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.8148148148148148, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.278800904750824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.717948717948718, f1=0.6857142857142857, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11993704736232758\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7857142857142857, f1=0.8148148148148148, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07881737500429153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.75, f1=0.75, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08299315720796585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.75, f1=0.75, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12747129797935486\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.75, f1=0.75, best_f1=0.6666666666666666\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 117912.16it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.6666666666666666\n",
            "real_f1 = 0.6428571428571429\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 180.67it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VIiiAcAgw8B",
        "outputId": "1b23da72-034c-4e4c-f97e-16d403dff4f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6038411855697632\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5301123261451721\n",
            "step: 20, loss: 0.6014789938926697\n",
            "step: 30, loss: 0.30411961674690247\n",
            "step: 40, loss: 0.3494540750980377\n",
            "step: 50, loss: 0.5497896671295166\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.28429946303367615\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 70, loss: 0.08185417205095291\n",
            "step: 80, loss: 0.107674740254879\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "step: 90, loss: 0.3745182752609253\n",
            "step: 100, loss: 0.17719881236553192\n",
            "step: 110, loss: 0.18002092838287354\n",
            "step: 120, loss: 0.08473823219537735\n",
            "step: 130, loss: 0.01753227226436138\n",
            "step: 140, loss: 0.007738980930298567\n",
            "step: 150, loss: 0.011298316530883312\n",
            "step: 160, loss: 0.02510567381978035\n",
            "step: 170, loss: 0.01909954659640789\n",
            "step: 180, loss: 0.018365895375609398\n",
            "step: 190, loss: 0.15328770875930786\n",
            "step: 200, loss: 0.0867021307349205\n",
            "step: 210, loss: 0.13015566766262054\n",
            "step: 220, loss: 0.019597221165895462\n",
            "step: 230, loss: 0.0025635443162173033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.972972972972973, f1=0.9774266365688488, best_f1=0.9774266365688488\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04358610138297081\n",
            "step: 10, loss: 0.089680515229702\n",
            "step: 20, loss: 0.022929158061742783\n",
            "step: 30, loss: 0.01038315985351801\n",
            "step: 40, loss: 0.030466875061392784\n",
            "step: 50, loss: 0.007589625660330057\n",
            "step: 60, loss: 0.009373089298605919\n",
            "step: 70, loss: 0.016704903915524483\n",
            "step: 80, loss: 0.005564062390476465\n",
            "step: 90, loss: 0.0009619499323889613\n",
            "step: 100, loss: 0.0034850446972995996\n",
            "step: 110, loss: 0.028369758278131485\n",
            "step: 120, loss: 0.010964998044073582\n",
            "step: 130, loss: 0.004078483209013939\n",
            "step: 140, loss: 0.002125371713191271\n",
            "step: 150, loss: 0.12121588736772537\n",
            "step: 160, loss: 0.0025350532960146666\n",
            "step: 170, loss: 0.004536310210824013\n",
            "step: 180, loss: 0.005794174503535032\n",
            "step: 190, loss: 0.07629721611738205\n",
            "step: 200, loss: 0.005066392477601767\n",
            "step: 210, loss: 0.06630777567625046\n",
            "step: 220, loss: 0.012047173455357552\n",
            "step: 230, loss: 0.0004906051326543093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9740698985343857, f1=0.9728506787330317, best_f1=0.9728506787330317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008596598170697689\n",
            "step: 10, loss: 0.055974237620830536\n",
            "step: 20, loss: 0.11630965769290924\n",
            "step: 30, loss: 0.007309585344046354\n",
            "step: 40, loss: 0.007133653853088617\n",
            "step: 50, loss: 0.05791918560862541\n",
            "step: 60, loss: 0.026545312255620956\n",
            "step: 70, loss: 0.008017185144126415\n",
            "step: 80, loss: 0.0008488500607199967\n",
            "step: 90, loss: 0.004187884274870157\n",
            "step: 100, loss: 0.0016344566829502583\n",
            "step: 110, loss: 0.0018789914902299643\n",
            "step: 120, loss: 0.0006771547487005591\n",
            "step: 130, loss: 0.0015622101491317153\n",
            "step: 140, loss: 0.001145332818850875\n",
            "step: 150, loss: 0.018273087218403816\n",
            "step: 160, loss: 0.008028673939406872\n",
            "step: 170, loss: 0.0007157225627452135\n",
            "step: 180, loss: 0.033637192100286484\n",
            "step: 190, loss: 0.02557273581624031\n",
            "step: 200, loss: 0.04070090502500534\n",
            "step: 210, loss: 0.009982981719076633\n",
            "step: 220, loss: 0.002694448223337531\n",
            "step: 230, loss: 0.011391636915504932\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9863636363636363, f1=0.9759450171821306, best_f1=0.9759450171821306\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03972074016928673\n",
            "step: 10, loss: 0.004017112776637077\n",
            "step: 20, loss: 0.0009985683718696237\n",
            "step: 30, loss: 0.00036581672611646354\n",
            "step: 40, loss: 0.09453441202640533\n",
            "step: 50, loss: 0.0017132731154561043\n",
            "step: 60, loss: 0.0007656878442503512\n",
            "step: 70, loss: 0.002896663034334779\n",
            "step: 80, loss: 0.0007528535206802189\n",
            "step: 90, loss: 0.003918540198355913\n",
            "step: 100, loss: 0.00077995122410357\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
            "step: 110, loss: 0.0011243289336562157\n",
            "step: 120, loss: 0.08197766542434692\n",
            "step: 130, loss: 0.0148170692846179\n",
            "step: 140, loss: 0.01788390800356865\n",
            "step: 150, loss: 0.009826863184571266\n",
            "step: 160, loss: 0.08080146461725235\n",
            "step: 170, loss: 0.016158320009708405\n",
            "step: 180, loss: 0.11261770129203796\n",
            "step: 190, loss: 0.011433849111199379\n",
            "step: 200, loss: 0.027632759883999825\n",
            "step: 210, loss: 0.018326958641409874\n",
            "step: 220, loss: 0.009605174884200096\n",
            "step: 230, loss: 0.0028083648066967726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9807909604519773, f1=0.9772209567198178, best_f1=0.9759450171821306\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016643961425870657\n",
            "step: 10, loss: 0.005552122835069895\n",
            "step: 20, loss: 0.0011312603019177914\n",
            "step: 30, loss: 0.0028447872027754784\n",
            "step: 40, loss: 0.0035580191761255264\n",
            "step: 50, loss: 0.041008323431015015\n",
            "step: 60, loss: 0.0021863970905542374\n",
            "step: 70, loss: 0.00396115193143487\n",
            "step: 80, loss: 0.08101394027471542\n",
            "step: 90, loss: 0.08545149862766266\n",
            "step: 100, loss: 0.0015613065334036946\n",
            "step: 110, loss: 0.05453092232346535\n",
            "step: 120, loss: 0.017782509326934814\n",
            "step: 130, loss: 0.0013431069673970342\n",
            "step: 140, loss: 0.00045358954230323434\n",
            "step: 150, loss: 0.039901554584503174\n",
            "step: 160, loss: 0.0007982976967468858\n",
            "step: 170, loss: 0.030331986024975777\n",
            "step: 180, loss: 0.00597818149253726\n",
            "step: 190, loss: 0.013421095907688141\n",
            "step: 200, loss: 0.0009661101503297687\n",
            "step: 210, loss: 0.019105764105916023\n",
            "step: 220, loss: 0.0009838031837716699\n",
            "step: 230, loss: 0.025688879191875458\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.987598647125141, f1=0.9830890642615557, best_f1=0.9830890642615557\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00048443235573358834\n",
            "step: 10, loss: 0.002376082818955183\n",
            "step: 20, loss: 0.0014768503606319427\n",
            "step: 30, loss: 0.0014398639323189855\n",
            "step: 40, loss: 0.0002815349434968084\n",
            "step: 50, loss: 0.00045228892122395337\n",
            "step: 60, loss: 0.005222144071012735\n",
            "step: 70, loss: 0.009486712515354156\n",
            "step: 80, loss: 0.0017146425088867545\n",
            "step: 90, loss: 0.003809885121881962\n",
            "step: 100, loss: 0.0015525036724284291\n",
            "step: 110, loss: 0.006912811193615198\n",
            "step: 120, loss: 0.0003961448965128511\n",
            "step: 130, loss: 0.001030834624543786\n",
            "step: 140, loss: 0.001892576925456524\n",
            "step: 150, loss: 0.0005835911724716425\n",
            "step: 160, loss: 0.011153494007885456\n",
            "step: 170, loss: 0.0004911596770398319\n",
            "step: 180, loss: 0.03378280624747276\n",
            "step: 190, loss: 0.00020021099771838635\n",
            "step: 200, loss: 0.0006397637771442533\n",
            "step: 210, loss: 0.0006013587699271739\n",
            "step: 220, loss: 0.003928000573068857\n",
            "step: 230, loss: 0.0026995260268449783\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9887892376681614, f1=0.9765886287625419, best_f1=0.9765886287625419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002813709434121847\n",
            "step: 10, loss: 0.0007939597126096487\n",
            "step: 20, loss: 0.0029874658212065697\n",
            "step: 30, loss: 0.0006070485105738044\n",
            "step: 40, loss: 0.0006962298066355288\n",
            "step: 50, loss: 0.001147167175076902\n",
            "step: 60, loss: 0.014950500801205635\n",
            "step: 70, loss: 0.0005547511391341686\n",
            "step: 80, loss: 0.00461762398481369\n",
            "step: 90, loss: 0.0019460961921140552\n",
            "step: 100, loss: 0.00033740897197276354\n",
            "step: 110, loss: 0.0006306427530944347\n",
            "step: 120, loss: 0.004704300779849291\n",
            "step: 130, loss: 0.007726042065769434\n",
            "step: 140, loss: 0.0007859526085667312\n",
            "step: 150, loss: 0.005797394551336765\n",
            "step: 160, loss: 0.0005112368380650878\n",
            "step: 170, loss: 0.00964822806417942\n",
            "step: 180, loss: 0.00026070483727380633\n",
            "step: 190, loss: 0.0002990987559314817\n",
            "step: 200, loss: 0.053120147436857224\n",
            "step: 210, loss: 0.049500782042741776\n",
            "step: 220, loss: 0.0003748654271475971\n",
            "step: 230, loss: 0.0017798530170693994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9842342342342343, f1=0.9774266365688488, best_f1=0.9765886287625419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018715152982622385\n",
            "step: 10, loss: 0.005974013824015856\n",
            "step: 20, loss: 0.001494915341027081\n",
            "step: 30, loss: 0.0015995721332728863\n",
            "step: 40, loss: 0.00456612603738904\n",
            "step: 50, loss: 0.0014252708060666919\n",
            "step: 60, loss: 0.0007499870262108743\n",
            "step: 70, loss: 0.0001862765639089048\n",
            "step: 80, loss: 0.028395699337124825\n",
            "step: 90, loss: 0.0009411269566044211\n",
            "step: 100, loss: 0.0008635988342575729\n",
            "step: 110, loss: 0.001729748211801052\n",
            "step: 120, loss: 0.0026938433293253183\n",
            "step: 130, loss: 0.0006972153205424547\n",
            "step: 140, loss: 0.007799461018294096\n",
            "step: 150, loss: 0.027800889685750008\n",
            "step: 160, loss: 0.0154071981087327\n",
            "step: 170, loss: 0.03440683335065842\n",
            "step: 180, loss: 0.00040985149098560214\n",
            "step: 190, loss: 0.0009168870747089386\n",
            "step: 200, loss: 0.010413861833512783\n",
            "step: 210, loss: 0.001662153284996748\n",
            "step: 220, loss: 0.0009122680639848113\n",
            "step: 230, loss: 0.00031728227622807026\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9854423292273236, f1=0.9787709497206705, best_f1=0.9765886287625419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021377911616582423\n",
            "step: 10, loss: 0.0008868036675266922\n",
            "step: 20, loss: 0.0006400028942152858\n",
            "step: 30, loss: 0.001038572401739657\n",
            "step: 40, loss: 0.0005756777245551348\n",
            "step: 50, loss: 0.0009292070171795785\n",
            "step: 60, loss: 0.0005877428338862956\n",
            "step: 70, loss: 0.023757144808769226\n",
            "step: 80, loss: 0.0006365804583765566\n",
            "step: 90, loss: 0.0310650784522295\n",
            "step: 100, loss: 0.00030576030258089304\n",
            "step: 110, loss: 0.0002195903070969507\n",
            "step: 120, loss: 0.008298464119434357\n",
            "step: 130, loss: 0.0031813844107091427\n",
            "step: 140, loss: 0.0003339955292176455\n",
            "step: 150, loss: 0.0005596216069534421\n",
            "step: 160, loss: 0.0036277147009968758\n",
            "step: 170, loss: 0.0013685954036191106\n",
            "step: 180, loss: 0.0012165920343250036\n",
            "step: 190, loss: 0.0002032312040682882\n",
            "step: 200, loss: 0.00021372965420596302\n",
            "step: 210, loss: 0.009862393140792847\n",
            "step: 220, loss: 0.00016830085951369256\n",
            "step: 230, loss: 0.001579958712682128\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9865470852017937, f1=0.9819819819819819, best_f1=0.9765886287625419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020427012350410223\n",
            "step: 10, loss: 0.0005356986657716334\n",
            "step: 20, loss: 0.00032545262365601957\n",
            "step: 30, loss: 9.569587564328685e-05\n",
            "step: 40, loss: 0.0007737097912468016\n",
            "step: 50, loss: 6.279148510657251e-05\n",
            "step: 60, loss: 0.0001912511943373829\n",
            "step: 70, loss: 0.0007670070044696331\n",
            "step: 80, loss: 0.00016917353786993772\n",
            "step: 90, loss: 0.000117877236334607\n",
            "step: 100, loss: 0.00010241389099974185\n",
            "step: 110, loss: 0.012753610499203205\n",
            "step: 120, loss: 0.00011131393694086\n",
            "step: 130, loss: 0.00026449200231581926\n",
            "step: 140, loss: 0.00046796276001259685\n",
            "step: 150, loss: 0.0008762399666011333\n",
            "step: 160, loss: 7.142400136217475e-05\n",
            "step: 170, loss: 0.00016268545005004853\n",
            "step: 180, loss: 0.0008334098965860903\n",
            "step: 190, loss: 0.00023402941587846726\n",
            "step: 200, loss: 0.000676424300763756\n",
            "step: 210, loss: 0.032044630497694016\n",
            "step: 220, loss: 7.812853436917067e-05\n",
            "step: 230, loss: 8.219205483328551e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9887387387387387, f1=0.9819004524886877, best_f1=0.9765886287625419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.886570140020922e-05\n",
            "step: 10, loss: 0.00014068960445001721\n",
            "step: 20, loss: 0.0001243615843122825\n",
            "step: 30, loss: 0.00010296619439031929\n",
            "step: 40, loss: 2.655868956935592e-05\n",
            "step: 50, loss: 6.28834095550701e-05\n",
            "step: 60, loss: 0.009896083734929562\n",
            "step: 70, loss: 6.198789196787402e-05\n",
            "step: 80, loss: 0.0008812181185930967\n",
            "step: 90, loss: 0.04242029786109924\n",
            "step: 100, loss: 0.002356862183660269\n",
            "step: 110, loss: 0.0007545602857135236\n",
            "step: 120, loss: 0.00021564806229434907\n",
            "step: 130, loss: 0.00013822065375279635\n",
            "step: 140, loss: 0.0002749140840023756\n",
            "step: 150, loss: 0.0001289007777813822\n",
            "step: 160, loss: 0.07851088792085648\n",
            "step: 170, loss: 0.0009854048257693648\n",
            "step: 180, loss: 0.000642909377347678\n",
            "step: 190, loss: 0.0001732706732582301\n",
            "step: 200, loss: 0.0029363995417952538\n",
            "step: 210, loss: 0.0005510759074240923\n",
            "step: 220, loss: 0.0018644160591065884\n",
            "step: 230, loss: 0.00020333443535491824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.987598647125141, f1=0.9807909604519773, best_f1=0.9765886287625419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018969419761560857\n",
            "step: 10, loss: 5.981950380373746e-05\n",
            "step: 20, loss: 0.01978805847465992\n",
            "step: 30, loss: 0.02950344793498516\n",
            "step: 40, loss: 9.950847743311897e-05\n",
            "step: 50, loss: 0.0001068541023414582\n",
            "step: 60, loss: 0.00019783001334872097\n",
            "step: 70, loss: 6.496915739262477e-05\n",
            "step: 80, loss: 3.709707743837498e-05\n",
            "step: 90, loss: 0.0005180159932933748\n",
            "step: 100, loss: 7.890518463682383e-05\n",
            "step: 110, loss: 5.467044684337452e-05\n",
            "step: 120, loss: 9.164876973954961e-05\n",
            "step: 130, loss: 6.412194488802925e-05\n",
            "step: 140, loss: 0.00010294420644640923\n",
            "step: 150, loss: 7.894742157077417e-05\n",
            "step: 160, loss: 0.00037176060141064227\n",
            "step: 170, loss: 6.218014459591359e-05\n",
            "step: 180, loss: 5.7385070249438286e-05\n",
            "step: 190, loss: 0.011286252178251743\n",
            "step: 200, loss: 9.539545135339722e-05\n",
            "step: 210, loss: 0.0002779607893899083\n",
            "step: 220, loss: 0.020872173830866814\n",
            "step: 230, loss: 0.00040469097439199686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9886621315192743, f1=0.9829351535836178, best_f1=0.9765886287625419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.058662256691605e-05\n",
            "step: 10, loss: 6.296303035924211e-05\n",
            "step: 20, loss: 8.247750520240515e-05\n",
            "step: 30, loss: 2.8827971618738957e-05\n",
            "step: 40, loss: 6.683432002319023e-05\n",
            "step: 50, loss: 0.00159190664999187\n",
            "step: 60, loss: 5.682041228283197e-05\n",
            "step: 70, loss: 3.854671012959443e-05\n",
            "step: 80, loss: 3.2094816560857e-05\n",
            "step: 90, loss: 4.297896521165967e-05\n",
            "step: 100, loss: 8.73986937222071e-05\n",
            "step: 110, loss: 0.0002532113285269588\n",
            "step: 120, loss: 8.478414383716881e-05\n",
            "step: 130, loss: 6.390047929016873e-05\n",
            "step: 140, loss: 4.86919452669099e-05\n",
            "step: 150, loss: 2.398214019194711e-05\n",
            "step: 160, loss: 0.026197075843811035\n",
            "step: 170, loss: 5.9036297898273915e-05\n",
            "step: 180, loss: 0.036377862095832825\n",
            "step: 190, loss: 6.698729703202844e-05\n",
            "step: 200, loss: 3.456833655945957e-05\n",
            "step: 210, loss: 0.0001902063231682405\n",
            "step: 220, loss: 5.171047450858168e-05\n",
            "step: 230, loss: 0.00011404855467844754\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9887892376681614, f1=0.9808342728297633, best_f1=0.9765886287625419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.047962571959943e-05\n",
            "step: 10, loss: 5.114505620440468e-05\n",
            "step: 20, loss: 7.163298869272694e-05\n",
            "step: 30, loss: 8.822892414173111e-05\n",
            "step: 40, loss: 4.682708822656423e-05\n",
            "step: 50, loss: 5.33322490809951e-05\n",
            "step: 60, loss: 5.189944204175845e-05\n",
            "step: 70, loss: 4.3359133997000754e-05\n",
            "step: 80, loss: 3.5179255064576864e-05\n",
            "step: 90, loss: 7.092089072102681e-05\n",
            "step: 100, loss: 4.426038867677562e-05\n",
            "step: 110, loss: 5.532054274226539e-05\n",
            "step: 120, loss: 2.0704304915852845e-05\n",
            "step: 130, loss: 6.41403385088779e-05\n",
            "step: 140, loss: 3.7909620004938915e-05\n",
            "step: 150, loss: 3.136470331810415e-05\n",
            "step: 160, loss: 4.9999740440398455e-05\n",
            "step: 170, loss: 5.579539720201865e-05\n",
            "step: 180, loss: 4.43128046754282e-05\n",
            "step: 190, loss: 0.0003559836186468601\n",
            "step: 200, loss: 9.639786730986089e-05\n",
            "step: 210, loss: 0.00010876672604354098\n",
            "step: 220, loss: 4.322913082432933e-05\n",
            "step: 230, loss: 2.5990182621171698e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9865771812080537, f1=0.9775784753363228, best_f1=0.9765886287625419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015217810869216919\n",
            "step: 10, loss: 3.9526548789581284e-05\n",
            "step: 20, loss: 6.857956759631634e-05\n",
            "step: 30, loss: 3.643138916231692e-05\n",
            "step: 40, loss: 2.7800442694569938e-05\n",
            "step: 50, loss: 3.614788511185907e-05\n",
            "step: 60, loss: 0.025640057399868965\n",
            "step: 70, loss: 3.245680272812024e-05\n",
            "step: 80, loss: 3.5663866583490744e-05\n",
            "step: 90, loss: 3.3421350963180885e-05\n",
            "step: 100, loss: 2.8794898753403686e-05\n",
            "step: 110, loss: 0.0002152538945665583\n",
            "step: 120, loss: 0.02230699174106121\n",
            "step: 130, loss: 2.7319958462612703e-05\n",
            "step: 140, loss: 0.024392640218138695\n",
            "step: 150, loss: 0.000124288271763362\n",
            "step: 160, loss: 4.579054075293243e-05\n",
            "step: 170, loss: 2.7118518119095825e-05\n",
            "step: 180, loss: 5.0268346967641264e-05\n",
            "step: 190, loss: 9.768297604750842e-05\n",
            "step: 200, loss: 0.00015084221377037466\n",
            "step: 210, loss: 0.04562801867723465\n",
            "step: 220, loss: 4.3235981138423085e-05\n",
            "step: 230, loss: 5.311031418386847e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9876819708846584, f1=0.9786276715410572, best_f1=0.9765886287625419\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:13, 180.09it/s]\n",
            "load_f1 = 0.9887892376681614\n",
            "real_f1 = 0.9854748603351955\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:26, 167.93it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4v1tmXbgw8B"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUUIV1IBgw8B",
        "outputId": "28cb048a-eadc-4a0d-8db4-0213718a697d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6633002161979675\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4148438274860382\n",
            "step: 20, loss: 0.2740328907966614\n",
            "step: 30, loss: 0.34824463725090027\n",
            "step: 40, loss: 0.3712451159954071\n",
            "step: 50, loss: 0.5943734049797058\n",
            "step: 60, loss: 0.32668256759643555\n",
            "step: 70, loss: 0.4528818726539612\n",
            "step: 80, loss: 0.28014758229255676\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.3125389516353607\n",
            "step: 100, loss: 0.28614190220832825\n",
            "step: 110, loss: 0.16533403098583221\n",
            "step: 120, loss: 0.23268966376781464\n",
            "step: 130, loss: 0.22107017040252686\n",
            "step: 140, loss: 0.24529296159744263\n",
            "step: 150, loss: 0.10652834177017212\n",
            "step: 160, loss: 0.4318292737007141\n",
            "step: 170, loss: 0.24340985715389252\n",
            "step: 180, loss: 0.17252673208713531\n",
            "step: 190, loss: 0.1444459855556488\n",
            "step: 200, loss: 0.027625590562820435\n",
            "step: 210, loss: 0.06703818589448929\n",
            "step: 220, loss: 0.21251754462718964\n",
            "step: 230, loss: 0.15305893123149872\n",
            "step: 240, loss: 0.03334831818938255\n",
            "step: 250, loss: 0.16459417343139648\n",
            "step: 260, loss: 0.13200300931930542\n",
            "step: 270, loss: 0.5005769729614258\n",
            "step: 280, loss: 0.07783783972263336\n",
            "step: 290, loss: 0.08746127784252167\n",
            "step: 300, loss: 0.06664042174816132\n",
            "step: 310, loss: 0.10136877000331879\n",
            "step: 320, loss: 0.05649387463927269\n",
            "step: 330, loss: 0.110294409096241\n",
            "step: 340, loss: 0.34135305881500244\n",
            "step: 350, loss: 0.16996128857135773\n",
            "step: 360, loss: 0.07086192816495895\n",
            "step: 370, loss: 0.08073178678750992\n",
            "step: 380, loss: 0.2821148931980133\n",
            "step: 390, loss: 0.05135170370340347\n",
            "step: 400, loss: 0.041219860315322876\n",
            "step: 410, loss: 0.36446523666381836\n",
            "step: 420, loss: 0.047029171139001846\n",
            "step: 430, loss: 0.046573080122470856\n",
            "step: 440, loss: 0.05256876349449158\n",
            "step: 450, loss: 0.13140837848186493\n",
            "step: 460, loss: 0.12140364199876785\n",
            "step: 470, loss: 0.04102642089128494\n",
            "step: 480, loss: 0.157316654920578\n",
            "step: 490, loss: 0.13170652091503143\n",
            "step: 500, loss: 0.040289971977472305\n",
            "step: 510, loss: 0.05028347298502922\n",
            "step: 520, loss: 0.07637792825698853\n",
            "step: 530, loss: 0.22066886723041534\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9242009132420091, f1=0.9245454545454544, best_f1=0.9245454545454544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08949863165616989\n",
            "step: 10, loss: 0.055857449769973755\n",
            "step: 20, loss: 0.04780464246869087\n",
            "step: 30, loss: 0.04032830148935318\n",
            "step: 40, loss: 0.07635578513145447\n",
            "step: 50, loss: 0.07894771546125412\n",
            "step: 60, loss: 0.049136653542518616\n",
            "step: 70, loss: 0.12516506016254425\n",
            "step: 80, loss: 0.0965457558631897\n",
            "step: 90, loss: 0.03204255551099777\n",
            "step: 100, loss: 0.18007643520832062\n",
            "step: 110, loss: 0.012471344321966171\n",
            "step: 120, loss: 0.061274368315935135\n",
            "step: 130, loss: 0.049883272498846054\n",
            "step: 140, loss: 0.11205585300922394\n",
            "step: 150, loss: 0.03700680658221245\n",
            "step: 160, loss: 0.049664195626974106\n",
            "step: 170, loss: 0.023966170847415924\n",
            "step: 180, loss: 0.018221024423837662\n",
            "step: 190, loss: 0.041271861642599106\n",
            "step: 200, loss: 0.2161022573709488\n",
            "step: 210, loss: 0.027680503204464912\n",
            "step: 220, loss: 0.002421043114736676\n",
            "step: 230, loss: 0.041640955954790115\n",
            "step: 240, loss: 0.023105962201952934\n",
            "step: 250, loss: 0.04812060669064522\n",
            "step: 260, loss: 0.107023686170578\n",
            "step: 270, loss: 0.08186066895723343\n",
            "step: 280, loss: 0.04170987755060196\n",
            "step: 290, loss: 0.016361473128199577\n",
            "step: 300, loss: 0.023874381557106972\n",
            "step: 310, loss: 0.028567012399435043\n",
            "step: 320, loss: 0.06116918474435806\n",
            "step: 330, loss: 0.10188967734575272\n",
            "step: 340, loss: 0.06922028213739395\n",
            "step: 350, loss: 0.018361438065767288\n",
            "step: 360, loss: 0.059423040598630905\n",
            "step: 370, loss: 0.04467778280377388\n",
            "step: 380, loss: 0.11413725465536118\n",
            "step: 390, loss: 0.01401477586477995\n",
            "step: 400, loss: 0.02519138529896736\n",
            "step: 410, loss: 0.043841708451509476\n",
            "step: 420, loss: 0.15103694796562195\n",
            "step: 430, loss: 0.16211281716823578\n",
            "step: 440, loss: 0.0071416194550693035\n",
            "step: 450, loss: 0.0496302954852581\n",
            "step: 460, loss: 0.057666487991809845\n",
            "step: 470, loss: 0.01958024501800537\n",
            "step: 480, loss: 0.012840621173381805\n",
            "step: 490, loss: 0.10608899593353271\n",
            "step: 500, loss: 0.030106021091341972\n",
            "step: 510, loss: 0.03706321120262146\n",
            "step: 520, loss: 0.380759060382843\n",
            "step: 530, loss: 0.16642238199710846\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9459962756052142, f1=0.9412861136999067, best_f1=0.9412861136999067\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10141416639089584\n",
            "step: 10, loss: 0.029501279816031456\n",
            "step: 20, loss: 0.010505826212465763\n",
            "step: 30, loss: 0.08849829435348511\n",
            "step: 40, loss: 0.056656595319509506\n",
            "step: 50, loss: 0.029805276542901993\n",
            "step: 60, loss: 0.057562824338674545\n",
            "step: 70, loss: 0.004698685370385647\n",
            "step: 80, loss: 0.01716931164264679\n",
            "step: 90, loss: 0.032052118331193924\n",
            "step: 100, loss: 0.042644258588552475\n",
            "step: 110, loss: 0.041154537349939346\n",
            "step: 120, loss: 0.20637866854667664\n",
            "step: 130, loss: 0.055472951382398605\n",
            "step: 140, loss: 0.01623453199863434\n",
            "step: 150, loss: 0.038815416395664215\n",
            "step: 160, loss: 0.011357192881405354\n",
            "step: 170, loss: 0.012323918752372265\n",
            "step: 180, loss: 0.02295919694006443\n",
            "step: 190, loss: 0.014086339622735977\n",
            "step: 200, loss: 0.02605254389345646\n",
            "step: 210, loss: 0.03477204591035843\n",
            "step: 220, loss: 0.09323188662528992\n",
            "step: 230, loss: 0.04963100329041481\n",
            "step: 240, loss: 0.10949353128671646\n",
            "step: 250, loss: 0.20394931733608246\n",
            "step: 260, loss: 0.10624699294567108\n",
            "step: 270, loss: 0.02867463417351246\n",
            "step: 280, loss: 0.06135249882936478\n",
            "step: 290, loss: 0.12668099999427795\n",
            "step: 300, loss: 0.11801011860370636\n",
            "step: 310, loss: 0.09548146277666092\n",
            "step: 320, loss: 0.046513646841049194\n",
            "step: 330, loss: 0.0074676028452813625\n",
            "step: 340, loss: 0.014503000304102898\n",
            "step: 350, loss: 0.09589576721191406\n",
            "step: 360, loss: 0.009421447291970253\n",
            "step: 370, loss: 0.09232217818498611\n",
            "step: 380, loss: 0.018594952300190926\n",
            "step: 390, loss: 0.00419800728559494\n",
            "step: 400, loss: 0.07247129827737808\n",
            "step: 410, loss: 0.02592460811138153\n",
            "step: 420, loss: 0.020667267963290215\n",
            "step: 430, loss: 0.027538076043128967\n",
            "step: 440, loss: 0.22676828503608704\n",
            "step: 450, loss: 0.01134430430829525\n",
            "step: 460, loss: 0.0641363263130188\n",
            "step: 470, loss: 0.01061617024242878\n",
            "step: 480, loss: 0.09748475253582001\n",
            "step: 490, loss: 0.0050543323159217834\n",
            "step: 500, loss: 0.04013563692569733\n",
            "step: 510, loss: 0.08667320758104324\n",
            "step: 520, loss: 0.016191110014915466\n",
            "step: 530, loss: 0.005028291139751673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9475638051044084, f1=0.9407407407407408, best_f1=0.9407407407407408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.051063355058431625\n",
            "step: 10, loss: 0.00392047269269824\n",
            "step: 20, loss: 0.174769327044487\n",
            "step: 30, loss: 0.09162045270204544\n",
            "step: 40, loss: 0.06841558963060379\n",
            "step: 50, loss: 0.1054617166519165\n",
            "step: 60, loss: 0.003989634104073048\n",
            "step: 70, loss: 0.019989101216197014\n",
            "step: 80, loss: 0.11267782747745514\n",
            "step: 90, loss: 0.10011187195777893\n",
            "step: 100, loss: 0.0012275835033506155\n",
            "step: 110, loss: 0.04466306045651436\n",
            "step: 120, loss: 0.001739670056849718\n",
            "step: 130, loss: 0.06533178687095642\n",
            "step: 140, loss: 0.043475642800331116\n",
            "step: 150, loss: 0.002968843560665846\n",
            "step: 160, loss: 0.029862895607948303\n",
            "step: 170, loss: 0.030103206634521484\n",
            "step: 180, loss: 0.0387626513838768\n",
            "step: 190, loss: 0.07137357443571091\n",
            "step: 200, loss: 0.10470839589834213\n",
            "step: 210, loss: 0.005065463948994875\n",
            "step: 220, loss: 0.1302160769701004\n",
            "step: 230, loss: 0.004015476442873478\n",
            "step: 240, loss: 0.0035426500253379345\n",
            "step: 250, loss: 0.18818065524101257\n",
            "step: 260, loss: 0.0017500227550044656\n",
            "step: 270, loss: 0.09140487760305405\n",
            "step: 280, loss: 0.012478198856115341\n",
            "step: 290, loss: 0.07081977277994156\n",
            "step: 300, loss: 0.02349177934229374\n",
            "step: 310, loss: 0.0048771449364721775\n",
            "step: 320, loss: 0.16081944108009338\n",
            "step: 330, loss: 0.017105545848608017\n",
            "step: 340, loss: 0.0020675405394285917\n",
            "step: 350, loss: 0.24486367404460907\n",
            "step: 360, loss: 0.05228249728679657\n",
            "step: 370, loss: 0.007409068290144205\n",
            "step: 380, loss: 0.007212957367300987\n",
            "step: 390, loss: 0.003638830268755555\n",
            "step: 400, loss: 0.017117232084274292\n",
            "step: 410, loss: 0.009843673557043076\n",
            "step: 420, loss: 0.003442309098318219\n",
            "step: 430, loss: 0.006785341538488865\n",
            "step: 440, loss: 0.0031444188207387924\n",
            "step: 450, loss: 0.0037661301903426647\n",
            "step: 460, loss: 0.04609248414635658\n",
            "step: 470, loss: 0.021742699667811394\n",
            "step: 480, loss: 0.007332568988204002\n",
            "step: 490, loss: 0.011863185092806816\n",
            "step: 500, loss: 0.014603016898036003\n",
            "step: 510, loss: 0.2020021229982376\n",
            "step: 520, loss: 0.008481043390929699\n",
            "step: 530, loss: 0.06720175594091415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9347826086956521, f1=0.9352720450281427, best_f1=0.9407407407407408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00345848617143929\n",
            "step: 10, loss: 0.015576564706861973\n",
            "step: 20, loss: 0.0026730732060968876\n",
            "step: 30, loss: 0.03444598242640495\n",
            "step: 40, loss: 0.0047210403718054295\n",
            "step: 50, loss: 0.1525445431470871\n",
            "step: 60, loss: 0.09606759995222092\n",
            "step: 70, loss: 0.004166719503700733\n",
            "step: 80, loss: 0.0016459355829283595\n",
            "step: 90, loss: 0.008116289041936398\n",
            "step: 100, loss: 0.006884741596877575\n",
            "step: 110, loss: 0.02853008732199669\n",
            "step: 120, loss: 0.1340763121843338\n",
            "step: 130, loss: 0.015573451295495033\n",
            "step: 140, loss: 0.02809293381869793\n",
            "step: 150, loss: 0.011937426403164864\n",
            "step: 160, loss: 0.004329086281359196\n",
            "step: 170, loss: 0.09987609833478928\n",
            "step: 180, loss: 0.012094035744667053\n",
            "step: 190, loss: 0.002181239193305373\n",
            "step: 200, loss: 0.012692553922533989\n",
            "step: 210, loss: 0.006559773813933134\n",
            "step: 220, loss: 0.006166442763060331\n",
            "step: 230, loss: 0.007490357384085655\n",
            "step: 240, loss: 0.0021318630315363407\n",
            "step: 250, loss: 0.10241948068141937\n",
            "step: 260, loss: 0.0030514297541230917\n",
            "step: 270, loss: 0.002162665594369173\n",
            "step: 280, loss: 0.009775730781257153\n",
            "step: 290, loss: 0.03849324584007263\n",
            "step: 300, loss: 0.10218816250562668\n",
            "step: 310, loss: 0.058354027569293976\n",
            "step: 320, loss: 0.0248107872903347\n",
            "step: 330, loss: 0.0007387064397335052\n",
            "step: 340, loss: 0.017529375851154327\n",
            "step: 350, loss: 0.011567327193915844\n",
            "step: 360, loss: 0.0009648058330640197\n",
            "step: 370, loss: 0.002088970970362425\n",
            "step: 380, loss: 0.00339457206428051\n",
            "step: 390, loss: 0.02492385357618332\n",
            "step: 400, loss: 0.012181345373392105\n",
            "step: 410, loss: 0.07798461616039276\n",
            "step: 420, loss: 0.1924920380115509\n",
            "step: 430, loss: 0.0791456550359726\n",
            "step: 440, loss: 0.0011186142219230533\n",
            "step: 450, loss: 0.015625586733222008\n",
            "step: 460, loss: 0.004163761157542467\n",
            "step: 470, loss: 0.009213329292833805\n",
            "step: 480, loss: 0.0018323963740840554\n",
            "step: 490, loss: 0.0061677927151322365\n",
            "step: 500, loss: 0.06218675523996353\n",
            "step: 510, loss: 0.005562108475714922\n",
            "step: 520, loss: 0.09015243500471115\n",
            "step: 530, loss: 0.007972460240125656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9436947417403444, f1=0.9420491423273065, best_f1=0.9407407407407408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0035181406419724226\n",
            "step: 10, loss: 0.0010742545127868652\n",
            "step: 20, loss: 0.0015767146833240986\n",
            "step: 30, loss: 0.002117904368788004\n",
            "step: 40, loss: 0.037603817880153656\n",
            "step: 50, loss: 0.0004237226094119251\n",
            "step: 60, loss: 0.001563016208820045\n",
            "step: 70, loss: 0.017232853919267654\n",
            "step: 80, loss: 0.00017938134260475636\n",
            "step: 90, loss: 0.056798674166202545\n",
            "step: 100, loss: 0.0010517328046262264\n",
            "step: 110, loss: 0.0022451584227383137\n",
            "step: 120, loss: 0.0043974947184324265\n",
            "step: 130, loss: 0.014241470023989677\n",
            "step: 140, loss: 0.002595874946564436\n",
            "step: 150, loss: 0.0022599902004003525\n",
            "step: 160, loss: 0.007076721638441086\n",
            "step: 170, loss: 0.001357779954560101\n",
            "step: 180, loss: 0.003915566951036453\n",
            "step: 190, loss: 0.035121217370033264\n",
            "step: 200, loss: 0.027078501880168915\n",
            "step: 210, loss: 0.0014767477987334132\n",
            "step: 220, loss: 0.005164914298802614\n",
            "step: 230, loss: 0.0011115085799247026\n",
            "step: 240, loss: 0.0006016873521730304\n",
            "step: 250, loss: 0.14038372039794922\n",
            "step: 260, loss: 0.00020592391956597567\n",
            "step: 270, loss: 0.0015049122739583254\n",
            "step: 280, loss: 0.001230369322001934\n",
            "step: 290, loss: 0.0011199346045032144\n",
            "step: 300, loss: 0.007621431723237038\n",
            "step: 310, loss: 0.07843036949634552\n",
            "step: 320, loss: 0.0005194679251872003\n",
            "step: 330, loss: 0.003319766139611602\n",
            "step: 340, loss: 0.0009772221092134714\n",
            "step: 350, loss: 0.003225277177989483\n",
            "step: 360, loss: 0.00606259610503912\n",
            "step: 370, loss: 0.003768308786675334\n",
            "step: 380, loss: 0.001842169906012714\n",
            "step: 390, loss: 0.004791955929249525\n",
            "step: 400, loss: 0.0032524950802326202\n",
            "step: 410, loss: 0.0009972768602892756\n",
            "step: 420, loss: 0.0029691578820347786\n",
            "step: 430, loss: 0.002742951735854149\n",
            "step: 440, loss: 0.02984588034451008\n",
            "step: 450, loss: 0.18628087639808655\n",
            "step: 460, loss: 0.001452928176149726\n",
            "step: 470, loss: 0.0017512327758595347\n",
            "step: 480, loss: 0.009689467959105968\n",
            "step: 490, loss: 0.017411191016435623\n",
            "step: 500, loss: 0.004648905247449875\n",
            "step: 510, loss: 0.08949360251426697\n",
            "step: 520, loss: 0.0026903704274445772\n",
            "step: 530, loss: 0.005595784168690443\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.94357515410147, f1=0.9408424041646947, best_f1=0.9407407407407408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005565283354371786\n",
            "step: 10, loss: 0.0016898594330996275\n",
            "step: 20, loss: 0.005012927111238241\n",
            "step: 30, loss: 0.02338939905166626\n",
            "step: 40, loss: 0.08841172605752945\n",
            "step: 50, loss: 0.013403872027993202\n",
            "step: 60, loss: 0.003830250585451722\n",
            "step: 70, loss: 0.0012486891355365515\n",
            "step: 80, loss: 0.0004424243816174567\n",
            "step: 90, loss: 0.0003363527648616582\n",
            "step: 100, loss: 0.04327262192964554\n",
            "step: 110, loss: 0.00014276517322286963\n",
            "step: 120, loss: 0.0002838599029928446\n",
            "step: 130, loss: 0.0007002754718996584\n",
            "step: 140, loss: 0.0005459652747958899\n",
            "step: 150, loss: 0.0007154330960474908\n",
            "step: 160, loss: 0.0040311310440301895\n",
            "step: 170, loss: 0.00031638069776818156\n",
            "step: 180, loss: 0.0005326357204467058\n",
            "step: 190, loss: 0.023459644988179207\n",
            "step: 200, loss: 0.00015505115152336657\n",
            "step: 210, loss: 0.001470611896365881\n",
            "step: 220, loss: 0.0033659012988209724\n",
            "step: 230, loss: 0.0035306329373270273\n",
            "step: 240, loss: 0.00601207185536623\n",
            "step: 250, loss: 0.0014615264954045415\n",
            "step: 260, loss: 0.00018857588293030858\n",
            "step: 270, loss: 0.00016703159781172872\n",
            "step: 280, loss: 0.02190690115094185\n",
            "step: 290, loss: 0.0020987954922020435\n",
            "step: 300, loss: 0.0007223711581900716\n",
            "step: 310, loss: 0.0012580036418512464\n",
            "step: 320, loss: 0.0003087654185947031\n",
            "step: 330, loss: 0.003336156252771616\n",
            "step: 340, loss: 0.0004732045345008373\n",
            "step: 350, loss: 0.0014132140204310417\n",
            "step: 360, loss: 0.07212752848863602\n",
            "step: 370, loss: 0.07843141257762909\n",
            "step: 380, loss: 0.012461748905479908\n",
            "step: 390, loss: 0.004683241713792086\n",
            "step: 400, loss: 0.1519060879945755\n",
            "step: 410, loss: 0.004558388609439135\n",
            "step: 420, loss: 0.012035132385790348\n",
            "step: 430, loss: 0.0028921174816787243\n",
            "step: 440, loss: 0.0005927865859121084\n",
            "step: 450, loss: 0.0036579580046236515\n",
            "step: 460, loss: 0.0044378796592354774\n",
            "step: 470, loss: 0.16118201613426208\n",
            "step: 480, loss: 0.009555208496749401\n",
            "step: 490, loss: 0.0014718043385073543\n",
            "step: 500, loss: 0.004427107982337475\n",
            "step: 510, loss: 0.0007734039681963623\n",
            "step: 520, loss: 0.000837378844153136\n",
            "step: 530, loss: 0.004193959292024374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9508497932935233, f1=0.9457720588235294, best_f1=0.9457720588235294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005718591273762286\n",
            "step: 10, loss: 0.0006227887351997197\n",
            "step: 20, loss: 0.0004749142681248486\n",
            "step: 30, loss: 0.00018997212464455515\n",
            "step: 40, loss: 0.00010104570537805557\n",
            "step: 50, loss: 0.0032329782843589783\n",
            "step: 60, loss: 0.0003179177874699235\n",
            "step: 70, loss: 0.0003768336900975555\n",
            "step: 80, loss: 0.00027335959021002054\n",
            "step: 90, loss: 0.00018108125368598849\n",
            "step: 100, loss: 7.249003101605922e-05\n",
            "step: 110, loss: 0.0001874118606792763\n",
            "step: 120, loss: 0.00034126886748708785\n",
            "step: 130, loss: 0.0128393629565835\n",
            "step: 140, loss: 0.00045726820826530457\n",
            "step: 150, loss: 9.851998038357124e-05\n",
            "step: 160, loss: 0.00018576468573883176\n",
            "step: 170, loss: 0.046619266271591187\n",
            "step: 180, loss: 0.000329270405927673\n",
            "step: 190, loss: 0.0011765939416363835\n",
            "step: 200, loss: 0.0027124546468257904\n",
            "step: 210, loss: 0.038732416927814484\n",
            "step: 220, loss: 0.00014921546971891075\n",
            "step: 230, loss: 0.082218237221241\n",
            "step: 240, loss: 0.0005860731471329927\n",
            "step: 250, loss: 0.0008017407380975783\n",
            "step: 260, loss: 0.0005088448524475098\n",
            "step: 270, loss: 0.005281486082822084\n",
            "step: 280, loss: 0.0003179551858920604\n",
            "step: 290, loss: 0.0016496900934726\n",
            "step: 300, loss: 0.00013123768439982086\n",
            "step: 310, loss: 0.0017792544094845653\n",
            "step: 320, loss: 0.00020789091649930924\n",
            "step: 330, loss: 0.0003673468600027263\n",
            "step: 340, loss: 0.03601280599832535\n",
            "step: 350, loss: 7.506808469770476e-05\n",
            "step: 360, loss: 0.07981230318546295\n",
            "step: 370, loss: 0.004620030056685209\n",
            "step: 380, loss: 0.0018395379884168506\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 390, loss: 0.0022296514362096786\n",
            "step: 400, loss: 0.013984136283397675\n",
            "step: 410, loss: 0.0010743607999756932\n",
            "step: 420, loss: 0.0007012312998995185\n",
            "step: 430, loss: 0.0014684763737022877\n",
            "step: 440, loss: 0.04536813870072365\n",
            "step: 450, loss: 0.0011643313337117434\n",
            "step: 460, loss: 0.0020510887261480093\n",
            "step: 470, loss: 0.09835527092218399\n",
            "step: 480, loss: 0.03837912902235985\n",
            "step: 490, loss: 0.00031395984115079045\n",
            "step: 500, loss: 0.00011043033009627834\n",
            "step: 510, loss: 0.005708071403205395\n",
            "step: 520, loss: 0.00028662860859185457\n",
            "step: 530, loss: 0.06863473355770111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9479022591055786, f1=0.9404157043879908, best_f1=0.9457720588235294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020988118194509298\n",
            "step: 10, loss: 0.0030445209704339504\n",
            "step: 20, loss: 0.0023620896972715855\n",
            "step: 30, loss: 0.04578807204961777\n",
            "step: 40, loss: 0.004818170331418514\n",
            "step: 50, loss: 0.0012112336698919535\n",
            "step: 60, loss: 0.0018608501413837075\n",
            "step: 70, loss: 0.07366760820150375\n",
            "step: 80, loss: 0.03945314884185791\n",
            "step: 90, loss: 0.05018068850040436\n",
            "step: 100, loss: 0.01215367577970028\n",
            "step: 110, loss: 0.009688557125627995\n",
            "step: 120, loss: 0.0008930990588851273\n",
            "step: 130, loss: 0.0006724462728016078\n",
            "step: 140, loss: 0.0003542932972777635\n",
            "step: 150, loss: 0.023562144488096237\n",
            "step: 160, loss: 0.018061628565192223\n",
            "step: 170, loss: 0.002690351102501154\n",
            "step: 180, loss: 0.0002086600143229589\n",
            "step: 190, loss: 0.0003448408388067037\n",
            "step: 200, loss: 0.0012035410618409514\n",
            "step: 210, loss: 0.13170741498470306\n",
            "step: 220, loss: 0.09910598397254944\n",
            "step: 230, loss: 0.0005513360374607146\n",
            "step: 240, loss: 0.000697821902576834\n",
            "step: 250, loss: 0.00043977523455396295\n",
            "step: 260, loss: 0.0009821628918871284\n",
            "step: 270, loss: 0.024636128917336464\n",
            "step: 280, loss: 0.0012242100201547146\n",
            "step: 290, loss: 0.00023398928169626743\n",
            "step: 300, loss: 0.0004264888702891767\n",
            "step: 310, loss: 0.0619385801255703\n",
            "step: 320, loss: 0.0016748050693422556\n",
            "step: 330, loss: 0.000272925419267267\n",
            "step: 340, loss: 0.003059764625504613\n",
            "step: 350, loss: 0.1408718377351761\n",
            "step: 360, loss: 0.0004402172635309398\n",
            "step: 370, loss: 0.00022663341951556504\n",
            "step: 380, loss: 0.0036313964519649744\n",
            "step: 390, loss: 0.00012221832002978772\n",
            "step: 400, loss: 0.04233304411172867\n",
            "step: 410, loss: 0.00022955001622904092\n",
            "step: 420, loss: 0.00017952480993699282\n",
            "step: 430, loss: 0.0014065635623410344\n",
            "step: 440, loss: 0.0010270995553582907\n",
            "step: 450, loss: 0.0013716361718252301\n",
            "step: 460, loss: 0.0003304299316368997\n",
            "step: 470, loss: 0.0003013382665812969\n",
            "step: 480, loss: 6.896359991515055e-05\n",
            "step: 490, loss: 0.04900793731212616\n",
            "step: 500, loss: 0.00020475378551054746\n",
            "step: 510, loss: 0.023879630491137505\n",
            "step: 520, loss: 0.0016411221586167812\n",
            "step: 530, loss: 0.012652105651795864\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9499536607970344, f1=0.9453015427769986, best_f1=0.9457720588235294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016966609982773662\n",
            "step: 10, loss: 0.0023294396232813597\n",
            "step: 20, loss: 0.0012008537305518985\n",
            "step: 30, loss: 0.005379924550652504\n",
            "step: 40, loss: 0.0001651729253353551\n",
            "step: 50, loss: 7.052782893879339e-05\n",
            "step: 60, loss: 0.00018740609812084585\n",
            "step: 70, loss: 0.00011448313307482749\n",
            "step: 80, loss: 4.793186963070184e-05\n",
            "step: 90, loss: 0.0006442955927923322\n",
            "step: 100, loss: 0.003082016948610544\n",
            "step: 110, loss: 0.0031744525767862797\n",
            "step: 120, loss: 9.722570393932983e-05\n",
            "step: 130, loss: 0.0013826315989717841\n",
            "step: 140, loss: 0.001794298063032329\n",
            "step: 150, loss: 0.0003803647996392101\n",
            "step: 160, loss: 0.021856792271137238\n",
            "step: 170, loss: 0.0001520083169452846\n",
            "step: 180, loss: 0.000347673223586753\n",
            "step: 190, loss: 0.00021503861353266984\n",
            "step: 200, loss: 0.0482378713786602\n",
            "step: 210, loss: 0.0004203195567242801\n",
            "step: 220, loss: 0.00029568205354735255\n",
            "step: 230, loss: 0.00020368538389448076\n",
            "step: 240, loss: 0.00023789727129042149\n",
            "step: 250, loss: 0.0004327953211031854\n",
            "step: 260, loss: 0.0003434538666624576\n",
            "step: 270, loss: 0.0002056693920167163\n",
            "step: 280, loss: 0.011412782594561577\n",
            "step: 290, loss: 0.0009536782163195312\n",
            "step: 300, loss: 0.000504643889144063\n",
            "step: 310, loss: 0.01841694861650467\n",
            "step: 320, loss: 0.00249294051900506\n",
            "step: 330, loss: 0.0009721522219479084\n",
            "step: 340, loss: 8.737675671000034e-05\n",
            "step: 350, loss: 0.003432716941460967\n",
            "step: 360, loss: 0.0008463856065645814\n",
            "step: 370, loss: 0.0005343094817362726\n",
            "step: 380, loss: 0.001148969866335392\n",
            "step: 390, loss: 0.0004110117442905903\n",
            "step: 400, loss: 0.0002474125358276069\n",
            "step: 410, loss: 8.661361789563671e-05\n",
            "step: 420, loss: 7.997419743333012e-05\n",
            "step: 430, loss: 5.829726796946488e-05\n",
            "step: 440, loss: 8.99929873412475e-05\n",
            "step: 450, loss: 0.00021343972184695303\n",
            "step: 460, loss: 8.350752614205703e-05\n",
            "step: 470, loss: 0.01216052658855915\n",
            "step: 480, loss: 0.001236757612787187\n",
            "step: 490, loss: 0.024200551211833954\n",
            "step: 500, loss: 0.0083750756457448\n",
            "step: 510, loss: 5.9512854932108894e-05\n",
            "step: 520, loss: 0.00011722042108885944\n",
            "step: 530, loss: 0.011355127207934856\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9502304147465437, f1=0.9480037140204272, best_f1=0.9457720588235294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00022814172552898526\n",
            "step: 10, loss: 0.00012421673454809934\n",
            "step: 20, loss: 0.00039950941572897136\n",
            "step: 30, loss: 0.0004812834376934916\n",
            "step: 40, loss: 0.00023612129734829068\n",
            "step: 50, loss: 0.00017754676810000092\n",
            "step: 60, loss: 0.0001286200131289661\n",
            "step: 70, loss: 0.0005188880022615194\n",
            "step: 80, loss: 0.0002874484343919903\n",
            "step: 90, loss: 0.0004545847768895328\n",
            "step: 100, loss: 0.00046790274791419506\n",
            "step: 110, loss: 0.0004647033056244254\n",
            "step: 120, loss: 7.298964919755235e-05\n",
            "step: 130, loss: 4.1395993321202695e-05\n",
            "step: 140, loss: 0.000153832821524702\n",
            "step: 150, loss: 0.0003549831744749099\n",
            "step: 160, loss: 0.00022628255828749388\n",
            "step: 170, loss: 0.00015544742927886546\n",
            "step: 180, loss: 0.0002973258087877184\n",
            "step: 190, loss: 0.0007639571558684111\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 200, loss: 9.710261656437069e-05\n",
            "step: 210, loss: 0.0003838048141915351\n",
            "step: 220, loss: 0.017385011538863182\n",
            "step: 230, loss: 3.26414083247073e-05\n",
            "step: 240, loss: 0.011531814001500607\n",
            "step: 250, loss: 0.00011860082304337993\n",
            "step: 260, loss: 0.0010028829565271735\n",
            "step: 270, loss: 0.00045349393622018397\n",
            "step: 280, loss: 0.00012086331844329834\n",
            "step: 290, loss: 7.677663234062493e-05\n",
            "step: 300, loss: 7.889031985541806e-05\n",
            "step: 310, loss: 0.002103217178955674\n",
            "step: 320, loss: 7.080104842316359e-05\n",
            "step: 330, loss: 6.0107293393230066e-05\n",
            "step: 340, loss: 5.7503493735566735e-05\n",
            "step: 350, loss: 7.172356708906591e-05\n",
            "step: 360, loss: 0.00011819760402431712\n",
            "step: 370, loss: 0.001182957785204053\n",
            "step: 380, loss: 0.05886521190404892\n",
            "step: 390, loss: 6.230571307241917e-05\n",
            "step: 400, loss: 0.008809964172542095\n",
            "step: 410, loss: 0.00034293666249141097\n",
            "step: 420, loss: 0.0004384904750622809\n",
            "step: 430, loss: 0.0021308762952685356\n",
            "step: 440, loss: 0.00012358918320387602\n",
            "step: 450, loss: 0.0037815968971699476\n",
            "step: 460, loss: 0.0002845269627869129\n",
            "step: 470, loss: 0.00041539600351825356\n",
            "step: 480, loss: 0.0004915300523862243\n",
            "step: 490, loss: 8.974951197160408e-05\n",
            "step: 500, loss: 0.0010819716844707727\n",
            "step: 510, loss: 0.0003530748945195228\n",
            "step: 520, loss: 0.00026843976229429245\n",
            "step: 530, loss: 0.011114147491753101\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9466852109411219, f1=0.9431870669745959, best_f1=0.9457720588235294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016708907787688076\n",
            "step: 10, loss: 0.01948426477611065\n",
            "step: 20, loss: 0.014195389114320278\n",
            "step: 30, loss: 0.00017088036111090332\n",
            "step: 40, loss: 0.006438602693378925\n",
            "step: 50, loss: 0.015375752933323383\n",
            "step: 60, loss: 0.00011997437832178548\n",
            "step: 70, loss: 0.013421637937426567\n",
            "step: 80, loss: 9.747822332428768e-05\n",
            "step: 90, loss: 0.006473036482930183\n",
            "step: 100, loss: 0.263052374124527\n",
            "step: 110, loss: 0.00010169806773774326\n",
            "step: 120, loss: 0.003981973510235548\n",
            "step: 130, loss: 0.000662481936160475\n",
            "step: 140, loss: 0.00016246653103735298\n",
            "step: 150, loss: 0.0009848610498011112\n",
            "step: 160, loss: 0.0002936555538326502\n",
            "step: 170, loss: 0.000219769382965751\n",
            "step: 180, loss: 0.00010304337774869055\n",
            "step: 190, loss: 0.00018368106975685805\n",
            "step: 200, loss: 0.006168482359498739\n",
            "step: 210, loss: 0.00015224417438730597\n",
            "step: 220, loss: 0.00010457296593813226\n",
            "step: 230, loss: 0.00036312727024778724\n",
            "step: 240, loss: 0.0003135574224870652\n",
            "step: 250, loss: 0.000737700960598886\n",
            "step: 260, loss: 7.113766332622617e-05\n",
            "step: 270, loss: 0.0006814018706791103\n",
            "step: 280, loss: 0.07474581152200699\n",
            "step: 290, loss: 0.0001164171117125079\n",
            "step: 300, loss: 0.00033077894477173686\n",
            "step: 310, loss: 0.006113688461482525\n",
            "step: 320, loss: 0.0011317231692373753\n",
            "step: 330, loss: 0.00031366405892185867\n",
            "step: 340, loss: 9.891838271869346e-05\n",
            "step: 350, loss: 8.312634599860758e-05\n",
            "step: 360, loss: 0.00016560325457248837\n",
            "step: 370, loss: 8.747872925596312e-05\n",
            "step: 380, loss: 0.0011031798785552382\n",
            "step: 390, loss: 0.003724090987816453\n",
            "step: 400, loss: 5.536886601475999e-05\n",
            "step: 410, loss: 0.010766529478132725\n",
            "step: 420, loss: 3.809064219240099e-05\n",
            "step: 430, loss: 0.0019363322062417865\n",
            "step: 440, loss: 0.001766126835718751\n",
            "step: 450, loss: 0.00017242228204850107\n",
            "step: 460, loss: 0.0010830489918589592\n",
            "step: 470, loss: 0.0005425084382295609\n",
            "step: 480, loss: 0.0002078386169159785\n",
            "step: 490, loss: 0.0001434189616702497\n",
            "step: 500, loss: 0.000247280957410112\n",
            "step: 510, loss: 0.00017916571232490242\n",
            "step: 520, loss: 0.0009607716929167509\n",
            "step: 530, loss: 0.0004890264826826751\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9518127581459385, f1=0.9457221711131555, best_f1=0.9457221711131555\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15281304717063904\n",
            "step: 10, loss: 0.0005488135502673686\n",
            "step: 20, loss: 0.00011833229655167088\n",
            "step: 30, loss: 0.00012875095126219094\n",
            "step: 40, loss: 0.0001311754749622196\n",
            "step: 50, loss: 0.0012561503099277616\n",
            "step: 60, loss: 0.00015014447853900492\n",
            "step: 70, loss: 0.0014516967348754406\n",
            "step: 80, loss: 0.00022052564600016922\n",
            "step: 90, loss: 0.0001731603406369686\n",
            "step: 100, loss: 0.0001522970269434154\n",
            "step: 110, loss: 4.794376218342222e-05\n",
            "step: 120, loss: 6.351996853481978e-05\n",
            "step: 130, loss: 5.7657780416775495e-05\n",
            "step: 140, loss: 0.0001070866928785108\n",
            "step: 150, loss: 0.0011344769736751914\n",
            "step: 160, loss: 0.00016292993677780032\n",
            "step: 170, loss: 3.765444125747308e-05\n",
            "step: 180, loss: 0.00011923282727366313\n",
            "step: 190, loss: 0.00019520550267770886\n",
            "step: 200, loss: 6.646794645348564e-05\n",
            "step: 210, loss: 7.477445615222678e-05\n",
            "step: 220, loss: 5.192526805330999e-05\n",
            "step: 230, loss: 0.003367544850334525\n",
            "step: 240, loss: 9.825693268794566e-05\n",
            "step: 250, loss: 0.00021452023065648973\n",
            "step: 260, loss: 0.022134484723210335\n",
            "step: 270, loss: 0.011578948237001896\n",
            "step: 280, loss: 8.06389725767076e-05\n",
            "step: 290, loss: 3.143831418128684e-05\n",
            "step: 300, loss: 6.337830564007163e-05\n",
            "step: 310, loss: 7.052904402371496e-05\n",
            "step: 320, loss: 6.582021887879819e-05\n",
            "step: 330, loss: 0.00012977671576663852\n",
            "step: 340, loss: 0.00014596089022234082\n",
            "step: 350, loss: 3.2827108952915296e-05\n",
            "step: 360, loss: 0.0879112258553505\n",
            "step: 370, loss: 0.00010363533510826528\n",
            "step: 380, loss: 5.940158007433638e-05\n",
            "step: 390, loss: 0.00020665445481427014\n",
            "step: 400, loss: 0.0001730201911414042\n",
            "step: 410, loss: 0.0004893119912594557\n",
            "step: 420, loss: 5.927484744461253e-05\n",
            "step: 430, loss: 0.00015479352441616356\n",
            "step: 440, loss: 4.6243272663559765e-05\n",
            "step: 450, loss: 0.0002475048240739852\n",
            "step: 460, loss: 0.0021166722290217876\n",
            "step: 470, loss: 1.6848205632413737e-05\n",
            "step: 480, loss: 5.9895464801229537e-05\n",
            "step: 490, loss: 0.00010650509648257867\n",
            "step: 500, loss: 0.0001276816619792953\n",
            "step: 510, loss: 0.0001591074833413586\n",
            "step: 520, loss: 0.00020353170111775398\n",
            "step: 530, loss: 6.354976358124986e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9484440315838365, f1=0.945707656612529, best_f1=0.9457221711131555\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00029858911875635386\n",
            "step: 10, loss: 8.451424946542829e-05\n",
            "step: 20, loss: 6.18120830040425e-05\n",
            "step: 30, loss: 0.0005001974059268832\n",
            "step: 40, loss: 0.00045765924733132124\n",
            "step: 50, loss: 0.0005454337224364281\n",
            "step: 60, loss: 0.00013509212294593453\n",
            "step: 70, loss: 7.321382872760296e-05\n",
            "step: 80, loss: 0.00012046597839798778\n",
            "step: 90, loss: 4.286606781533919e-05\n",
            "step: 100, loss: 5.144132592249662e-05\n",
            "step: 110, loss: 0.0006111850379966199\n",
            "step: 120, loss: 6.27937915851362e-05\n",
            "step: 130, loss: 0.00010485750681255013\n",
            "step: 140, loss: 0.0008835049229674041\n",
            "step: 150, loss: 0.00033112181699834764\n",
            "step: 160, loss: 8.875672938302159e-05\n",
            "step: 170, loss: 0.023241017013788223\n",
            "step: 180, loss: 5.100432099425234e-05\n",
            "step: 190, loss: 0.0006038694991730154\n",
            "step: 200, loss: 7.722307782387361e-05\n",
            "step: 210, loss: 0.0005382876843214035\n",
            "step: 220, loss: 4.4883279770147055e-05\n",
            "step: 230, loss: 0.008243129588663578\n",
            "step: 240, loss: 0.00011697630543494597\n",
            "step: 250, loss: 0.00011432875180616975\n",
            "step: 260, loss: 6.324069545371458e-05\n",
            "step: 270, loss: 0.00026253313990309834\n",
            "step: 280, loss: 4.45301593572367e-05\n",
            "step: 290, loss: 0.00013450533151626587\n",
            "step: 300, loss: 7.877437019487843e-05\n",
            "step: 310, loss: 3.4108095860574394e-05\n",
            "step: 320, loss: 9.106918150791898e-05\n",
            "step: 330, loss: 7.428489334415644e-05\n",
            "step: 340, loss: 0.0004717679403256625\n",
            "step: 350, loss: 6.322105036815628e-05\n",
            "step: 360, loss: 7.00641394359991e-05\n",
            "step: 370, loss: 0.003967850003391504\n",
            "step: 380, loss: 0.00011077929229941219\n",
            "step: 390, loss: 0.007773424498736858\n",
            "step: 400, loss: 0.009163563139736652\n",
            "step: 410, loss: 0.00014691684918943793\n",
            "step: 420, loss: 0.0001253648370038718\n",
            "step: 430, loss: 0.00021694770839530975\n",
            "step: 440, loss: 7.650681800441816e-05\n",
            "step: 450, loss: 8.312782301800326e-05\n",
            "step: 460, loss: 0.003910931758582592\n",
            "step: 470, loss: 7.694357918808237e-05\n",
            "step: 480, loss: 5.350371066015214e-05\n",
            "step: 490, loss: 7.181888213381171e-05\n",
            "step: 500, loss: 0.0005302278441376984\n",
            "step: 510, loss: 0.0002553272352088243\n",
            "step: 520, loss: 4.613661440089345e-05\n",
            "step: 530, loss: 0.001337038935162127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9503676470588236, f1=0.9457221711131555, best_f1=0.9457221711131555\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.92364589869976e-05\n",
            "step: 10, loss: 0.0008272346458397806\n",
            "step: 20, loss: 0.008334164507687092\n",
            "step: 30, loss: 0.02567734196782112\n",
            "step: 40, loss: 7.372957043116912e-05\n",
            "step: 50, loss: 0.00028416758868843317\n",
            "step: 60, loss: 9.809016046347097e-05\n",
            "step: 70, loss: 0.00017402517551090568\n",
            "step: 80, loss: 7.706101314397529e-05\n",
            "step: 90, loss: 4.006892777397297e-05\n",
            "step: 100, loss: 2.2014344722265378e-05\n",
            "step: 110, loss: 9.434700041310862e-05\n",
            "step: 120, loss: 4.5960681745782495e-05\n",
            "step: 130, loss: 0.0001082337403204292\n",
            "step: 140, loss: 4.8634781705914065e-05\n",
            "step: 150, loss: 5.625746780424379e-05\n",
            "step: 160, loss: 6.878888962091878e-05\n",
            "step: 170, loss: 3.403371010790579e-05\n",
            "step: 180, loss: 0.00021924771135672927\n",
            "step: 190, loss: 0.00040320472908206284\n",
            "step: 200, loss: 0.0004814044514205307\n",
            "step: 210, loss: 0.00028423621552065015\n",
            "step: 220, loss: 6.287856376729906e-05\n",
            "step: 230, loss: 6.557350570801646e-05\n",
            "step: 240, loss: 4.4598513341043144e-05\n",
            "step: 250, loss: 6.260205555008724e-05\n",
            "step: 260, loss: 4.124939368921332e-05\n",
            "step: 270, loss: 7.902624201960862e-05\n",
            "step: 280, loss: 0.0003438879793975502\n",
            "step: 290, loss: 0.00020962451526429504\n",
            "step: 300, loss: 4.1178875108016655e-05\n",
            "step: 310, loss: 0.03600265830755234\n",
            "step: 320, loss: 6.868335913168266e-05\n",
            "step: 330, loss: 6.689513975288719e-05\n",
            "step: 340, loss: 8.198370778700337e-05\n",
            "step: 350, loss: 0.0024500067811459303\n",
            "step: 360, loss: 0.00012089385563740507\n",
            "step: 370, loss: 8.330192940775305e-05\n",
            "step: 380, loss: 6.710628076689318e-05\n",
            "step: 390, loss: 0.003309238702058792\n",
            "step: 400, loss: 0.00015595760487485677\n",
            "step: 410, loss: 0.00010778185242088512\n",
            "step: 420, loss: 7.987410936038941e-05\n",
            "step: 430, loss: 2.5068939066841267e-05\n",
            "step: 440, loss: 4.695201278082095e-05\n",
            "step: 450, loss: 0.00465276837348938\n",
            "step: 460, loss: 0.0011086869053542614\n",
            "step: 470, loss: 4.758194336318411e-05\n",
            "step: 480, loss: 0.002520486246794462\n",
            "step: 490, loss: 0.006431929301470518\n",
            "step: 500, loss: 0.0013435266446322203\n",
            "step: 510, loss: 4.5029635657556355e-05\n",
            "step: 520, loss: 0.00023590789351146668\n",
            "step: 530, loss: 3.472721073194407e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.950668510834486, f1=0.9474169741697417, best_f1=0.9457221711131555\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:27, 212.39it/s]\n",
            "load_f1 = 0.9512867647058824\n",
            "real_f1 = 0.9501845018450183\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "3072it [00:19, 159.20it/s]"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqkZ1fXggw8C",
        "outputId": "d387fa51-fcb6-4b48-c4fc-715ed713d019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 432kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 664kB/s] \n",
            "Downloading: 100% 456k/456k [00:00<00:00, 514kB/s]\n",
            "Downloading: 100% 501M/501M [00:06<00:00, 72.1MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.48997870087623596\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4913041889667511\n",
            "step: 20, loss: 0.4295283555984497\n",
            "step: 30, loss: 0.4171154797077179\n",
            "step: 40, loss: 0.3133029043674469\n",
            "step: 50, loss: 0.4287368655204773\n",
            "step: 60, loss: 0.5041627287864685\n",
            "step: 70, loss: 0.3040984570980072\n",
            "step: 80, loss: 0.3563336133956909\n",
            "step: 90, loss: 0.23365052044391632\n",
            "step: 100, loss: 0.2409820854663849\n",
            "step: 110, loss: 0.26834815740585327\n",
            "step: 120, loss: 0.4066017270088196\n",
            "step: 130, loss: 0.22527725994586945\n",
            "step: 140, loss: 0.41087082028388977\n",
            "step: 150, loss: 0.21109294891357422\n",
            "step: 160, loss: 0.3493611514568329\n",
            "step: 170, loss: 0.10069160163402557\n",
            "step: 180, loss: 0.2217995822429657\n",
            "step: 190, loss: 0.6060208678245544\n",
            "step: 200, loss: 0.24462489783763885\n",
            "step: 210, loss: 0.34817683696746826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.45701357466063347, f1=0.5109489051094891, best_f1=0.5109489051094891\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1995459645986557\n",
            "step: 10, loss: 0.04545178264379501\n",
            "step: 20, loss: 0.3148389458656311\n",
            "step: 30, loss: 0.28009238839149475\n",
            "step: 40, loss: 0.4094841778278351\n",
            "step: 50, loss: 0.2823280096054077\n",
            "step: 60, loss: 0.2826738655567169\n",
            "step: 70, loss: 0.1816072165966034\n",
            "step: 80, loss: 0.25284072756767273\n",
            "step: 90, loss: 0.10630038380622864\n",
            "step: 100, loss: 0.4729175269603729\n",
            "step: 110, loss: 0.2529750466346741\n",
            "step: 120, loss: 0.11586282402276993\n",
            "step: 130, loss: 0.2717822194099426\n",
            "step: 140, loss: 0.055757392197847366\n",
            "step: 150, loss: 0.3431849479675293\n",
            "step: 160, loss: 0.07513298094272614\n",
            "step: 170, loss: 0.30956676602363586\n",
            "step: 180, loss: 0.18584804236888885\n",
            "step: 190, loss: 0.3024771809577942\n",
            "step: 200, loss: 0.08096962422132492\n",
            "step: 210, loss: 0.15322071313858032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5911708253358925, f1=0.6490566037735849, best_f1=0.6490566037735849\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07438531517982483\n",
            "step: 10, loss: 0.03942955657839775\n",
            "step: 20, loss: 0.2968551516532898\n",
            "step: 30, loss: 0.04832638055086136\n",
            "step: 40, loss: 0.3245123028755188\n",
            "step: 50, loss: 0.6663655042648315\n",
            "step: 60, loss: 0.22819513082504272\n",
            "step: 70, loss: 0.09533371776342392\n",
            "step: 80, loss: 0.09382502734661102\n",
            "step: 90, loss: 0.032218847423791885\n",
            "step: 100, loss: 0.19503512978553772\n",
            "step: 110, loss: 0.027850110083818436\n",
            "step: 120, loss: 0.15770511329174042\n",
            "step: 130, loss: 0.11313892155885696\n",
            "step: 140, loss: 0.1005442664027214\n",
            "step: 150, loss: 0.20582224428653717\n",
            "step: 160, loss: 0.20707392692565918\n",
            "step: 170, loss: 0.18214698135852814\n",
            "step: 180, loss: 0.10383421927690506\n",
            "step: 190, loss: 0.06752997636795044\n",
            "step: 200, loss: 0.1276053637266159\n",
            "step: 210, loss: 0.2559078335762024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6160337552742615, f1=0.630901287553648, best_f1=0.630901287553648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026565004140138626\n",
            "step: 10, loss: 0.1060129851102829\n",
            "step: 20, loss: 0.06923678517341614\n",
            "step: 30, loss: 0.13275252282619476\n",
            "step: 40, loss: 0.10282894968986511\n",
            "step: 50, loss: 0.11573521047830582\n",
            "step: 60, loss: 0.15089291334152222\n",
            "step: 70, loss: 0.05608930066227913\n",
            "step: 80, loss: 0.0693850889801979\n",
            "step: 90, loss: 0.04993879050016403\n",
            "step: 100, loss: 0.2064078450202942\n",
            "step: 110, loss: 0.2881375253200531\n",
            "step: 120, loss: 0.22028443217277527\n",
            "step: 130, loss: 0.1536373794078827\n",
            "step: 140, loss: 0.20378366112709045\n",
            "step: 150, loss: 0.12874437868595123\n",
            "step: 160, loss: 0.07021689414978027\n",
            "step: 170, loss: 0.03189394250512123\n",
            "step: 180, loss: 0.012673833407461643\n",
            "step: 190, loss: 0.13228140771389008\n",
            "step: 200, loss: 0.02659166231751442\n",
            "step: 210, loss: 0.3405245542526245\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6219512195121951, f1=0.6464646464646464, best_f1=0.6464646464646464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18909864127635956\n",
            "step: 10, loss: 0.022873099893331528\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.23217450082302094\n",
            "step: 30, loss: 0.032679446041584015\n",
            "step: 40, loss: 0.0691877007484436\n",
            "step: 50, loss: 0.07784590125083923\n",
            "step: 60, loss: 0.012318750843405724\n",
            "step: 70, loss: 0.20921748876571655\n",
            "step: 80, loss: 0.0700816810131073\n",
            "step: 90, loss: 0.00959338154643774\n",
            "step: 100, loss: 0.04948166385293007\n",
            "step: 110, loss: 0.0314820222556591\n",
            "step: 120, loss: 0.09533495455980301\n",
            "step: 130, loss: 0.05852266028523445\n",
            "step: 140, loss: 0.09706690907478333\n",
            "step: 150, loss: 0.055425144731998444\n",
            "step: 160, loss: 0.0826314389705658\n",
            "step: 170, loss: 0.09565312415361404\n",
            "step: 180, loss: 0.09721894562244415\n",
            "step: 190, loss: 0.11385904997587204\n",
            "step: 200, loss: 0.08505335450172424\n",
            "step: 210, loss: 0.06761366873979568\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6279999999999999, f1=0.6262230919765167, best_f1=0.6262230919765167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0108219338580966\n",
            "step: 10, loss: 0.029696546494960785\n",
            "step: 20, loss: 0.013139784336090088\n",
            "step: 30, loss: 0.16186951100826263\n",
            "step: 40, loss: 0.02791205607354641\n",
            "step: 50, loss: 0.19665439426898956\n",
            "step: 60, loss: 0.1821356862783432\n",
            "step: 70, loss: 0.03436017036437988\n",
            "step: 80, loss: 0.13917528092861176\n",
            "step: 90, loss: 0.11147172749042511\n",
            "step: 100, loss: 0.15671610832214355\n",
            "step: 110, loss: 0.11562304943799973\n",
            "step: 120, loss: 0.021679555997252464\n",
            "step: 130, loss: 0.014746226370334625\n",
            "step: 140, loss: 0.04575882852077484\n",
            "step: 150, loss: 0.004073515068739653\n",
            "step: 160, loss: 0.024334365501999855\n",
            "step: 170, loss: 0.029098201543092728\n",
            "step: 180, loss: 0.04423961415886879\n",
            "step: 190, loss: 0.0937507227063179\n",
            "step: 200, loss: 0.22653008997440338\n",
            "step: 210, loss: 0.07765081524848938\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.632183908045977, f1=0.6529080675422138, best_f1=0.6529080675422138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14027981460094452\n",
            "step: 10, loss: 0.01665184088051319\n",
            "step: 20, loss: 0.019298618659377098\n",
            "step: 30, loss: 0.04848788306117058\n",
            "step: 40, loss: 0.012622109614312649\n",
            "step: 50, loss: 0.0033412473276257515\n",
            "step: 60, loss: 0.010022720322012901\n",
            "step: 70, loss: 0.05490158870816231\n",
            "step: 80, loss: 0.059461560100317\n",
            "step: 90, loss: 0.02027590200304985\n",
            "step: 100, loss: 0.05945947766304016\n",
            "step: 110, loss: 0.08284947276115417\n",
            "step: 120, loss: 0.06628581136465073\n",
            "step: 130, loss: 0.08979469537734985\n",
            "step: 140, loss: 0.05283847823739052\n",
            "step: 150, loss: 0.035605959594249725\n",
            "step: 160, loss: 0.3220904469490051\n",
            "step: 170, loss: 0.06795283406972885\n",
            "step: 180, loss: 0.01403819676488638\n",
            "step: 190, loss: 0.05995499715209007\n",
            "step: 200, loss: 0.09083256870508194\n",
            "step: 210, loss: 0.2574966251850128\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6180555555555555, f1=0.6348122866894198, best_f1=0.6529080675422138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01127727422863245\n",
            "step: 10, loss: 0.06511002779006958\n",
            "step: 20, loss: 0.007499145809561014\n",
            "step: 30, loss: 0.00513075664639473\n",
            "step: 40, loss: 0.0162037443369627\n",
            "step: 50, loss: 0.0009367917664349079\n",
            "step: 60, loss: 0.004478253424167633\n",
            "step: 70, loss: 0.06039496138691902\n",
            "step: 80, loss: 0.12584325671195984\n",
            "step: 90, loss: 0.018595585599541664\n",
            "step: 100, loss: 0.017059920355677605\n",
            "step: 110, loss: 0.04148821160197258\n",
            "step: 120, loss: 0.029245909303426743\n",
            "step: 130, loss: 0.012104939669370651\n",
            "step: 140, loss: 0.12581849098205566\n",
            "step: 150, loss: 0.15041698515415192\n",
            "step: 160, loss: 0.15729652345180511\n",
            "step: 170, loss: 0.06632950156927109\n",
            "step: 180, loss: 0.1028166338801384\n",
            "step: 190, loss: 0.022988326847553253\n",
            "step: 200, loss: 0.0752691850066185\n",
            "step: 210, loss: 0.1599026769399643\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5916666666666667, f1=0.6575875486381323, best_f1=0.6529080675422138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17491255700588226\n",
            "step: 10, loss: 0.025704538449645042\n",
            "step: 20, loss: 0.1235373392701149\n",
            "step: 30, loss: 0.04950091615319252\n",
            "step: 40, loss: 0.001446588896214962\n",
            "step: 50, loss: 0.08267734199762344\n",
            "step: 60, loss: 0.0012093264376744628\n",
            "step: 70, loss: 0.01996835693717003\n",
            "step: 80, loss: 0.035126298666000366\n",
            "step: 90, loss: 0.036350298672914505\n",
            "step: 100, loss: 0.006310855038464069\n",
            "step: 110, loss: 0.03165159747004509\n",
            "step: 120, loss: 0.005282060708850622\n",
            "step: 130, loss: 0.08876372873783112\n",
            "step: 140, loss: 0.04493141546845436\n",
            "step: 150, loss: 0.015112076885998249\n",
            "step: 160, loss: 0.08728092163801193\n",
            "step: 170, loss: 0.08422859758138657\n",
            "step: 180, loss: 0.12130557745695114\n",
            "step: 190, loss: 0.0051775467582046986\n",
            "step: 200, loss: 0.0027192735578864813\n",
            "step: 210, loss: 0.2503019869327545\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6290672451193059, f1=0.6425531914893616, best_f1=0.6529080675422138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002190419239923358\n",
            "step: 10, loss: 0.0024581067264080048\n",
            "step: 20, loss: 0.006344596389681101\n",
            "step: 30, loss: 0.007497986312955618\n",
            "step: 40, loss: 0.005523958709090948\n",
            "step: 50, loss: 0.041372478008270264\n",
            "step: 60, loss: 0.004050040151923895\n",
            "step: 70, loss: 0.012075434438884258\n",
            "step: 80, loss: 0.011999307200312614\n",
            "step: 90, loss: 0.02140599489212036\n",
            "step: 100, loss: 0.13670068979263306\n",
            "step: 110, loss: 0.004072379320859909\n",
            "step: 120, loss: 0.017742132768034935\n",
            "step: 130, loss: 0.008165913634002209\n",
            "step: 140, loss: 0.007389216683804989\n",
            "step: 150, loss: 0.02250286005437374\n",
            "step: 160, loss: 0.01569509692490101\n",
            "step: 170, loss: 0.0537242628633976\n",
            "step: 180, loss: 0.002819550223648548\n",
            "step: 190, loss: 0.16235309839248657\n",
            "step: 200, loss: 0.03313417360186577\n",
            "step: 210, loss: 0.01376986876130104\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6117136659436009, f1=0.6570841889117043, best_f1=0.6529080675422138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07864436507225037\n",
            "step: 10, loss: 0.0038328575901687145\n",
            "step: 20, loss: 0.05047287046909332\n",
            "step: 30, loss: 0.021919287741184235\n",
            "step: 40, loss: 0.060737382620573044\n",
            "step: 50, loss: 0.08578158169984818\n",
            "step: 60, loss: 0.0029789558611810207\n",
            "step: 70, loss: 0.01155425701290369\n",
            "step: 80, loss: 0.003195652738213539\n",
            "step: 90, loss: 0.12952034175395966\n",
            "step: 100, loss: 0.01570882275700569\n",
            "step: 110, loss: 0.009769337251782417\n",
            "step: 120, loss: 0.02142399549484253\n",
            "step: 130, loss: 0.0017724739154800773\n",
            "step: 140, loss: 0.05150997266173363\n",
            "step: 150, loss: 0.01365633960813284\n",
            "step: 160, loss: 0.008012956008315086\n",
            "step: 170, loss: 0.003291117027401924\n",
            "step: 180, loss: 0.13828854262828827\n",
            "step: 190, loss: 0.011474746279418468\n",
            "step: 200, loss: 0.0020464088302105665\n",
            "step: 210, loss: 0.06736191362142563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6317907444668007, f1=0.6693069306930693, best_f1=0.6529080675422138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010145921260118484\n",
            "step: 10, loss: 0.0031461473554372787\n",
            "step: 20, loss: 0.028789963573217392\n",
            "step: 30, loss: 0.0017948143649846315\n",
            "step: 40, loss: 0.0020784884691238403\n",
            "step: 50, loss: 0.029871691018342972\n",
            "step: 60, loss: 0.00011812973389169201\n",
            "step: 70, loss: 0.0631651058793068\n",
            "step: 80, loss: 0.12320003658533096\n",
            "step: 90, loss: 0.0019517220789566636\n",
            "step: 100, loss: 0.004767616279423237\n",
            "step: 110, loss: 0.008421561680734158\n",
            "step: 120, loss: 0.000731404812540859\n",
            "step: 130, loss: 0.002498430898413062\n",
            "step: 140, loss: 0.07283590734004974\n",
            "step: 150, loss: 5.240128666628152e-05\n",
            "step: 160, loss: 0.0008409464498981833\n",
            "step: 170, loss: 0.1581185758113861\n",
            "step: 180, loss: 0.0031294000800698996\n",
            "step: 190, loss: 0.0020185718312859535\n",
            "step: 200, loss: 0.027105774730443954\n",
            "step: 210, loss: 0.0018127872608602047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6049382716049383, f1=0.6734279918864097, best_f1=0.6529080675422138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001386292395181954\n",
            "step: 10, loss: 0.017484283074736595\n",
            "step: 20, loss: 0.0009730517631396651\n",
            "step: 30, loss: 0.000765630102250725\n",
            "step: 40, loss: 0.0019003867637366056\n",
            "step: 50, loss: 0.1608768105506897\n",
            "step: 60, loss: 0.003973386250436306\n",
            "step: 70, loss: 0.008828266523778439\n",
            "step: 80, loss: 0.1388845592737198\n",
            "step: 90, loss: 0.06180627644062042\n",
            "step: 100, loss: 0.002189646940678358\n",
            "step: 110, loss: 0.010883791372179985\n",
            "step: 120, loss: 0.012864246964454651\n",
            "step: 130, loss: 0.004337153863161802\n",
            "step: 140, loss: 0.0472613163292408\n",
            "step: 150, loss: 0.0014988286420702934\n",
            "step: 160, loss: 0.0032776256557554007\n",
            "step: 170, loss: 0.0009046490304172039\n",
            "step: 180, loss: 0.0006068386719562113\n",
            "step: 190, loss: 0.01724270172417164\n",
            "step: 200, loss: 0.0022915585432201624\n",
            "step: 210, loss: 0.009688161313533783\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6119733924611973, f1=0.6535087719298245, best_f1=0.6529080675422138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001220264588482678\n",
            "step: 10, loss: 0.009963509626686573\n",
            "step: 20, loss: 0.002201601630076766\n",
            "step: 30, loss: 0.006598723586648703\n",
            "step: 40, loss: 0.010194428265094757\n",
            "step: 50, loss: 0.0009412798681296408\n",
            "step: 60, loss: 0.042046111077070236\n",
            "step: 70, loss: 0.0007149601588025689\n",
            "step: 80, loss: 0.009382717311382294\n",
            "step: 90, loss: 0.0002094357623718679\n",
            "step: 100, loss: 0.0004947561537846923\n",
            "step: 110, loss: 0.02598848007619381\n",
            "step: 120, loss: 0.000522151414770633\n",
            "step: 130, loss: 0.008197974413633347\n",
            "step: 140, loss: 0.0007587178843095899\n",
            "step: 150, loss: 0.0013836967991665006\n",
            "step: 160, loss: 0.0003197974874638021\n",
            "step: 170, loss: 0.022127648815512657\n",
            "step: 180, loss: 0.010767209343612194\n",
            "step: 190, loss: 0.002963251667097211\n",
            "step: 200, loss: 0.0012701739324256778\n",
            "step: 210, loss: 0.0018642987124621868\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5815602836879433, f1=0.6374133949191686, best_f1=0.6529080675422138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008687230874784291\n",
            "step: 10, loss: 0.0007351858075708151\n",
            "step: 20, loss: 0.014517168514430523\n",
            "step: 30, loss: 0.006976981181651354\n",
            "step: 40, loss: 0.0044340514577925205\n",
            "step: 50, loss: 0.0023712425027042627\n",
            "step: 60, loss: 0.01247647125273943\n",
            "step: 70, loss: 0.0026460441295057535\n",
            "step: 80, loss: 0.009348362684249878\n",
            "step: 90, loss: 0.003322921460494399\n",
            "step: 100, loss: 0.003566832048818469\n",
            "step: 110, loss: 0.003746827132999897\n",
            "step: 120, loss: 0.0021776396315544844\n",
            "step: 130, loss: 0.0006331360200420022\n",
            "step: 140, loss: 0.0037612980231642723\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 150, loss: 0.005297404248267412\n",
            "step: 160, loss: 0.04192742332816124\n",
            "step: 170, loss: 0.007231942377984524\n",
            "step: 180, loss: 0.0010233931243419647\n",
            "step: 190, loss: 0.0012944458285346627\n",
            "step: 200, loss: 0.0018351764883846045\n",
            "step: 210, loss: 0.0026944917626678944\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5977528089887639, f1=0.6403508771929826, best_f1=0.6529080675422138\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:10, 223.89it/s]\n",
            "load_f1 = 0.6234309623430961\n",
            "real_f1 = 0.6029723991507431\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 142.40it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXvTChDGgw8D"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5747b6d-e123-42ba-8127-7184ceca2982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.45684614777565\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4493238031864166\n",
            "step: 20, loss: 0.2752213478088379\n",
            "step: 30, loss: 0.41212794184684753\n",
            "step: 40, loss: 0.2367752492427826\n",
            "step: 50, loss: 0.3174777626991272\n",
            "step: 60, loss: 0.46438562870025635\n",
            "step: 70, loss: 0.4132888913154602\n",
            "step: 80, loss: 0.1885468065738678\n",
            "step: 90, loss: 0.366184264421463\n",
            "step: 100, loss: 0.44298481941223145\n",
            "step: 110, loss: 0.2425331175327301\n",
            "step: 120, loss: 0.3414851725101471\n",
            "step: 130, loss: 0.31588447093963623\n",
            "step: 140, loss: 0.16788288950920105\n",
            "step: 150, loss: 0.3125526010990143\n",
            "step: 160, loss: 0.23601576685905457\n",
            "step: 170, loss: 0.36983436346054077\n",
            "step: 180, loss: 0.18523991107940674\n",
            "step: 190, loss: 0.1793089509010315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4122224748134613\n",
            "step: 10, loss: 0.336031436920166\n",
            "step: 20, loss: 0.6138032078742981\n",
            "step: 30, loss: 0.25099024176597595\n",
            "step: 40, loss: 0.6064199805259705\n",
            "step: 50, loss: 0.31144025921821594\n",
            "step: 60, loss: 0.46132487058639526\n",
            "step: 70, loss: 0.32125362753868103\n",
            "step: 80, loss: 0.16638898849487305\n",
            "step: 90, loss: 0.3202342987060547\n",
            "step: 100, loss: 0.27338308095932007\n",
            "step: 110, loss: 0.37246009707450867\n",
            "step: 120, loss: 0.23750533163547516\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 130, loss: 0.5357649326324463\n",
            "step: 140, loss: 0.31338179111480713\n",
            "step: 150, loss: 0.298373818397522\n",
            "step: 160, loss: 0.30970993638038635\n",
            "step: 170, loss: 0.23507127165794373\n",
            "step: 180, loss: 0.17659997940063477\n",
            "step: 190, loss: 0.2488642781972885\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38108476996421814\n",
            "step: 10, loss: 0.3692318797111511\n",
            "step: 20, loss: 0.4792785942554474\n",
            "step: 30, loss: 0.31184256076812744\n",
            "step: 40, loss: 0.08475299924612045\n",
            "step: 50, loss: 0.3816800117492676\n",
            "step: 60, loss: 0.16129866242408752\n",
            "step: 70, loss: 0.38006138801574707\n",
            "step: 80, loss: 0.31251874566078186\n",
            "step: 90, loss: 0.37946730852127075\n",
            "step: 100, loss: 0.5558768510818481\n",
            "step: 110, loss: 0.6842050552368164\n",
            "step: 120, loss: 0.3851817846298218\n",
            "step: 130, loss: 0.16854235529899597\n",
            "step: 140, loss: 0.3782051205635071\n",
            "step: 150, loss: 0.3191101551055908\n",
            "step: 160, loss: 0.6105090975761414\n",
            "step: 170, loss: 0.44137823581695557\n",
            "step: 180, loss: 0.3887787461280823\n",
            "step: 190, loss: 0.1634047031402588\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24206779897212982\n",
            "step: 10, loss: 0.23090168833732605\n",
            "step: 20, loss: 0.3039539158344269\n",
            "step: 30, loss: 0.24358269572257996\n",
            "step: 40, loss: 0.5479764938354492\n",
            "step: 50, loss: 0.23073604702949524\n",
            "step: 60, loss: 0.4028238356113434\n",
            "step: 70, loss: 0.30401793122291565\n",
            "step: 80, loss: 0.2559671998023987\n",
            "step: 90, loss: 0.18734267354011536\n",
            "step: 100, loss: 0.30260276794433594\n",
            "step: 110, loss: 0.38322123885154724\n",
            "step: 120, loss: 0.24145051836967468\n",
            "step: 130, loss: 0.4542091190814972\n",
            "step: 140, loss: 0.3117624819278717\n",
            "step: 150, loss: 0.23353931307792664\n",
            "step: 160, loss: 0.3145466446876526\n",
            "step: 170, loss: 0.44398269057273865\n",
            "step: 180, loss: 0.3715381324291229\n",
            "step: 190, loss: 0.16088518500328064\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3939831256866455\n",
            "step: 10, loss: 0.4041374623775482\n",
            "step: 20, loss: 0.1746644228696823\n",
            "step: 30, loss: 0.11528606712818146\n",
            "step: 40, loss: 0.3044593632221222\n",
            "step: 50, loss: 0.5036059021949768\n",
            "step: 60, loss: 0.24943989515304565\n",
            "step: 70, loss: 0.37247180938720703\n",
            "step: 80, loss: 0.3766288161277771\n",
            "step: 90, loss: 0.3040508031845093\n",
            "step: 100, loss: 0.4515043795108795\n",
            "step: 110, loss: 0.40569445490837097\n",
            "step: 120, loss: 0.2382752001285553\n",
            "step: 130, loss: 0.5534902811050415\n",
            "step: 140, loss: 0.38410645723342896\n",
            "step: 150, loss: 0.3119066655635834\n",
            "step: 160, loss: 0.16470129787921906\n",
            "step: 170, loss: 0.3876091241836548\n",
            "step: 180, loss: 0.24886056780815125\n",
            "step: 190, loss: 0.3114710748195648\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.303220272064209\n",
            "step: 10, loss: 0.24216626584529877\n",
            "step: 20, loss: 0.30625173449516296\n",
            "step: 30, loss: 0.47788986563682556\n",
            "step: 40, loss: 0.24016405642032623\n",
            "step: 50, loss: 0.3078833818435669\n",
            "step: 60, loss: 0.4517662227153778\n",
            "step: 70, loss: 0.3009762763977051\n",
            "step: 80, loss: 0.3138546645641327\n",
            "step: 90, loss: 0.24650812149047852\n",
            "step: 100, loss: 0.4730309844017029\n",
            "step: 110, loss: 0.23017454147338867\n",
            "step: 120, loss: 0.4669490456581116\n",
            "step: 130, loss: 0.5234544277191162\n",
            "step: 140, loss: 0.18550655245780945\n",
            "step: 150, loss: 0.37263572216033936\n",
            "step: 160, loss: 0.3796660006046295\n",
            "step: 170, loss: 0.39084935188293457\n",
            "step: 180, loss: 0.1794779747724533\n",
            "step: 190, loss: 0.2968323528766632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3165417015552521\n",
            "step: 10, loss: 0.3188442885875702\n",
            "step: 20, loss: 0.3117900490760803\n",
            "step: 30, loss: 0.18920032680034637\n",
            "step: 40, loss: 0.3205529749393463\n",
            "step: 50, loss: 0.09418133646249771\n",
            "step: 60, loss: 0.1583709865808487\n",
            "step: 70, loss: 0.15615281462669373\n",
            "step: 80, loss: 0.23891586065292358\n",
            "step: 90, loss: 0.23813077807426453\n",
            "step: 100, loss: 0.5815250277519226\n",
            "step: 110, loss: 0.44547414779663086\n",
            "step: 120, loss: 0.3908972144126892\n",
            "step: 130, loss: 0.3119947910308838\n",
            "step: 140, loss: 0.23819875717163086\n",
            "step: 150, loss: 0.31248435378074646\n",
            "step: 160, loss: 0.37979093194007874\n",
            "step: 170, loss: 0.3137996196746826\n",
            "step: 180, loss: 0.23692891001701355\n",
            "step: 190, loss: 0.30611366033554077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23810426890850067\n",
            "step: 10, loss: 0.3788575828075409\n",
            "step: 20, loss: 0.23317654430866241\n",
            "step: 30, loss: 0.3787134885787964\n",
            "step: 40, loss: 0.16957710683345795\n",
            "step: 50, loss: 0.3240986466407776\n",
            "step: 60, loss: 0.44645819067955017\n",
            "step: 70, loss: 0.23801563680171967\n",
            "step: 80, loss: 0.38123172521591187\n",
            "step: 90, loss: 0.23900309205055237\n",
            "step: 100, loss: 0.310198575258255\n",
            "step: 110, loss: 0.3848428428173065\n",
            "step: 120, loss: 0.3797273337841034\n",
            "step: 130, loss: 0.3063049614429474\n",
            "step: 140, loss: 0.3975204825401306\n",
            "step: 150, loss: 0.32162395119667053\n",
            "step: 160, loss: 0.189456969499588\n",
            "step: 170, loss: 0.23549029231071472\n",
            "step: 180, loss: 0.3161049783229828\n",
            "step: 190, loss: 0.3131697475910187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2483399212360382\n",
            "step: 10, loss: 0.10454124212265015\n",
            "step: 20, loss: 0.31123417615890503\n",
            "step: 30, loss: 0.17999160289764404\n",
            "step: 40, loss: 0.30457767844200134\n",
            "step: 50, loss: 0.3774728775024414\n",
            "step: 60, loss: 0.38415947556495667\n",
            "step: 70, loss: 0.15339705348014832\n",
            "step: 80, loss: 0.39186543226242065\n",
            "step: 90, loss: 0.6830716729164124\n",
            "step: 100, loss: 0.3751152455806732\n",
            "step: 110, loss: 0.3876441717147827\n",
            "step: 120, loss: 0.5906596779823303\n",
            "step: 130, loss: 0.3193076550960541\n",
            "step: 140, loss: 0.31318211555480957\n",
            "step: 150, loss: 0.2402643859386444\n",
            "step: 160, loss: 0.23108047246932983\n",
            "step: 170, loss: 0.4656487703323364\n",
            "step: 180, loss: 0.31344932317733765\n",
            "step: 190, loss: 0.24173028767108917\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24157670140266418\n",
            "step: 10, loss: 0.16649997234344482\n",
            "step: 20, loss: 0.31188341975212097\n",
            "step: 30, loss: 0.3777919113636017\n",
            "step: 40, loss: 0.3151613473892212\n",
            "step: 50, loss: 0.0954471305012703\n",
            "step: 60, loss: 0.31191641092300415\n",
            "step: 70, loss: 0.3156558871269226\n",
            "step: 80, loss: 0.39581236243247986\n",
            "step: 90, loss: 0.1628040224313736\n",
            "step: 100, loss: 0.23311279714107513\n",
            "step: 110, loss: 0.23152361810207367\n",
            "step: 120, loss: 0.38951870799064636\n",
            "step: 130, loss: 0.5116056203842163\n",
            "step: 140, loss: 0.3700537383556366\n",
            "step: 150, loss: 0.31404438614845276\n",
            "step: 160, loss: 0.24050620198249817\n",
            "step: 170, loss: 0.3792445659637451\n",
            "step: 180, loss: 0.23547926545143127\n",
            "step: 190, loss: 0.16329023241996765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5439802408218384\n",
            "step: 10, loss: 0.09766943752765656\n",
            "step: 20, loss: 0.234677255153656\n",
            "step: 30, loss: 0.23904335498809814\n",
            "step: 40, loss: 0.08698387444019318\n",
            "step: 50, loss: 0.3179932236671448\n",
            "step: 60, loss: 0.23470327258110046\n",
            "step: 70, loss: 0.6170253753662109\n",
            "step: 80, loss: 0.3769751489162445\n",
            "step: 90, loss: 0.313738077878952\n",
            "step: 100, loss: 0.1752840131521225\n",
            "step: 110, loss: 0.25041577219963074\n",
            "step: 120, loss: 0.3166857659816742\n",
            "step: 130, loss: 0.611445963382721\n",
            "step: 140, loss: 0.45719197392463684\n",
            "step: 150, loss: 0.3157779276371002\n",
            "step: 160, loss: 0.3160971999168396\n",
            "step: 170, loss: 0.4431585669517517\n",
            "step: 180, loss: 0.3163556754589081\n",
            "step: 190, loss: 0.10849975794553757\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37746915221214294\n",
            "step: 10, loss: 0.30805283784866333\n",
            "step: 20, loss: 0.5388367772102356\n",
            "step: 30, loss: 0.3825536370277405\n",
            "step: 40, loss: 0.3061036765575409\n",
            "step: 50, loss: 0.5294281244277954\n",
            "step: 60, loss: 0.4576943516731262\n",
            "step: 70, loss: 0.3884565234184265\n",
            "step: 80, loss: 0.3777627944946289\n",
            "step: 90, loss: 0.45182347297668457\n",
            "step: 100, loss: 0.3179713487625122\n",
            "step: 110, loss: 0.4598005414009094\n",
            "step: 120, loss: 0.16987088322639465\n",
            "step: 130, loss: 0.24756833910942078\n",
            "step: 140, loss: 0.3845612704753876\n",
            "step: 150, loss: 0.3776514232158661\n",
            "step: 160, loss: 0.243598073720932\n",
            "step: 170, loss: 0.455814391374588\n",
            "step: 180, loss: 0.31439805030822754\n",
            "step: 190, loss: 0.16956643760204315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31558936834335327\n",
            "step: 10, loss: 0.3124021291732788\n",
            "step: 20, loss: 0.40269720554351807\n",
            "step: 30, loss: 0.3921460509300232\n",
            "step: 40, loss: 0.24705329537391663\n",
            "step: 50, loss: 0.24713429808616638\n",
            "step: 60, loss: 0.31046929955482483\n",
            "step: 70, loss: 0.1652788519859314\n",
            "step: 80, loss: 0.31080567836761475\n",
            "step: 90, loss: 0.3138391673564911\n",
            "step: 100, loss: 0.16162168979644775\n",
            "step: 110, loss: 0.6043486595153809\n",
            "step: 120, loss: 0.16696317493915558\n",
            "step: 130, loss: 0.23469585180282593\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 140, loss: 0.3128146529197693\n",
            "step: 150, loss: 0.37754741311073303\n",
            "step: 160, loss: 0.4501582980155945\n",
            "step: 170, loss: 0.10869179666042328\n",
            "step: 180, loss: 0.24573028087615967\n",
            "step: 190, loss: 0.3109624683856964\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31468477845191956\n",
            "step: 10, loss: 0.2418137639760971\n",
            "step: 20, loss: 0.3882666528224945\n",
            "step: 30, loss: 0.10351122170686722\n",
            "step: 40, loss: 0.3116414546966553\n",
            "step: 50, loss: 0.3831918239593506\n",
            "step: 60, loss: 0.31706956028938293\n",
            "step: 70, loss: 0.244558185338974\n",
            "step: 80, loss: 0.37839147448539734\n",
            "step: 90, loss: 0.17216727137565613\n",
            "step: 100, loss: 0.3111822307109833\n",
            "step: 110, loss: 0.4454711079597473\n",
            "step: 120, loss: 0.3064422011375427\n",
            "step: 130, loss: 0.3144829571247101\n",
            "step: 140, loss: 0.17180778086185455\n",
            "step: 150, loss: 0.4518009424209595\n",
            "step: 160, loss: 0.38834747672080994\n",
            "step: 170, loss: 0.2445935755968094\n",
            "step: 180, loss: 0.31021249294281006\n",
            "step: 190, loss: 0.5273017287254333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5428845882415771\n",
            "step: 10, loss: 0.23364295065402985\n",
            "step: 20, loss: 0.2421085685491562\n",
            "step: 30, loss: 0.3135065734386444\n",
            "step: 40, loss: 0.6660134792327881\n",
            "step: 50, loss: 0.3101087212562561\n",
            "step: 60, loss: 0.176253080368042\n",
            "step: 70, loss: 0.2364198863506317\n",
            "step: 80, loss: 0.2434941679239273\n",
            "step: 90, loss: 0.3148539662361145\n",
            "step: 100, loss: 0.39001893997192383\n",
            "step: 110, loss: 0.4545298218727112\n",
            "step: 120, loss: 0.24000994861125946\n",
            "step: 130, loss: 0.31167182326316833\n",
            "step: 140, loss: 0.445420503616333\n",
            "step: 150, loss: 0.1669899970293045\n",
            "step: 160, loss: 0.30531638860702515\n",
            "step: 170, loss: 0.7443451285362244\n",
            "step: 180, loss: 0.31143617630004883\n",
            "step: 190, loss: 0.45634958148002625\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:13, 150.10it/s]\n",
            "load_f1 = 0.17216770740410348\n",
            "real_f1 = 0.17216770740410348\n",
            "733it [00:00, 3545.39it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 141.95it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSCCmtSggw8E"
      },
      "source": [
        "## DK TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbIZQYfgw8E"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4692a5-09c6-4aff-a06c-f67e10ce3753"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5224134922027588\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.43570470809936523\n",
            "step: 20, loss: 0.3386564254760742\n",
            "step: 30, loss: 0.4864845871925354\n",
            "step: 40, loss: 0.5513207912445068\n",
            "step: 50, loss: 0.3058820068836212\n",
            "step: 60, loss: 0.6161268949508667\n",
            "step: 70, loss: 0.31029242277145386\n",
            "step: 80, loss: 0.22977179288864136\n",
            "step: 90, loss: 0.19007262587547302\n",
            "step: 100, loss: 0.18375064432621002\n",
            "step: 110, loss: 0.4068055748939514\n",
            "step: 120, loss: 0.332864910364151\n",
            "step: 130, loss: 0.33719414472579956\n",
            "step: 140, loss: 0.3461642861366272\n",
            "step: 150, loss: 0.3085441589355469\n",
            "step: 160, loss: 0.38738834857940674\n",
            "step: 170, loss: 0.3276815116405487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32663947343826294\n",
            "step: 10, loss: 0.4489154517650604\n",
            "step: 20, loss: 0.311973512172699\n",
            "step: 30, loss: 0.3248671889305115\n",
            "step: 40, loss: 0.09652520716190338\n",
            "step: 50, loss: 0.4534939229488373\n",
            "step: 60, loss: 0.19815103709697723\n",
            "step: 70, loss: 0.4873529076576233\n",
            "step: 80, loss: 0.2504025995731354\n",
            "step: 90, loss: 0.23449283838272095\n",
            "step: 100, loss: 0.5136857628822327\n",
            "step: 110, loss: 0.26196008920669556\n",
            "step: 120, loss: 0.2552545666694641\n",
            "step: 130, loss: 0.5388697385787964\n",
            "step: 140, loss: 0.5181161761283875\n",
            "step: 150, loss: 0.43537476658821106\n",
            "step: 160, loss: 0.4303972125053406\n",
            "step: 170, loss: 0.37198373675346375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6010770201683044\n",
            "step: 10, loss: 0.30089306831359863\n",
            "step: 20, loss: 0.23111656308174133\n",
            "step: 30, loss: 0.23997394740581512\n",
            "step: 40, loss: 0.3856006860733032\n",
            "step: 50, loss: 0.5737312436103821\n",
            "step: 60, loss: 0.31186139583587646\n",
            "step: 70, loss: 0.23026776313781738\n",
            "step: 80, loss: 0.3747130036354065\n",
            "step: 90, loss: 0.5237874984741211\n",
            "step: 100, loss: 0.30894196033477783\n",
            "step: 110, loss: 0.16972661018371582\n",
            "step: 120, loss: 0.5781854391098022\n",
            "step: 130, loss: 0.512714147567749\n",
            "step: 140, loss: 0.43094122409820557\n",
            "step: 150, loss: 0.19514276087284088\n",
            "step: 160, loss: 0.17397497594356537\n",
            "step: 170, loss: 0.3072023391723633\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4033510982990265\n",
            "step: 10, loss: 0.5287095308303833\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.23550793528556824\n",
            "step: 30, loss: 0.4498196244239807\n",
            "step: 40, loss: 0.24383828043937683\n",
            "step: 50, loss: 0.38628238439559937\n",
            "step: 60, loss: 0.6455332636833191\n",
            "step: 70, loss: 0.32093605399131775\n",
            "step: 80, loss: 0.46076318621635437\n",
            "step: 90, loss: 0.32136183977127075\n",
            "step: 100, loss: 0.3771967589855194\n",
            "step: 110, loss: 0.4422902464866638\n",
            "step: 120, loss: 0.469236820936203\n",
            "step: 130, loss: 0.3115960359573364\n",
            "step: 140, loss: 0.23853453993797302\n",
            "step: 150, loss: 0.6925938725471497\n",
            "step: 160, loss: 0.13616178929805756\n",
            "step: 170, loss: 0.23973330855369568\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38206979632377625\n",
            "step: 10, loss: 0.3002461791038513\n",
            "step: 20, loss: 0.3522549271583557\n",
            "step: 30, loss: 0.2979086637496948\n",
            "step: 40, loss: 0.23861420154571533\n",
            "step: 50, loss: 0.23809799551963806\n",
            "step: 60, loss: 0.3088415563106537\n",
            "step: 70, loss: 0.32274994254112244\n",
            "step: 80, loss: 0.07553055882453918\n",
            "step: 90, loss: 0.6234117746353149\n",
            "step: 100, loss: 0.25978484749794006\n",
            "step: 110, loss: 0.26905491948127747\n",
            "step: 120, loss: 0.13239283859729767\n",
            "step: 130, loss: 0.23931871354579926\n",
            "step: 140, loss: 0.16747640073299408\n",
            "step: 150, loss: 0.2987862527370453\n",
            "step: 160, loss: 0.2363199144601822\n",
            "step: 170, loss: 0.37887316942214966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.2315227070347284, f1=0.23260247855100097, best_f1=0.23260247855100097\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3040986657142639\n",
            "step: 10, loss: 0.10812019556760788\n",
            "step: 20, loss: 0.35027897357940674\n",
            "step: 30, loss: 0.25723958015441895\n",
            "step: 40, loss: 0.450551837682724\n",
            "step: 50, loss: 0.23476389050483704\n",
            "step: 60, loss: 0.3711256682872772\n",
            "step: 70, loss: 0.3677956759929657\n",
            "step: 80, loss: 0.18618735671043396\n",
            "step: 90, loss: 0.3080659508705139\n",
            "step: 100, loss: 0.1614590436220169\n",
            "step: 110, loss: 0.457817405462265\n",
            "step: 120, loss: 0.4307559132575989\n",
            "step: 130, loss: 0.5421786308288574\n",
            "step: 140, loss: 0.25127190351486206\n",
            "step: 150, loss: 0.14907178282737732\n",
            "step: 160, loss: 0.3210066556930542\n",
            "step: 170, loss: 0.22419333457946777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.28079470198675494, f1=0.26720647773279355, best_f1=0.26720647773279355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26566728949546814\n",
            "step: 10, loss: 0.4422398805618286\n",
            "step: 20, loss: 0.46231937408447266\n",
            "step: 30, loss: 0.4653851389884949\n",
            "step: 40, loss: 0.12200221419334412\n",
            "step: 50, loss: 0.3856482207775116\n",
            "step: 60, loss: 0.53426194190979\n",
            "step: 70, loss: 0.33202752470970154\n",
            "step: 80, loss: 0.17256517708301544\n",
            "step: 90, loss: 0.4491376578807831\n",
            "step: 100, loss: 0.27592507004737854\n",
            "step: 110, loss: 0.3402920365333557\n",
            "step: 120, loss: 0.3485150635242462\n",
            "step: 130, loss: 0.24777241051197052\n",
            "step: 140, loss: 0.1400597095489502\n",
            "step: 150, loss: 0.27838245034217834\n",
            "step: 160, loss: 0.35033002495765686\n",
            "step: 170, loss: 0.3041612505912781\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.31558935361216733, f1=0.27184466019417475, best_f1=0.27184466019417475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4057004451751709\n",
            "step: 10, loss: 0.45555421710014343\n",
            "step: 20, loss: 0.2114461213350296\n",
            "step: 30, loss: 0.2379002571105957\n",
            "step: 40, loss: 0.20314805209636688\n",
            "step: 50, loss: 0.3260626792907715\n",
            "step: 60, loss: 0.3439945876598358\n",
            "step: 70, loss: 0.13149790465831757\n",
            "step: 80, loss: 0.23917439579963684\n",
            "step: 90, loss: 0.3075391352176666\n",
            "step: 100, loss: 0.16885268688201904\n",
            "step: 110, loss: 0.3430001139640808\n",
            "step: 120, loss: 0.45840683579444885\n",
            "step: 130, loss: 0.22471940517425537\n",
            "step: 140, loss: 0.36341404914855957\n",
            "step: 150, loss: 0.21012230217456818\n",
            "step: 160, loss: 0.04184854030609131\n",
            "step: 170, loss: 0.32737305760383606\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.5111111111111111, f1=0.5107913669064748, best_f1=0.5107913669064748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15873926877975464\n",
            "step: 10, loss: 0.33107584714889526\n",
            "step: 20, loss: 0.2887733578681946\n",
            "step: 30, loss: 0.12931105494499207\n",
            "step: 40, loss: 0.2327919900417328\n",
            "step: 50, loss: 0.12048786878585815\n",
            "step: 60, loss: 0.2560717463493347\n",
            "step: 70, loss: 0.27690640091896057\n",
            "step: 80, loss: 0.0652107521891594\n",
            "step: 90, loss: 0.3683861792087555\n",
            "step: 100, loss: 0.22373196482658386\n",
            "step: 110, loss: 0.1421109288930893\n",
            "step: 120, loss: 0.4050123691558838\n",
            "step: 130, loss: 0.19916735589504242\n",
            "step: 140, loss: 0.11499611288309097\n",
            "step: 150, loss: 0.3012973964214325\n",
            "step: 160, loss: 0.1798066943883896\n",
            "step: 170, loss: 0.183517724275589\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.5905172413793105, f1=0.6004140786749482, best_f1=0.6004140786749482\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14966821670532227\n",
            "step: 10, loss: 0.11000782251358032\n",
            "step: 20, loss: 0.11786771565675735\n",
            "step: 30, loss: 0.33142703771591187\n",
            "step: 40, loss: 0.3250548839569092\n",
            "step: 50, loss: 0.31806644797325134\n",
            "step: 60, loss: 0.3754658102989197\n",
            "step: 70, loss: 0.14593444764614105\n",
            "step: 80, loss: 0.22078245878219604\n",
            "step: 90, loss: 0.07587716728448868\n",
            "step: 100, loss: 0.28691521286964417\n",
            "step: 110, loss: 0.10530348122119904\n",
            "step: 120, loss: 0.13329382240772247\n",
            "step: 130, loss: 0.02025538496673107\n",
            "step: 140, loss: 0.056089192628860474\n",
            "step: 150, loss: 0.2473679482936859\n",
            "step: 160, loss: 0.0777050256729126\n",
            "step: 170, loss: 0.25564202666282654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.6150234741784038, f1=0.6605080831408776, best_f1=0.6605080831408776\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2711876630783081\n",
            "step: 10, loss: 0.20467163622379303\n",
            "step: 20, loss: 0.11962679028511047\n",
            "step: 30, loss: 0.3253336250782013\n",
            "step: 40, loss: 0.052735086530447006\n",
            "step: 50, loss: 0.1387292593717575\n",
            "step: 60, loss: 0.36775025725364685\n",
            "step: 70, loss: 0.17952892184257507\n",
            "step: 80, loss: 0.07757637649774551\n",
            "step: 90, loss: 0.09068766236305237\n",
            "step: 100, loss: 0.3823051154613495\n",
            "step: 110, loss: 0.14194858074188232\n",
            "step: 120, loss: 0.21721746027469635\n",
            "step: 130, loss: 0.16307522356510162\n",
            "step: 140, loss: 0.30255889892578125\n",
            "step: 150, loss: 0.043906837701797485\n",
            "step: 160, loss: 0.07221173495054245\n",
            "step: 170, loss: 0.1796836405992508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.6372549019607844, f1=0.65083135391924, best_f1=0.65083135391924\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23494018614292145\n",
            "step: 10, loss: 0.1253843605518341\n",
            "step: 20, loss: 0.04874030500650406\n",
            "step: 30, loss: 0.23375681042671204\n",
            "step: 40, loss: 0.09184136986732483\n",
            "step: 50, loss: 0.09575656801462173\n",
            "step: 60, loss: 0.11857355386018753\n",
            "step: 70, loss: 0.02821836806833744\n",
            "step: 80, loss: 0.06997161358594894\n",
            "step: 90, loss: 0.1720402091741562\n",
            "step: 100, loss: 0.03144475072622299\n",
            "step: 110, loss: 0.2017461061477661\n",
            "step: 120, loss: 0.09840720891952515\n",
            "step: 130, loss: 0.19714058935642242\n",
            "step: 140, loss: 0.15901316702365875\n",
            "step: 150, loss: 0.05177291855216026\n",
            "step: 160, loss: 0.3843386173248291\n",
            "step: 170, loss: 0.34623900055885315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.6453089244851259, f1=0.6802721088435375, best_f1=0.6802721088435375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.053392939269542694\n",
            "step: 10, loss: 0.06714648753404617\n",
            "step: 20, loss: 0.05164986103773117\n",
            "step: 30, loss: 0.1639413982629776\n",
            "step: 40, loss: 0.15190184116363525\n",
            "step: 50, loss: 0.08667688816785812\n",
            "step: 60, loss: 0.05180596560239792\n",
            "step: 70, loss: 0.2251470983028412\n",
            "step: 80, loss: 0.048560578376054764\n",
            "step: 90, loss: 0.0936276987195015\n",
            "step: 100, loss: 0.04376468434929848\n",
            "step: 110, loss: 0.19664238393306732\n",
            "step: 120, loss: 0.03118598647415638\n",
            "step: 130, loss: 0.023097224533557892\n",
            "step: 140, loss: 0.24996815621852875\n",
            "step: 150, loss: 0.1854085624217987\n",
            "step: 160, loss: 0.14240983128547668\n",
            "step: 170, loss: 0.061763741075992584\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6368159203980098, f1=0.7014218009478673, best_f1=0.6802721088435375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.060375623404979706\n",
            "step: 10, loss: 0.04751350358128548\n",
            "step: 20, loss: 0.19994410872459412\n",
            "step: 30, loss: 0.15146008133888245\n",
            "step: 40, loss: 0.06977199018001556\n",
            "step: 50, loss: 0.03141534700989723\n",
            "step: 60, loss: 0.16314558684825897\n",
            "step: 70, loss: 0.07681863009929657\n",
            "step: 80, loss: 0.02540522627532482\n",
            "step: 90, loss: 0.09598355740308762\n",
            "step: 100, loss: 0.5308424234390259\n",
            "step: 110, loss: 0.18308450281620026\n",
            "step: 120, loss: 0.15432685613632202\n",
            "step: 130, loss: 0.137806236743927\n",
            "step: 140, loss: 0.14560078084468842\n",
            "step: 150, loss: 0.055990926921367645\n",
            "step: 160, loss: 0.09214131534099579\n",
            "step: 170, loss: 0.2506602108478546\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.6499999999999999, f1=0.6821052631578948, best_f1=0.6821052631578948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02796945534646511\n",
            "step: 10, loss: 0.06627184897661209\n",
            "step: 20, loss: 0.017916670069098473\n",
            "step: 30, loss: 0.15545184910297394\n",
            "step: 40, loss: 0.04530680552124977\n",
            "step: 50, loss: 0.05775542929768562\n",
            "step: 60, loss: 0.040231604129076004\n",
            "step: 70, loss: 0.18604779243469238\n",
            "step: 80, loss: 0.038952842354774475\n",
            "step: 90, loss: 0.1946214735507965\n",
            "step: 100, loss: 0.19153544306755066\n",
            "step: 110, loss: 0.1723310798406601\n",
            "step: 120, loss: 0.04669990763068199\n",
            "step: 130, loss: 0.2823021411895752\n",
            "step: 140, loss: 0.08788474649190903\n",
            "step: 150, loss: 0.023760942742228508\n",
            "step: 160, loss: 0.10568135231733322\n",
            "step: 170, loss: 0.06069212406873703\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.6711409395973154, f1=0.7022222222222223, best_f1=0.7022222222222223\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 227.92it/s]\n",
            "load_f1 = 0.5632458233890215\n",
            "real_f1 = 0.558252427184466\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 142.21it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HZE1zMQgw8F"
      },
      "source": [
        "## DK DIRTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jg7qrOQgw8F"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c35e970a-5517-4e85-ec07-aa86c27a62dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5979142189025879\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.43974819779396057\n",
            "step: 20, loss: 0.5192098617553711\n",
            "step: 30, loss: 0.3724230229854584\n",
            "step: 40, loss: 0.32762035727500916\n",
            "step: 50, loss: 0.5776663422584534\n",
            "step: 60, loss: 0.47590023279190063\n",
            "step: 70, loss: 0.44028374552726746\n",
            "step: 80, loss: 0.45457297563552856\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.24901996552944183\n",
            "step: 100, loss: 0.14870940148830414\n",
            "step: 110, loss: 0.36679503321647644\n",
            "step: 120, loss: 0.2251165509223938\n",
            "step: 130, loss: 0.13468994200229645\n",
            "step: 140, loss: 0.015590266324579716\n",
            "step: 150, loss: 0.14457473158836365\n",
            "step: 160, loss: 0.05450360104441643\n",
            "step: 170, loss: 0.18422673642635345\n",
            "step: 180, loss: 0.06268929690122604\n",
            "step: 190, loss: 0.07920637726783752\n",
            "step: 200, loss: 0.05996327102184296\n",
            "step: 210, loss: 0.06184128299355507\n",
            "step: 220, loss: 0.10505848377943039\n",
            "step: 230, loss: 0.0589505136013031\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9288793103448276, f1=0.9393605292171995, best_f1=0.9393605292171995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010152212344110012\n",
            "step: 10, loss: 0.10773037374019623\n",
            "step: 20, loss: 0.02154688537120819\n",
            "step: 30, loss: 0.04101619869470596\n",
            "step: 40, loss: 0.04598267376422882\n",
            "step: 50, loss: 0.020777912810444832\n",
            "step: 60, loss: 0.10956314951181412\n",
            "step: 70, loss: 0.033505573868751526\n",
            "step: 80, loss: 0.04454544186592102\n",
            "step: 90, loss: 0.02890956401824951\n",
            "step: 100, loss: 0.029684094712138176\n",
            "step: 110, loss: 0.03516850993037224\n",
            "step: 120, loss: 0.038363974541425705\n",
            "step: 130, loss: 0.06068016588687897\n",
            "step: 140, loss: 0.011324673891067505\n",
            "step: 150, loss: 0.2201681286096573\n",
            "step: 160, loss: 0.009802674874663353\n",
            "step: 170, loss: 0.001951185055077076\n",
            "step: 180, loss: 0.015689648687839508\n",
            "step: 190, loss: 0.03518122062087059\n",
            "step: 200, loss: 0.21628141403198242\n",
            "step: 210, loss: 0.029797684401273727\n",
            "step: 220, loss: 0.003201527986675501\n",
            "step: 230, loss: 0.0010501743527129292\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9610678531701891, f1=0.958659217877095, best_f1=0.958659217877095\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05671127885580063\n",
            "step: 10, loss: 0.06249891594052315\n",
            "step: 20, loss: 0.00806399155408144\n",
            "step: 30, loss: 0.004731875378638506\n",
            "step: 40, loss: 0.01776645891368389\n",
            "step: 50, loss: 0.03138623759150505\n",
            "step: 60, loss: 0.0053806910291314125\n",
            "step: 70, loss: 0.004765905439853668\n",
            "step: 80, loss: 0.13458870351314545\n",
            "step: 90, loss: 0.006155447568744421\n",
            "step: 100, loss: 0.020609281957149506\n",
            "step: 110, loss: 0.01170342043042183\n",
            "step: 120, loss: 0.07286889851093292\n",
            "step: 130, loss: 0.011447442695498466\n",
            "step: 140, loss: 0.014716355130076408\n",
            "step: 150, loss: 0.03486248478293419\n",
            "step: 160, loss: 0.029231654480099678\n",
            "step: 170, loss: 0.002287516836076975\n",
            "step: 180, loss: 0.01033734530210495\n",
            "step: 190, loss: 0.05286332592368126\n",
            "step: 200, loss: 0.006833489052951336\n",
            "step: 210, loss: 0.004358714446425438\n",
            "step: 220, loss: 0.06377564370632172\n",
            "step: 230, loss: 0.0032149769831448793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9597315436241611, f1=0.9515219842164601, best_f1=0.958659217877095\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007411854807287455\n",
            "step: 10, loss: 0.005643106065690517\n",
            "step: 20, loss: 0.0024781033862382174\n",
            "step: 30, loss: 0.028024787083268166\n",
            "step: 40, loss: 0.03441194072365761\n",
            "step: 50, loss: 0.00836469978094101\n",
            "step: 60, loss: 0.015653930604457855\n",
            "step: 70, loss: 0.0014090933836996555\n",
            "step: 80, loss: 0.0026050955057144165\n",
            "step: 90, loss: 0.02394113875925541\n",
            "step: 100, loss: 0.025860587134957314\n",
            "step: 110, loss: 0.005856078118085861\n",
            "step: 120, loss: 0.02509853057563305\n",
            "step: 130, loss: 0.01922258734703064\n",
            "step: 140, loss: 0.001737719401717186\n",
            "step: 150, loss: 0.0020551877096295357\n",
            "step: 160, loss: 0.0025387529749423265\n",
            "step: 170, loss: 0.002203996991738677\n",
            "step: 180, loss: 0.15785184502601624\n",
            "step: 190, loss: 0.0018684528768062592\n",
            "step: 200, loss: 0.005214964505285025\n",
            "step: 210, loss: 0.0036381399258971214\n",
            "step: 220, loss: 0.004178878851234913\n",
            "step: 230, loss: 0.0012273527681827545\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9707865168539327, f1=0.9579067121729239, best_f1=0.9579067121729239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007847091765142977\n",
            "step: 10, loss: 0.002923512365669012\n",
            "step: 20, loss: 0.004008636809885502\n",
            "step: 30, loss: 0.003671307349577546\n",
            "step: 40, loss: 0.01601197011768818\n",
            "step: 50, loss: 0.013792771846055984\n",
            "step: 60, loss: 0.13889168202877045\n",
            "step: 70, loss: 0.001808722736313939\n",
            "step: 80, loss: 0.006557678338140249\n",
            "step: 90, loss: 0.022779548540711403\n",
            "step: 100, loss: 0.0005174692487344146\n",
            "step: 110, loss: 0.0012832535430788994\n",
            "step: 120, loss: 0.010965442284941673\n",
            "step: 130, loss: 0.0007203927962109447\n",
            "step: 140, loss: 0.016500404104590416\n",
            "step: 150, loss: 0.0036914353258907795\n",
            "step: 160, loss: 0.00745379738509655\n",
            "step: 170, loss: 0.15137679874897003\n",
            "step: 180, loss: 0.08554988354444504\n",
            "step: 190, loss: 0.03533066064119339\n",
            "step: 200, loss: 0.0019591364543884993\n",
            "step: 210, loss: 0.00474750017747283\n",
            "step: 220, loss: 0.002946150256320834\n",
            "step: 230, loss: 0.004235965199768543\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9648924122310306, f1=0.9576174112256587, best_f1=0.9579067121729239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006558767636306584\n",
            "step: 10, loss: 0.004351991694420576\n",
            "step: 20, loss: 0.0018532747635617852\n",
            "step: 30, loss: 0.0010179925011470914\n",
            "step: 40, loss: 0.000429611187428236\n",
            "step: 50, loss: 0.0004918023478239775\n",
            "step: 60, loss: 0.0014585691969841719\n",
            "step: 70, loss: 0.04166656732559204\n",
            "step: 80, loss: 0.005797484889626503\n",
            "step: 90, loss: 0.0028149655554443598\n",
            "step: 100, loss: 0.0005226911744102836\n",
            "step: 110, loss: 0.0058654919266700745\n",
            "step: 120, loss: 0.00041720259469002485\n",
            "step: 130, loss: 0.000572561810258776\n",
            "step: 140, loss: 0.00837730523198843\n",
            "step: 150, loss: 0.0012084355112165213\n",
            "step: 160, loss: 0.009739595465362072\n",
            "step: 170, loss: 0.069234699010849\n",
            "step: 180, loss: 0.013407486490905285\n",
            "step: 190, loss: 0.004736336413770914\n",
            "step: 200, loss: 0.004168704617768526\n",
            "step: 210, loss: 0.0013167030410841107\n",
            "step: 220, loss: 0.010824727825820446\n",
            "step: 230, loss: 0.0023995577357709408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9720044792833147, f1=0.9704545454545453, best_f1=0.9704545454545453\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002695647068321705\n",
            "step: 10, loss: 0.0009634889429435134\n",
            "step: 20, loss: 0.00034907704684883356\n",
            "step: 30, loss: 0.0004072723095305264\n",
            "step: 40, loss: 0.0014796770410612226\n",
            "step: 50, loss: 0.0012225176906213164\n",
            "step: 60, loss: 0.0012191931018605828\n",
            "step: 70, loss: 0.0007110853912308812\n",
            "step: 80, loss: 0.0014118656981736422\n",
            "step: 90, loss: 0.0006033413228578866\n",
            "step: 100, loss: 0.0005564825260080397\n",
            "step: 110, loss: 0.0004649000184144825\n",
            "step: 120, loss: 0.0009599559125490487\n",
            "step: 130, loss: 0.0015707287238910794\n",
            "step: 140, loss: 0.0002783865202218294\n",
            "step: 150, loss: 0.010302890092134476\n",
            "step: 160, loss: 0.00032064612605609\n",
            "step: 170, loss: 0.009474161081016064\n",
            "step: 180, loss: 0.0006306513678282499\n",
            "step: 190, loss: 0.015309489332139492\n",
            "step: 200, loss: 0.003561084158718586\n",
            "step: 210, loss: 0.0010394882410764694\n",
            "step: 220, loss: 0.0005286672385409474\n",
            "step: 230, loss: 0.000827137497253716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9774266365688488, f1=0.9741282339707535, best_f1=0.9741282339707535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006643295637331903\n",
            "step: 10, loss: 0.0021995059214532375\n",
            "step: 20, loss: 0.003238845383748412\n",
            "step: 30, loss: 0.001720731146633625\n",
            "step: 40, loss: 0.0032200815621763468\n",
            "step: 50, loss: 0.0005496556404978037\n",
            "step: 60, loss: 0.0007529343129135668\n",
            "step: 70, loss: 0.0004768540384247899\n",
            "step: 80, loss: 0.022370925173163414\n",
            "step: 90, loss: 0.0021529102232307196\n",
            "step: 100, loss: 0.001088427845388651\n",
            "step: 110, loss: 0.004459598567336798\n",
            "step: 120, loss: 0.00044783749035559595\n",
            "step: 130, loss: 0.0009647779515944421\n",
            "step: 140, loss: 0.0005263531347736716\n",
            "step: 150, loss: 0.058689579367637634\n",
            "step: 160, loss: 0.000521199603099376\n",
            "step: 170, loss: 0.013489334844052792\n",
            "step: 180, loss: 0.0009617999894544482\n",
            "step: 190, loss: 0.022518504410982132\n",
            "step: 200, loss: 0.05441886559128761\n",
            "step: 210, loss: 0.002001391025260091\n",
            "step: 220, loss: 0.0008731010602787137\n",
            "step: 230, loss: 0.00037444400368258357\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9774266365688488, f1=0.9726027397260274, best_f1=0.9741282339707535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010390623938292265\n",
            "step: 10, loss: 0.001784015097655356\n",
            "step: 20, loss: 0.0009925502818077803\n",
            "step: 30, loss: 0.002172334585338831\n",
            "step: 40, loss: 0.0011534662917256355\n",
            "step: 50, loss: 0.0009222966036759317\n",
            "step: 60, loss: 0.0009638680494390428\n",
            "step: 70, loss: 0.056951526552438736\n",
            "step: 80, loss: 0.0024483134038746357\n",
            "step: 90, loss: 0.02031221054494381\n",
            "step: 100, loss: 0.0005038008093833923\n",
            "step: 110, loss: 0.0005953013896942139\n",
            "step: 120, loss: 0.010871898382902145\n",
            "step: 130, loss: 0.0012828364269807935\n",
            "step: 140, loss: 0.000825157854706049\n",
            "step: 150, loss: 0.004624711349606514\n",
            "step: 160, loss: 0.00707043893635273\n",
            "step: 170, loss: 0.0002836265484802425\n",
            "step: 180, loss: 0.010008223354816437\n",
            "step: 190, loss: 0.00020636072440538555\n",
            "step: 200, loss: 0.0002189340302720666\n",
            "step: 210, loss: 0.0013835112331435084\n",
            "step: 220, loss: 0.004271875601261854\n",
            "step: 230, loss: 0.0005290705012157559\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9786276715410572, f1=0.9785310734463276, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008619112195447087\n",
            "step: 10, loss: 0.003274794900789857\n",
            "step: 20, loss: 0.0008746837265789509\n",
            "step: 30, loss: 0.0004080715880263597\n",
            "step: 40, loss: 0.0032385983504354954\n",
            "step: 50, loss: 0.000601286708842963\n",
            "step: 60, loss: 0.0012090844102203846\n",
            "step: 70, loss: 0.008310348726809025\n",
            "step: 80, loss: 0.001742631895467639\n",
            "step: 90, loss: 0.0005156337283551693\n",
            "step: 100, loss: 0.0003988728567492217\n",
            "step: 110, loss: 0.0009523688349872828\n",
            "step: 120, loss: 0.0005309226689860225\n",
            "step: 130, loss: 0.0008586474577896297\n",
            "step: 140, loss: 0.00034501292975619435\n",
            "step: 150, loss: 0.0011109693441540003\n",
            "step: 160, loss: 0.00014913732593413442\n",
            "step: 170, loss: 0.0003010663203895092\n",
            "step: 180, loss: 0.0010187654988840222\n",
            "step: 190, loss: 0.00040814929525367916\n",
            "step: 200, loss: 0.0010485327802598476\n",
            "step: 210, loss: 0.05217184126377106\n",
            "step: 220, loss: 0.0009399367263540626\n",
            "step: 230, loss: 0.00031116779427975416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9761634506242906, f1=0.9727272727272728, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002826851268764585\n",
            "step: 10, loss: 0.00042476426460780203\n",
            "step: 20, loss: 0.00031422049505636096\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 30, loss: 0.0005101457936689258\n",
            "step: 40, loss: 5.4267125960905105e-05\n",
            "step: 50, loss: 0.00017756257147993892\n",
            "step: 60, loss: 0.014154382981359959\n",
            "step: 70, loss: 0.0007547697750851512\n",
            "step: 80, loss: 0.025964342057704926\n",
            "step: 90, loss: 0.18743599951267242\n",
            "step: 100, loss: 0.002803550101816654\n",
            "step: 110, loss: 0.0003954597341362387\n",
            "step: 120, loss: 0.0030291101429611444\n",
            "step: 130, loss: 0.0004425695224199444\n",
            "step: 140, loss: 0.0018375799991190434\n",
            "step: 150, loss: 0.0009974241256713867\n",
            "step: 160, loss: 0.0027335137128829956\n",
            "step: 170, loss: 0.0016421866603195667\n",
            "step: 180, loss: 0.0013575946213677526\n",
            "step: 190, loss: 0.0012692271266132593\n",
            "step: 200, loss: 0.006110710557550192\n",
            "step: 210, loss: 0.0008249871898442507\n",
            "step: 220, loss: 0.0012552484404295683\n",
            "step: 230, loss: 0.000638590136077255\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9763779527559054, f1=0.9684684684684683, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010599942179396749\n",
            "step: 10, loss: 0.0003862686571665108\n",
            "step: 20, loss: 0.0010855947621166706\n",
            "step: 30, loss: 0.0470668226480484\n",
            "step: 40, loss: 0.0011672339169308543\n",
            "step: 50, loss: 0.0007003373466432095\n",
            "step: 60, loss: 0.001618965994566679\n",
            "step: 70, loss: 0.00033749762224033475\n",
            "step: 80, loss: 0.0005412072059698403\n",
            "step: 90, loss: 0.00038790807593613863\n",
            "step: 100, loss: 0.0005790450377389789\n",
            "step: 110, loss: 0.00028899015160277486\n",
            "step: 120, loss: 0.0031947060488164425\n",
            "step: 130, loss: 0.0017420303774997592\n",
            "step: 140, loss: 0.0006504691555164754\n",
            "step: 150, loss: 0.0006733782356604934\n",
            "step: 160, loss: 0.004730419255793095\n",
            "step: 170, loss: 0.0002682568447198719\n",
            "step: 180, loss: 0.0008831224404275417\n",
            "step: 190, loss: 0.001867205137386918\n",
            "step: 200, loss: 0.001542786369100213\n",
            "step: 210, loss: 0.0003145380178466439\n",
            "step: 220, loss: 0.0007877483149059117\n",
            "step: 230, loss: 0.0014309256803244352\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9819004524886877, f1=0.9727272727272728, best_f1=0.9727272727272728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007371920510195196\n",
            "step: 10, loss: 0.0008396076736971736\n",
            "step: 20, loss: 0.0010678916005417705\n",
            "step: 30, loss: 0.009816392324864864\n",
            "step: 40, loss: 0.0007472258294001222\n",
            "step: 50, loss: 0.0018767430447041988\n",
            "step: 60, loss: 0.00037059857277199626\n",
            "step: 70, loss: 0.000156951486133039\n",
            "step: 80, loss: 9.270204463973641e-05\n",
            "step: 90, loss: 0.0001970810699276626\n",
            "step: 100, loss: 0.0007784431800246239\n",
            "step: 110, loss: 0.000536116014700383\n",
            "step: 120, loss: 0.00039518577978014946\n",
            "step: 130, loss: 0.0004875545564573258\n",
            "step: 140, loss: 0.001444666413590312\n",
            "step: 150, loss: 6.913291872479022e-05\n",
            "step: 160, loss: 0.0023040231317281723\n",
            "step: 170, loss: 0.0006595898885279894\n",
            "step: 180, loss: 0.029028838500380516\n",
            "step: 190, loss: 0.00024532689712941647\n",
            "step: 200, loss: 4.773797627422027e-05\n",
            "step: 210, loss: 0.0006029165233485401\n",
            "step: 220, loss: 0.00015535883721895516\n",
            "step: 230, loss: 0.0001300879375776276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9806157354618015, f1=0.9702517162471395, best_f1=0.9727272727272728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011333864676998928\n",
            "step: 10, loss: 0.00010812225809786469\n",
            "step: 20, loss: 0.00012684284592978656\n",
            "step: 30, loss: 0.000278992869425565\n",
            "step: 40, loss: 0.00039110853685997427\n",
            "step: 50, loss: 9.495155973127112e-05\n",
            "step: 60, loss: 0.0003506613720674068\n",
            "step: 70, loss: 0.00046431980445049703\n",
            "step: 80, loss: 0.00010987724090227857\n",
            "step: 90, loss: 0.0008656770805828273\n",
            "step: 100, loss: 0.0003941546310670674\n",
            "step: 110, loss: 0.0006772708729840815\n",
            "step: 120, loss: 3.7277703086147085e-05\n",
            "step: 130, loss: 0.0006340260151773691\n",
            "step: 140, loss: 0.0005039240932092071\n",
            "step: 150, loss: 6.623627268709242e-05\n",
            "step: 160, loss: 0.0001622192357899621\n",
            "step: 170, loss: 0.00020950843463651836\n",
            "step: 180, loss: 0.00021053769160062075\n",
            "step: 190, loss: 0.0011629292275756598\n",
            "step: 200, loss: 0.00019406831415835768\n",
            "step: 210, loss: 0.00043045455822721124\n",
            "step: 220, loss: 0.00043261994142085314\n",
            "step: 230, loss: 0.00013276255049277097\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9786276715410572, f1=0.9774266365688488, best_f1=0.9727272727272728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0215901006013155\n",
            "step: 10, loss: 0.00041371717816218734\n",
            "step: 20, loss: 0.0003284417907707393\n",
            "step: 30, loss: 0.00021591248514596373\n",
            "step: 40, loss: 9.542836050968617e-05\n",
            "step: 50, loss: 0.0013763055903837085\n",
            "step: 60, loss: 0.008747619576752186\n",
            "step: 70, loss: 9.226417023455724e-05\n",
            "step: 80, loss: 0.00013256481906864792\n",
            "step: 90, loss: 7.146182178985327e-05\n",
            "step: 100, loss: 6.883819150971249e-05\n",
            "step: 110, loss: 0.0001811133261071518\n",
            "step: 120, loss: 0.023283224552869797\n",
            "step: 130, loss: 7.077717600623146e-05\n",
            "step: 140, loss: 0.00029884057585150003\n",
            "step: 150, loss: 0.00014342715439852327\n",
            "step: 160, loss: 0.011614852584898472\n",
            "step: 170, loss: 5.613160828943364e-05\n",
            "step: 180, loss: 0.0003933763946406543\n",
            "step: 190, loss: 0.00021164222562219948\n",
            "step: 200, loss: 9.433337254449725e-05\n",
            "step: 210, loss: 0.05304417759180069\n",
            "step: 220, loss: 0.0009045694023370743\n",
            "step: 230, loss: 0.000119248048576992\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9785794813979707, f1=0.9762174405436014, best_f1=0.9727272727272728\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:17, 143.28it/s]\n",
            "load_f1 = 0.9831271091113611\n",
            "real_f1 = 0.9820224719101124\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:33, 130.31it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYEeB2m-gw8G"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcfb34ca-b490-4d6c-8ffd-d4521d119f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6222481727600098\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.4687756597995758\n",
            "step: 20, loss: 0.25222116708755493\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.398689866065979\n",
            "step: 40, loss: 0.2763591706752777\n",
            "step: 50, loss: 0.42517539858818054\n",
            "step: 60, loss: 0.23702993988990784\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 70, loss: 0.16185471415519714\n",
            "step: 80, loss: 0.13969337940216064\n",
            "step: 90, loss: 0.14175757765769958\n",
            "step: 100, loss: 0.21059833467006683\n",
            "step: 110, loss: 0.20550650358200073\n",
            "step: 120, loss: 0.20189253985881805\n",
            "step: 130, loss: 0.09982144832611084\n",
            "step: 140, loss: 0.3466428518295288\n",
            "step: 150, loss: 0.21334826946258545\n",
            "step: 160, loss: 0.2653432786464691\n",
            "step: 170, loss: 0.06638039648532867\n",
            "step: 180, loss: 0.10087010264396667\n",
            "step: 190, loss: 0.12132685631513596\n",
            "step: 200, loss: 0.05892464518547058\n",
            "step: 210, loss: 0.11885136365890503\n",
            "step: 220, loss: 0.03264876455068588\n",
            "step: 230, loss: 0.14015783369541168\n",
            "step: 240, loss: 0.029243450611829758\n",
            "step: 250, loss: 0.13082334399223328\n",
            "step: 260, loss: 0.25772568583488464\n",
            "step: 270, loss: 0.22044460475444794\n",
            "step: 280, loss: 0.08009412884712219\n",
            "step: 290, loss: 0.11576090008020401\n",
            "step: 300, loss: 0.0910399928689003\n",
            "step: 310, loss: 0.10463692992925644\n",
            "step: 320, loss: 0.04567507654428482\n",
            "step: 330, loss: 0.03423400595784187\n",
            "step: 340, loss: 0.32875901460647583\n",
            "step: 350, loss: 0.10525234788656235\n",
            "step: 360, loss: 0.034358568489551544\n",
            "step: 370, loss: 0.04010371118783951\n",
            "step: 380, loss: 0.09423958510160446\n",
            "step: 390, loss: 0.013202971778810024\n",
            "step: 400, loss: 0.03537483140826225\n",
            "step: 410, loss: 0.11058027297258377\n",
            "step: 420, loss: 0.03165515139698982\n",
            "step: 430, loss: 0.03906974941492081\n",
            "step: 440, loss: 0.20854276418685913\n",
            "step: 450, loss: 0.07616932690143585\n",
            "step: 460, loss: 0.05421217903494835\n",
            "step: 470, loss: 0.04189828410744667\n",
            "step: 480, loss: 0.14764080941677094\n",
            "step: 490, loss: 0.1152690127491951\n",
            "step: 500, loss: 0.07191697508096695\n",
            "step: 510, loss: 0.11227802187204361\n",
            "step: 520, loss: 0.09530756622552872\n",
            "step: 530, loss: 0.007885810919106007\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9314685314685315, f1=0.9260628465804066, best_f1=0.9260628465804066\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1345495879650116\n",
            "step: 10, loss: 0.06485123187303543\n",
            "step: 20, loss: 0.022376805543899536\n",
            "step: 30, loss: 0.019989600405097008\n",
            "step: 40, loss: 0.030318697914481163\n",
            "step: 50, loss: 0.05083683878183365\n",
            "step: 60, loss: 0.010921316221356392\n",
            "step: 70, loss: 0.016290972009301186\n",
            "step: 80, loss: 0.06810081005096436\n",
            "step: 90, loss: 0.023998968303203583\n",
            "step: 100, loss: 0.17576728761196136\n",
            "step: 110, loss: 0.031835075467824936\n",
            "step: 120, loss: 0.029723048210144043\n",
            "step: 130, loss: 0.0018795274663716555\n",
            "step: 140, loss: 0.015264054760336876\n",
            "step: 150, loss: 0.07465682923793793\n",
            "step: 160, loss: 0.04463962838053703\n",
            "step: 170, loss: 0.035091616213321686\n",
            "step: 180, loss: 0.012480552308261395\n",
            "step: 190, loss: 0.019205499440431595\n",
            "step: 200, loss: 0.20750173926353455\n",
            "step: 210, loss: 0.04304223507642746\n",
            "step: 220, loss: 0.0034228952135890722\n",
            "step: 230, loss: 0.03806084766983986\n",
            "step: 240, loss: 0.013469639234244823\n",
            "step: 250, loss: 0.0539759062230587\n",
            "step: 260, loss: 0.009317677468061447\n",
            "step: 270, loss: 0.05423774570226669\n",
            "step: 280, loss: 0.0889991968870163\n",
            "step: 290, loss: 0.01610414683818817\n",
            "step: 300, loss: 0.04930848628282547\n",
            "step: 310, loss: 0.034769393503665924\n",
            "step: 320, loss: 0.039739955216646194\n",
            "step: 330, loss: 0.0157471876591444\n",
            "step: 340, loss: 0.3224872052669525\n",
            "step: 350, loss: 0.0028161462396383286\n",
            "step: 360, loss: 0.07619332522153854\n",
            "step: 370, loss: 0.020131580531597137\n",
            "step: 380, loss: 0.12902800738811493\n",
            "step: 390, loss: 0.017865633592009544\n",
            "step: 400, loss: 0.07320193946361542\n",
            "step: 410, loss: 0.04803461208939552\n",
            "step: 420, loss: 0.09432290494441986\n",
            "step: 430, loss: 0.12849515676498413\n",
            "step: 440, loss: 0.016255797818303108\n",
            "step: 450, loss: 0.07359765470027924\n",
            "step: 460, loss: 0.034134168177843094\n",
            "step: 470, loss: 0.02348903939127922\n",
            "step: 480, loss: 0.019897034391760826\n",
            "step: 490, loss: 0.06281629204750061\n",
            "step: 500, loss: 0.005218866281211376\n",
            "step: 510, loss: 0.04502669349312782\n",
            "step: 520, loss: 0.3545747399330139\n",
            "step: 530, loss: 0.11252402514219284\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.945387792565397, f1=0.940045766590389, best_f1=0.940045766590389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13286632299423218\n",
            "step: 10, loss: 0.021036136895418167\n",
            "step: 20, loss: 0.005271764937788248\n",
            "step: 30, loss: 0.046679720282554626\n",
            "step: 40, loss: 0.02264568768441677\n",
            "step: 50, loss: 0.004098015837371349\n",
            "step: 60, loss: 0.007866010069847107\n",
            "step: 70, loss: 0.0028933004941791296\n",
            "step: 80, loss: 0.09491503238677979\n",
            "step: 90, loss: 0.011383970268070698\n",
            "step: 100, loss: 0.017856242135167122\n",
            "step: 110, loss: 0.0744975283741951\n",
            "step: 120, loss: 0.10477468371391296\n",
            "step: 130, loss: 0.07511209696531296\n",
            "step: 140, loss: 0.005986340809613466\n",
            "step: 150, loss: 0.025110263377428055\n",
            "step: 160, loss: 0.011649766005575657\n",
            "step: 170, loss: 0.03445901721715927\n",
            "step: 180, loss: 0.06447825580835342\n",
            "step: 190, loss: 0.0288309995085001\n",
            "step: 200, loss: 0.03712981939315796\n",
            "step: 210, loss: 0.006330684293061495\n",
            "step: 220, loss: 0.14269395172595978\n",
            "step: 230, loss: 0.03933484107255936\n",
            "step: 240, loss: 0.05436641350388527\n",
            "step: 250, loss: 0.051975067704916\n",
            "step: 260, loss: 0.009552464820444584\n",
            "step: 270, loss: 0.001095696585252881\n",
            "step: 280, loss: 0.07883300632238388\n",
            "step: 290, loss: 0.01242312602698803\n",
            "step: 300, loss: 0.08091383427381516\n",
            "step: 310, loss: 0.11877026408910751\n",
            "step: 320, loss: 0.027559446170926094\n",
            "step: 330, loss: 0.010814286768436432\n",
            "step: 340, loss: 0.007905038073658943\n",
            "step: 350, loss: 0.07953839004039764\n",
            "step: 360, loss: 0.013201374560594559\n",
            "step: 370, loss: 0.08290015161037445\n",
            "step: 380, loss: 0.0625743642449379\n",
            "step: 390, loss: 0.00530386483296752\n",
            "step: 400, loss: 0.1795840561389923\n",
            "step: 410, loss: 0.11029128730297089\n",
            "step: 420, loss: 0.009505809284746647\n",
            "step: 430, loss: 0.034392740577459335\n",
            "step: 440, loss: 0.12730289995670319\n",
            "step: 450, loss: 0.041595183312892914\n",
            "step: 460, loss: 0.08564400672912598\n",
            "step: 470, loss: 0.07376062870025635\n",
            "step: 480, loss: 0.08614934980869293\n",
            "step: 490, loss: 0.03138057887554169\n",
            "step: 500, loss: 0.027612466365098953\n",
            "step: 510, loss: 0.020161088556051254\n",
            "step: 520, loss: 0.11835591495037079\n",
            "step: 530, loss: 0.0006343710119836032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9171058936272161, f1=0.9103117505995204, best_f1=0.940045766590389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03359047695994377\n",
            "step: 10, loss: 0.02946394868195057\n",
            "step: 20, loss: 0.06286229938268661\n",
            "step: 30, loss: 0.021447593346238136\n",
            "step: 40, loss: 0.011991795152425766\n",
            "step: 50, loss: 0.040909405797719955\n",
            "step: 60, loss: 0.0020738327875733376\n",
            "step: 70, loss: 0.006451255176216364\n",
            "step: 80, loss: 0.19707857072353363\n",
            "step: 90, loss: 0.11332274973392487\n",
            "step: 100, loss: 0.0032006357796490192\n",
            "step: 110, loss: 0.07925282418727875\n",
            "step: 120, loss: 0.006335715297609568\n",
            "step: 130, loss: 0.02408979833126068\n",
            "step: 140, loss: 0.009111779741942883\n",
            "step: 150, loss: 0.0224230345338583\n",
            "step: 160, loss: 0.06643597781658173\n",
            "step: 170, loss: 0.01785655878484249\n",
            "step: 180, loss: 0.04790192097425461\n",
            "step: 190, loss: 0.013291759416460991\n",
            "step: 200, loss: 0.09718431532382965\n",
            "step: 210, loss: 0.0013410215033218265\n",
            "step: 220, loss: 0.0045060706324875355\n",
            "step: 230, loss: 0.004652773030102253\n",
            "step: 240, loss: 0.021170655265450478\n",
            "step: 250, loss: 0.09899384528398514\n",
            "step: 260, loss: 0.01007918082177639\n",
            "step: 270, loss: 0.14631766080856323\n",
            "step: 280, loss: 0.0077437409199774265\n",
            "step: 290, loss: 0.043924491852521896\n",
            "step: 300, loss: 0.003729134565219283\n",
            "step: 310, loss: 0.005497942678630352\n",
            "step: 320, loss: 0.028839675709605217\n",
            "step: 330, loss: 0.011832996271550655\n",
            "step: 340, loss: 0.025748729705810547\n",
            "step: 350, loss: 0.08173467218875885\n",
            "step: 360, loss: 0.07610231637954712\n",
            "step: 370, loss: 0.0010103541426360607\n",
            "step: 380, loss: 0.0018285701517015696\n",
            "step: 390, loss: 0.0003827608306892216\n",
            "step: 400, loss: 0.01530643180012703\n",
            "step: 410, loss: 0.05396891385316849\n",
            "step: 420, loss: 0.012416386976838112\n",
            "step: 430, loss: 0.05873722955584526\n",
            "step: 440, loss: 0.027591612190008163\n",
            "step: 450, loss: 0.02614790014922619\n",
            "step: 460, loss: 0.01663120836019516\n",
            "step: 470, loss: 0.000201158516574651\n",
            "step: 480, loss: 0.028284600004553795\n",
            "step: 490, loss: 0.014871789142489433\n",
            "step: 500, loss: 0.0753554180264473\n",
            "step: 510, loss: 0.16413117945194244\n",
            "step: 520, loss: 0.018794331699609756\n",
            "step: 530, loss: 0.1182691901922226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9493258949325895, f1=0.9389846297158826, best_f1=0.9389846297158826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012742032995447516\n",
            "step: 10, loss: 0.002428963780403137\n",
            "step: 20, loss: 0.0007411343394778669\n",
            "step: 30, loss: 0.03116689808666706\n",
            "step: 40, loss: 0.0008420202066190541\n",
            "step: 50, loss: 0.01961652934551239\n",
            "step: 60, loss: 0.01903395727276802\n",
            "step: 70, loss: 0.007721927482634783\n",
            "step: 80, loss: 0.00021950635709799826\n",
            "step: 90, loss: 0.04792984575033188\n",
            "step: 100, loss: 0.0018149006646126509\n",
            "step: 110, loss: 0.05102889984846115\n",
            "step: 120, loss: 0.02522006258368492\n",
            "step: 130, loss: 0.008450550027191639\n",
            "step: 140, loss: 0.015193958766758442\n",
            "step: 150, loss: 0.0010988936992362142\n",
            "step: 160, loss: 0.011728701181709766\n",
            "step: 170, loss: 0.07097039371728897\n",
            "step: 180, loss: 0.010517891496419907\n",
            "step: 190, loss: 0.004783803131431341\n",
            "step: 200, loss: 0.0015025314642116427\n",
            "step: 210, loss: 0.002335903001949191\n",
            "step: 220, loss: 0.0003187531838193536\n",
            "step: 230, loss: 0.0016635481733828783\n",
            "step: 240, loss: 0.003451660042628646\n",
            "step: 250, loss: 0.2541583776473999\n",
            "step: 260, loss: 0.004620358347892761\n",
            "step: 270, loss: 0.0017597691621631384\n",
            "step: 280, loss: 0.0026466932613402605\n",
            "step: 290, loss: 0.010382218286395073\n",
            "step: 300, loss: 0.006498419679701328\n",
            "step: 310, loss: 0.007942970842123032\n",
            "step: 320, loss: 0.007986573502421379\n",
            "step: 330, loss: 0.027338039129972458\n",
            "step: 340, loss: 0.00043381229625083506\n",
            "step: 350, loss: 0.00021743483375757933\n",
            "step: 360, loss: 0.00011790457210736349\n",
            "step: 370, loss: 0.0005832333117723465\n",
            "step: 380, loss: 0.00010575290070846677\n",
            "step: 390, loss: 0.000689418229740113\n",
            "step: 400, loss: 0.00046610485878773034\n",
            "step: 410, loss: 0.21178539097309113\n",
            "step: 420, loss: 0.05504967272281647\n",
            "step: 430, loss: 0.020293308421969414\n",
            "step: 440, loss: 0.00025272637140005827\n",
            "step: 450, loss: 0.022501662373542786\n",
            "step: 460, loss: 0.016251619905233383\n",
            "step: 470, loss: 0.0300800409168005\n",
            "step: 480, loss: 0.04442698135972023\n",
            "step: 490, loss: 0.008555860258638859\n",
            "step: 500, loss: 0.020220598205924034\n",
            "step: 510, loss: 0.0030984412878751755\n",
            "step: 520, loss: 0.09443891048431396\n",
            "step: 530, loss: 0.04454914852976799\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.946135831381733, f1=0.9366852886405959, best_f1=0.9389846297158826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012378288432955742\n",
            "step: 10, loss: 0.00033582249307073653\n",
            "step: 20, loss: 0.00044621567940339446\n",
            "step: 30, loss: 0.0006944040069356561\n",
            "step: 40, loss: 0.00028168881544843316\n",
            "step: 50, loss: 0.00013020010374020785\n",
            "step: 60, loss: 0.002224575961008668\n",
            "step: 70, loss: 0.0006728843436576426\n",
            "step: 80, loss: 0.00030954796238802373\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.00302052590996027\n",
            "step: 100, loss: 0.004413262475281954\n",
            "step: 110, loss: 0.010607964359223843\n",
            "step: 120, loss: 0.004564314614981413\n",
            "step: 130, loss: 0.008211871609091759\n",
            "step: 140, loss: 0.0005124518647789955\n",
            "step: 150, loss: 0.00041549309389665723\n",
            "step: 160, loss: 0.0005557386903092265\n",
            "step: 170, loss: 0.0006152383284643292\n",
            "step: 180, loss: 0.00030789183801971376\n",
            "step: 190, loss: 0.018026508390903473\n",
            "step: 200, loss: 0.013017880730330944\n",
            "step: 210, loss: 0.002529399236664176\n",
            "step: 220, loss: 0.0009187771938741207\n",
            "step: 230, loss: 0.010343356058001518\n",
            "step: 240, loss: 0.0036010381300002337\n",
            "step: 250, loss: 0.0199010968208313\n",
            "step: 260, loss: 0.00034361970028840005\n",
            "step: 270, loss: 0.0024585069622844458\n",
            "step: 280, loss: 0.01404501497745514\n",
            "step: 290, loss: 0.0012409951305016875\n",
            "step: 300, loss: 0.0019567168783396482\n",
            "step: 310, loss: 0.03300861269235611\n",
            "step: 320, loss: 0.08929108828306198\n",
            "step: 330, loss: 0.0006775101064704359\n",
            "step: 340, loss: 0.0005133011727593839\n",
            "step: 350, loss: 0.001238225377164781\n",
            "step: 360, loss: 0.12678059935569763\n",
            "step: 370, loss: 0.0024996446445584297\n",
            "step: 380, loss: 0.005308326333761215\n",
            "step: 390, loss: 0.005457165651023388\n",
            "step: 400, loss: 0.0033769418951123953\n",
            "step: 410, loss: 0.00023514588247053325\n",
            "step: 420, loss: 0.000825725554022938\n",
            "step: 430, loss: 0.04784563183784485\n",
            "step: 440, loss: 0.019069159403443336\n",
            "step: 450, loss: 0.0169255081564188\n",
            "step: 460, loss: 0.0011115088127553463\n",
            "step: 470, loss: 0.0013937588082626462\n",
            "step: 480, loss: 0.033165231347084045\n",
            "step: 490, loss: 0.011094223707914352\n",
            "step: 500, loss: 0.009856962598860264\n",
            "step: 510, loss: 0.02275203913450241\n",
            "step: 520, loss: 0.0018366053700447083\n",
            "step: 530, loss: 0.0054027908481657505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9489322191272052, f1=0.9282385834109972, best_f1=0.9389846297158826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02364286221563816\n",
            "step: 10, loss: 0.0012537947623059154\n",
            "step: 20, loss: 0.013326931744813919\n",
            "step: 30, loss: 0.005746346898376942\n",
            "step: 40, loss: 0.003037018235772848\n",
            "step: 50, loss: 0.0034999907948076725\n",
            "step: 60, loss: 0.013255074620246887\n",
            "step: 70, loss: 0.003875237889587879\n",
            "step: 80, loss: 0.00021355500211939216\n",
            "step: 90, loss: 0.0007093104650266469\n",
            "step: 100, loss: 0.00033450796036049724\n",
            "step: 110, loss: 0.00016909802798181772\n",
            "step: 120, loss: 0.0001925992692122236\n",
            "step: 130, loss: 6.409981870092452e-05\n",
            "step: 140, loss: 0.00012576766312122345\n",
            "step: 150, loss: 0.0003964120987802744\n",
            "step: 160, loss: 0.0001914207241497934\n",
            "step: 170, loss: 0.02457437478005886\n",
            "step: 180, loss: 0.05306071788072586\n",
            "step: 190, loss: 0.0078406548127532\n",
            "step: 200, loss: 0.0013947099214419723\n",
            "step: 210, loss: 0.0010821755276992917\n",
            "step: 220, loss: 7.665294833714142e-05\n",
            "step: 230, loss: 5.325497841113247e-05\n",
            "step: 240, loss: 0.0007383369375020266\n",
            "step: 250, loss: 0.0009319951059296727\n",
            "step: 260, loss: 0.0007277869153767824\n",
            "step: 270, loss: 0.00045324431266635656\n",
            "step: 280, loss: 0.0028129240963608027\n",
            "step: 290, loss: 0.019961830228567123\n",
            "step: 300, loss: 0.000522296701092273\n",
            "step: 310, loss: 0.0015573303680866957\n",
            "step: 320, loss: 0.004006346222013235\n",
            "step: 330, loss: 0.0001732376404106617\n",
            "step: 340, loss: 0.0011409217258915305\n",
            "step: 350, loss: 0.000482320407172665\n",
            "step: 360, loss: 0.0032405827660113573\n",
            "step: 370, loss: 0.000371068948879838\n",
            "step: 380, loss: 0.0003760383988264948\n",
            "step: 390, loss: 0.0003449520154390484\n",
            "step: 400, loss: 0.01152227632701397\n",
            "step: 410, loss: 0.017998071387410164\n",
            "step: 420, loss: 0.010868764482438564\n",
            "step: 430, loss: 0.00969739817082882\n",
            "step: 440, loss: 0.008055963553488255\n",
            "step: 450, loss: 0.0021054937969893217\n",
            "step: 460, loss: 0.00025737719261087477\n",
            "step: 470, loss: 0.12727707624435425\n",
            "step: 480, loss: 0.04502425342798233\n",
            "step: 490, loss: 0.005454534664750099\n",
            "step: 500, loss: 0.005036966875195503\n",
            "step: 510, loss: 0.0007516186451539397\n",
            "step: 520, loss: 0.00130177428945899\n",
            "step: 530, loss: 0.0004153829941060394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9480580252690688, f1=0.9345099860659545, best_f1=0.9389846297158826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000719383591786027\n",
            "step: 10, loss: 0.001111445832066238\n",
            "step: 20, loss: 0.0005145479226484895\n",
            "step: 30, loss: 0.00022879788593854755\n",
            "step: 40, loss: 0.0004138169751968235\n",
            "step: 50, loss: 0.00011804657697211951\n",
            "step: 60, loss: 0.00014096482482273132\n",
            "step: 70, loss: 0.00020961584232281893\n",
            "step: 80, loss: 0.0007571913301944733\n",
            "step: 90, loss: 0.0004878333129454404\n",
            "step: 100, loss: 0.00010170792666031048\n",
            "step: 110, loss: 0.0002680437173694372\n",
            "step: 120, loss: 0.00017596494581084698\n",
            "step: 130, loss: 0.000594042707234621\n",
            "step: 140, loss: 0.028394442051649094\n",
            "step: 150, loss: 9.882164158625528e-05\n",
            "step: 160, loss: 0.0005771723226644099\n",
            "step: 170, loss: 0.05886177346110344\n",
            "step: 180, loss: 0.0006831549108028412\n",
            "step: 190, loss: 0.05451106280088425\n",
            "step: 200, loss: 0.0024644858203828335\n",
            "step: 210, loss: 0.02610117569565773\n",
            "step: 220, loss: 0.0005867881118319929\n",
            "step: 230, loss: 0.05286649987101555\n",
            "step: 240, loss: 0.0004326918860897422\n",
            "step: 250, loss: 0.001568696927279234\n",
            "step: 260, loss: 4.635624645743519e-05\n",
            "step: 270, loss: 0.0008891703910194337\n",
            "step: 280, loss: 5.683256677002646e-05\n",
            "step: 290, loss: 0.00039420576649717987\n",
            "step: 300, loss: 4.868861651630141e-05\n",
            "step: 310, loss: 0.01667185313999653\n",
            "step: 320, loss: 0.015372291207313538\n",
            "step: 330, loss: 8.668813825352117e-05\n",
            "step: 340, loss: 0.030918069183826447\n",
            "step: 350, loss: 0.0005122103029862046\n",
            "step: 360, loss: 0.02135869860649109\n",
            "step: 370, loss: 0.00046935916179791093\n",
            "step: 380, loss: 7.199988613137975e-05\n",
            "step: 390, loss: 0.005224170163273811\n",
            "step: 400, loss: 0.012328331358730793\n",
            "step: 410, loss: 0.00037162890657782555\n",
            "step: 420, loss: 0.0003336295485496521\n",
            "step: 430, loss: 0.004995232447981834\n",
            "step: 440, loss: 0.0006729665910825133\n",
            "step: 450, loss: 0.0013991784071549773\n",
            "step: 460, loss: 0.0007602116675116122\n",
            "step: 470, loss: 0.0890517458319664\n",
            "step: 480, loss: 0.001935733132995665\n",
            "step: 490, loss: 0.001832183450460434\n",
            "step: 500, loss: 0.0021512589883059263\n",
            "step: 510, loss: 0.0004155548813287169\n",
            "step: 520, loss: 0.0002134311362169683\n",
            "step: 530, loss: 0.06730292737483978\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.946135831381733, f1=0.9313543599257884, best_f1=0.9389846297158826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.99349225719925e-05\n",
            "step: 10, loss: 0.003384467214345932\n",
            "step: 20, loss: 0.0003640954091679305\n",
            "step: 30, loss: 0.022027384489774704\n",
            "step: 40, loss: 0.0007071195286698639\n",
            "step: 50, loss: 0.0004872626159340143\n",
            "step: 60, loss: 0.044036976993083954\n",
            "step: 70, loss: 0.00039884159923531115\n",
            "step: 80, loss: 0.0035217730328440666\n",
            "step: 90, loss: 0.058980464935302734\n",
            "step: 100, loss: 0.00024146059877239168\n",
            "step: 110, loss: 0.017645955085754395\n",
            "step: 120, loss: 0.01122195366770029\n",
            "step: 130, loss: 0.00042916418169625103\n",
            "step: 140, loss: 0.04524214193224907\n",
            "step: 150, loss: 0.0024953153915703297\n",
            "step: 160, loss: 0.002722739474847913\n",
            "step: 170, loss: 0.004447183571755886\n",
            "step: 180, loss: 0.0005699030007235706\n",
            "step: 190, loss: 0.0008797499467618763\n",
            "step: 200, loss: 0.0002817089552991092\n",
            "step: 210, loss: 0.0004353294207248837\n",
            "step: 220, loss: 0.0002471976913511753\n",
            "step: 230, loss: 0.0003008613712154329\n",
            "step: 240, loss: 0.00014502019621431828\n",
            "step: 250, loss: 0.01972167380154133\n",
            "step: 260, loss: 8.071066986303777e-05\n",
            "step: 270, loss: 0.0007145727286115289\n",
            "step: 280, loss: 0.001318154507316649\n",
            "step: 290, loss: 0.00256825122050941\n",
            "step: 300, loss: 0.009756739251315594\n",
            "step: 310, loss: 0.07482026517391205\n",
            "step: 320, loss: 0.001100457040593028\n",
            "step: 330, loss: 0.0008170292130671442\n",
            "step: 340, loss: 0.030764590948820114\n",
            "step: 350, loss: 0.03370002284646034\n",
            "step: 360, loss: 0.0011907437583431602\n",
            "step: 370, loss: 0.0006864645984023809\n",
            "step: 380, loss: 0.0008852713508531451\n",
            "step: 390, loss: 0.00019388024520594627\n",
            "step: 400, loss: 0.07121815532445908\n",
            "step: 410, loss: 0.0004907197435386479\n",
            "step: 420, loss: 0.0021747625432908535\n",
            "step: 430, loss: 0.0029067066498100758\n",
            "step: 440, loss: 0.0034474933054298162\n",
            "step: 450, loss: 0.001085328753106296\n",
            "step: 460, loss: 0.00248893559910357\n",
            "step: 470, loss: 0.00030727911507710814\n",
            "step: 480, loss: 0.00023368133406620473\n",
            "step: 490, loss: 0.002837395528331399\n",
            "step: 500, loss: 0.0020042634569108486\n",
            "step: 510, loss: 0.07942042499780655\n",
            "step: 520, loss: 0.0050955661572515965\n",
            "step: 530, loss: 0.004820825532078743\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9507139567019807, f1=0.9393382352941178, best_f1=0.9393382352941178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002078496851027012\n",
            "step: 10, loss: 0.0016287385951727629\n",
            "step: 20, loss: 0.0028432002291083336\n",
            "step: 30, loss: 0.0013165846467018127\n",
            "step: 40, loss: 0.0002627279900480062\n",
            "step: 50, loss: 0.00015552231343463063\n",
            "step: 60, loss: 0.00027377717196941376\n",
            "step: 70, loss: 0.00010161295358557254\n",
            "step: 80, loss: 0.00016607315046712756\n",
            "step: 90, loss: 0.00027528207283467054\n",
            "step: 100, loss: 0.0004273467347957194\n",
            "step: 110, loss: 0.06555613875389099\n",
            "step: 120, loss: 0.00032432383159175515\n",
            "step: 130, loss: 5.75586236664094e-05\n",
            "step: 140, loss: 9.942797623807564e-05\n",
            "step: 150, loss: 8.366393740288913e-05\n",
            "step: 160, loss: 0.010257037356495857\n",
            "step: 170, loss: 0.00015636299212928861\n",
            "step: 180, loss: 0.007361717522144318\n",
            "step: 190, loss: 0.0002685925574041903\n",
            "step: 200, loss: 0.026290684938430786\n",
            "step: 210, loss: 0.0005974387167952955\n",
            "step: 220, loss: 0.0004261165449861437\n",
            "step: 230, loss: 0.00011873942275997251\n",
            "step: 240, loss: 0.026001915335655212\n",
            "step: 250, loss: 0.00013411729014478624\n",
            "step: 260, loss: 0.0005076061352156103\n",
            "step: 270, loss: 0.00016831133689265698\n",
            "step: 280, loss: 0.0004130890010856092\n",
            "step: 290, loss: 9.534809942124411e-05\n",
            "step: 300, loss: 0.0005956512759439647\n",
            "step: 310, loss: 0.04515087604522705\n",
            "step: 320, loss: 0.0005667465156875551\n",
            "step: 330, loss: 0.00016088529082480818\n",
            "step: 340, loss: 0.00012835106463171542\n",
            "step: 350, loss: 0.01476207934319973\n",
            "step: 360, loss: 7.130565063562244e-05\n",
            "step: 370, loss: 0.00011892869952134788\n",
            "step: 380, loss: 0.0013288382906466722\n",
            "step: 390, loss: 7.815597200533375e-05\n",
            "step: 400, loss: 8.470557077089325e-05\n",
            "step: 410, loss: 7.426032971125096e-05\n",
            "step: 420, loss: 7.495099271181971e-05\n",
            "step: 430, loss: 0.00012859527487307787\n",
            "step: 440, loss: 6.671746814390644e-05\n",
            "step: 450, loss: 8.037292718654498e-05\n",
            "step: 460, loss: 7.380392344202846e-05\n",
            "step: 470, loss: 0.00348391174338758\n",
            "step: 480, loss: 0.00032199989072978497\n",
            "step: 490, loss: 0.00113195541780442\n",
            "step: 500, loss: 0.00016940620844252408\n",
            "step: 510, loss: 8.528976468369365e-05\n",
            "step: 520, loss: 0.0020911958999931812\n",
            "step: 530, loss: 0.02310180477797985\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9478584729981377, f1=0.9385474860335196, best_f1=0.9393382352941178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.427281863987446e-05\n",
            "step: 10, loss: 0.00019317913393024355\n",
            "step: 20, loss: 0.0002714560541789979\n",
            "step: 30, loss: 8.975001401267946e-05\n",
            "step: 40, loss: 4.819026435143314e-05\n",
            "step: 50, loss: 7.279706187546253e-05\n",
            "step: 60, loss: 0.002001057378947735\n",
            "step: 70, loss: 0.0006370561895892024\n",
            "step: 80, loss: 0.0013881586492061615\n",
            "step: 90, loss: 0.0013227590825408697\n",
            "step: 100, loss: 4.989479930372909e-05\n",
            "step: 110, loss: 0.00024061260046437383\n",
            "step: 120, loss: 0.00026660694857127964\n",
            "step: 130, loss: 0.00014726493100170046\n",
            "step: 140, loss: 0.00013828124792780727\n",
            "step: 150, loss: 0.0002049892646027729\n",
            "step: 160, loss: 0.002390161156654358\n",
            "step: 170, loss: 0.00021986589126754552\n",
            "step: 180, loss: 0.0036795083433389664\n",
            "step: 190, loss: 0.0006850940990261734\n",
            "step: 200, loss: 0.0001277425471926108\n",
            "step: 210, loss: 0.00020645701442845166\n",
            "step: 220, loss: 0.0019502323120832443\n",
            "step: 230, loss: 5.174801481189206e-05\n",
            "step: 240, loss: 0.0021937438286840916\n",
            "step: 250, loss: 0.00014462783292401582\n",
            "step: 260, loss: 0.003253370989114046\n",
            "step: 270, loss: 0.007317772600799799\n",
            "step: 280, loss: 0.0002235369465779513\n",
            "step: 290, loss: 0.00018187062232755125\n",
            "step: 300, loss: 0.00013526109978556633\n",
            "step: 310, loss: 0.019326163455843925\n",
            "step: 320, loss: 0.0006501174648292363\n",
            "step: 330, loss: 7.270820788107812e-05\n",
            "step: 340, loss: 0.031086456030607224\n",
            "step: 350, loss: 0.00013255765952635556\n",
            "step: 360, loss: 0.003987668082118034\n",
            "step: 370, loss: 0.0007082860101945698\n",
            "step: 380, loss: 0.00010662278509698808\n",
            "step: 390, loss: 0.0008805729448795319\n",
            "step: 400, loss: 9.22640465432778e-05\n",
            "step: 410, loss: 0.0012554700952023268\n",
            "step: 420, loss: 0.0010459921322762966\n",
            "step: 430, loss: 0.0014280856121331453\n",
            "step: 440, loss: 0.00014079218090046197\n",
            "step: 450, loss: 0.0004957271157763898\n",
            "step: 460, loss: 0.0001087643249775283\n",
            "step: 470, loss: 0.0005671812687069178\n",
            "step: 480, loss: 0.00021496314730029553\n",
            "step: 490, loss: 0.00024485596804879606\n",
            "step: 500, loss: 0.0008221847820095718\n",
            "step: 510, loss: 4.9709204176906496e-05\n",
            "step: 520, loss: 6.993956776568666e-05\n",
            "step: 530, loss: 0.0032598599791526794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9522058823529412, f1=0.9338201734367868, best_f1=0.9338201734367868\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002923830761574209\n",
            "step: 10, loss: 4.494113818509504e-05\n",
            "step: 20, loss: 0.00047912204172462225\n",
            "step: 30, loss: 8.625042391940951e-05\n",
            "step: 40, loss: 0.00015670373977627605\n",
            "step: 50, loss: 3.644884782261215e-05\n",
            "step: 60, loss: 0.00044365739449858665\n",
            "step: 70, loss: 0.00042751830187626183\n",
            "step: 80, loss: 5.405253614298999e-05\n",
            "step: 90, loss: 0.010348938405513763\n",
            "step: 100, loss: 0.241901695728302\n",
            "step: 110, loss: 0.0001872439170256257\n",
            "step: 120, loss: 0.00018530247325543314\n",
            "step: 130, loss: 0.0006155224400572479\n",
            "step: 140, loss: 0.0002384747058385983\n",
            "step: 150, loss: 0.00011408785940147936\n",
            "step: 160, loss: 0.056557029485702515\n",
            "step: 170, loss: 0.00019943079678341746\n",
            "step: 180, loss: 0.00018467854533810169\n",
            "step: 190, loss: 0.0002072154893539846\n",
            "step: 200, loss: 0.004901987034827471\n",
            "step: 210, loss: 0.00021012268553022295\n",
            "step: 220, loss: 0.00020551065972540528\n",
            "step: 230, loss: 0.00035584671422839165\n",
            "step: 240, loss: 0.0011565437307581306\n",
            "step: 250, loss: 0.0001290489744860679\n",
            "step: 260, loss: 0.00011652115790639073\n",
            "step: 270, loss: 0.0018573951674625278\n",
            "step: 280, loss: 0.00040966051165014505\n",
            "step: 290, loss: 9.265559492632747e-05\n",
            "step: 300, loss: 0.001354669569991529\n",
            "step: 310, loss: 8.915713260648772e-05\n",
            "step: 320, loss: 0.016642937436699867\n",
            "step: 330, loss: 0.0018486991757526994\n",
            "step: 340, loss: 7.095919136190787e-05\n",
            "step: 350, loss: 0.0002800476213451475\n",
            "step: 360, loss: 0.0010408245725557208\n",
            "step: 370, loss: 2.880879765143618e-05\n",
            "step: 380, loss: 0.00015964070917107165\n",
            "step: 390, loss: 0.001539001241326332\n",
            "step: 400, loss: 0.05587216094136238\n",
            "step: 410, loss: 0.00010321143781766295\n",
            "step: 420, loss: 5.738630352425389e-05\n",
            "step: 430, loss: 3.903930701198988e-05\n",
            "step: 440, loss: 9.973069245461375e-05\n",
            "step: 450, loss: 8.751015411689878e-05\n",
            "step: 460, loss: 4.579768210533075e-05\n",
            "step: 470, loss: 0.0012372260680422187\n",
            "step: 480, loss: 8.119390258798376e-05\n",
            "step: 490, loss: 6.056006168364547e-05\n",
            "step: 500, loss: 0.00013393483823165298\n",
            "step: 510, loss: 9.621229401091114e-05\n",
            "step: 520, loss: 0.00020959821995347738\n",
            "step: 530, loss: 0.0036693792790174484\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.951846657316503, f1=0.9402501157943491, best_f1=0.9338201734367868\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.875498704379424e-05\n",
            "step: 10, loss: 8.232596883317456e-05\n",
            "step: 20, loss: 0.00044085472472943366\n",
            "step: 30, loss: 2.9450224246829748e-05\n",
            "step: 40, loss: 5.02353796036914e-05\n",
            "step: 50, loss: 0.0005344478995539248\n",
            "step: 60, loss: 3.729452146217227e-05\n",
            "step: 70, loss: 6.807444151490927e-05\n",
            "step: 80, loss: 0.00034736329689621925\n",
            "step: 90, loss: 5.52220044482965e-05\n",
            "step: 100, loss: 6.393383227987215e-05\n",
            "step: 110, loss: 2.1695104805985466e-05\n",
            "step: 120, loss: 3.13763739541173e-05\n",
            "step: 130, loss: 6.855648825876415e-05\n",
            "step: 140, loss: 4.8119210987351835e-05\n",
            "step: 150, loss: 0.00010356508573750034\n",
            "step: 160, loss: 0.0007380509050562978\n",
            "step: 170, loss: 5.196618803893216e-05\n",
            "step: 180, loss: 3.8401525671361014e-05\n",
            "step: 190, loss: 0.0004358281148597598\n",
            "step: 200, loss: 0.09082396328449249\n",
            "step: 210, loss: 4.245017044013366e-05\n",
            "step: 220, loss: 2.9551189072662964e-05\n",
            "step: 230, loss: 0.00018133378762286156\n",
            "step: 240, loss: 0.0006861196598038077\n",
            "step: 250, loss: 8.338066982105374e-05\n",
            "step: 260, loss: 4.00334938603919e-05\n",
            "step: 270, loss: 0.000578688457608223\n",
            "step: 280, loss: 4.836656080442481e-05\n",
            "step: 290, loss: 3.181404827046208e-05\n",
            "step: 300, loss: 3.6047465982846916e-05\n",
            "step: 310, loss: 6.635452882619575e-05\n",
            "step: 320, loss: 3.5931181628257036e-05\n",
            "step: 330, loss: 4.7833756980253384e-05\n",
            "step: 340, loss: 6.0905906138941646e-05\n",
            "step: 350, loss: 3.140329499728978e-05\n",
            "step: 360, loss: 0.14318138360977173\n",
            "step: 370, loss: 4.511345105129294e-05\n",
            "step: 380, loss: 0.00013572172611020505\n",
            "step: 390, loss: 0.00016562595556024462\n",
            "step: 400, loss: 0.0002329127019038424\n",
            "step: 410, loss: 8.907736628316343e-05\n",
            "step: 420, loss: 8.653708209749311e-05\n",
            "step: 430, loss: 0.0001451534335501492\n",
            "step: 440, loss: 0.00046537129674106836\n",
            "step: 450, loss: 0.00010194064088864252\n",
            "step: 460, loss: 0.00029838437330909073\n",
            "step: 470, loss: 0.0017537453677505255\n",
            "step: 480, loss: 2.6722387701738626e-05\n",
            "step: 490, loss: 4.7980462113628164e-05\n",
            "step: 500, loss: 3.313714842079207e-05\n",
            "step: 510, loss: 9.563873027218506e-05\n",
            "step: 520, loss: 4.5469842007150874e-05\n",
            "step: 530, loss: 5.9904235968133435e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9503940658321743, f1=0.9348623853211009, best_f1=0.9338201734367868\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013646339066326618\n",
            "step: 10, loss: 0.004650454968214035\n",
            "step: 20, loss: 9.433754894416779e-05\n",
            "step: 30, loss: 6.524235504912212e-05\n",
            "step: 40, loss: 0.0006133003043942153\n",
            "step: 50, loss: 9.669832797953859e-05\n",
            "step: 60, loss: 8.117278048302978e-05\n",
            "step: 70, loss: 0.00010127606219612062\n",
            "step: 80, loss: 0.0012954027624800801\n",
            "step: 90, loss: 4.27869672421366e-05\n",
            "step: 100, loss: 9.00767045095563e-05\n",
            "step: 110, loss: 9.41652397159487e-05\n",
            "step: 120, loss: 8.949008770287037e-05\n",
            "step: 130, loss: 0.00013207494339440018\n",
            "step: 140, loss: 8.936157246353105e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 150, loss: 0.0007677260437048972\n",
            "step: 160, loss: 5.5426284234272316e-05\n",
            "step: 170, loss: 6.527562800329179e-05\n",
            "step: 180, loss: 5.64095753361471e-05\n",
            "step: 190, loss: 8.083778811851516e-05\n",
            "step: 200, loss: 7.730431389063597e-05\n",
            "step: 210, loss: 0.00036188701051287353\n",
            "step: 220, loss: 3.085798380197957e-05\n",
            "step: 230, loss: 5.107977631269023e-05\n",
            "step: 240, loss: 0.00032304503838531673\n",
            "step: 250, loss: 0.0002855208585970104\n",
            "step: 260, loss: 5.863184196641669e-05\n",
            "step: 270, loss: 0.0001832716225180775\n",
            "step: 280, loss: 0.00014959636610001326\n",
            "step: 290, loss: 0.00010624215065035969\n",
            "step: 300, loss: 0.00015234516467899084\n",
            "step: 310, loss: 0.11716316640377045\n",
            "step: 320, loss: 0.00013478480104822665\n",
            "step: 330, loss: 0.0008169678039848804\n",
            "step: 340, loss: 0.000839850923512131\n",
            "step: 350, loss: 8.600085857324302e-05\n",
            "step: 360, loss: 0.00011243332119192928\n",
            "step: 370, loss: 0.0006351217161864042\n",
            "step: 380, loss: 0.007598604075610638\n",
            "step: 390, loss: 0.00012664451787713915\n",
            "step: 400, loss: 0.005855197086930275\n",
            "step: 410, loss: 4.820611502509564e-05\n",
            "step: 420, loss: 9.628627594793215e-05\n",
            "step: 430, loss: 0.00010334038233850151\n",
            "step: 440, loss: 0.00011488874588394538\n",
            "step: 450, loss: 6.346838199533522e-05\n",
            "step: 460, loss: 0.00033271132269874215\n",
            "step: 470, loss: 8.372389856958762e-05\n",
            "step: 480, loss: 7.0676498580724e-05\n",
            "step: 490, loss: 0.00010229738109046593\n",
            "step: 500, loss: 0.0008096711244434118\n",
            "step: 510, loss: 0.000824293470941484\n",
            "step: 520, loss: 0.00048016716027632356\n",
            "step: 530, loss: 0.0027499289717525244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9523368810735771, f1=0.9378166743436204, best_f1=0.9378166743436204\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.987274875631556e-05\n",
            "step: 10, loss: 0.0011759274639189243\n",
            "step: 20, loss: 0.0006226691766642034\n",
            "step: 30, loss: 0.00012735901691485196\n",
            "step: 40, loss: 6.702105747535825e-05\n",
            "step: 50, loss: 0.0017370772548019886\n",
            "step: 60, loss: 9.567289089318365e-05\n",
            "step: 70, loss: 5.949223123025149e-05\n",
            "step: 80, loss: 5.0091304728994146e-05\n",
            "step: 90, loss: 6.294681952567771e-05\n",
            "step: 100, loss: 3.04470413539093e-05\n",
            "step: 110, loss: 0.0012027695775032043\n",
            "step: 120, loss: 4.730835644295439e-05\n",
            "step: 130, loss: 4.798820737050846e-05\n",
            "step: 140, loss: 4.3066407670266926e-05\n",
            "step: 150, loss: 7.971705053932965e-05\n",
            "step: 160, loss: 7.111068407539278e-05\n",
            "step: 170, loss: 2.9463617465808056e-05\n",
            "step: 180, loss: 7.5070362072438e-05\n",
            "step: 190, loss: 0.0016639773966744542\n",
            "step: 200, loss: 0.0007522067753598094\n",
            "step: 210, loss: 2.4771357857389376e-05\n",
            "step: 220, loss: 5.834191324538551e-05\n",
            "step: 230, loss: 0.0038220915012061596\n",
            "step: 240, loss: 5.399970541475341e-05\n",
            "step: 250, loss: 0.00011421630915720016\n",
            "step: 260, loss: 3.315576759632677e-05\n",
            "step: 270, loss: 5.437595973489806e-05\n",
            "step: 280, loss: 4.734229878522456e-05\n",
            "step: 290, loss: 5.822563616675325e-05\n",
            "step: 300, loss: 0.00015303082182072103\n",
            "step: 310, loss: 0.0001018872790155001\n",
            "step: 320, loss: 7.650892075616866e-05\n",
            "step: 330, loss: 6.978552119107917e-05\n",
            "step: 340, loss: 7.36522997613065e-05\n",
            "step: 350, loss: 0.0011226864298805594\n",
            "step: 360, loss: 8.253263513324782e-05\n",
            "step: 370, loss: 0.00010165785352000967\n",
            "step: 380, loss: 7.272957736859098e-05\n",
            "step: 390, loss: 4.378282756078988e-05\n",
            "step: 400, loss: 0.0014413567259907722\n",
            "step: 410, loss: 7.13135814294219e-05\n",
            "step: 420, loss: 8.99264050531201e-05\n",
            "step: 430, loss: 2.278585816384293e-05\n",
            "step: 440, loss: 9.958365262718871e-05\n",
            "step: 450, loss: 0.0041175405494868755\n",
            "step: 460, loss: 0.00014048667799215764\n",
            "step: 470, loss: 5.712547499570064e-05\n",
            "step: 480, loss: 0.0005371120641939342\n",
            "step: 490, loss: 5.889406020287424e-05\n",
            "step: 500, loss: 0.0002714828005991876\n",
            "step: 510, loss: 6.148598913569003e-05\n",
            "step: 520, loss: 8.31475990707986e-05\n",
            "step: 530, loss: 8.44916285132058e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9522041763341067, f1=0.9372114496768237, best_f1=0.9378166743436204\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:35, 160.04it/s]\n",
            "load_f1 = 0.9503019043195542\n",
            "real_f1 = 0.9466357308584686\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 135.07it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnXzXaaYhstq"
      },
      "source": [
        "# DITTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r23AxFPnhstr"
      },
      "source": [
        "## DITTO STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db04d36-0ed2-401d-a97e-e110ab5ee022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.43207401037216187\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.2692307692307693, f1=0.2745098039215686, best_f1=0.2745098039215686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.48630404472351074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.29213483146067415, f1=0.23728813559322035, best_f1=0.23728813559322035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.41819339990615845\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.2916666666666667, f1=0.29508196721311475, best_f1=0.23728813559322035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29861390590667725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.3058823529411765, f1=0.18181818181818182, best_f1=0.18181818181818182\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3425007462501526\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.38095238095238093, f1=0.23076923076923075, best_f1=0.23076923076923075\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18534091114997864\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.43999999999999995, f1=0.6428571428571429, best_f1=0.6428571428571429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3248424530029297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.48, f1=0.5945945945945946, best_f1=0.5945945945945946\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17480316758155823\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.6470588235294117, f1=0.5517241379310344, best_f1=0.5517241379310344\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22203126549720764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6451612903225806, f1=0.5925925925925927, best_f1=0.5517241379310344\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22670526802539825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.7857142857142857, f1=0.6153846153846153, best_f1=0.6153846153846153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1504192054271698\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.8125000000000001, f1=0.6875000000000001, best_f1=0.6875000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10314523428678513\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.8275862068965518, f1=0.689655172413793, best_f1=0.689655172413793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.034026551991701126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8571428571428571, f1=0.689655172413793, best_f1=0.689655172413793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04310120269656181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8571428571428571, f1=0.689655172413793, best_f1=0.689655172413793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03754141554236412\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8571428571428571, f1=0.689655172413793, best_f1=0.689655172413793\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 119874.89it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.631578947368421\n",
            "real_f1 = 0.6190476190476191\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 135.47it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bang43Lhsts"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa505403-70de-4598-e076-7048aa50195b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 398kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 745kB/s] \n",
            "Downloading: 100% 456k/456k [00:00<00:00, 504kB/s]\n",
            "Downloading: 100% 501M/501M [00:07<00:00, 68.2MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5694915652275085\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.4613169729709625\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.507539689540863\n",
            "step: 30, loss: 0.21069364249706268\n",
            "step: 40, loss: 0.10192585736513138\n",
            "step: 50, loss: 0.12339179217815399\n",
            "step: 60, loss: 0.1507759690284729\n",
            "step: 70, loss: 0.03984959051012993\n",
            "step: 80, loss: 0.009184087626636028\n",
            "step: 90, loss: 0.1408015489578247\n",
            "step: 100, loss: 0.13787153363227844\n",
            "step: 110, loss: 0.03728071227669716\n",
            "step: 120, loss: 0.04943274334073067\n",
            "step: 130, loss: 0.0033714494202286005\n",
            "step: 140, loss: 0.11942315846681595\n",
            "step: 150, loss: 0.049290772527456284\n",
            "step: 160, loss: 0.0023574314545840025\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 170, loss: 0.15831008553504944\n",
            "step: 180, loss: 0.011704863049089909\n",
            "step: 190, loss: 0.17506876587867737\n",
            "step: 200, loss: 0.013803992420434952\n",
            "step: 210, loss: 0.15868806838989258\n",
            "step: 220, loss: 0.057718031108379364\n",
            "step: 230, loss: 0.0022640316747128963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9743589743589743, f1=0.979591836734694, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022502713836729527\n",
            "step: 10, loss: 0.15093767642974854\n",
            "step: 20, loss: 0.00595100549980998\n",
            "step: 30, loss: 0.07108789682388306\n",
            "step: 40, loss: 0.05230899900197983\n",
            "step: 50, loss: 0.009586113505065441\n",
            "step: 60, loss: 0.01238249707967043\n",
            "step: 70, loss: 0.010657447390258312\n",
            "step: 80, loss: 0.002654158743098378\n",
            "step: 90, loss: 0.01016421988606453\n",
            "step: 100, loss: 0.002609098795801401\n",
            "step: 110, loss: 0.002627011388540268\n",
            "step: 120, loss: 0.07292088121175766\n",
            "step: 130, loss: 0.006501822266727686\n",
            "step: 140, loss: 0.0020083419512957335\n",
            "step: 150, loss: 0.054591692984104156\n",
            "step: 160, loss: 0.0019229196477681398\n",
            "step: 170, loss: 0.0007623211131431162\n",
            "step: 180, loss: 0.009562785737216473\n",
            "step: 190, loss: 0.0017743132775649428\n",
            "step: 200, loss: 0.007869131863117218\n",
            "step: 210, loss: 0.02951963059604168\n",
            "step: 220, loss: 0.000314743141643703\n",
            "step: 230, loss: 0.0015910138608887792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9672316384180792, f1=0.9705215419501134, best_f1=0.979591836734694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017738018650561571\n",
            "step: 10, loss: 0.1445198655128479\n",
            "step: 20, loss: 0.02079848013818264\n",
            "step: 30, loss: 0.00152308598626405\n",
            "step: 40, loss: 0.013963916338980198\n",
            "step: 50, loss: 0.0908055305480957\n",
            "step: 60, loss: 0.0030108068604022264\n",
            "step: 70, loss: 0.0017682993784546852\n",
            "step: 80, loss: 0.0455356128513813\n",
            "step: 90, loss: 0.0007067496189847589\n",
            "step: 100, loss: 0.0013472149148583412\n",
            "step: 110, loss: 0.0012834100052714348\n",
            "step: 120, loss: 0.003845105180516839\n",
            "step: 130, loss: 0.00270848092623055\n",
            "step: 140, loss: 0.002570709679275751\n",
            "step: 150, loss: 0.009603319689631462\n",
            "step: 160, loss: 0.0016790678491815925\n",
            "step: 170, loss: 0.0010168240405619144\n",
            "step: 180, loss: 0.00691266031935811\n",
            "step: 190, loss: 0.01516785193234682\n",
            "step: 200, loss: 0.041997842490673065\n",
            "step: 210, loss: 0.0018605379154905677\n",
            "step: 220, loss: 0.00804523378610611\n",
            "step: 230, loss: 0.01937943696975708\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.983050847457627, f1=0.9782857142857143, best_f1=0.9782857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006432270165532827\n",
            "step: 10, loss: 0.014370436780154705\n",
            "step: 20, loss: 0.03250017762184143\n",
            "step: 30, loss: 0.0005452783079817891\n",
            "step: 40, loss: 0.0006902352324686944\n",
            "step: 50, loss: 0.03703226149082184\n",
            "step: 60, loss: 0.002944356994703412\n",
            "step: 70, loss: 0.001936351298354566\n",
            "step: 80, loss: 0.0002704510116018355\n",
            "step: 90, loss: 0.005270557478070259\n",
            "step: 100, loss: 0.002450656145811081\n",
            "step: 110, loss: 0.0010409998940303922\n",
            "step: 120, loss: 0.0007167558069340885\n",
            "step: 130, loss: 0.004830727819353342\n",
            "step: 140, loss: 0.003644038923084736\n",
            "step: 150, loss: 0.024808522313833237\n",
            "step: 160, loss: 0.0028994339518249035\n",
            "step: 170, loss: 0.011192851699888706\n",
            "step: 180, loss: 0.19180767238140106\n",
            "step: 190, loss: 0.016708463430404663\n",
            "step: 200, loss: 0.01250758022069931\n",
            "step: 210, loss: 0.0009586401283740997\n",
            "step: 220, loss: 0.00721599068492651\n",
            "step: 230, loss: 0.006919607520103455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9828962371721778, f1=0.9759450171821306, best_f1=0.9782857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004664764739573002\n",
            "step: 10, loss: 0.0026731400284916162\n",
            "step: 20, loss: 0.0016041317721828818\n",
            "step: 30, loss: 0.0004350129747763276\n",
            "step: 40, loss: 0.0008097626268863678\n",
            "step: 50, loss: 0.0008459163946099579\n",
            "step: 60, loss: 0.007708555553108454\n",
            "step: 70, loss: 0.0015971404500305653\n",
            "step: 80, loss: 0.06798835843801498\n",
            "step: 90, loss: 0.0678967610001564\n",
            "step: 100, loss: 0.0004653066862374544\n",
            "step: 110, loss: 0.001554856076836586\n",
            "step: 120, loss: 0.00032402490614913404\n",
            "step: 130, loss: 0.01663735881447792\n",
            "step: 140, loss: 0.00039615685818716884\n",
            "step: 150, loss: 0.0313178151845932\n",
            "step: 160, loss: 0.0011719001922756433\n",
            "step: 170, loss: 0.0007589748711325228\n",
            "step: 180, loss: 0.0005692386766895652\n",
            "step: 190, loss: 0.0005768210394307971\n",
            "step: 200, loss: 0.04995523393154144\n",
            "step: 210, loss: 0.051226209849119186\n",
            "step: 220, loss: 0.0006408441695384681\n",
            "step: 230, loss: 0.004609519615769386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9853438556933484, f1=0.9852440408626559, best_f1=0.9852440408626559\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003009194042533636\n",
            "step: 10, loss: 0.0011953309876844287\n",
            "step: 20, loss: 0.0014507743762806058\n",
            "step: 30, loss: 0.0007055288879200816\n",
            "step: 40, loss: 0.000321993138641119\n",
            "step: 50, loss: 0.0029765008948743343\n",
            "step: 60, loss: 0.0075348783284425735\n",
            "step: 70, loss: 0.0032271852251142263\n",
            "step: 80, loss: 0.0009091655374504626\n",
            "step: 90, loss: 0.00588660454377532\n",
            "step: 100, loss: 0.0032878268975764513\n",
            "step: 110, loss: 0.0138554647564888\n",
            "step: 120, loss: 0.0003563401405699551\n",
            "step: 130, loss: 0.003484228625893593\n",
            "step: 140, loss: 0.0006283211405389011\n",
            "step: 150, loss: 0.0005776354810222983\n",
            "step: 160, loss: 0.0287152249366045\n",
            "step: 170, loss: 0.0027523718308657408\n",
            "step: 180, loss: 9.067869541468099e-05\n",
            "step: 190, loss: 0.00026656800764612854\n",
            "step: 200, loss: 0.020388148725032806\n",
            "step: 210, loss: 0.02328602410852909\n",
            "step: 220, loss: 0.03387392312288284\n",
            "step: 230, loss: 0.0034003991167992353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9853768278965129, f1=0.9841628959276018, best_f1=0.9841628959276018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016626982251182199\n",
            "step: 10, loss: 0.0004479703784454614\n",
            "step: 20, loss: 0.0008143963059410453\n",
            "step: 30, loss: 0.001710855751298368\n",
            "step: 40, loss: 0.020338337868452072\n",
            "step: 50, loss: 0.004768391605466604\n",
            "step: 60, loss: 0.0012602602364495397\n",
            "step: 70, loss: 0.0019362817984074354\n",
            "step: 80, loss: 0.0011020818492397666\n",
            "step: 90, loss: 0.020069967955350876\n",
            "step: 100, loss: 0.00023465807316824794\n",
            "step: 110, loss: 0.0010817135917022824\n",
            "step: 120, loss: 0.028427261859178543\n",
            "step: 130, loss: 0.0046137250028550625\n",
            "step: 140, loss: 0.0002380896476097405\n",
            "step: 150, loss: 0.04187675192952156\n",
            "step: 160, loss: 0.0001810515095712617\n",
            "step: 170, loss: 0.028932882472872734\n",
            "step: 180, loss: 0.0011870766757056117\n",
            "step: 190, loss: 0.007633597124367952\n",
            "step: 200, loss: 0.009779025800526142\n",
            "step: 210, loss: 0.10584932565689087\n",
            "step: 220, loss: 0.00023663636238779873\n",
            "step: 230, loss: 0.0005730170523747802\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.984090909090909, f1=0.9807474518686297, best_f1=0.9841628959276018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005363897071219981\n",
            "step: 10, loss: 0.01955900900065899\n",
            "step: 20, loss: 0.0004288141499273479\n",
            "step: 30, loss: 0.00035757457953877747\n",
            "step: 40, loss: 0.0028571265283972025\n",
            "step: 50, loss: 0.001139925909228623\n",
            "step: 60, loss: 0.00034536735620349646\n",
            "step: 70, loss: 0.00016576026973780245\n",
            "step: 80, loss: 0.020123867318034172\n",
            "step: 90, loss: 0.001693632686510682\n",
            "step: 100, loss: 0.00034452060936018825\n",
            "step: 110, loss: 0.004175763111561537\n",
            "step: 120, loss: 0.0006478888099081814\n",
            "step: 130, loss: 0.0017193349776789546\n",
            "step: 140, loss: 0.0005979957059025764\n",
            "step: 150, loss: 0.08098382502794266\n",
            "step: 160, loss: 0.007891025394201279\n",
            "step: 170, loss: 0.011208636686205864\n",
            "step: 180, loss: 0.00029945725691504776\n",
            "step: 190, loss: 0.0009459995198994875\n",
            "step: 200, loss: 0.0024857677053660154\n",
            "step: 210, loss: 0.005779838189482689\n",
            "step: 220, loss: 0.00022472592536360025\n",
            "step: 230, loss: 8.504246216034517e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9842696629213483, f1=0.9807909604519773, best_f1=0.9841628959276018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001998286898015067\n",
            "step: 10, loss: 0.0012532470282167196\n",
            "step: 20, loss: 0.0001818884047679603\n",
            "step: 30, loss: 0.0012933035613968968\n",
            "step: 40, loss: 0.0025668616872280836\n",
            "step: 50, loss: 0.011644613929092884\n",
            "step: 60, loss: 0.0014631320955231786\n",
            "step: 70, loss: 0.035567834973335266\n",
            "step: 80, loss: 0.00869214441627264\n",
            "step: 90, loss: 0.03323604539036751\n",
            "step: 100, loss: 0.00018975819693878293\n",
            "step: 110, loss: 0.00015430210623890162\n",
            "step: 120, loss: 0.03501712903380394\n",
            "step: 130, loss: 0.0002469326718710363\n",
            "step: 140, loss: 0.00492213387042284\n",
            "step: 150, loss: 0.00021205018856562674\n",
            "step: 160, loss: 0.00043325175647623837\n",
            "step: 170, loss: 0.00017511684563942254\n",
            "step: 180, loss: 0.0001175965735455975\n",
            "step: 190, loss: 0.0004888399271294475\n",
            "step: 200, loss: 0.000900097016710788\n",
            "step: 210, loss: 0.0049918945878744125\n",
            "step: 220, loss: 0.0002360124053666368\n",
            "step: 230, loss: 0.008025788702070713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9875424688561721, f1=0.9783845278725825, best_f1=0.9783845278725825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003638937370851636\n",
            "step: 10, loss: 0.0008248501690104604\n",
            "step: 20, loss: 0.00029974390054121614\n",
            "step: 30, loss: 9.485911868978292e-05\n",
            "step: 40, loss: 0.049444183707237244\n",
            "step: 50, loss: 0.011743253096938133\n",
            "step: 60, loss: 8.110709313768893e-05\n",
            "step: 70, loss: 0.0003236409684177488\n",
            "step: 80, loss: 0.00010121728701051325\n",
            "step: 90, loss: 0.000272795936325565\n",
            "step: 100, loss: 0.00014726108929608017\n",
            "step: 110, loss: 0.0037405395414680243\n",
            "step: 120, loss: 7.096554327290505e-05\n",
            "step: 130, loss: 0.00011270320828771219\n",
            "step: 140, loss: 0.0001072974264388904\n",
            "step: 150, loss: 0.0005142163718119264\n",
            "step: 160, loss: 0.0013226292794570327\n",
            "step: 170, loss: 0.00013878862955607474\n",
            "step: 180, loss: 0.0010031687561422586\n",
            "step: 190, loss: 0.002458944683894515\n",
            "step: 200, loss: 0.0004864686634391546\n",
            "step: 210, loss: 0.0020726146176457405\n",
            "step: 220, loss: 0.0033794178161770105\n",
            "step: 230, loss: 0.00017139124975074083\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9853438556933484, f1=0.9853438556933484, best_f1=0.9783845278725825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011860465019708499\n",
            "step: 10, loss: 0.00012914507533423603\n",
            "step: 20, loss: 0.00019757109112106264\n",
            "step: 30, loss: 0.0006777283269912004\n",
            "step: 40, loss: 0.0008123057777993381\n",
            "step: 50, loss: 0.0004929104470647871\n",
            "step: 60, loss: 0.006315091159194708\n",
            "step: 70, loss: 0.15796814858913422\n",
            "step: 80, loss: 0.019466308876872063\n",
            "step: 90, loss: 0.08412351459264755\n",
            "step: 100, loss: 0.005442132707685232\n",
            "step: 110, loss: 0.013899503275752068\n",
            "step: 120, loss: 0.0004095017211511731\n",
            "step: 130, loss: 0.00024817293160595\n",
            "step: 140, loss: 0.000636428885627538\n",
            "step: 150, loss: 0.0011013188632205129\n",
            "step: 160, loss: 0.042892176657915115\n",
            "step: 170, loss: 0.0002780552313197404\n",
            "step: 180, loss: 0.00014605297474190593\n",
            "step: 190, loss: 0.0002954913070425391\n",
            "step: 200, loss: 0.004977263044565916\n",
            "step: 210, loss: 0.0002083014405798167\n",
            "step: 220, loss: 0.0011995803797617555\n",
            "step: 230, loss: 0.00015631182759534568\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9865771812080537, f1=0.9798206278026906, best_f1=0.9783845278725825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001172863645479083\n",
            "step: 10, loss: 0.00011832373274955899\n",
            "step: 20, loss: 0.0068227252922952175\n",
            "step: 30, loss: 0.11596833914518356\n",
            "step: 40, loss: 0.00012345437426120043\n",
            "step: 50, loss: 0.0008707887609489262\n",
            "step: 60, loss: 0.00020220484293531626\n",
            "step: 70, loss: 0.00034803844755515456\n",
            "step: 80, loss: 0.0001341018360108137\n",
            "step: 90, loss: 0.0018143391935154796\n",
            "step: 100, loss: 8.913539932109416e-05\n",
            "step: 110, loss: 0.00016934028826653957\n",
            "step: 120, loss: 0.00015117487055249512\n",
            "step: 130, loss: 0.00010886645031860098\n",
            "step: 140, loss: 8.404238178627566e-05\n",
            "step: 150, loss: 0.0007400392787531018\n",
            "step: 160, loss: 0.0002505868033040315\n",
            "step: 170, loss: 0.0005075481021776795\n",
            "step: 180, loss: 7.555662887170911e-05\n",
            "step: 190, loss: 0.00019890637486241758\n",
            "step: 200, loss: 0.00025124027160927653\n",
            "step: 210, loss: 0.0002735171001404524\n",
            "step: 220, loss: 0.0006083884509280324\n",
            "step: 230, loss: 0.00020209577633067966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9865771812080537, f1=0.9776785714285714, best_f1=0.9783845278725825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012827612226828933\n",
            "step: 10, loss: 0.00010379629384260625\n",
            "step: 20, loss: 0.00013892391871195287\n",
            "step: 30, loss: 7.61329720262438e-05\n",
            "step: 40, loss: 0.0005943099386058748\n",
            "step: 50, loss: 0.0002892198972404003\n",
            "step: 60, loss: 9.108435187954456e-05\n",
            "step: 70, loss: 0.0001320799347013235\n",
            "step: 80, loss: 8.414711192017421e-05\n",
            "step: 90, loss: 0.00026652473025023937\n",
            "step: 100, loss: 0.0002559784916229546\n",
            "step: 110, loss: 0.00011648366489680484\n",
            "step: 120, loss: 0.001258753938600421\n",
            "step: 130, loss: 7.248381734825671e-05\n",
            "step: 140, loss: 0.00038712978130206466\n",
            "step: 150, loss: 0.0232806745916605\n",
            "step: 160, loss: 0.00025608859141357243\n",
            "step: 170, loss: 9.503385808784515e-05\n",
            "step: 180, loss: 0.023506058380007744\n",
            "step: 190, loss: 0.0001828851964091882\n",
            "step: 200, loss: 3.9914153603604063e-05\n",
            "step: 210, loss: 0.0018980479799211025\n",
            "step: 220, loss: 0.000814126746263355\n",
            "step: 230, loss: 0.00031494026188738644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9865470852017937, f1=0.9820627802690582, best_f1=0.9783845278725825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.984079714631662e-05\n",
            "step: 10, loss: 7.370977255050093e-05\n",
            "step: 20, loss: 0.00026892084861174226\n",
            "step: 30, loss: 0.0001179735190817155\n",
            "step: 40, loss: 0.00013109839346725494\n",
            "step: 50, loss: 4.7017030738061294e-05\n",
            "step: 60, loss: 9.863166633294895e-05\n",
            "step: 70, loss: 7.066014950396493e-05\n",
            "step: 80, loss: 5.085350858280435e-05\n",
            "step: 90, loss: 8.228201477322727e-05\n",
            "step: 100, loss: 8.351852738996968e-05\n",
            "step: 110, loss: 0.00016536563634872437\n",
            "step: 120, loss: 0.0002387937856838107\n",
            "step: 130, loss: 9.081680764211342e-05\n",
            "step: 140, loss: 0.00011093800276285037\n",
            "step: 150, loss: 6.789807957829908e-05\n",
            "step: 160, loss: 7.287931657629088e-05\n",
            "step: 170, loss: 0.00016110904107335955\n",
            "step: 180, loss: 8.811177394818515e-05\n",
            "step: 190, loss: 0.0005318194744177163\n",
            "step: 200, loss: 0.00040860011358745396\n",
            "step: 210, loss: 0.0001469938870286569\n",
            "step: 220, loss: 0.0001719787687761709\n",
            "step: 230, loss: 7.157767686294392e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9865771812080537, f1=0.977728285077951, best_f1=0.9783845278725825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01566254533827305\n",
            "step: 10, loss: 6.078672595322132e-05\n",
            "step: 20, loss: 0.00040293586789630353\n",
            "step: 30, loss: 0.0005762849468737841\n",
            "step: 40, loss: 0.00021294064936228096\n",
            "step: 50, loss: 5.149361822986975e-05\n",
            "step: 60, loss: 0.022717667743563652\n",
            "step: 70, loss: 0.00017823050438892096\n",
            "step: 80, loss: 0.0003068339719902724\n",
            "step: 90, loss: 5.820422666147351e-05\n",
            "step: 100, loss: 3.900482988683507e-05\n",
            "step: 110, loss: 8.452699694316834e-05\n",
            "step: 120, loss: 0.025097601115703583\n",
            "step: 130, loss: 8.256967703346163e-05\n",
            "step: 140, loss: 0.022570330649614334\n",
            "step: 150, loss: 0.00017993392248172313\n",
            "step: 160, loss: 0.003323034616187215\n",
            "step: 170, loss: 7.802828622516245e-05\n",
            "step: 180, loss: 0.016489550471305847\n",
            "step: 190, loss: 0.010693062096834183\n",
            "step: 200, loss: 0.00019365563639439642\n",
            "step: 210, loss: 0.11426997184753418\n",
            "step: 220, loss: 8.275217987829819e-05\n",
            "step: 230, loss: 0.00013292775838635862\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9854748603351955, f1=0.9788182831661093, best_f1=0.9783845278725825\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:16, 150.41it/s]\n",
            "load_f1 = 0.9853438556933484\n",
            "real_f1 = 0.9853438556933484\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 138.75it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrVM9KP9hstt"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f6cf39-0800-46d6-bdaa-1e78a63a3be6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.606137216091156\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.3813115358352661\n",
            "step: 20, loss: 0.3339666426181793\n",
            "step: 30, loss: 0.34618377685546875\n",
            "step: 40, loss: 0.29740557074546814\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.26631200313568115\n",
            "step: 60, loss: 0.2901190221309662\n",
            "step: 70, loss: 0.21533145010471344\n",
            "step: 80, loss: 0.21680372953414917\n",
            "step: 90, loss: 0.2564099133014679\n",
            "step: 100, loss: 0.1649821698665619\n",
            "step: 110, loss: 0.1858101487159729\n",
            "step: 120, loss: 0.41602399945259094\n",
            "step: 130, loss: 0.15681520104408264\n",
            "step: 140, loss: 0.20255662500858307\n",
            "step: 150, loss: 0.1738733947277069\n",
            "step: 160, loss: 0.17416957020759583\n",
            "step: 170, loss: 0.12701022624969482\n",
            "step: 180, loss: 0.10339148342609406\n",
            "step: 190, loss: 0.21306608617305756\n",
            "step: 200, loss: 0.06310432404279709\n",
            "step: 210, loss: 0.03468667343258858\n",
            "step: 220, loss: 0.1239701583981514\n",
            "step: 230, loss: 0.30094337463378906\n",
            "step: 240, loss: 0.08727677166461945\n",
            "step: 250, loss: 0.05579999089241028\n",
            "step: 260, loss: 0.1561049073934555\n",
            "step: 270, loss: 0.3873644769191742\n",
            "step: 280, loss: 0.04489349201321602\n",
            "step: 290, loss: 0.07782834023237228\n",
            "step: 300, loss: 0.07372915744781494\n",
            "step: 310, loss: 0.4715902805328369\n",
            "step: 320, loss: 0.07925978302955627\n",
            "step: 330, loss: 0.06272384524345398\n",
            "step: 340, loss: 0.30843979120254517\n",
            "step: 350, loss: 0.13164041936397552\n",
            "step: 360, loss: 0.00876579713076353\n",
            "step: 370, loss: 0.005978443659842014\n",
            "step: 380, loss: 0.18496258556842804\n",
            "step: 390, loss: 0.02200998179614544\n",
            "step: 400, loss: 0.08904711157083511\n",
            "step: 410, loss: 0.2905789315700531\n",
            "step: 420, loss: 0.028638944029808044\n",
            "step: 430, loss: 0.05943340063095093\n",
            "step: 440, loss: 0.01543080247938633\n",
            "step: 450, loss: 0.13209523260593414\n",
            "step: 460, loss: 0.07415017485618591\n",
            "step: 470, loss: 0.062124721705913544\n",
            "step: 480, loss: 0.1052551120519638\n",
            "step: 490, loss: 0.20712143182754517\n",
            "step: 500, loss: 0.22502431273460388\n",
            "step: 510, loss: 0.15266625583171844\n",
            "step: 520, loss: 0.16174015402793884\n",
            "step: 530, loss: 0.10321137309074402\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9208962048468221, f1=0.9241942805265547, best_f1=0.9241942805265547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13317248225212097\n",
            "step: 10, loss: 0.05261015519499779\n",
            "step: 20, loss: 0.026897408068180084\n",
            "step: 30, loss: 0.04680647328495979\n",
            "step: 40, loss: 0.03810486942529678\n",
            "step: 50, loss: 0.05564562976360321\n",
            "step: 60, loss: 0.013887295499444008\n",
            "step: 70, loss: 0.016162976622581482\n",
            "step: 80, loss: 0.029929231852293015\n",
            "step: 90, loss: 0.01784241385757923\n",
            "step: 100, loss: 0.2047959417104721\n",
            "step: 110, loss: 0.019473344087600708\n",
            "step: 120, loss: 0.17577141523361206\n",
            "step: 130, loss: 0.003411678597331047\n",
            "step: 140, loss: 0.2796936333179474\n",
            "step: 150, loss: 0.019736764952540398\n",
            "step: 160, loss: 0.07065130770206451\n",
            "step: 170, loss: 0.07055088132619858\n",
            "step: 180, loss: 0.023690907284617424\n",
            "step: 190, loss: 0.022595256567001343\n",
            "step: 200, loss: 0.2100331038236618\n",
            "step: 210, loss: 0.03379223868250847\n",
            "step: 220, loss: 0.0025464606005698442\n",
            "step: 230, loss: 0.039422657340765\n",
            "step: 240, loss: 0.11287260800600052\n",
            "step: 250, loss: 0.019246544688940048\n",
            "step: 260, loss: 0.12159252166748047\n",
            "step: 270, loss: 0.13476449251174927\n",
            "step: 280, loss: 0.07590527087450027\n",
            "step: 290, loss: 0.05892631411552429\n",
            "step: 300, loss: 0.026032704859972\n",
            "step: 310, loss: 0.05722814053297043\n",
            "step: 320, loss: 0.07739106565713882\n",
            "step: 330, loss: 0.052425138652324677\n",
            "step: 340, loss: 0.12815186381340027\n",
            "step: 350, loss: 0.0034228800795972347\n",
            "step: 360, loss: 0.11864417046308517\n",
            "step: 370, loss: 0.022051004692912102\n",
            "step: 380, loss: 0.10011057555675507\n",
            "step: 390, loss: 0.006891404744237661\n",
            "step: 400, loss: 0.04820229858160019\n",
            "step: 410, loss: 0.06843344867229462\n",
            "step: 420, loss: 0.11726196855306625\n",
            "step: 430, loss: 0.10012684762477875\n",
            "step: 440, loss: 0.008054491132497787\n",
            "step: 450, loss: 0.0628364160656929\n",
            "step: 460, loss: 0.03143753856420517\n",
            "step: 470, loss: 0.03738342598080635\n",
            "step: 480, loss: 0.10280831903219223\n",
            "step: 490, loss: 0.06577471643686295\n",
            "step: 500, loss: 0.026015792042016983\n",
            "step: 510, loss: 0.03544866666197777\n",
            "step: 520, loss: 0.37374669313430786\n",
            "step: 530, loss: 0.03510481119155884\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9396035039188566, f1=0.9382488479262673, best_f1=0.9382488479262673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07316377013921738\n",
            "step: 10, loss: 0.04281625896692276\n",
            "step: 20, loss: 0.01631820946931839\n",
            "step: 30, loss: 0.08127475529909134\n",
            "step: 40, loss: 0.04697435349225998\n",
            "step: 50, loss: 0.010258747264742851\n",
            "step: 60, loss: 0.04668303206562996\n",
            "step: 70, loss: 0.005345093552023172\n",
            "step: 80, loss: 0.10208022594451904\n",
            "step: 90, loss: 0.053034424781799316\n",
            "step: 100, loss: 0.046690817922353745\n",
            "step: 110, loss: 0.03751806914806366\n",
            "step: 120, loss: 0.05420231446623802\n",
            "step: 130, loss: 0.06194634735584259\n",
            "step: 140, loss: 0.020359523594379425\n",
            "step: 150, loss: 0.04749861732125282\n",
            "step: 160, loss: 0.04622239992022514\n",
            "step: 170, loss: 0.01747662015259266\n",
            "step: 180, loss: 0.012719821184873581\n",
            "step: 190, loss: 0.007900817319750786\n",
            "step: 200, loss: 0.02136331982910633\n",
            "step: 210, loss: 0.08450683206319809\n",
            "step: 220, loss: 0.1682722568511963\n",
            "step: 230, loss: 0.08784327656030655\n",
            "step: 240, loss: 0.06080523505806923\n",
            "step: 250, loss: 0.1537419855594635\n",
            "step: 260, loss: 0.18353396654129028\n",
            "step: 270, loss: 0.012953092344105244\n",
            "step: 280, loss: 0.03796635568141937\n",
            "step: 290, loss: 0.06130484491586685\n",
            "step: 300, loss: 0.14984072744846344\n",
            "step: 310, loss: 0.0789572224020958\n",
            "step: 320, loss: 0.07684384286403656\n",
            "step: 330, loss: 0.022490402683615685\n",
            "step: 340, loss: 0.07074639201164246\n",
            "step: 350, loss: 0.2649002969264984\n",
            "step: 360, loss: 0.04028891399502754\n",
            "step: 370, loss: 0.10721617937088013\n",
            "step: 380, loss: 0.011283115483820438\n",
            "step: 390, loss: 0.007688898127526045\n",
            "step: 400, loss: 0.1757425218820572\n",
            "step: 410, loss: 0.022735584527254105\n",
            "step: 420, loss: 0.010358680970966816\n",
            "step: 430, loss: 0.04032671079039574\n",
            "step: 440, loss: 0.26247814297676086\n",
            "step: 450, loss: 0.03932075947523117\n",
            "step: 460, loss: 0.11997988820075989\n",
            "step: 470, loss: 0.06252006441354752\n",
            "step: 480, loss: 0.08338947594165802\n",
            "step: 490, loss: 0.0070066386833786964\n",
            "step: 500, loss: 0.018661944195628166\n",
            "step: 510, loss: 0.14007945358753204\n",
            "step: 520, loss: 0.04873284697532654\n",
            "step: 530, loss: 0.010572012513875961\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9416745061147694, f1=0.9317004239284032, best_f1=0.9317004239284032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022154806181788445\n",
            "step: 10, loss: 0.01835184544324875\n",
            "step: 20, loss: 0.17791423201560974\n",
            "step: 30, loss: 0.12820959091186523\n",
            "step: 40, loss: 0.04535778984427452\n",
            "step: 50, loss: 0.06600817292928696\n",
            "step: 60, loss: 0.00863068550825119\n",
            "step: 70, loss: 0.024742990732192993\n",
            "step: 80, loss: 0.14743158221244812\n",
            "step: 90, loss: 0.029787929728627205\n",
            "step: 100, loss: 0.04672364890575409\n",
            "step: 110, loss: 0.08934522420167923\n",
            "step: 120, loss: 0.006670564413070679\n",
            "step: 130, loss: 0.021267782896757126\n",
            "step: 140, loss: 0.16181395947933197\n",
            "step: 150, loss: 0.013502643443644047\n",
            "step: 160, loss: 0.006910014897584915\n",
            "step: 170, loss: 0.048135433346033096\n",
            "step: 180, loss: 0.07782328873872757\n",
            "step: 190, loss: 0.0760607123374939\n",
            "step: 200, loss: 0.1124832034111023\n",
            "step: 210, loss: 0.0065460121259093285\n",
            "step: 220, loss: 0.03736262023448944\n",
            "step: 230, loss: 0.006350819021463394\n",
            "step: 240, loss: 0.008487462997436523\n",
            "step: 250, loss: 0.19775064289569855\n",
            "step: 260, loss: 0.0013966687256470323\n",
            "step: 270, loss: 0.06365086138248444\n",
            "step: 280, loss: 0.0011491129407659173\n",
            "step: 290, loss: 0.007872110232710838\n",
            "step: 300, loss: 0.007234976626932621\n",
            "step: 310, loss: 0.0029694801196455956\n",
            "step: 320, loss: 0.07045415043830872\n",
            "step: 330, loss: 0.008470803499221802\n",
            "step: 340, loss: 0.019922126084566116\n",
            "step: 350, loss: 0.16380469501018524\n",
            "step: 360, loss: 0.04913759604096413\n",
            "step: 370, loss: 0.003966163843870163\n",
            "step: 380, loss: 0.003832476679235697\n",
            "step: 390, loss: 0.003697273787111044\n",
            "step: 400, loss: 0.04776689037680626\n",
            "step: 410, loss: 0.0035624064039438963\n",
            "step: 420, loss: 0.007249858230352402\n",
            "step: 430, loss: 0.011952937580645084\n",
            "step: 440, loss: 0.025097176432609558\n",
            "step: 450, loss: 0.020912615582346916\n",
            "step: 460, loss: 0.023274948820471764\n",
            "step: 470, loss: 0.007091139443218708\n",
            "step: 480, loss: 0.009568140842020512\n",
            "step: 490, loss: 0.023100553080439568\n",
            "step: 500, loss: 0.03412175178527832\n",
            "step: 510, loss: 0.141281396150589\n",
            "step: 520, loss: 0.03086145967245102\n",
            "step: 530, loss: 0.06278584152460098\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9466357308584686, f1=0.9471733086190918, best_f1=0.9471733086190918\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014932365156710148\n",
            "step: 10, loss: 0.005031618755310774\n",
            "step: 20, loss: 0.0054555367678403854\n",
            "step: 30, loss: 0.021153060719370842\n",
            "step: 40, loss: 0.012434188276529312\n",
            "step: 50, loss: 0.005781815852969885\n",
            "step: 60, loss: 0.12940531969070435\n",
            "step: 70, loss: 0.0032514366321265697\n",
            "step: 80, loss: 0.0026372119318693876\n",
            "step: 90, loss: 0.01799567975103855\n",
            "step: 100, loss: 0.01084202155470848\n",
            "step: 110, loss: 0.011569601483643055\n",
            "step: 120, loss: 0.0998581275343895\n",
            "step: 130, loss: 0.004069410730153322\n",
            "step: 140, loss: 0.06353927403688431\n",
            "step: 150, loss: 0.014125900343060493\n",
            "step: 160, loss: 0.03478146344423294\n",
            "step: 170, loss: 0.07610362768173218\n",
            "step: 180, loss: 0.003680173773318529\n",
            "step: 190, loss: 0.006741248071193695\n",
            "step: 200, loss: 0.020960986614227295\n",
            "step: 210, loss: 0.014063806273043156\n",
            "step: 220, loss: 0.02724333666265011\n",
            "step: 230, loss: 0.0025621610693633556\n",
            "step: 240, loss: 0.002139428863301873\n",
            "step: 250, loss: 0.13625997304916382\n",
            "step: 260, loss: 0.0034429491497576237\n",
            "step: 270, loss: 0.0021824820432811975\n",
            "step: 280, loss: 0.00470541650429368\n",
            "step: 290, loss: 0.0008850482990965247\n",
            "step: 300, loss: 0.07385066896677017\n",
            "step: 310, loss: 0.0814322680234909\n",
            "step: 320, loss: 0.0834287479519844\n",
            "step: 330, loss: 0.0072808717377483845\n",
            "step: 340, loss: 0.005370112136006355\n",
            "step: 350, loss: 0.0020600873976945877\n",
            "step: 360, loss: 0.0036305496469140053\n",
            "step: 370, loss: 0.001188992871902883\n",
            "step: 380, loss: 0.00078268360812217\n",
            "step: 390, loss: 0.0028429620433598757\n",
            "step: 400, loss: 0.014043509028851986\n",
            "step: 410, loss: 0.10081568360328674\n",
            "step: 420, loss: 0.1764100044965744\n",
            "step: 430, loss: 0.15545393526554108\n",
            "step: 440, loss: 0.0028862745966762304\n",
            "step: 450, loss: 0.0744299665093422\n",
            "step: 460, loss: 0.06024523451924324\n",
            "step: 470, loss: 0.08876417577266693\n",
            "step: 480, loss: 0.05068576708436012\n",
            "step: 490, loss: 0.043092142790555954\n",
            "step: 500, loss: 0.05440157279372215\n",
            "step: 510, loss: 0.005979736801236868\n",
            "step: 520, loss: 0.031096097081899643\n",
            "step: 530, loss: 0.00821936409920454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9443929564411491, f1=0.9377593360995851, best_f1=0.9471733086190918\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0058310073800385\n",
            "step: 10, loss: 0.016502194106578827\n",
            "step: 20, loss: 0.013568859547376633\n",
            "step: 30, loss: 0.0005879816017113626\n",
            "step: 40, loss: 0.0009337028604932129\n",
            "step: 50, loss: 0.0025594234466552734\n",
            "step: 60, loss: 0.006940608378499746\n",
            "step: 70, loss: 0.023079222068190575\n",
            "step: 80, loss: 0.00030531344236806035\n",
            "step: 90, loss: 0.002775172470137477\n",
            "step: 100, loss: 0.07166601717472076\n",
            "step: 110, loss: 0.0151442252099514\n",
            "step: 120, loss: 0.027859238907694817\n",
            "step: 130, loss: 0.0024020280689001083\n",
            "step: 140, loss: 0.0009936299175024033\n",
            "step: 150, loss: 0.0008103420841507614\n",
            "step: 160, loss: 0.08162069320678711\n",
            "step: 170, loss: 0.008264065720140934\n",
            "step: 180, loss: 0.04255475476384163\n",
            "step: 190, loss: 0.19601735472679138\n",
            "step: 200, loss: 0.012622278183698654\n",
            "step: 210, loss: 0.004113473929464817\n",
            "step: 220, loss: 0.019340183585882187\n",
            "step: 230, loss: 0.006378346122801304\n",
            "step: 240, loss: 0.02351589873433113\n",
            "step: 250, loss: 0.09315440058708191\n",
            "step: 260, loss: 0.0008353099110536277\n",
            "step: 270, loss: 0.005912075750529766\n",
            "step: 280, loss: 0.08176819980144501\n",
            "step: 290, loss: 0.0037846534978598356\n",
            "step: 300, loss: 0.018126370385289192\n",
            "step: 310, loss: 0.039399489760398865\n",
            "step: 320, loss: 0.0011082241544499993\n",
            "step: 330, loss: 0.005292197223752737\n",
            "step: 340, loss: 0.00034680782118812203\n",
            "step: 350, loss: 0.005121369380503893\n",
            "step: 360, loss: 0.03772885724902153\n",
            "step: 370, loss: 0.01914963871240616\n",
            "step: 380, loss: 0.002284039743244648\n",
            "step: 390, loss: 0.00233170366846025\n",
            "step: 400, loss: 0.005287799518555403\n",
            "step: 410, loss: 0.00023576797684654593\n",
            "step: 420, loss: 0.007376644294708967\n",
            "step: 430, loss: 0.010490200482308865\n",
            "step: 440, loss: 0.054601363837718964\n",
            "step: 450, loss: 0.25887465476989746\n",
            "step: 460, loss: 0.0028668621089309454\n",
            "step: 470, loss: 0.008637366816401482\n",
            "step: 480, loss: 0.006758435629308224\n",
            "step: 490, loss: 0.00469678221270442\n",
            "step: 500, loss: 0.03657536953687668\n",
            "step: 510, loss: 0.0055360132828354836\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 520, loss: 0.00043652861495502293\n",
            "step: 530, loss: 0.011150254867970943\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9487418452935694, f1=0.9399720800372265, best_f1=0.9399720800372265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014206666965037584\n",
            "step: 10, loss: 0.01293089147657156\n",
            "step: 20, loss: 0.0026962950360029936\n",
            "step: 30, loss: 0.021569784730672836\n",
            "step: 40, loss: 0.07341066747903824\n",
            "step: 50, loss: 0.010645671747624874\n",
            "step: 60, loss: 0.01602829061448574\n",
            "step: 70, loss: 0.022798730060458183\n",
            "step: 80, loss: 0.0011981852585449815\n",
            "step: 90, loss: 0.0014885213458910584\n",
            "step: 100, loss: 0.00836638268083334\n",
            "step: 110, loss: 0.001604898483492434\n",
            "step: 120, loss: 0.0012592682614922523\n",
            "step: 130, loss: 0.0027831955812871456\n",
            "step: 140, loss: 0.004111242946237326\n",
            "step: 150, loss: 0.004650472197681665\n",
            "step: 160, loss: 0.0004686176835093647\n",
            "step: 170, loss: 0.0041988203302025795\n",
            "step: 180, loss: 0.17467302083969116\n",
            "step: 190, loss: 0.11747804284095764\n",
            "step: 200, loss: 0.0003316267975606024\n",
            "step: 210, loss: 0.0011544371955096722\n",
            "step: 220, loss: 0.005992097780108452\n",
            "step: 230, loss: 0.002538035623729229\n",
            "step: 240, loss: 0.04585814103484154\n",
            "step: 250, loss: 0.0055759078823029995\n",
            "step: 260, loss: 0.0008416881901212037\n",
            "step: 270, loss: 0.02098550647497177\n",
            "step: 280, loss: 0.0062393732368946075\n",
            "step: 290, loss: 0.012894675135612488\n",
            "step: 300, loss: 0.0004961987142451108\n",
            "step: 310, loss: 0.002068087924271822\n",
            "step: 320, loss: 0.005859371740370989\n",
            "step: 330, loss: 0.020264193415641785\n",
            "step: 340, loss: 0.007309906184673309\n",
            "step: 350, loss: 0.005239480175077915\n",
            "step: 360, loss: 0.007459644693881273\n",
            "step: 370, loss: 0.031967610120773315\n",
            "step: 380, loss: 0.022208090871572495\n",
            "step: 390, loss: 0.028261195868253708\n",
            "step: 400, loss: 0.08877413719892502\n",
            "step: 410, loss: 0.03677643463015556\n",
            "step: 420, loss: 0.012775941751897335\n",
            "step: 430, loss: 0.0007754291291348636\n",
            "step: 440, loss: 0.004089847672730684\n",
            "step: 450, loss: 0.00850016251206398\n",
            "step: 460, loss: 0.02185203693807125\n",
            "step: 470, loss: 0.13748152554035187\n",
            "step: 480, loss: 0.005674354266375303\n",
            "step: 490, loss: 0.010912875644862652\n",
            "step: 500, loss: 0.013223385438323021\n",
            "step: 510, loss: 0.0021013759542256594\n",
            "step: 520, loss: 0.0010090019786730409\n",
            "step: 530, loss: 0.0019997877534478903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9421028253821214, f1=0.9377901578458682, best_f1=0.9399720800372265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002069563139230013\n",
            "step: 10, loss: 0.0019284005975350738\n",
            "step: 20, loss: 0.0011496013030409813\n",
            "step: 30, loss: 0.0006752717890776694\n",
            "step: 40, loss: 0.00022162377717904747\n",
            "step: 50, loss: 0.00044391624396666884\n",
            "step: 60, loss: 0.003026674734428525\n",
            "step: 70, loss: 0.0024018683470785618\n",
            "step: 80, loss: 0.043764613568782806\n",
            "step: 90, loss: 0.0009156158776022494\n",
            "step: 100, loss: 0.00162324879784137\n",
            "step: 110, loss: 0.0044313655234873295\n",
            "step: 120, loss: 0.0004778243019245565\n",
            "step: 130, loss: 0.0009122313931584358\n",
            "step: 140, loss: 0.03972174599766731\n",
            "step: 150, loss: 0.0009108323720283806\n",
            "step: 160, loss: 0.00010731707880040631\n",
            "step: 170, loss: 0.15172021090984344\n",
            "step: 180, loss: 0.002429247833788395\n",
            "step: 190, loss: 0.04995483160018921\n",
            "step: 200, loss: 0.009895405732095242\n",
            "step: 210, loss: 0.18221309781074524\n",
            "step: 220, loss: 0.0015077907592058182\n",
            "step: 230, loss: 0.008135531097650528\n",
            "step: 240, loss: 0.03179322928190231\n",
            "step: 250, loss: 0.00561326602473855\n",
            "step: 260, loss: 0.0004110360750928521\n",
            "step: 270, loss: 0.020417237654328346\n",
            "step: 280, loss: 0.00024149577075149864\n",
            "step: 290, loss: 0.03798707202076912\n",
            "step: 300, loss: 2.8735372325172648e-05\n",
            "step: 310, loss: 0.0018658485496416688\n",
            "step: 320, loss: 0.00015113467816263437\n",
            "step: 330, loss: 0.0001241179706994444\n",
            "step: 340, loss: 0.005048224236816168\n",
            "step: 350, loss: 6.645944085903466e-05\n",
            "step: 360, loss: 0.20523115992546082\n",
            "step: 370, loss: 0.1413818597793579\n",
            "step: 380, loss: 0.001422283472493291\n",
            "step: 390, loss: 0.029978666454553604\n",
            "step: 400, loss: 0.007643405348062515\n",
            "step: 410, loss: 0.0011241957545280457\n",
            "step: 420, loss: 0.0016274229856207967\n",
            "step: 430, loss: 0.001807165564969182\n",
            "step: 440, loss: 0.14102846384048462\n",
            "step: 450, loss: 0.0011914579663425684\n",
            "step: 460, loss: 0.005523762200027704\n",
            "step: 470, loss: 0.12594151496887207\n",
            "step: 480, loss: 0.010330304503440857\n",
            "step: 490, loss: 0.002976295305415988\n",
            "step: 500, loss: 0.0029989504255354404\n",
            "step: 510, loss: 0.007789120078086853\n",
            "step: 520, loss: 0.0028056404553353786\n",
            "step: 530, loss: 0.004769053775817156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.943758573388203, f1=0.9419237749546279, best_f1=0.9399720800372265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006963172345422208\n",
            "step: 10, loss: 0.002032934920862317\n",
            "step: 20, loss: 0.008060493506491184\n",
            "step: 30, loss: 0.06687192618846893\n",
            "step: 40, loss: 0.0005011147004552186\n",
            "step: 50, loss: 0.0016683556605130434\n",
            "step: 60, loss: 0.00035380979534238577\n",
            "step: 70, loss: 0.015431622974574566\n",
            "step: 80, loss: 0.01239364966750145\n",
            "step: 90, loss: 0.10816390812397003\n",
            "step: 100, loss: 0.0008125798194669187\n",
            "step: 110, loss: 0.029460521414875984\n",
            "step: 120, loss: 0.0010916125029325485\n",
            "step: 130, loss: 0.001586219179444015\n",
            "step: 140, loss: 0.0022148967254906893\n",
            "step: 150, loss: 0.0006220002542249858\n",
            "step: 160, loss: 0.0010194857604801655\n",
            "step: 170, loss: 0.0011923181591555476\n",
            "step: 180, loss: 0.010465383529663086\n",
            "step: 190, loss: 0.0001632122293813154\n",
            "step: 200, loss: 0.0013701559510082006\n",
            "step: 210, loss: 0.003933302592486143\n",
            "step: 220, loss: 0.0015761548420414329\n",
            "step: 230, loss: 0.00032666471088305116\n",
            "step: 240, loss: 0.0005168510251678526\n",
            "step: 250, loss: 0.0036687033716589212\n",
            "step: 260, loss: 0.0001979904918698594\n",
            "step: 270, loss: 0.033314693719148636\n",
            "step: 280, loss: 0.00821802020072937\n",
            "step: 290, loss: 0.0011843318352475762\n",
            "step: 300, loss: 4.9583526561036706e-05\n",
            "step: 310, loss: 0.014687690883874893\n",
            "step: 320, loss: 0.0008058559033088386\n",
            "step: 330, loss: 0.0009686945704743266\n",
            "step: 340, loss: 0.004434369970113039\n",
            "step: 350, loss: 0.03342191129922867\n",
            "step: 360, loss: 0.001648598350584507\n",
            "step: 370, loss: 0.0035744053311645985\n",
            "step: 380, loss: 0.0046987892128527164\n",
            "step: 390, loss: 0.00016570401203352958\n",
            "step: 400, loss: 0.01804722473025322\n",
            "step: 410, loss: 0.005330492742359638\n",
            "step: 420, loss: 0.007219989784061909\n",
            "step: 430, loss: 0.003895118599757552\n",
            "step: 440, loss: 0.001074014464393258\n",
            "step: 450, loss: 0.01742270216345787\n",
            "step: 460, loss: 0.0003272337489761412\n",
            "step: 470, loss: 0.019054502248764038\n",
            "step: 480, loss: 8.236078429035842e-05\n",
            "step: 490, loss: 0.008235668763518333\n",
            "step: 500, loss: 0.0009524050401523709\n",
            "step: 510, loss: 0.09146641194820404\n",
            "step: 520, loss: 0.002571417484432459\n",
            "step: 530, loss: 0.07977344840765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9464368886818817, f1=0.9387186629526463, best_f1=0.9399720800372265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011019648518413305\n",
            "step: 10, loss: 0.0011096912203356624\n",
            "step: 20, loss: 0.00037904366035945714\n",
            "step: 30, loss: 0.003543023020029068\n",
            "step: 40, loss: 0.0038753810804337263\n",
            "step: 50, loss: 0.0011779807973653078\n",
            "step: 60, loss: 0.017380928620696068\n",
            "step: 70, loss: 0.007621806114912033\n",
            "step: 80, loss: 0.001387071912176907\n",
            "step: 90, loss: 0.006735313683748245\n",
            "step: 100, loss: 0.0007744583999738097\n",
            "step: 110, loss: 0.009941195137798786\n",
            "step: 120, loss: 7.709467899985611e-05\n",
            "step: 130, loss: 3.6002547858515754e-05\n",
            "step: 140, loss: 0.00034708782914094627\n",
            "step: 150, loss: 0.0016489882254973054\n",
            "step: 160, loss: 0.028180843219161034\n",
            "step: 170, loss: 0.00011254784476477653\n",
            "step: 180, loss: 0.0967201218008995\n",
            "step: 190, loss: 3.174308221787214e-05\n",
            "step: 200, loss: 0.0003340561524964869\n",
            "step: 210, loss: 0.013782596215605736\n",
            "step: 220, loss: 0.0005492665222845972\n",
            "step: 230, loss: 0.0010388484224677086\n",
            "step: 240, loss: 0.0012008694466203451\n",
            "step: 250, loss: 0.0063714757561683655\n",
            "step: 260, loss: 0.009553574956953526\n",
            "step: 270, loss: 0.005015430506318808\n",
            "step: 280, loss: 0.027589788660407066\n",
            "step: 290, loss: 0.01998789794743061\n",
            "step: 300, loss: 0.04831964150071144\n",
            "step: 310, loss: 0.017288682982325554\n",
            "step: 320, loss: 0.0288830678910017\n",
            "step: 330, loss: 0.004391706082969904\n",
            "step: 340, loss: 9.950394451152533e-05\n",
            "step: 350, loss: 0.000586509529966861\n",
            "step: 360, loss: 0.005106027238070965\n",
            "step: 370, loss: 0.0033167111687362194\n",
            "step: 380, loss: 0.015364197082817554\n",
            "step: 390, loss: 0.000987196690402925\n",
            "step: 400, loss: 0.00047461941721849144\n",
            "step: 410, loss: 0.0016917112516239285\n",
            "step: 420, loss: 0.0006501654861494899\n",
            "step: 430, loss: 0.0010394565761089325\n",
            "step: 440, loss: 0.0005033882334828377\n",
            "step: 450, loss: 0.002725919708609581\n",
            "step: 460, loss: 0.00021603572531603277\n",
            "step: 470, loss: 0.002581023843958974\n",
            "step: 480, loss: 0.0017012248281389475\n",
            "step: 490, loss: 0.00106367701664567\n",
            "step: 500, loss: 0.0045937844552099705\n",
            "step: 510, loss: 0.0012043163878843188\n",
            "step: 520, loss: 0.0020562249701470137\n",
            "step: 530, loss: 0.01772071234881878\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9441860465116279, f1=0.9456572224802601, best_f1=0.9399720800372265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010393512202426791\n",
            "step: 10, loss: 0.0008013302576728165\n",
            "step: 20, loss: 0.001594889909029007\n",
            "step: 30, loss: 0.0012602830538526177\n",
            "step: 40, loss: 0.000827243085950613\n",
            "step: 50, loss: 0.00046439748257398605\n",
            "step: 60, loss: 0.0008816817426122725\n",
            "step: 70, loss: 0.0003803735598921776\n",
            "step: 80, loss: 0.0008017220534384251\n",
            "step: 90, loss: 0.02824646793305874\n",
            "step: 100, loss: 0.0009416568209417164\n",
            "step: 110, loss: 0.030266549438238144\n",
            "step: 120, loss: 0.05125528201460838\n",
            "step: 130, loss: 0.0003803988511208445\n",
            "step: 140, loss: 0.0006547037046402693\n",
            "step: 150, loss: 0.014969924464821815\n",
            "step: 160, loss: 0.007360141724348068\n",
            "step: 170, loss: 0.0016672657802700996\n",
            "step: 180, loss: 5.238961966824718e-05\n",
            "step: 190, loss: 0.0005137899424880743\n",
            "step: 200, loss: 0.0005041087861172855\n",
            "step: 210, loss: 0.001000786549411714\n",
            "step: 220, loss: 0.05376293137669563\n",
            "step: 230, loss: 0.0028927272651344538\n",
            "step: 240, loss: 0.0021266506519168615\n",
            "step: 250, loss: 2.2007958250469528e-05\n",
            "step: 260, loss: 0.004342691507190466\n",
            "step: 270, loss: 0.001575217698700726\n",
            "step: 280, loss: 0.0016079460037872195\n",
            "step: 290, loss: 0.0381520576775074\n",
            "step: 300, loss: 0.00024461280554533005\n",
            "step: 310, loss: 0.026430681347846985\n",
            "step: 320, loss: 0.004552175290882587\n",
            "step: 330, loss: 0.0012475945986807346\n",
            "step: 340, loss: 0.005385211668908596\n",
            "step: 350, loss: 0.0003407453768886626\n",
            "step: 360, loss: 0.0008931186748668551\n",
            "step: 370, loss: 0.0028606040868908167\n",
            "step: 380, loss: 0.0011067073792219162\n",
            "step: 390, loss: 0.0016131665324792266\n",
            "step: 400, loss: 0.00012824639270547777\n",
            "step: 410, loss: 0.000956783420406282\n",
            "step: 420, loss: 0.01971958950161934\n",
            "step: 430, loss: 0.0006491238600574434\n",
            "step: 440, loss: 0.00039926619501784444\n",
            "step: 450, loss: 0.0024630350526422262\n",
            "step: 460, loss: 0.0033210469409823418\n",
            "step: 470, loss: 0.0008854490588419139\n",
            "step: 480, loss: 0.00012481991143431515\n",
            "step: 490, loss: 0.0017012576572597027\n",
            "step: 500, loss: 0.00020542244601529092\n",
            "step: 510, loss: 0.0038531741593033075\n",
            "step: 520, loss: 0.00017180187569465488\n",
            "step: 530, loss: 0.010655799880623817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9386814200092208, f1=0.9407441433164906, best_f1=0.9399720800372265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005416920757852495\n",
            "step: 10, loss: 0.0030191789846867323\n",
            "step: 20, loss: 0.008045295253396034\n",
            "step: 30, loss: 0.0001804508501663804\n",
            "step: 40, loss: 0.021252600476145744\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.005906923674046993\n",
            "step: 60, loss: 5.5472231906605884e-05\n",
            "step: 70, loss: 0.08517491817474365\n",
            "step: 80, loss: 0.0006654148455709219\n",
            "step: 90, loss: 0.002410299377515912\n",
            "step: 100, loss: 0.048734769225120544\n",
            "step: 110, loss: 0.01803717389702797\n",
            "step: 120, loss: 0.004966528620570898\n",
            "step: 130, loss: 0.02160760946571827\n",
            "step: 140, loss: 0.0004012945864815265\n",
            "step: 150, loss: 0.009529437869787216\n",
            "step: 160, loss: 0.011669199913740158\n",
            "step: 170, loss: 1.084052291844273e-05\n",
            "step: 180, loss: 0.00018498986901249737\n",
            "step: 190, loss: 0.0009715129854157567\n",
            "step: 200, loss: 0.003256396623328328\n",
            "step: 210, loss: 2.079015212075319e-05\n",
            "step: 220, loss: 0.00020653038518503308\n",
            "step: 230, loss: 9.366277663502842e-05\n",
            "step: 240, loss: 0.0002324259257875383\n",
            "step: 250, loss: 9.87565545074176e-06\n",
            "step: 260, loss: 0.0002930421906057745\n",
            "step: 270, loss: 0.05288795381784439\n",
            "step: 280, loss: 0.0001231919159181416\n",
            "step: 290, loss: 8.935870573623106e-05\n",
            "step: 300, loss: 0.0019279164262115955\n",
            "step: 310, loss: 0.0007368004880845547\n",
            "step: 320, loss: 0.007244590204209089\n",
            "step: 330, loss: 0.04057278856635094\n",
            "step: 340, loss: 0.0004211770719848573\n",
            "step: 350, loss: 8.124815394694451e-06\n",
            "step: 360, loss: 3.395224121049978e-05\n",
            "step: 370, loss: 0.00016258655523415655\n",
            "step: 380, loss: 0.0009590075351297855\n",
            "step: 390, loss: 0.05646579712629318\n",
            "step: 400, loss: 0.0001541431265650317\n",
            "step: 410, loss: 0.010519666597247124\n",
            "step: 420, loss: 0.14760597050189972\n",
            "step: 430, loss: 0.008799615316092968\n",
            "step: 440, loss: 0.08824067562818527\n",
            "step: 450, loss: 0.0003940226451959461\n",
            "step: 460, loss: 4.3568819819483906e-05\n",
            "step: 470, loss: 0.0021325796842575073\n",
            "step: 480, loss: 0.025933345779776573\n",
            "step: 490, loss: 0.0011634171241894364\n",
            "step: 500, loss: 0.001192216295748949\n",
            "step: 510, loss: 0.02350088767707348\n",
            "step: 520, loss: 0.0020621041767299175\n",
            "step: 530, loss: 0.0015763159608468413\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9460966542750929, f1=0.9435185185185185, best_f1=0.9399720800372265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001718884304864332\n",
            "step: 10, loss: 0.0008498140960000455\n",
            "step: 20, loss: 0.0012137622106820345\n",
            "step: 30, loss: 0.0004610228934325278\n",
            "step: 40, loss: 0.005300554446876049\n",
            "step: 50, loss: 0.0035548065789043903\n",
            "step: 60, loss: 0.0042627532966434956\n",
            "step: 70, loss: 0.0026377784088253975\n",
            "step: 80, loss: 0.004655235446989536\n",
            "step: 90, loss: 0.004406034015119076\n",
            "step: 100, loss: 0.0003870803629979491\n",
            "step: 110, loss: 0.0007479244377464056\n",
            "step: 120, loss: 5.507865716936067e-05\n",
            "step: 130, loss: 0.0025708647444844246\n",
            "step: 140, loss: 0.0012015984393656254\n",
            "step: 150, loss: 0.0025784350000321865\n",
            "step: 160, loss: 0.0003541026380844414\n",
            "step: 170, loss: 0.006263740826398134\n",
            "step: 180, loss: 0.00042538391426205635\n",
            "step: 190, loss: 0.00013398431474342942\n",
            "step: 200, loss: 0.00024383066920563579\n",
            "step: 210, loss: 3.139975160593167e-05\n",
            "step: 220, loss: 0.024809470400214195\n",
            "step: 230, loss: 0.0035514351911842823\n",
            "step: 240, loss: 0.04478289932012558\n",
            "step: 250, loss: 0.0007616423536092043\n",
            "step: 260, loss: 0.0016656778752803802\n",
            "step: 270, loss: 0.0008718307944945991\n",
            "step: 280, loss: 6.233088788576424e-05\n",
            "step: 290, loss: 0.0009173667640425265\n",
            "step: 300, loss: 0.0001924815442180261\n",
            "step: 310, loss: 0.00041824832442216575\n",
            "step: 320, loss: 0.00018929000361822546\n",
            "step: 330, loss: 0.0015671945875510573\n",
            "step: 340, loss: 0.0018706333357840776\n",
            "step: 350, loss: 0.0009073209948837757\n",
            "step: 360, loss: 0.09006711840629578\n",
            "step: 370, loss: 0.009352491237223148\n",
            "step: 380, loss: 0.0010864357464015484\n",
            "step: 390, loss: 0.0007525191176682711\n",
            "step: 400, loss: 0.0010410556569695473\n",
            "step: 410, loss: 0.0009712653118185699\n",
            "step: 420, loss: 0.0003228779823984951\n",
            "step: 430, loss: 6.002683949191123e-05\n",
            "step: 440, loss: 0.000119554199045524\n",
            "step: 450, loss: 0.00022963466471992433\n",
            "step: 460, loss: 0.0002694023714866489\n",
            "step: 470, loss: 0.0020900273229926825\n",
            "step: 480, loss: 3.356425077072345e-05\n",
            "step: 490, loss: 0.00011003702820744365\n",
            "step: 500, loss: 0.00414184108376503\n",
            "step: 510, loss: 8.713373972568661e-06\n",
            "step: 520, loss: 0.0001759072911227122\n",
            "step: 530, loss: 2.7810481697088107e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9484440315838365, f1=0.9425393883225208, best_f1=0.9399720800372265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0033210739493370056\n",
            "step: 10, loss: 0.0006740076933056116\n",
            "step: 20, loss: 0.00012021810107398778\n",
            "step: 30, loss: 0.00026716195861808956\n",
            "step: 40, loss: 0.00010406568617327139\n",
            "step: 50, loss: 0.001000580727122724\n",
            "step: 60, loss: 0.014827493578195572\n",
            "step: 70, loss: 0.002666595159098506\n",
            "step: 80, loss: 0.0001376466389046982\n",
            "step: 90, loss: 6.31857619737275e-05\n",
            "step: 100, loss: 8.962559513747692e-05\n",
            "step: 110, loss: 0.00041654842789284885\n",
            "step: 120, loss: 1.003570287139155e-05\n",
            "step: 130, loss: 1.5652429283363745e-05\n",
            "step: 140, loss: 0.008842481300234795\n",
            "step: 150, loss: 0.002394341630861163\n",
            "step: 160, loss: 0.0023133072536438704\n",
            "step: 170, loss: 0.001966077834367752\n",
            "step: 180, loss: 0.00022811314556747675\n",
            "step: 190, loss: 0.00267805065959692\n",
            "step: 200, loss: 0.0008573210216127336\n",
            "step: 210, loss: 0.008236976340413094\n",
            "step: 220, loss: 1.3593332369055133e-05\n",
            "step: 230, loss: 0.021153178066015244\n",
            "step: 240, loss: 0.0027509192004799843\n",
            "step: 250, loss: 0.0013288843911141157\n",
            "step: 260, loss: 0.00025193122564814985\n",
            "step: 270, loss: 0.0006665404653176665\n",
            "step: 280, loss: 0.001106725656427443\n",
            "step: 290, loss: 8.139306009979919e-05\n",
            "step: 300, loss: 2.205930468335282e-05\n",
            "step: 310, loss: 0.0001768874062690884\n",
            "step: 320, loss: 2.237163243989926e-05\n",
            "step: 330, loss: 0.001477074809372425\n",
            "step: 340, loss: 0.0004230674239806831\n",
            "step: 350, loss: 0.0005607568309642375\n",
            "step: 360, loss: 0.0017032341565936804\n",
            "step: 370, loss: 0.0002616470737848431\n",
            "step: 380, loss: 0.026149094104766846\n",
            "step: 390, loss: 0.0011412266176193953\n",
            "step: 400, loss: 0.0019973262678831816\n",
            "step: 410, loss: 9.782781853573397e-05\n",
            "step: 420, loss: 0.00020019781368318945\n",
            "step: 430, loss: 0.0008946319576352835\n",
            "step: 440, loss: 0.0010601809481158853\n",
            "step: 450, loss: 0.00021110981469973922\n",
            "step: 460, loss: 0.048686861991882324\n",
            "step: 470, loss: 2.8937356546521187e-05\n",
            "step: 480, loss: 0.00035304256016388535\n",
            "step: 490, loss: 0.0003169897827319801\n",
            "step: 500, loss: 0.0022023681085556746\n",
            "step: 510, loss: 0.00010166376887355\n",
            "step: 520, loss: 0.0008445028797723353\n",
            "step: 530, loss: 0.0017690520035102963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9488372093023255, f1=0.9427640763145649, best_f1=0.9427640763145649\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.3740662072668783e-05\n",
            "step: 10, loss: 0.001756120822392404\n",
            "step: 20, loss: 0.0032718167640268803\n",
            "step: 30, loss: 0.005966573487967253\n",
            "step: 40, loss: 0.0020950925536453724\n",
            "step: 50, loss: 0.0010154034243896604\n",
            "step: 60, loss: 0.0005187590140849352\n",
            "step: 70, loss: 0.00010132090392289683\n",
            "step: 80, loss: 0.00021035855752415955\n",
            "step: 90, loss: 9.197795588988811e-05\n",
            "step: 100, loss: 4.314568650443107e-05\n",
            "step: 110, loss: 0.004734191577881575\n",
            "step: 120, loss: 0.00032148530590347946\n",
            "step: 130, loss: 0.0011030384339392185\n",
            "step: 140, loss: 2.4742164896451868e-05\n",
            "step: 150, loss: 0.0018957266584038734\n",
            "step: 160, loss: 0.0002765564131550491\n",
            "step: 170, loss: 3.582668796298094e-05\n",
            "step: 180, loss: 0.0004650465853046626\n",
            "step: 190, loss: 0.019813258200883865\n",
            "step: 200, loss: 0.0018813046626746655\n",
            "step: 210, loss: 0.0009020080324262381\n",
            "step: 220, loss: 0.002054166281595826\n",
            "step: 230, loss: 0.0003474121622275561\n",
            "step: 240, loss: 0.00037259457167237997\n",
            "step: 250, loss: 0.0007393735577352345\n",
            "step: 260, loss: 1.848818465077784e-05\n",
            "step: 270, loss: 1.8200702470494434e-05\n",
            "step: 280, loss: 0.0003641430812422186\n",
            "step: 290, loss: 0.0002904823049902916\n",
            "step: 300, loss: 0.0003690998419187963\n",
            "step: 310, loss: 0.033612485975027084\n",
            "step: 320, loss: 0.00870937667787075\n",
            "step: 330, loss: 1.620839975657873e-05\n",
            "step: 340, loss: 0.0001594827335793525\n",
            "step: 350, loss: 0.007523437961935997\n",
            "step: 360, loss: 0.00035103957634419203\n",
            "step: 370, loss: 0.0014760716585442424\n",
            "step: 380, loss: 0.0016408697701990604\n",
            "step: 390, loss: 0.0002358187921345234\n",
            "step: 400, loss: 0.046700023114681244\n",
            "step: 410, loss: 0.00014051454490981996\n",
            "step: 420, loss: 0.00023291638353839517\n",
            "step: 430, loss: 1.31240831251489e-05\n",
            "step: 440, loss: 0.002349437214434147\n",
            "step: 450, loss: 0.0018351614708080888\n",
            "step: 460, loss: 0.0031108027324080467\n",
            "step: 470, loss: 3.680684312712401e-05\n",
            "step: 480, loss: 0.0028766789473593235\n",
            "step: 490, loss: 0.0007081442163325846\n",
            "step: 500, loss: 0.0028623396065086126\n",
            "step: 510, loss: 0.0005862120306119323\n",
            "step: 520, loss: 5.3974876209395006e-05\n",
            "step: 530, loss: 0.0005201288149692118\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.9492314857941314, f1=0.9438515081206497, best_f1=0.9438515081206497\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:34, 168.61it/s]\n",
            "load_f1 = 0.950420954162769\n",
            "real_f1 = 0.9510032664489033\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 140.21it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyGyaWAphstt"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d73f317-74a4-49bb-f320-38431c0ee9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5011301040649414\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.392741322517395\n",
            "step: 20, loss: 0.44149646162986755\n",
            "step: 30, loss: 0.28895673155784607\n",
            "step: 40, loss: 0.3257209360599518\n",
            "step: 50, loss: 0.4405916929244995\n",
            "step: 60, loss: 0.5206957459449768\n",
            "step: 70, loss: 0.30991455912590027\n",
            "step: 80, loss: 0.3594716191291809\n",
            "step: 90, loss: 0.20611999928951263\n",
            "step: 100, loss: 0.26736995577812195\n",
            "step: 110, loss: 0.28333747386932373\n",
            "step: 120, loss: 0.3862968385219574\n",
            "step: 130, loss: 0.2909175455570221\n",
            "step: 140, loss: 0.4689903259277344\n",
            "step: 150, loss: 0.3101464509963989\n",
            "step: 160, loss: 0.5036044120788574\n",
            "step: 170, loss: 0.2471337765455246\n",
            "step: 180, loss: 0.4061252176761627\n",
            "step: 190, loss: 0.6458840370178223\n",
            "step: 200, loss: 0.35336530208587646\n",
            "step: 210, loss: 0.4897250831127167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3947436511516571\n",
            "step: 10, loss: 0.19612517952919006\n",
            "step: 20, loss: 0.5242892503738403\n",
            "step: 30, loss: 0.49789705872535706\n",
            "step: 40, loss: 0.45098644495010376\n",
            "step: 50, loss: 0.23830491304397583\n",
            "step: 60, loss: 0.30653733015060425\n",
            "step: 70, loss: 0.4419209659099579\n",
            "step: 80, loss: 0.31485021114349365\n",
            "step: 90, loss: 0.377852201461792\n",
            "step: 100, loss: 0.4885789155960083\n",
            "step: 110, loss: 0.3902382552623749\n",
            "step: 120, loss: 0.23774540424346924\n",
            "step: 130, loss: 0.17385733127593994\n",
            "step: 140, loss: 0.25331830978393555\n",
            "step: 150, loss: 0.4273577332496643\n",
            "step: 160, loss: 0.17344622313976288\n",
            "step: 170, loss: 0.5708234310150146\n",
            "step: 180, loss: 0.3014583885669708\n",
            "step: 190, loss: 0.2965922951698303\n",
            "step: 200, loss: 0.14939595758914948\n",
            "step: 210, loss: 0.3289080262184143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.2167729279058362, f1=0.21613691931540344, best_f1=0.21613691931540344\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23383620381355286\n",
            "step: 10, loss: 0.2261795550584793\n",
            "step: 20, loss: 0.4571560025215149\n",
            "step: 30, loss: 0.25982901453971863\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 40, loss: 0.5110940337181091\n",
            "step: 50, loss: 0.43725740909576416\n",
            "step: 60, loss: 0.507220983505249\n",
            "step: 70, loss: 0.2237580567598343\n",
            "step: 80, loss: 0.45031020045280457\n",
            "step: 90, loss: 0.24039466679096222\n",
            "step: 100, loss: 0.39209914207458496\n",
            "step: 110, loss: 0.23125125467777252\n",
            "step: 120, loss: 0.2203129082918167\n",
            "step: 130, loss: 0.21664494276046753\n",
            "step: 140, loss: 0.3610980808734894\n",
            "step: 150, loss: 0.25611013174057007\n",
            "step: 160, loss: 0.22505584359169006\n",
            "step: 170, loss: 0.35321617126464844\n",
            "step: 180, loss: 0.29992586374282837\n",
            "step: 190, loss: 0.1664671152830124\n",
            "step: 200, loss: 0.22435739636421204\n",
            "step: 210, loss: 0.2802746891975403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.2323232323232323, f1=0.22535211267605634, best_f1=0.22535211267605634\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3234795331954956\n",
            "step: 10, loss: 0.283830851316452\n",
            "step: 20, loss: 0.27005016803741455\n",
            "step: 30, loss: 0.2255268096923828\n",
            "step: 40, loss: 0.19011689722537994\n",
            "step: 50, loss: 0.2235378473997116\n",
            "step: 60, loss: 0.4585704207420349\n",
            "step: 70, loss: 0.24064795672893524\n",
            "step: 80, loss: 0.23027303814888\n",
            "step: 90, loss: 0.3767027258872986\n",
            "step: 100, loss: 0.3445942997932434\n",
            "step: 110, loss: 0.6447015404701233\n",
            "step: 120, loss: 0.33716729283332825\n",
            "step: 130, loss: 0.6761392951011658\n",
            "step: 140, loss: 0.43267232179641724\n",
            "step: 150, loss: 0.3459579050540924\n",
            "step: 160, loss: 0.31400617957115173\n",
            "step: 170, loss: 0.21880091726779938\n",
            "step: 180, loss: 0.07834969460964203\n",
            "step: 190, loss: 0.16618020832538605\n",
            "step: 200, loss: 0.29763850569725037\n",
            "step: 210, loss: 0.44044047594070435\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.2531089978054133, f1=0.262992125984252, best_f1=0.262992125984252\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.33618083596229553\n",
            "step: 10, loss: 0.33296382427215576\n",
            "step: 20, loss: 0.30615881085395813\n",
            "step: 30, loss: 0.2367420494556427\n",
            "step: 40, loss: 0.39903101325035095\n",
            "step: 50, loss: 0.36926132440567017\n",
            "step: 60, loss: 0.38533860445022583\n",
            "step: 70, loss: 0.25451451539993286\n",
            "step: 80, loss: 0.41172224283218384\n",
            "step: 90, loss: 0.37729862332344055\n",
            "step: 100, loss: 0.2012505829334259\n",
            "step: 110, loss: 0.1621803194284439\n",
            "step: 120, loss: 0.2631208598613739\n",
            "step: 130, loss: 0.2662941813468933\n",
            "step: 140, loss: 0.48208001255989075\n",
            "step: 150, loss: 0.2285022735595703\n",
            "step: 160, loss: 0.19549071788787842\n",
            "step: 170, loss: 0.3324127197265625\n",
            "step: 180, loss: 0.1961325854063034\n",
            "step: 190, loss: 0.3664124310016632\n",
            "step: 200, loss: 0.4489586055278778\n",
            "step: 210, loss: 0.17064017057418823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.38408304498269896, f1=0.34307992202729043, best_f1=0.34307992202729043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08751457929611206\n",
            "step: 10, loss: 0.3888103663921356\n",
            "step: 20, loss: 0.25144854187965393\n",
            "step: 30, loss: 0.26586002111434937\n",
            "step: 40, loss: 0.2622690200805664\n",
            "step: 50, loss: 0.5548990368843079\n",
            "step: 60, loss: 0.2613653242588043\n",
            "step: 70, loss: 0.3456341624259949\n",
            "step: 80, loss: 0.2763305902481079\n",
            "step: 90, loss: 0.33100780844688416\n",
            "step: 100, loss: 0.35170403122901917\n",
            "step: 110, loss: 0.3150678873062134\n",
            "step: 120, loss: 0.2049165517091751\n",
            "step: 130, loss: 0.1745574027299881\n",
            "step: 140, loss: 0.3685385286808014\n",
            "step: 150, loss: 0.1603647768497467\n",
            "step: 160, loss: 0.11768542975187302\n",
            "step: 170, loss: 0.4022179841995239\n",
            "step: 180, loss: 0.30048441886901855\n",
            "step: 190, loss: 0.23590746521949768\n",
            "step: 200, loss: 0.35030457377433777\n",
            "step: 210, loss: 0.2476889193058014\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.4045676998368678, f1=0.39542483660130723, best_f1=0.39542483660130723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3688492774963379\n",
            "step: 10, loss: 0.19004447758197784\n",
            "step: 20, loss: 0.41049832105636597\n",
            "step: 30, loss: 0.18954139947891235\n",
            "step: 40, loss: 0.2033160775899887\n",
            "step: 50, loss: 0.2901553809642792\n",
            "step: 60, loss: 0.2771437466144562\n",
            "step: 70, loss: 0.1963931769132614\n",
            "step: 80, loss: 0.34725186228752136\n",
            "step: 90, loss: 0.3558911085128784\n",
            "step: 100, loss: 0.27116233110427856\n",
            "step: 110, loss: 0.22938191890716553\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 120, loss: 0.2751947045326233\n",
            "step: 130, loss: 0.38844600319862366\n",
            "step: 140, loss: 0.2797927260398865\n",
            "step: 150, loss: 0.29025998711586\n",
            "step: 160, loss: 0.615401029586792\n",
            "step: 170, loss: 0.45735061168670654\n",
            "step: 180, loss: 0.20187394320964813\n",
            "step: 190, loss: 0.2497951090335846\n",
            "step: 200, loss: 0.24435852468013763\n",
            "step: 210, loss: 0.44579610228538513\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.45029239766081863, f1=0.45538461538461533, best_f1=0.45538461538461533\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37403199076652527\n",
            "step: 10, loss: 0.2405380755662918\n",
            "step: 20, loss: 0.15845024585723877\n",
            "step: 30, loss: 0.20497210323810577\n",
            "step: 40, loss: 0.28127503395080566\n",
            "step: 50, loss: 0.07948990166187286\n",
            "step: 60, loss: 0.09182386100292206\n",
            "step: 70, loss: 0.321272075176239\n",
            "step: 80, loss: 0.2410350739955902\n",
            "step: 90, loss: 0.35967501997947693\n",
            "step: 100, loss: 0.3704088628292084\n",
            "step: 110, loss: 0.24347876012325287\n",
            "step: 120, loss: 0.23214401304721832\n",
            "step: 130, loss: 0.05979140102863312\n",
            "step: 140, loss: 0.23603834211826324\n",
            "step: 150, loss: 0.35098740458488464\n",
            "step: 160, loss: 0.44349634647369385\n",
            "step: 170, loss: 0.43581220507621765\n",
            "step: 180, loss: 0.2000882774591446\n",
            "step: 190, loss: 0.11687412858009338\n",
            "step: 200, loss: 0.20685356855392456\n",
            "step: 210, loss: 0.3646913468837738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.466887417218543, f1=0.508361204013378, best_f1=0.508361204013378\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.36883339285850525\n",
            "step: 10, loss: 0.25110599398612976\n",
            "step: 20, loss: 0.29223254323005676\n",
            "step: 30, loss: 0.07976295799016953\n",
            "step: 40, loss: 0.09496256709098816\n",
            "step: 50, loss: 0.4225676655769348\n",
            "step: 60, loss: 0.09428815543651581\n",
            "step: 70, loss: 0.17140506207942963\n",
            "step: 80, loss: 0.10588499903678894\n",
            "step: 90, loss: 0.2696187496185303\n",
            "step: 100, loss: 0.2812052071094513\n",
            "step: 110, loss: 0.2438315898180008\n",
            "step: 120, loss: 0.31436794996261597\n",
            "step: 130, loss: 0.1373656988143921\n",
            "step: 140, loss: 0.3177257180213928\n",
            "step: 150, loss: 0.0956415981054306\n",
            "step: 160, loss: 0.2618246376514435\n",
            "step: 170, loss: 0.20299848914146423\n",
            "step: 180, loss: 0.3024667203426361\n",
            "step: 190, loss: 0.11391552537679672\n",
            "step: 200, loss: 0.14171497523784637\n",
            "step: 210, loss: 0.2173279970884323\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.5719237435008666, f1=0.6181172291296626, best_f1=0.6181172291296626\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13786953687667847\n",
            "step: 10, loss: 0.2045954316854477\n",
            "step: 20, loss: 0.11155945062637329\n",
            "step: 30, loss: 0.08461833000183105\n",
            "step: 40, loss: 0.15843479335308075\n",
            "step: 50, loss: 0.19439958035945892\n",
            "step: 60, loss: 0.1139703020453453\n",
            "step: 70, loss: 0.16714514791965485\n",
            "step: 80, loss: 0.16494764387607574\n",
            "step: 90, loss: 0.15443657338619232\n",
            "step: 100, loss: 0.32333070039749146\n",
            "step: 110, loss: 0.05766941234469414\n",
            "step: 120, loss: 0.462240993976593\n",
            "step: 130, loss: 0.09302252531051636\n",
            "step: 140, loss: 0.14762389659881592\n",
            "step: 150, loss: 0.15600039064884186\n",
            "step: 160, loss: 0.08614084869623184\n",
            "step: 170, loss: 0.05591551959514618\n",
            "step: 180, loss: 0.13042345643043518\n",
            "step: 190, loss: 0.11125023663043976\n",
            "step: 200, loss: 0.3179999589920044\n",
            "step: 210, loss: 0.1456880420446396\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.5862785862785863, f1=0.5882352941176471, best_f1=0.5882352941176471\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10467777401208878\n",
            "step: 10, loss: 0.18448664247989655\n",
            "step: 20, loss: 0.0687481015920639\n",
            "step: 30, loss: 0.13422542810440063\n",
            "step: 40, loss: 0.1814388930797577\n",
            "step: 50, loss: 0.37483009696006775\n",
            "step: 60, loss: 0.16461892426013947\n",
            "step: 70, loss: 0.10320284962654114\n",
            "step: 80, loss: 0.2301318198442459\n",
            "step: 90, loss: 0.239568293094635\n",
            "step: 100, loss: 0.19552625715732574\n",
            "step: 110, loss: 0.39231643080711365\n",
            "step: 120, loss: 0.2061842679977417\n",
            "step: 130, loss: 0.09703681617975235\n",
            "step: 140, loss: 0.1345282346010208\n",
            "step: 150, loss: 0.17693912982940674\n",
            "step: 160, loss: 0.03431306034326553\n",
            "step: 170, loss: 0.14454330503940582\n",
            "step: 180, loss: 0.1649475246667862\n",
            "step: 190, loss: 0.2987404465675354\n",
            "step: 200, loss: 0.06870967894792557\n",
            "step: 210, loss: 0.1805436760187149\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.5870841487279843, f1=0.6015037593984963, best_f1=0.6015037593984963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09990900754928589\n",
            "step: 10, loss: 0.14214661717414856\n",
            "step: 20, loss: 0.2508689761161804\n",
            "step: 30, loss: 0.10374996811151505\n",
            "step: 40, loss: 0.06879756599664688\n",
            "step: 50, loss: 0.1834484487771988\n",
            "step: 60, loss: 0.09959220886230469\n",
            "step: 70, loss: 0.22803714871406555\n",
            "step: 80, loss: 0.11473318934440613\n",
            "step: 90, loss: 0.26551735401153564\n",
            "step: 100, loss: 0.038478605449199677\n",
            "step: 110, loss: 0.18449756503105164\n",
            "step: 120, loss: 0.037998978048563004\n",
            "step: 130, loss: 0.13889454305171967\n",
            "step: 140, loss: 0.3440394997596741\n",
            "step: 150, loss: 0.02511814795434475\n",
            "step: 160, loss: 0.10579463839530945\n",
            "step: 170, loss: 0.23210792243480682\n",
            "step: 180, loss: 0.10918058454990387\n",
            "step: 190, loss: 0.11919084936380386\n",
            "step: 200, loss: 0.05314529314637184\n",
            "step: 210, loss: 0.08226174116134644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5852713178294574, f1=0.6132812500000001, best_f1=0.6015037593984963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08774582296609879\n",
            "step: 10, loss: 0.03120950050652027\n",
            "step: 20, loss: 0.06438732892274857\n",
            "step: 30, loss: 0.07687344402074814\n",
            "step: 40, loss: 0.1701233983039856\n",
            "step: 50, loss: 0.1011376827955246\n",
            "step: 60, loss: 0.22070841491222382\n",
            "step: 70, loss: 0.0580952949821949\n",
            "step: 80, loss: 0.21306493878364563\n",
            "step: 90, loss: 0.22786280512809753\n",
            "step: 100, loss: 0.11100469529628754\n",
            "step: 110, loss: 0.1383911371231079\n",
            "step: 120, loss: 0.09449122101068497\n",
            "step: 130, loss: 0.0392831452190876\n",
            "step: 140, loss: 0.1503104716539383\n",
            "step: 150, loss: 0.14400233328342438\n",
            "step: 160, loss: 0.12205982953310013\n",
            "step: 170, loss: 0.0670977458357811\n",
            "step: 180, loss: 0.035241831094026566\n",
            "step: 190, loss: 0.07642853260040283\n",
            "step: 200, loss: 0.0930481106042862\n",
            "step: 210, loss: 0.12365309149026871\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.600375234521576, f1=0.6303939962476548, best_f1=0.6303939962476548\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01580742560327053\n",
            "step: 10, loss: 0.06905801594257355\n",
            "step: 20, loss: 0.16898073256015778\n",
            "step: 30, loss: 0.02300153486430645\n",
            "step: 40, loss: 0.12355843186378479\n",
            "step: 50, loss: 0.07078752666711807\n",
            "step: 60, loss: 0.19675908982753754\n",
            "step: 70, loss: 0.13316892087459564\n",
            "step: 80, loss: 0.09116436541080475\n",
            "step: 90, loss: 0.03927777707576752\n",
            "step: 100, loss: 0.03548962622880936\n",
            "step: 110, loss: 0.17129945755004883\n",
            "step: 120, loss: 0.03974289819598198\n",
            "step: 130, loss: 0.05046622455120087\n",
            "step: 140, loss: 0.11438176035881042\n",
            "step: 150, loss: 0.13149720430374146\n",
            "step: 160, loss: 0.09750247001647949\n",
            "step: 170, loss: 0.0924677774310112\n",
            "step: 180, loss: 0.05589789152145386\n",
            "step: 190, loss: 0.13836334645748138\n",
            "step: 200, loss: 0.05225968733429909\n",
            "step: 210, loss: 0.03476965054869652\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5967078189300411, f1=0.6237424547283703, best_f1=0.6303939962476548\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07128214836120605\n",
            "step: 10, loss: 0.08281781524419785\n",
            "step: 20, loss: 0.17198599874973297\n",
            "step: 30, loss: 0.1060311421751976\n",
            "step: 40, loss: 0.20753273367881775\n",
            "step: 50, loss: 0.12983380258083344\n",
            "step: 60, loss: 0.21059304475784302\n",
            "step: 70, loss: 0.08508757501840591\n",
            "step: 80, loss: 0.27973201870918274\n",
            "step: 90, loss: 0.05300929397344589\n",
            "step: 100, loss: 0.08735425770282745\n",
            "step: 110, loss: 0.11046550422906876\n",
            "step: 120, loss: 0.2321690171957016\n",
            "step: 130, loss: 0.10764671117067337\n",
            "step: 140, loss: 0.09022337198257446\n",
            "step: 150, loss: 0.25942301750183105\n",
            "step: 160, loss: 0.10832173377275467\n",
            "step: 170, loss: 0.11957346647977829\n",
            "step: 180, loss: 0.07166190445423126\n",
            "step: 190, loss: 0.19786356389522552\n",
            "step: 200, loss: 0.026024116203188896\n",
            "step: 210, loss: 0.24228531122207642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.6065573770491803, f1=0.6372745490981965, best_f1=0.6372745490981965\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:09, 231.59it/s]\n",
            "load_f1 = 0.5735294117647058\n",
            "real_f1 = 0.5650557620817844\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 140.16it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL0eWrGYhstu"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e93c7b-568d-40ce-d400-9cd31f218794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.46744653582572937\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.41393136978149414\n",
            "step: 20, loss: 0.25977447628974915\n",
            "step: 30, loss: 0.3676232397556305\n",
            "step: 40, loss: 0.24466176331043243\n",
            "step: 50, loss: 0.30821937322616577\n",
            "step: 60, loss: 0.4532616138458252\n",
            "step: 70, loss: 0.47150784730911255\n",
            "step: 80, loss: 0.13873308897018433\n",
            "step: 90, loss: 0.31394392251968384\n",
            "step: 100, loss: 0.4249928295612335\n",
            "step: 110, loss: 0.23392236232757568\n",
            "step: 120, loss: 0.2964828312397003\n",
            "step: 130, loss: 0.33533498644828796\n",
            "step: 140, loss: 0.17481926083564758\n",
            "step: 150, loss: 0.3305782675743103\n",
            "step: 160, loss: 0.2325257658958435\n",
            "step: 170, loss: 0.37359631061553955\n",
            "step: 180, loss: 0.17324253916740417\n",
            "step: 190, loss: 0.1706366240978241\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38239696621894836\n",
            "step: 10, loss: 0.31537097692489624\n",
            "step: 20, loss: 0.6689420342445374\n",
            "step: 30, loss: 0.2597464323043823\n",
            "step: 40, loss: 0.5669799447059631\n",
            "step: 50, loss: 0.32314878702163696\n",
            "step: 60, loss: 0.4528667628765106\n",
            "step: 70, loss: 0.31035879254341125\n",
            "step: 80, loss: 0.17141291499137878\n",
            "step: 90, loss: 0.30480527877807617\n",
            "step: 100, loss: 0.24539199471473694\n",
            "step: 110, loss: 0.3952009379863739\n",
            "step: 120, loss: 0.23327261209487915\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 130, loss: 0.5534840822219849\n",
            "step: 140, loss: 0.31851014494895935\n",
            "step: 150, loss: 0.3012409508228302\n",
            "step: 160, loss: 0.3137798607349396\n",
            "step: 170, loss: 0.2438904345035553\n",
            "step: 180, loss: 0.17774373292922974\n",
            "step: 190, loss: 0.23183034360408783\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38025811314582825\n",
            "step: 10, loss: 0.3890708088874817\n",
            "step: 20, loss: 0.4776931405067444\n",
            "step: 30, loss: 0.3104410469532013\n",
            "step: 40, loss: 0.08333691209554672\n",
            "step: 50, loss: 0.37867921590805054\n",
            "step: 60, loss: 0.16261635720729828\n",
            "step: 70, loss: 0.3801961839199066\n",
            "step: 80, loss: 0.3306877017021179\n",
            "step: 90, loss: 0.3767678439617157\n",
            "step: 100, loss: 0.527411699295044\n",
            "step: 110, loss: 0.6669667959213257\n",
            "step: 120, loss: 0.3767663538455963\n",
            "step: 130, loss: 0.15590117871761322\n",
            "step: 140, loss: 0.37651512026786804\n",
            "step: 150, loss: 0.31058168411254883\n",
            "step: 160, loss: 0.6298076510429382\n",
            "step: 170, loss: 0.44800782203674316\n",
            "step: 180, loss: 0.37996795773506165\n",
            "step: 190, loss: 0.16028639674186707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23376372456550598\n",
            "step: 10, loss: 0.23352785408496857\n",
            "step: 20, loss: 0.31099066138267517\n",
            "step: 30, loss: 0.24782051146030426\n",
            "step: 40, loss: 0.5425788760185242\n",
            "step: 50, loss: 0.24466556310653687\n",
            "step: 60, loss: 0.37936243414878845\n",
            "step: 70, loss: 0.31194254755973816\n",
            "step: 80, loss: 0.24940112233161926\n",
            "step: 90, loss: 0.17628227174282074\n",
            "step: 100, loss: 0.31003254652023315\n",
            "step: 110, loss: 0.3755688965320587\n",
            "step: 120, loss: 0.2490304559469223\n",
            "step: 130, loss: 0.45998165011405945\n",
            "step: 140, loss: 0.3106916844844818\n",
            "step: 150, loss: 0.24649158120155334\n",
            "step: 160, loss: 0.30391302704811096\n",
            "step: 170, loss: 0.4316120743751526\n",
            "step: 180, loss: 0.3806852102279663\n",
            "step: 190, loss: 0.1607212871313095\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4027289152145386\n",
            "step: 10, loss: 0.36649665236473083\n",
            "step: 20, loss: 0.1578369289636612\n",
            "step: 30, loss: 0.1199699267745018\n",
            "step: 40, loss: 0.30706366896629333\n",
            "step: 50, loss: 0.5077413320541382\n",
            "step: 60, loss: 0.23619024455547333\n",
            "step: 70, loss: 0.39276638627052307\n",
            "step: 80, loss: 0.34698718786239624\n",
            "step: 90, loss: 0.2742856740951538\n",
            "step: 100, loss: 0.444031685590744\n",
            "step: 110, loss: 0.40527695417404175\n",
            "step: 120, loss: 0.24129436910152435\n",
            "step: 130, loss: 0.6094657182693481\n",
            "step: 140, loss: 0.3455657660961151\n",
            "step: 150, loss: 0.2717099189758301\n",
            "step: 160, loss: 0.15522736310958862\n",
            "step: 170, loss: 0.37911415100097656\n",
            "step: 180, loss: 0.23969294130802155\n",
            "step: 190, loss: 0.2879285514354706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.2017353579175705, f1=0.20501635768811338, best_f1=0.20501635768811338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2872099280357361\n",
            "step: 10, loss: 0.23756049573421478\n",
            "step: 20, loss: 0.2761126458644867\n",
            "step: 30, loss: 0.4468994140625\n",
            "step: 40, loss: 0.2827906310558319\n",
            "step: 50, loss: 0.29179349541664124\n",
            "step: 60, loss: 0.42119839787483215\n",
            "step: 70, loss: 0.29643967747688293\n",
            "step: 80, loss: 0.29227694869041443\n",
            "step: 90, loss: 0.21512582898139954\n",
            "step: 100, loss: 0.45278191566467285\n",
            "step: 110, loss: 0.22981467843055725\n",
            "step: 120, loss: 0.4254300594329834\n",
            "step: 130, loss: 0.5757023692131042\n",
            "step: 140, loss: 0.17315351963043213\n",
            "step: 150, loss: 0.3722694218158722\n",
            "step: 160, loss: 0.37452346086502075\n",
            "step: 170, loss: 0.36564064025878906\n",
            "step: 180, loss: 0.17888039350509644\n",
            "step: 190, loss: 0.31043198704719543\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.2017353579175705, f1=0.20501635768811338, best_f1=0.20501635768811338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30092722177505493\n",
            "step: 10, loss: 0.2813752293586731\n",
            "step: 20, loss: 0.2788507342338562\n",
            "step: 30, loss: 0.20131905376911163\n",
            "step: 40, loss: 0.2905016541481018\n",
            "step: 50, loss: 0.08591541647911072\n",
            "step: 60, loss: 0.1461159735918045\n",
            "step: 70, loss: 0.14952099323272705\n",
            "step: 80, loss: 0.23136359453201294\n",
            "step: 90, loss: 0.23028238117694855\n",
            "step: 100, loss: 0.5298022627830505\n",
            "step: 110, loss: 0.4274841845035553\n",
            "step: 120, loss: 0.35672444105148315\n",
            "step: 130, loss: 0.28357505798339844\n",
            "step: 140, loss: 0.2195817232131958\n",
            "step: 150, loss: 0.27832096815109253\n",
            "step: 160, loss: 0.3650111258029938\n",
            "step: 170, loss: 0.28259459137916565\n",
            "step: 180, loss: 0.21702182292938232\n",
            "step: 190, loss: 0.3105620741844177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.2029621503017005, f1=0.2056384742951907, best_f1=0.2056384742951907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22685867547988892\n",
            "step: 10, loss: 0.3555240333080292\n",
            "step: 20, loss: 0.221824049949646\n",
            "step: 30, loss: 0.36390185356140137\n",
            "step: 40, loss: 0.15906177461147308\n",
            "step: 50, loss: 0.29196685552597046\n",
            "step: 60, loss: 0.42839837074279785\n",
            "step: 70, loss: 0.21633818745613098\n",
            "step: 80, loss: 0.4009239673614502\n",
            "step: 90, loss: 0.2200249582529068\n",
            "step: 100, loss: 0.2693406343460083\n",
            "step: 110, loss: 0.36315906047821045\n",
            "step: 120, loss: 0.36866435408592224\n",
            "step: 130, loss: 0.3012605905532837\n",
            "step: 140, loss: 0.33360370993614197\n",
            "step: 150, loss: 0.2926342487335205\n",
            "step: 160, loss: 0.17460954189300537\n",
            "step: 170, loss: 0.22806166112422943\n",
            "step: 180, loss: 0.3103128671646118\n",
            "step: 190, loss: 0.2760572135448456\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.20162601626016258, f1=0.20501635768811338, best_f1=0.2056384742951907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24644383788108826\n",
            "step: 10, loss: 0.09800436347723007\n",
            "step: 20, loss: 0.29374784231185913\n",
            "step: 30, loss: 0.16731765866279602\n",
            "step: 40, loss: 0.30122143030166626\n",
            "step: 50, loss: 0.33798521757125854\n",
            "step: 60, loss: 0.3677380681037903\n",
            "step: 70, loss: 0.12806157767772675\n",
            "step: 80, loss: 0.3235834836959839\n",
            "step: 90, loss: 0.8222252726554871\n",
            "step: 100, loss: 0.3465018570423126\n",
            "step: 110, loss: 0.3338718116283417\n",
            "step: 120, loss: 0.578298032283783\n",
            "step: 130, loss: 0.28159716725349426\n",
            "step: 140, loss: 0.30895373225212097\n",
            "step: 150, loss: 0.23366256058216095\n",
            "step: 160, loss: 0.21232223510742188\n",
            "step: 170, loss: 0.4940967559814453\n",
            "step: 180, loss: 0.36249613761901855\n",
            "step: 190, loss: 0.20113274455070496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.24234354194407456, f1=0.19491525423728812, best_f1=0.19491525423728812\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2453760802745819\n",
            "step: 10, loss: 0.15017685294151306\n",
            "step: 20, loss: 0.28897014260292053\n",
            "step: 30, loss: 0.38524681329727173\n",
            "step: 40, loss: 0.27775973081588745\n",
            "step: 50, loss: 0.09142208844423294\n",
            "step: 60, loss: 0.2741999328136444\n",
            "step: 70, loss: 0.31017962098121643\n",
            "step: 80, loss: 0.41155514121055603\n",
            "step: 90, loss: 0.14896339178085327\n",
            "step: 100, loss: 0.18468479812145233\n",
            "step: 110, loss: 0.19563178718090057\n",
            "step: 120, loss: 0.3230625092983246\n",
            "step: 130, loss: 0.4750067889690399\n",
            "step: 140, loss: 0.3583833575248718\n",
            "step: 150, loss: 0.27423375844955444\n",
            "step: 160, loss: 0.21206310391426086\n",
            "step: 170, loss: 0.3056667447090149\n",
            "step: 180, loss: 0.20627905428409576\n",
            "step: 190, loss: 0.15944594144821167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.2657580919931857, f1=0.20598006644518274, best_f1=0.20598006644518274\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4604913890361786\n",
            "step: 10, loss: 0.09380017220973969\n",
            "step: 20, loss: 0.2005940079689026\n",
            "step: 30, loss: 0.18292009830474854\n",
            "step: 40, loss: 0.07256033271551132\n",
            "step: 50, loss: 0.312153697013855\n",
            "step: 60, loss: 0.2067435085773468\n",
            "step: 70, loss: 0.47050729393959045\n",
            "step: 80, loss: 0.3610112965106964\n",
            "step: 90, loss: 0.42439836263656616\n",
            "step: 100, loss: 0.17076539993286133\n",
            "step: 110, loss: 0.24628740549087524\n",
            "step: 120, loss: 0.33702558279037476\n",
            "step: 130, loss: 0.5681467056274414\n",
            "step: 140, loss: 0.46418312191963196\n",
            "step: 150, loss: 0.2654694616794586\n",
            "step: 160, loss: 0.2544904351234436\n",
            "step: 170, loss: 0.39883479475975037\n",
            "step: 180, loss: 0.319094181060791\n",
            "step: 190, loss: 0.10977345705032349\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.25846153846153846, f1=0.25079702444208285, best_f1=0.20598006644518274\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3498089909553528\n",
            "step: 10, loss: 0.24813570082187653\n",
            "step: 20, loss: 0.4925702214241028\n",
            "step: 30, loss: 0.3208407461643219\n",
            "step: 40, loss: 0.23202453553676605\n",
            "step: 50, loss: 0.45447826385498047\n",
            "step: 60, loss: 0.4253377914428711\n",
            "step: 70, loss: 0.3371959328651428\n",
            "step: 80, loss: 0.3228009343147278\n",
            "step: 90, loss: 0.43550023436546326\n",
            "step: 100, loss: 0.25968289375305176\n",
            "step: 110, loss: 0.4515277147293091\n",
            "step: 120, loss: 0.15655383467674255\n",
            "step: 130, loss: 0.2324681431055069\n",
            "step: 140, loss: 0.32042551040649414\n",
            "step: 150, loss: 0.3492547571659088\n",
            "step: 160, loss: 0.24204151332378387\n",
            "step: 170, loss: 0.35076603293418884\n",
            "step: 180, loss: 0.3196199834346771\n",
            "step: 190, loss: 0.14935480058193207\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.2656072644721907, f1=0.2541567695961995, best_f1=0.20598006644518274\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.27782583236694336\n",
            "step: 10, loss: 0.347313791513443\n",
            "step: 20, loss: 0.32525473833084106\n",
            "step: 30, loss: 0.3682195544242859\n",
            "step: 40, loss: 0.21411700546741486\n",
            "step: 50, loss: 0.21650265157222748\n",
            "step: 60, loss: 0.2569584846496582\n",
            "step: 70, loss: 0.1524791419506073\n",
            "step: 80, loss: 0.2685492932796478\n",
            "step: 90, loss: 0.272438645362854\n",
            "step: 100, loss: 0.14256127178668976\n",
            "step: 110, loss: 0.528336226940155\n",
            "step: 120, loss: 0.19958478212356567\n",
            "step: 130, loss: 0.17496339976787567\n",
            "step: 140, loss: 0.27240216732025146\n",
            "step: 150, loss: 0.33877161145210266\n",
            "step: 160, loss: 0.3804394602775574\n",
            "step: 170, loss: 0.10166409611701965\n",
            "step: 180, loss: 0.22060847282409668\n",
            "step: 190, loss: 0.32296356558799744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.27920227920227925, f1=0.24891461649782923, best_f1=0.24891461649782923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26578831672668457\n",
            "step: 10, loss: 0.1982097327709198\n",
            "step: 20, loss: 0.36865630745887756\n",
            "step: 30, loss: 0.10827303677797318\n",
            "step: 40, loss: 0.27096009254455566\n",
            "step: 50, loss: 0.308322548866272\n",
            "step: 60, loss: 0.2330688238143921\n",
            "step: 70, loss: 0.20983897149562836\n",
            "step: 80, loss: 0.3938664197921753\n",
            "step: 90, loss: 0.18716861307621002\n",
            "step: 100, loss: 0.2383958250284195\n",
            "step: 110, loss: 0.4100790023803711\n",
            "step: 120, loss: 0.2410387247800827\n",
            "step: 130, loss: 0.3500058054924011\n",
            "step: 140, loss: 0.17449474334716797\n",
            "step: 150, loss: 0.32893791794776917\n",
            "step: 160, loss: 0.29642435908317566\n",
            "step: 170, loss: 0.19511587917804718\n",
            "step: 180, loss: 0.25832709670066833\n",
            "step: 190, loss: 0.5141288042068481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.2807825086306099, f1=0.26265060240963856, best_f1=0.26265060240963856\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5362208485603333\n",
            "step: 10, loss: 0.19796651601791382\n",
            "step: 20, loss: 0.2753017544746399\n",
            "step: 30, loss: 0.2832903563976288\n",
            "step: 40, loss: 0.5505320429801941\n",
            "step: 50, loss: 0.29221537709236145\n",
            "step: 60, loss: 0.20564286410808563\n",
            "step: 70, loss: 0.19459837675094604\n",
            "step: 80, loss: 0.18262706696987152\n",
            "step: 90, loss: 0.29281437397003174\n",
            "step: 100, loss: 0.2622181177139282\n",
            "step: 110, loss: 0.36915138363838196\n",
            "step: 120, loss: 0.19272767007350922\n",
            "step: 130, loss: 0.26419246196746826\n",
            "step: 140, loss: 0.4637525975704193\n",
            "step: 150, loss: 0.13247817754745483\n",
            "step: 160, loss: 0.2482409030199051\n",
            "step: 170, loss: 0.7005040645599365\n",
            "step: 180, loss: 0.2418641448020935\n",
            "step: 190, loss: 0.3466905355453491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.2936416184971098, f1=0.2908224076281287, best_f1=0.2908224076281287\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:13, 149.93it/s]\n",
            "load_f1 = 0.28918322295805743\n",
            "real_f1 = 0.28540305010893247\n",
            "733it [00:00, 3275.51it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 136.41it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW6LV4zMhstv"
      },
      "source": [
        "## DITTO TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3an30TrShstv"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6e48c2-7dc5-49cc-a804-49e08b41d733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4819668233394623\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5082944631576538\n",
            "step: 20, loss: 0.3304861783981323\n",
            "step: 30, loss: 0.42148470878601074\n",
            "step: 40, loss: 0.5410178899765015\n",
            "step: 50, loss: 0.35009777545928955\n",
            "step: 60, loss: 0.5632655620574951\n",
            "step: 70, loss: 0.2953180968761444\n",
            "step: 80, loss: 0.19879402220249176\n",
            "step: 90, loss: 0.21649880707263947\n",
            "step: 100, loss: 0.15872855484485626\n",
            "step: 110, loss: 0.37564817070961\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.31783533096313477\n",
            "step: 130, loss: 0.31265515089035034\n",
            "step: 140, loss: 0.3940965533256531\n",
            "step: 150, loss: 0.3190653324127197\n",
            "step: 160, loss: 0.3712363541126251\n",
            "step: 170, loss: 0.3440922498703003\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3314468562602997\n",
            "step: 10, loss: 0.47133761644363403\n",
            "step: 20, loss: 0.3233257532119751\n",
            "step: 30, loss: 0.2798701524734497\n",
            "step: 40, loss: 0.09524879604578018\n",
            "step: 50, loss: 0.44891276955604553\n",
            "step: 60, loss: 0.18540668487548828\n",
            "step: 70, loss: 0.5001113414764404\n",
            "step: 80, loss: 0.2408686727285385\n",
            "step: 90, loss: 0.24753208458423615\n",
            "step: 100, loss: 0.512532651424408\n",
            "step: 110, loss: 0.25416994094848633\n",
            "step: 120, loss: 0.2505027949810028\n",
            "step: 130, loss: 0.5412604808807373\n",
            "step: 140, loss: 0.5239184498786926\n",
            "step: 150, loss: 0.43593356013298035\n",
            "step: 160, loss: 0.44778504967689514\n",
            "step: 170, loss: 0.37630361318588257\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6081399321556091\n",
            "step: 10, loss: 0.31332048773765564\n",
            "step: 20, loss: 0.2318788468837738\n",
            "step: 30, loss: 0.24745610356330872\n",
            "step: 40, loss: 0.3763751685619354\n",
            "step: 50, loss: 0.5780563950538635\n",
            "step: 60, loss: 0.31425729393959045\n",
            "step: 70, loss: 0.23585766553878784\n",
            "step: 80, loss: 0.3740309178829193\n",
            "step: 90, loss: 0.552177369594574\n",
            "step: 100, loss: 0.32403674721717834\n",
            "step: 110, loss: 0.16322743892669678\n",
            "step: 120, loss: 0.5928425788879395\n",
            "step: 130, loss: 0.5054849982261658\n",
            "step: 140, loss: 0.42938876152038574\n",
            "step: 150, loss: 0.2059735655784607\n",
            "step: 160, loss: 0.16996492445468903\n",
            "step: 170, loss: 0.3166041076183319\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3674633204936981\n",
            "step: 10, loss: 0.6028569936752319\n",
            "step: 20, loss: 0.2490580976009369\n",
            "step: 30, loss: 0.45613110065460205\n",
            "step: 40, loss: 0.24251244962215424\n",
            "step: 50, loss: 0.38030916452407837\n",
            "step: 60, loss: 0.7366475462913513\n",
            "step: 70, loss: 0.3214321732521057\n",
            "step: 80, loss: 0.45975515246391296\n",
            "step: 90, loss: 0.3121461272239685\n",
            "step: 100, loss: 0.3625066578388214\n",
            "step: 110, loss: 0.447975218296051\n",
            "step: 120, loss: 0.4912855327129364\n",
            "step: 130, loss: 0.30797773599624634\n",
            "step: 140, loss: 0.23604382574558258\n",
            "step: 150, loss: 0.6961967945098877\n",
            "step: 160, loss: 0.12500008940696716\n",
            "step: 170, loss: 0.2464105188846588\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3931814432144165\n",
            "step: 10, loss: 0.3147989809513092\n",
            "step: 20, loss: 0.3333466649055481\n",
            "step: 30, loss: 0.310776948928833\n",
            "step: 40, loss: 0.253919392824173\n",
            "step: 50, loss: 0.23728299140930176\n",
            "step: 60, loss: 0.3133068382740021\n",
            "step: 70, loss: 0.3101464509963989\n",
            "step: 80, loss: 0.07884672284126282\n",
            "step: 90, loss: 0.6183477640151978\n",
            "step: 100, loss: 0.2681829035282135\n",
            "step: 110, loss: 0.25486114621162415\n",
            "step: 120, loss: 0.1255919635295868\n",
            "step: 130, loss: 0.2410847395658493\n",
            "step: 140, loss: 0.16936388611793518\n",
            "step: 150, loss: 0.3069116175174713\n",
            "step: 160, loss: 0.23809736967086792\n",
            "step: 170, loss: 0.3669643700122833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3179009258747101\n",
            "step: 10, loss: 0.10681069642305374\n",
            "step: 20, loss: 0.381687194108963\n",
            "step: 30, loss: 0.24339887499809265\n",
            "step: 40, loss: 0.44543471932411194\n",
            "step: 50, loss: 0.2569812536239624\n",
            "step: 60, loss: 0.38779428601264954\n",
            "step: 70, loss: 0.39678141474723816\n",
            "step: 80, loss: 0.18842223286628723\n",
            "step: 90, loss: 0.31540292501449585\n",
            "step: 100, loss: 0.19178038835525513\n",
            "step: 110, loss: 0.45050811767578125\n",
            "step: 120, loss: 0.4456748962402344\n",
            "step: 130, loss: 0.4828118085861206\n",
            "step: 140, loss: 0.2544768154621124\n",
            "step: 150, loss: 0.17581911385059357\n",
            "step: 160, loss: 0.3107203543186188\n",
            "step: 170, loss: 0.23623913526535034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2447480708360672\n",
            "step: 10, loss: 0.4435291886329651\n",
            "step: 20, loss: 0.4532339870929718\n",
            "step: 30, loss: 0.514240562915802\n",
            "step: 40, loss: 0.12903255224227905\n",
            "step: 50, loss: 0.43289923667907715\n",
            "step: 60, loss: 0.558664858341217\n",
            "step: 70, loss: 0.37872567772865295\n",
            "step: 80, loss: 0.17515024542808533\n",
            "step: 90, loss: 0.45585405826568604\n",
            "step: 100, loss: 0.3081521987915039\n",
            "step: 110, loss: 0.38208356499671936\n",
            "step: 120, loss: 0.37299999594688416\n",
            "step: 130, loss: 0.19414512813091278\n",
            "step: 140, loss: 0.12601812183856964\n",
            "step: 150, loss: 0.31465792655944824\n",
            "step: 160, loss: 0.3841829001903534\n",
            "step: 170, loss: 0.3137539327144623\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.447349488735199\n",
            "step: 10, loss: 0.5051223039627075\n",
            "step: 20, loss: 0.24735093116760254\n",
            "step: 30, loss: 0.3120020031929016\n",
            "step: 40, loss: 0.2424827218055725\n",
            "step: 50, loss: 0.37476977705955505\n",
            "step: 60, loss: 0.3814496695995331\n",
            "step: 70, loss: 0.18193098902702332\n",
            "step: 80, loss: 0.3104762136936188\n",
            "step: 90, loss: 0.5742186307907104\n",
            "step: 100, loss: 0.253265380859375\n",
            "step: 110, loss: 0.5122968554496765\n",
            "step: 120, loss: 0.32040438055992126\n",
            "step: 130, loss: 0.31347212195396423\n",
            "step: 140, loss: 0.5019693970680237\n",
            "step: 150, loss: 0.2414027899503708\n",
            "step: 160, loss: 0.18288975954055786\n",
            "step: 170, loss: 0.45979106426239014\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37930554151535034\n",
            "step: 10, loss: 0.5826504230499268\n",
            "step: 20, loss: 0.3790084421634674\n",
            "step: 30, loss: 0.1805368959903717\n",
            "step: 40, loss: 0.38380858302116394\n",
            "step: 50, loss: 0.24983318150043488\n",
            "step: 60, loss: 0.3730914294719696\n",
            "step: 70, loss: 0.31065788865089417\n",
            "step: 80, loss: 0.1648465096950531\n",
            "step: 90, loss: 0.45446157455444336\n",
            "step: 100, loss: 0.5010538101196289\n",
            "step: 110, loss: 0.31822115182876587\n",
            "step: 120, loss: 0.44052034616470337\n",
            "step: 130, loss: 0.31158146262168884\n",
            "step: 140, loss: 0.37526077032089233\n",
            "step: 150, loss: 0.3732950985431671\n",
            "step: 160, loss: 0.24712713062763214\n",
            "step: 170, loss: 0.43425655364990234\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.44435080885887146\n",
            "step: 10, loss: 0.31531238555908203\n",
            "step: 20, loss: 0.25172165036201477\n",
            "step: 30, loss: 0.6407586336135864\n",
            "step: 40, loss: 0.3712623715400696\n",
            "step: 50, loss: 0.443731427192688\n",
            "step: 60, loss: 0.5730729699134827\n",
            "step: 70, loss: 0.2654813528060913\n",
            "step: 80, loss: 0.2542800307273865\n",
            "step: 90, loss: 0.23903034627437592\n",
            "step: 100, loss: 0.2377023547887802\n",
            "step: 110, loss: 0.3041459918022156\n",
            "step: 120, loss: 0.30799150466918945\n",
            "step: 130, loss: 0.17484726011753082\n",
            "step: 140, loss: 0.24546360969543457\n",
            "step: 150, loss: 0.31602928042411804\n",
            "step: 160, loss: 0.24097774922847748\n",
            "step: 170, loss: 0.4888654947280884\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3793162405490875\n",
            "step: 10, loss: 0.3176558017730713\n",
            "step: 20, loss: 0.1736985445022583\n",
            "step: 30, loss: 0.5055941343307495\n",
            "step: 40, loss: 0.24216288328170776\n",
            "step: 50, loss: 0.24468481540679932\n",
            "step: 60, loss: 0.3983828127384186\n",
            "step: 70, loss: 0.3196139931678772\n",
            "step: 80, loss: 0.17879533767700195\n",
            "step: 90, loss: 0.37948334217071533\n",
            "step: 100, loss: 0.5108690857887268\n",
            "step: 110, loss: 0.3669043779373169\n",
            "step: 120, loss: 0.45705172419548035\n",
            "step: 130, loss: 0.376505047082901\n",
            "step: 140, loss: 0.5008715987205505\n",
            "step: 150, loss: 0.4360989034175873\n",
            "step: 160, loss: 0.25095081329345703\n",
            "step: 170, loss: 0.18192699551582336\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.22626262626262628, f1=0.2318244170096022, best_f1=0.2318244170096022\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38385653495788574\n",
            "step: 10, loss: 0.29523035883903503\n",
            "step: 20, loss: 0.23748143017292023\n",
            "step: 30, loss: 0.2796154022216797\n",
            "step: 40, loss: 0.3042805790901184\n",
            "step: 50, loss: 0.3594854772090912\n",
            "step: 60, loss: 0.2933516800403595\n",
            "step: 70, loss: 0.16907097399234772\n",
            "step: 80, loss: 0.24585965275764465\n",
            "step: 90, loss: 0.3148726224899292\n",
            "step: 100, loss: 0.25942057371139526\n",
            "step: 110, loss: 0.4284327030181885\n",
            "step: 120, loss: 0.3041262626647949\n",
            "step: 130, loss: 0.37313926219940186\n",
            "step: 140, loss: 0.2752819359302521\n",
            "step: 150, loss: 0.28540900349617004\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 160, loss: 0.7509641647338867\n",
            "step: 170, loss: 0.3405455946922302\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.3059210526315789, f1=0.255663430420712, best_f1=0.255663430420712\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3146274983882904\n",
            "step: 10, loss: 0.19059337675571442\n",
            "step: 20, loss: 0.2270294576883316\n",
            "step: 30, loss: 0.44813472032546997\n",
            "step: 40, loss: 0.37493929266929626\n",
            "step: 50, loss: 0.1626380980014801\n",
            "step: 60, loss: 0.22191083431243896\n",
            "step: 70, loss: 0.31418463587760925\n",
            "step: 80, loss: 0.18422985076904297\n",
            "step: 90, loss: 0.2683088779449463\n",
            "step: 100, loss: 0.13755623996257782\n",
            "step: 110, loss: 0.41883352398872375\n",
            "step: 120, loss: 0.10955733805894852\n",
            "step: 130, loss: 0.12098664790391922\n",
            "step: 140, loss: 0.18640165030956268\n",
            "step: 150, loss: 0.3809284269809723\n",
            "step: 160, loss: 0.058205198496580124\n",
            "step: 170, loss: 0.08377908915281296\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.7162790697674418, f1=0.7500000000000001, best_f1=0.7500000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14539743959903717\n",
            "step: 10, loss: 0.2357078343629837\n",
            "step: 20, loss: 0.1644669622182846\n",
            "step: 30, loss: 0.08253846317529678\n",
            "step: 40, loss: 0.1283937692642212\n",
            "step: 50, loss: 0.09513119608163834\n",
            "step: 60, loss: 0.31934434175491333\n",
            "step: 70, loss: 0.15614654123783112\n",
            "step: 80, loss: 0.07233323156833649\n",
            "step: 90, loss: 0.07217639684677124\n",
            "step: 100, loss: 0.5204509496688843\n",
            "step: 110, loss: 0.11073817312717438\n",
            "step: 120, loss: 0.19626015424728394\n",
            "step: 130, loss: 0.20211289823055267\n",
            "step: 140, loss: 0.06622423976659775\n",
            "step: 150, loss: 0.11221908032894135\n",
            "step: 160, loss: 0.18441013991832733\n",
            "step: 170, loss: 0.16089729964733124\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.7632183908045976, f1=0.7845804988662132, best_f1=0.7845804988662132\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024508623406291008\n",
            "step: 10, loss: 0.13092878460884094\n",
            "step: 20, loss: 0.22634457051753998\n",
            "step: 30, loss: 0.2273322343826294\n",
            "step: 40, loss: 0.03623137250542641\n",
            "step: 50, loss: 0.054500512778759\n",
            "step: 60, loss: 0.06729590892791748\n",
            "step: 70, loss: 0.12521544098854065\n",
            "step: 80, loss: 0.063688725233078\n",
            "step: 90, loss: 0.05040152370929718\n",
            "step: 100, loss: 0.2782520055770874\n",
            "step: 110, loss: 0.21613483130931854\n",
            "step: 120, loss: 0.06258600205183029\n",
            "step: 130, loss: 0.3470067083835602\n",
            "step: 140, loss: 0.10307107865810394\n",
            "step: 150, loss: 0.01976454257965088\n",
            "step: 160, loss: 0.06190973520278931\n",
            "step: 170, loss: 0.046319734305143356\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.7704485488126649, f1=0.7846153846153846, best_f1=0.7846153846153846\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 225.14it/s]\n",
            "load_f1 = 0.6128133704735377\n",
            "real_f1 = 0.6293333333333333\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 137.08it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VngEb4vfhstw"
      },
      "source": [
        "## DITTO DIRTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfPaCqR4hstw"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ea8f80-f334-4da5-94af-a8d8f09dea52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 423kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 672kB/s] \n",
            "Downloading: 100% 456k/456k [00:04<00:00, 101kB/s]\n",
            "Downloading: 100% 501M/501M [00:06<00:00, 72.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5610620975494385\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.430954247713089\n",
            "step: 20, loss: 0.5140743851661682\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.27542760968208313\n",
            "step: 40, loss: 0.34273990988731384\n",
            "step: 50, loss: 0.5246987342834473\n",
            "step: 60, loss: 0.06645911186933517\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 70, loss: 0.21811118721961975\n",
            "step: 80, loss: 0.048100706189870834\n",
            "step: 90, loss: 0.3035450279712677\n",
            "step: 100, loss: 0.17456784844398499\n",
            "step: 110, loss: 0.1295962780714035\n",
            "step: 120, loss: 0.08706557750701904\n",
            "step: 130, loss: 0.0767948105931282\n",
            "step: 140, loss: 0.006550635676831007\n",
            "step: 150, loss: 0.20525068044662476\n",
            "step: 160, loss: 0.03511791303753853\n",
            "step: 170, loss: 0.13923417031764984\n",
            "step: 180, loss: 0.10847140848636627\n",
            "step: 190, loss: 0.01363984402269125\n",
            "step: 200, loss: 0.025564901530742645\n",
            "step: 210, loss: 0.03127725049853325\n",
            "step: 220, loss: 0.13989976048469543\n",
            "step: 230, loss: 0.005458390340209007\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9546961325966851, f1=0.9623893805309734, best_f1=0.9623893805309734\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009883681312203407\n",
            "step: 10, loss: 0.26959946751594543\n",
            "step: 20, loss: 0.011233880184590816\n",
            "step: 30, loss: 0.03317427262663841\n",
            "step: 40, loss: 0.03388640657067299\n",
            "step: 50, loss: 0.00903208926320076\n",
            "step: 60, loss: 0.006625481881201267\n",
            "step: 70, loss: 0.01289411075413227\n",
            "step: 80, loss: 0.010382524691522121\n",
            "step: 90, loss: 0.039094820618629456\n",
            "step: 100, loss: 0.021518534049391747\n",
            "step: 110, loss: 0.028021566569805145\n",
            "step: 120, loss: 0.005125239957123995\n",
            "step: 130, loss: 0.02388060837984085\n",
            "step: 140, loss: 0.006659325212240219\n",
            "step: 150, loss: 0.0650186836719513\n",
            "step: 160, loss: 0.07533200085163116\n",
            "step: 170, loss: 0.013244034722447395\n",
            "step: 180, loss: 0.052621182054281235\n",
            "step: 190, loss: 0.010918114334344864\n",
            "step: 200, loss: 0.034065138548612595\n",
            "step: 210, loss: 0.03098728321492672\n",
            "step: 220, loss: 0.003371114144101739\n",
            "step: 230, loss: 0.0017007802380248904\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9637760702524698, f1=0.9679558011049725, best_f1=0.9679558011049725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028761662542819977\n",
            "step: 10, loss: 0.022573431953787804\n",
            "step: 20, loss: 0.006911423057317734\n",
            "step: 30, loss: 0.010750651359558105\n",
            "step: 40, loss: 0.03385516628623009\n",
            "step: 50, loss: 0.01617206633090973\n",
            "step: 60, loss: 0.02716580405831337\n",
            "step: 70, loss: 0.005498893558979034\n",
            "step: 80, loss: 0.026336541399359703\n",
            "step: 90, loss: 0.0029762699268758297\n",
            "step: 100, loss: 0.012680809013545513\n",
            "step: 110, loss: 0.008458376862108707\n",
            "step: 120, loss: 0.021525094285607338\n",
            "step: 130, loss: 0.0064841099083423615\n",
            "step: 140, loss: 0.0030771519523113966\n",
            "step: 150, loss: 0.09086822718381882\n",
            "step: 160, loss: 0.012653230689466\n",
            "step: 170, loss: 0.009600980207324028\n",
            "step: 180, loss: 0.010466977022588253\n",
            "step: 190, loss: 0.012852521613240242\n",
            "step: 200, loss: 0.008492397144436836\n",
            "step: 210, loss: 0.0018939871806651354\n",
            "step: 220, loss: 0.013575333170592785\n",
            "step: 230, loss: 0.051920171827077866\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9753363228699552, f1=0.9617977528089887, best_f1=0.9617977528089887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009167445823550224\n",
            "step: 10, loss: 0.00860707089304924\n",
            "step: 20, loss: 0.006111018359661102\n",
            "step: 30, loss: 0.007533949799835682\n",
            "step: 40, loss: 0.06200440600514412\n",
            "step: 50, loss: 0.010554123669862747\n",
            "step: 60, loss: 0.003150197444483638\n",
            "step: 70, loss: 0.020466027781367302\n",
            "step: 80, loss: 0.10160302370786667\n",
            "step: 90, loss: 0.06416833400726318\n",
            "step: 100, loss: 0.012288463301956654\n",
            "step: 110, loss: 0.002292619552463293\n",
            "step: 120, loss: 0.025816919282078743\n",
            "step: 130, loss: 0.011011059395968914\n",
            "step: 140, loss: 0.02912019193172455\n",
            "step: 150, loss: 0.003469303948804736\n",
            "step: 160, loss: 0.0006252505118027329\n",
            "step: 170, loss: 0.011907643638551235\n",
            "step: 180, loss: 0.0596812441945076\n",
            "step: 190, loss: 0.001914511201903224\n",
            "step: 200, loss: 0.013030214235186577\n",
            "step: 210, loss: 0.11550282686948776\n",
            "step: 220, loss: 0.0016103253001347184\n",
            "step: 230, loss: 0.005221317522227764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9689578713968958, f1=0.967525195968645, best_f1=0.9617977528089887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00248138140887022\n",
            "step: 10, loss: 0.004228959791362286\n",
            "step: 20, loss: 0.012646044604480267\n",
            "step: 30, loss: 0.0031105480156838894\n",
            "step: 40, loss: 0.011697201989591122\n",
            "step: 50, loss: 0.021570872515439987\n",
            "step: 60, loss: 0.011643658392131329\n",
            "step: 70, loss: 0.01727566309273243\n",
            "step: 80, loss: 0.014355787076056004\n",
            "step: 90, loss: 0.009413894265890121\n",
            "step: 100, loss: 0.0007136146887205541\n",
            "step: 110, loss: 0.0034565322566777468\n",
            "step: 120, loss: 0.003632000647485256\n",
            "step: 130, loss: 0.0013433399144560099\n",
            "step: 140, loss: 0.018822042271494865\n",
            "step: 150, loss: 0.07542476803064346\n",
            "step: 160, loss: 0.000380150624550879\n",
            "step: 170, loss: 0.001750395866110921\n",
            "step: 180, loss: 0.10941356420516968\n",
            "step: 190, loss: 0.031077103689312935\n",
            "step: 200, loss: 0.003914902918040752\n",
            "step: 210, loss: 0.022839553654193878\n",
            "step: 220, loss: 0.0014277135487645864\n",
            "step: 230, loss: 0.07173743844032288\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.976271186440678, f1=0.9751693002257337, best_f1=0.9751693002257337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008597150444984436\n",
            "step: 10, loss: 0.027455730363726616\n",
            "step: 20, loss: 0.0017150429775938392\n",
            "step: 30, loss: 0.0026862621307373047\n",
            "step: 40, loss: 0.0006637066835537553\n",
            "step: 50, loss: 0.002925640670582652\n",
            "step: 60, loss: 0.03803754225373268\n",
            "step: 70, loss: 0.192081481218338\n",
            "step: 80, loss: 0.006686212494969368\n",
            "step: 90, loss: 0.018755868077278137\n",
            "step: 100, loss: 0.0015840906416997313\n",
            "step: 110, loss: 0.012051678262650967\n",
            "step: 120, loss: 0.0008821133524179459\n",
            "step: 130, loss: 0.0011275879805907607\n",
            "step: 140, loss: 0.00046008708886802197\n",
            "step: 150, loss: 0.00053820526227355\n",
            "step: 160, loss: 0.005966716445982456\n",
            "step: 170, loss: 0.021253066137433052\n",
            "step: 180, loss: 0.06793218851089478\n",
            "step: 190, loss: 0.0004688084591180086\n",
            "step: 200, loss: 0.013268179260194302\n",
            "step: 210, loss: 0.000663464074023068\n",
            "step: 220, loss: 0.04953497648239136\n",
            "step: 230, loss: 0.001853675814345479\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9709821428571428, f1=0.9740698985343857, best_f1=0.9751693002257337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005058820825070143\n",
            "step: 10, loss: 0.0013696375535801053\n",
            "step: 20, loss: 0.000572866469155997\n",
            "step: 30, loss: 0.0008055999642238021\n",
            "step: 40, loss: 0.001710567157715559\n",
            "step: 50, loss: 0.0019143999088555574\n",
            "step: 60, loss: 0.0013126203557476401\n",
            "step: 70, loss: 0.0005321292555890977\n",
            "step: 80, loss: 0.000470704777399078\n",
            "step: 90, loss: 0.006954679731279612\n",
            "step: 100, loss: 0.00034037212026305497\n",
            "step: 110, loss: 0.00067072146339342\n",
            "step: 120, loss: 0.00035483177634887397\n",
            "step: 130, loss: 0.003233653726056218\n",
            "step: 140, loss: 0.00023893119941931218\n",
            "step: 150, loss: 0.0015384237049147487\n",
            "step: 160, loss: 0.00021198684407863766\n",
            "step: 170, loss: 0.0011032733600586653\n",
            "step: 180, loss: 0.0002168829960282892\n",
            "step: 190, loss: 0.08527717739343643\n",
            "step: 200, loss: 0.030803393572568893\n",
            "step: 210, loss: 0.02996818721294403\n",
            "step: 220, loss: 0.0012909990036860108\n",
            "step: 230, loss: 0.002407104941084981\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9720670391061451, f1=0.9684684684684683, best_f1=0.9751693002257337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019083119696006179\n",
            "step: 10, loss: 0.011442933231592178\n",
            "step: 20, loss: 0.00035093995393253863\n",
            "step: 30, loss: 0.0006636119796894491\n",
            "step: 40, loss: 0.0015998382586985826\n",
            "step: 50, loss: 0.016620084643363953\n",
            "step: 60, loss: 0.0022901156917214394\n",
            "step: 70, loss: 0.000736708811018616\n",
            "step: 80, loss: 0.007640888914465904\n",
            "step: 90, loss: 0.002356412820518017\n",
            "step: 100, loss: 0.0009092765394598246\n",
            "step: 110, loss: 0.0023375260643661022\n",
            "step: 120, loss: 0.009217026643455029\n",
            "step: 130, loss: 0.00026962615083903074\n",
            "step: 140, loss: 0.00018458743579685688\n",
            "step: 150, loss: 0.07012981176376343\n",
            "step: 160, loss: 0.012737962417304516\n",
            "step: 170, loss: 0.11354441195726395\n",
            "step: 180, loss: 0.0008248916128650308\n",
            "step: 190, loss: 0.008957911282777786\n",
            "step: 200, loss: 0.018164262175559998\n",
            "step: 210, loss: 0.001118397107347846\n",
            "step: 220, loss: 0.00047508778516203165\n",
            "step: 230, loss: 0.0008991092327050865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9775784753363228, f1=0.9784824462061155, best_f1=0.9784824462061155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024224340450018644\n",
            "step: 10, loss: 0.0007750260992906988\n",
            "step: 20, loss: 0.0010711231734603643\n",
            "step: 30, loss: 0.001130552962422371\n",
            "step: 40, loss: 0.008777972310781479\n",
            "step: 50, loss: 0.0009526588837616146\n",
            "step: 60, loss: 0.0007475381135009229\n",
            "step: 70, loss: 0.04424620792269707\n",
            "step: 80, loss: 0.0001903298543766141\n",
            "step: 90, loss: 0.08566946536302567\n",
            "step: 100, loss: 0.006038610357791185\n",
            "step: 110, loss: 0.00024799746461212635\n",
            "step: 120, loss: 0.028649328276515007\n",
            "step: 130, loss: 0.0021375189535319805\n",
            "step: 140, loss: 0.0008021958637982607\n",
            "step: 150, loss: 0.004731275606900454\n",
            "step: 160, loss: 0.0027860745321959257\n",
            "step: 170, loss: 0.0013942563673481345\n",
            "step: 180, loss: 0.00044317994615994394\n",
            "step: 190, loss: 0.0003594313748180866\n",
            "step: 200, loss: 0.00036564661422744393\n",
            "step: 210, loss: 0.01574915461242199\n",
            "step: 220, loss: 0.0003698163782246411\n",
            "step: 230, loss: 0.00026272499235346913\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9752252252252253, f1=0.9738933030646991, best_f1=0.9784824462061155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004297657578717917\n",
            "step: 10, loss: 0.0033568809740245342\n",
            "step: 20, loss: 0.000752479478251189\n",
            "step: 30, loss: 0.0004975594347342849\n",
            "step: 40, loss: 0.02425774559378624\n",
            "step: 50, loss: 0.00036142338649369776\n",
            "step: 60, loss: 0.00027617596788331866\n",
            "step: 70, loss: 0.0021170219406485558\n",
            "step: 80, loss: 0.00012107406655559316\n",
            "step: 90, loss: 0.00012068015348631889\n",
            "step: 100, loss: 0.00014518265379592776\n",
            "step: 110, loss: 0.026753762736916542\n",
            "step: 120, loss: 0.00027204910293221474\n",
            "step: 130, loss: 0.0002875843201763928\n",
            "step: 140, loss: 0.00014250972890295088\n",
            "step: 150, loss: 0.0005662366165779531\n",
            "step: 160, loss: 0.00010397216829005629\n",
            "step: 170, loss: 0.0030775039922446012\n",
            "step: 180, loss: 0.00133711623493582\n",
            "step: 190, loss: 0.00029288590303622186\n",
            "step: 200, loss: 0.0012057459680363536\n",
            "step: 210, loss: 0.004700841847807169\n",
            "step: 220, loss: 0.004272400867193937\n",
            "step: 230, loss: 0.00018113819533027709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9817767653758542, f1=0.9690011481056257, best_f1=0.9690011481056257\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004364659253042191\n",
            "step: 10, loss: 0.0002896225778385997\n",
            "step: 20, loss: 0.0002254844584967941\n",
            "step: 30, loss: 0.0005304139340296388\n",
            "step: 40, loss: 0.0001547715364722535\n",
            "step: 50, loss: 0.007203148677945137\n",
            "step: 60, loss: 0.013541481457650661\n",
            "step: 70, loss: 0.001161237945780158\n",
            "step: 80, loss: 0.0034501194022595882\n",
            "step: 90, loss: 0.012618507258594036\n",
            "step: 100, loss: 0.00023890180455055088\n",
            "step: 110, loss: 0.0006282022222876549\n",
            "step: 120, loss: 0.0010055545717477798\n",
            "step: 130, loss: 0.00014293658023234457\n",
            "step: 140, loss: 0.004720584023743868\n",
            "step: 150, loss: 0.00019334148964844644\n",
            "step: 160, loss: 0.04035200551152229\n",
            "step: 170, loss: 0.0012933072866871953\n",
            "step: 180, loss: 0.0005134472739882767\n",
            "step: 190, loss: 0.0005691364640370011\n",
            "step: 200, loss: 0.0037928682286292315\n",
            "step: 210, loss: 0.0002680057077668607\n",
            "step: 220, loss: 0.000510798767209053\n",
            "step: 230, loss: 0.00028956690221093595\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9831271091113611, f1=0.9750566893424036, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00033870048355311155\n",
            "step: 10, loss: 0.0004946136032231152\n",
            "step: 20, loss: 0.010825594887137413\n",
            "step: 30, loss: 0.06545629352331161\n",
            "step: 40, loss: 0.0004730010114144534\n",
            "step: 50, loss: 0.0030867597088217735\n",
            "step: 60, loss: 0.0007034590817056596\n",
            "step: 70, loss: 0.00017479373491369188\n",
            "step: 80, loss: 0.0001265356841031462\n",
            "step: 90, loss: 0.0014521133853122592\n",
            "step: 100, loss: 0.00012815797526855022\n",
            "step: 110, loss: 0.00010350280354032293\n",
            "step: 120, loss: 0.002696906216442585\n",
            "step: 130, loss: 0.00011330096458550543\n",
            "step: 140, loss: 0.00012980549945496023\n",
            "step: 150, loss: 0.0001646235614316538\n",
            "step: 160, loss: 0.00014700007159262896\n",
            "step: 170, loss: 0.00018517229182180017\n",
            "step: 180, loss: 9.231795411324129e-05\n",
            "step: 190, loss: 0.0008928753086365759\n",
            "step: 200, loss: 0.00011964762234129012\n",
            "step: 210, loss: 0.005921445321291685\n",
            "step: 220, loss: 0.00036377841024659574\n",
            "step: 230, loss: 0.0007195041398517787\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9841628959276018, f1=0.9749430523917996, best_f1=0.9749430523917996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004243809264153242\n",
            "step: 10, loss: 0.00016452366253361106\n",
            "step: 20, loss: 0.00015531173266936094\n",
            "step: 30, loss: 0.001015752786770463\n",
            "step: 40, loss: 0.00013373589899856597\n",
            "step: 50, loss: 0.00033020315458998084\n",
            "step: 60, loss: 0.00010803047916851938\n",
            "step: 70, loss: 0.0008047367446124554\n",
            "step: 80, loss: 0.0006542385672219098\n",
            "step: 90, loss: 9.116355067817494e-05\n",
            "step: 100, loss: 0.00016961197252385318\n",
            "step: 110, loss: 0.003379751928150654\n",
            "step: 120, loss: 0.00010454290168127045\n",
            "step: 130, loss: 0.00011855840421048924\n",
            "step: 140, loss: 0.00010076074977405369\n",
            "step: 150, loss: 0.0005737023311667144\n",
            "step: 160, loss: 0.00013950855645816773\n",
            "step: 170, loss: 8.273531420854852e-05\n",
            "step: 180, loss: 0.04683757945895195\n",
            "step: 190, loss: 8.795916073722765e-05\n",
            "step: 200, loss: 5.223148764343932e-05\n",
            "step: 210, loss: 0.00016607956786174327\n",
            "step: 220, loss: 7.97935645096004e-05\n",
            "step: 230, loss: 0.00010575849591987208\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9841986455981941, f1=0.9785310734463276, best_f1=0.9785310734463276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011736356827896088\n",
            "step: 10, loss: 9.103344928007573e-05\n",
            "step: 20, loss: 8.352949225809425e-05\n",
            "step: 30, loss: 9.497864812146872e-05\n",
            "step: 40, loss: 9.239062637789175e-05\n",
            "step: 50, loss: 4.48972059530206e-05\n",
            "step: 60, loss: 6.753925117664039e-05\n",
            "step: 70, loss: 0.00028997453046031296\n",
            "step: 80, loss: 5.137399784871377e-05\n",
            "step: 90, loss: 0.00014331469719763845\n",
            "step: 100, loss: 8.05492527433671e-05\n",
            "step: 110, loss: 0.0014053593622520566\n",
            "step: 120, loss: 3.3890788472490385e-05\n",
            "step: 130, loss: 0.0001535168703412637\n",
            "step: 140, loss: 0.00021973319235257804\n",
            "step: 150, loss: 3.215459219063632e-05\n",
            "step: 160, loss: 0.00011851742601720616\n",
            "step: 170, loss: 0.00024528210633434355\n",
            "step: 180, loss: 0.00024913009838201106\n",
            "step: 190, loss: 0.00013868419046048075\n",
            "step: 200, loss: 0.0004474601009860635\n",
            "step: 210, loss: 8.985819295048714e-05\n",
            "step: 220, loss: 7.854992873035371e-05\n",
            "step: 230, loss: 0.0002837258798535913\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9842342342342343, f1=0.9796839729119639, best_f1=0.9796839729119639\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04046402499079704\n",
            "step: 10, loss: 0.00039500423008576035\n",
            "step: 20, loss: 0.00011858048674184829\n",
            "step: 30, loss: 8.439599332632497e-05\n",
            "step: 40, loss: 5.3956013289280236e-05\n",
            "step: 50, loss: 0.0001027741891448386\n",
            "step: 60, loss: 0.02552446722984314\n",
            "step: 70, loss: 0.0028094169683754444\n",
            "step: 80, loss: 7.434383587678894e-05\n",
            "step: 90, loss: 5.535249874810688e-05\n",
            "step: 100, loss: 5.914177017984912e-05\n",
            "step: 110, loss: 0.000974498107098043\n",
            "step: 120, loss: 0.02513054944574833\n",
            "step: 130, loss: 0.0031423273030668497\n",
            "step: 140, loss: 0.0003189142735209316\n",
            "step: 150, loss: 0.00021254913008306175\n",
            "step: 160, loss: 0.01932465471327305\n",
            "step: 170, loss: 0.000480469228932634\n",
            "step: 180, loss: 8.337973849847913e-05\n",
            "step: 190, loss: 0.00022076955065131187\n",
            "step: 200, loss: 0.0004012783756479621\n",
            "step: 210, loss: 0.03154318407177925\n",
            "step: 220, loss: 0.00011053606431232765\n",
            "step: 230, loss: 0.0001550772722112015\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.9853768278965129, f1=0.9796839729119639, best_f1=0.9796839729119639\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:16, 154.06it/s]\n",
            "load_f1 = 0.983050847457627\n",
            "real_f1 = 0.9830890642615557\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 142.48it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY0y_yZuhstx"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cedc9caf-e507-4a0b-84ee-eaa364a539aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6149146556854248\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.37048614025115967\n",
            "step: 20, loss: 0.32321396470069885\n",
            "step: 30, loss: 0.41160696744918823\n",
            "step: 40, loss: 0.36790943145751953\n",
            "step: 50, loss: 0.5762134790420532\n",
            "step: 60, loss: 0.4012279212474823\n",
            "step: 70, loss: 0.5705980658531189\n",
            "step: 80, loss: 0.454135924577713\n",
            "step: 90, loss: 0.21020159125328064\n",
            "step: 100, loss: 0.3319016695022583\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 110, loss: 0.13558802008628845\n",
            "step: 120, loss: 0.3617953658103943\n",
            "step: 130, loss: 0.15167774260044098\n",
            "step: 140, loss: 0.3159269392490387\n",
            "step: 150, loss: 0.17115367949008942\n",
            "step: 160, loss: 0.26486438512802124\n",
            "step: 170, loss: 0.07483869791030884\n",
            "step: 180, loss: 0.06923886388540268\n",
            "step: 190, loss: 0.0974944531917572\n",
            "step: 200, loss: 0.12998680770397186\n",
            "step: 210, loss: 0.0395134799182415\n",
            "step: 220, loss: 0.1047617644071579\n",
            "step: 230, loss: 0.15806081891059875\n",
            "step: 240, loss: 0.05936376377940178\n",
            "step: 250, loss: 0.03925534710288048\n",
            "step: 260, loss: 0.3248765170574188\n",
            "step: 270, loss: 0.16548578441143036\n",
            "step: 280, loss: 0.07729741185903549\n",
            "step: 290, loss: 0.15058167278766632\n",
            "step: 300, loss: 0.03681175783276558\n",
            "step: 310, loss: 0.12422551214694977\n",
            "step: 320, loss: 0.0649518221616745\n",
            "step: 330, loss: 0.047720205038785934\n",
            "step: 340, loss: 0.4042102098464966\n",
            "step: 350, loss: 0.10581620782613754\n",
            "step: 360, loss: 0.027014387771487236\n",
            "step: 370, loss: 0.03210932016372681\n",
            "step: 380, loss: 0.1830001324415207\n",
            "step: 390, loss: 0.025337237864732742\n",
            "step: 400, loss: 0.02359476312994957\n",
            "step: 410, loss: 0.24995043873786926\n",
            "step: 420, loss: 0.029510021209716797\n",
            "step: 430, loss: 0.027528945356607437\n",
            "step: 440, loss: 0.07609762996435165\n",
            "step: 450, loss: 0.03810938447713852\n",
            "step: 460, loss: 0.07773394137620926\n",
            "step: 470, loss: 0.04110444337129593\n",
            "step: 480, loss: 0.10374816507101059\n",
            "step: 490, loss: 0.32428961992263794\n",
            "step: 500, loss: 0.017511039972305298\n",
            "step: 510, loss: 0.07266008108854294\n",
            "step: 520, loss: 0.0851893201470375\n",
            "step: 530, loss: 0.013893461786210537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.933579335793358, f1=0.9348729792147806, best_f1=0.9348729792147806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09996750205755234\n",
            "step: 10, loss: 0.06461764127016068\n",
            "step: 20, loss: 0.021678607910871506\n",
            "step: 30, loss: 0.18688975274562836\n",
            "step: 40, loss: 0.10867141932249069\n",
            "step: 50, loss: 0.04241019859910011\n",
            "step: 60, loss: 0.03359553962945938\n",
            "step: 70, loss: 0.03615613281726837\n",
            "step: 80, loss: 0.007973221130669117\n",
            "step: 90, loss: 0.024246517568826675\n",
            "step: 100, loss: 0.08023350685834885\n",
            "step: 110, loss: 0.006417888216674328\n",
            "step: 120, loss: 0.04259994626045227\n",
            "step: 130, loss: 0.006736373528838158\n",
            "step: 140, loss: 0.04530206322669983\n",
            "step: 150, loss: 0.026899639517068863\n",
            "step: 160, loss: 0.025298859924077988\n",
            "step: 170, loss: 0.026738498359918594\n",
            "step: 180, loss: 0.02534114569425583\n",
            "step: 190, loss: 0.07827972620725632\n",
            "step: 200, loss: 0.26674774289131165\n",
            "step: 210, loss: 0.04109738767147064\n",
            "step: 220, loss: 0.0054700965993106365\n",
            "step: 230, loss: 0.10526023060083389\n",
            "step: 240, loss: 0.005850617308169603\n",
            "step: 250, loss: 0.11470288038253784\n",
            "step: 260, loss: 0.01770126260817051\n",
            "step: 270, loss: 0.016838116571307182\n",
            "step: 280, loss: 0.11214133352041245\n",
            "step: 290, loss: 0.06823026388883591\n",
            "step: 300, loss: 0.04357339069247246\n",
            "step: 310, loss: 0.028391582891345024\n",
            "step: 320, loss: 0.037370722740888596\n",
            "step: 330, loss: 0.037657495588064194\n",
            "step: 340, loss: 0.16256821155548096\n",
            "step: 350, loss: 0.002801097696647048\n",
            "step: 360, loss: 0.07705260813236237\n",
            "step: 370, loss: 0.026558425277471542\n",
            "step: 380, loss: 0.1757914125919342\n",
            "step: 390, loss: 0.015318808145821095\n",
            "step: 400, loss: 0.040738753974437714\n",
            "step: 410, loss: 0.055548712611198425\n",
            "step: 420, loss: 0.16375049948692322\n",
            "step: 430, loss: 0.13564617931842804\n",
            "step: 440, loss: 0.030113479122519493\n",
            "step: 450, loss: 0.11272721737623215\n",
            "step: 460, loss: 0.0333683155477047\n",
            "step: 470, loss: 0.02816881239414215\n",
            "step: 480, loss: 0.01914713904261589\n",
            "step: 490, loss: 0.07025297731161118\n",
            "step: 500, loss: 0.008007139898836613\n",
            "step: 510, loss: 0.046219754964113235\n",
            "step: 520, loss: 0.23035667836666107\n",
            "step: 530, loss: 0.11469404399394989\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9458082445576655, f1=0.9385113268608415, best_f1=0.9385113268608415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11040682345628738\n",
            "step: 10, loss: 0.036583442240953445\n",
            "step: 20, loss: 0.017104320228099823\n",
            "step: 30, loss: 0.03004266507923603\n",
            "step: 40, loss: 0.28167402744293213\n",
            "step: 50, loss: 0.004746992141008377\n",
            "step: 60, loss: 0.010507967323064804\n",
            "step: 70, loss: 0.022001156583428383\n",
            "step: 80, loss: 0.21277636289596558\n",
            "step: 90, loss: 0.0810089111328125\n",
            "step: 100, loss: 0.029980046674609184\n",
            "step: 110, loss: 0.025595763698220253\n",
            "step: 120, loss: 0.22270125150680542\n",
            "step: 130, loss: 0.07439468055963516\n",
            "step: 140, loss: 0.014060878194868565\n",
            "step: 150, loss: 0.04442329332232475\n",
            "step: 160, loss: 0.01858900859951973\n",
            "step: 170, loss: 0.007059051655232906\n",
            "step: 180, loss: 0.014272480271756649\n",
            "step: 190, loss: 0.00524531863629818\n",
            "step: 200, loss: 0.03615095093846321\n",
            "step: 210, loss: 0.06470004469156265\n",
            "step: 220, loss: 0.10624220222234726\n",
            "step: 230, loss: 0.04373599588871002\n",
            "step: 240, loss: 0.061998751014471054\n",
            "step: 250, loss: 0.08952081203460693\n",
            "step: 260, loss: 0.04343194141983986\n",
            "step: 270, loss: 0.0097477026283741\n",
            "step: 280, loss: 0.08627980947494507\n",
            "step: 290, loss: 0.032706718891859055\n",
            "step: 300, loss: 0.03737391531467438\n",
            "step: 310, loss: 0.11701931804418564\n",
            "step: 320, loss: 0.012512445449829102\n",
            "step: 330, loss: 0.005968870595097542\n",
            "step: 340, loss: 0.0100303515791893\n",
            "step: 350, loss: 0.04893994703888893\n",
            "step: 360, loss: 0.006573984865099192\n",
            "step: 370, loss: 0.09402123838663101\n",
            "step: 380, loss: 0.0031302371062338352\n",
            "step: 390, loss: 0.023933347314596176\n",
            "step: 400, loss: 0.18690523505210876\n",
            "step: 410, loss: 0.008351118303835392\n",
            "step: 420, loss: 0.012392650358378887\n",
            "step: 430, loss: 0.027126604691147804\n",
            "step: 440, loss: 0.3917285203933716\n",
            "step: 450, loss: 0.026070155203342438\n",
            "step: 460, loss: 0.0749126672744751\n",
            "step: 470, loss: 0.0384896956384182\n",
            "step: 480, loss: 0.07099904119968414\n",
            "step: 490, loss: 0.005853260867297649\n",
            "step: 500, loss: 0.08032682538032532\n",
            "step: 510, loss: 0.018879912793636322\n",
            "step: 520, loss: 0.028399556875228882\n",
            "step: 530, loss: 0.00853561982512474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9422263973696571, f1=0.9347826086956521, best_f1=0.9385113268608415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03221851587295532\n",
            "step: 10, loss: 0.015603815205395222\n",
            "step: 20, loss: 0.09595340490341187\n",
            "step: 30, loss: 0.028314698487520218\n",
            "step: 40, loss: 0.002648942871019244\n",
            "step: 50, loss: 0.07948268204927444\n",
            "step: 60, loss: 0.011493842117488384\n",
            "step: 70, loss: 0.20738717913627625\n",
            "step: 80, loss: 0.1309671550989151\n",
            "step: 90, loss: 0.047996945679187775\n",
            "step: 100, loss: 0.025402221828699112\n",
            "step: 110, loss: 0.20104491710662842\n",
            "step: 120, loss: 0.005695607513189316\n",
            "step: 130, loss: 0.015198170207440853\n",
            "step: 140, loss: 0.0605287104845047\n",
            "step: 150, loss: 0.08302759379148483\n",
            "step: 160, loss: 0.004825320560485125\n",
            "step: 170, loss: 0.018915699794888496\n",
            "step: 180, loss: 0.016765624284744263\n",
            "step: 190, loss: 0.04679407924413681\n",
            "step: 200, loss: 0.03361188620328903\n",
            "step: 210, loss: 0.0005443575792014599\n",
            "step: 220, loss: 0.0014254390262067318\n",
            "step: 230, loss: 0.01844940148293972\n",
            "step: 240, loss: 0.027611779049038887\n",
            "step: 250, loss: 0.08518551290035248\n",
            "step: 260, loss: 0.0036315557081252337\n",
            "step: 270, loss: 0.029032528400421143\n",
            "step: 280, loss: 0.002166267717257142\n",
            "step: 290, loss: 0.011370968073606491\n",
            "step: 300, loss: 0.0022070773411542177\n",
            "step: 310, loss: 0.0036215283907949924\n",
            "step: 320, loss: 0.021229926496744156\n",
            "step: 330, loss: 0.025650186464190483\n",
            "step: 340, loss: 0.004700613208115101\n",
            "step: 350, loss: 0.042695339769124985\n",
            "step: 360, loss: 0.09858176857233047\n",
            "step: 370, loss: 0.008663673885166645\n",
            "step: 380, loss: 0.006820142734795809\n",
            "step: 390, loss: 0.002255114493891597\n",
            "step: 400, loss: 0.007779676001518965\n",
            "step: 410, loss: 0.020024076104164124\n",
            "step: 420, loss: 0.012269662693142891\n",
            "step: 430, loss: 0.02971726842224598\n",
            "step: 440, loss: 0.032280001789331436\n",
            "step: 450, loss: 0.019435647875070572\n",
            "step: 460, loss: 0.0034803326707333326\n",
            "step: 470, loss: 0.0019238274544477463\n",
            "step: 480, loss: 0.03444177657365799\n",
            "step: 490, loss: 0.04418139532208443\n",
            "step: 500, loss: 0.04994890093803406\n",
            "step: 510, loss: 0.07294896990060806\n",
            "step: 520, loss: 0.011061903089284897\n",
            "step: 530, loss: 0.12113720178604126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9471715755025713, f1=0.9387186629526463, best_f1=0.9387186629526463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021317297592759132\n",
            "step: 10, loss: 0.013672389090061188\n",
            "step: 20, loss: 0.019078494980931282\n",
            "step: 30, loss: 0.0020507576409727335\n",
            "step: 40, loss: 0.0008604691247455776\n",
            "step: 50, loss: 0.00870898924767971\n",
            "step: 60, loss: 0.0053592948243021965\n",
            "step: 70, loss: 0.0016391624230891466\n",
            "step: 80, loss: 0.010825833305716515\n",
            "step: 90, loss: 0.02356080338358879\n",
            "step: 100, loss: 0.09150346368551254\n",
            "step: 110, loss: 0.04358268529176712\n",
            "step: 120, loss: 0.06851492077112198\n",
            "step: 130, loss: 0.047973498702049255\n",
            "step: 140, loss: 0.00455094687640667\n",
            "step: 150, loss: 0.0428510457277298\n",
            "step: 160, loss: 0.03860443830490112\n",
            "step: 170, loss: 0.04068596661090851\n",
            "step: 180, loss: 0.04618861898779869\n",
            "step: 190, loss: 0.006334200501441956\n",
            "step: 200, loss: 0.009361298754811287\n",
            "step: 210, loss: 0.0011321342317387462\n",
            "step: 220, loss: 0.010491423308849335\n",
            "step: 230, loss: 0.002499983413144946\n",
            "step: 240, loss: 0.041484713554382324\n",
            "step: 250, loss: 0.12533700466156006\n",
            "step: 260, loss: 0.002547109965234995\n",
            "step: 270, loss: 0.0025976577308028936\n",
            "step: 280, loss: 0.050970032811164856\n",
            "step: 290, loss: 0.0021434975787997246\n",
            "step: 300, loss: 0.08341429382562637\n",
            "step: 310, loss: 0.02646363526582718\n",
            "step: 320, loss: 0.01801822893321514\n",
            "step: 330, loss: 0.023430464789271355\n",
            "step: 340, loss: 0.033698756247758865\n",
            "step: 350, loss: 0.04587450623512268\n",
            "step: 360, loss: 0.00040978979086503386\n",
            "step: 370, loss: 0.0007854431751184165\n",
            "step: 380, loss: 0.0004597888619173318\n",
            "step: 390, loss: 0.0028764635790139437\n",
            "step: 400, loss: 0.07795874029397964\n",
            "step: 410, loss: 0.11894306540489197\n",
            "step: 420, loss: 0.11486788839101791\n",
            "step: 430, loss: 0.0025628595612943172\n",
            "step: 440, loss: 0.002677831333130598\n",
            "step: 450, loss: 0.005534104071557522\n",
            "step: 460, loss: 0.06525570899248123\n",
            "step: 470, loss: 0.047367360442876816\n",
            "step: 480, loss: 0.03288928046822548\n",
            "step: 490, loss: 0.09494378417730331\n",
            "step: 500, loss: 0.024636082351207733\n",
            "step: 510, loss: 0.007604556158185005\n",
            "step: 520, loss: 0.08811990171670914\n",
            "step: 530, loss: 0.014752744697034359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9462068965517241, f1=0.933271547729379, best_f1=0.9387186629526463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11896153539419174\n",
            "step: 10, loss: 0.00947427935898304\n",
            "step: 20, loss: 0.009938899427652359\n",
            "step: 30, loss: 0.0011242673499509692\n",
            "step: 40, loss: 0.0018598514143377542\n",
            "step: 50, loss: 0.004948865156620741\n",
            "step: 60, loss: 0.012945941649377346\n",
            "step: 70, loss: 0.0027748337015509605\n",
            "step: 80, loss: 0.07595986127853394\n",
            "step: 90, loss: 0.0020993100479245186\n",
            "step: 100, loss: 0.034506551921367645\n",
            "step: 110, loss: 0.008281100541353226\n",
            "step: 120, loss: 0.08585502952337265\n",
            "step: 130, loss: 0.018226850777864456\n",
            "step: 140, loss: 0.0038777452427893877\n",
            "step: 150, loss: 0.0013057123869657516\n",
            "step: 160, loss: 0.07705943286418915\n",
            "step: 170, loss: 0.00625836756080389\n",
            "step: 180, loss: 0.01850113645195961\n",
            "step: 190, loss: 0.314392626285553\n",
            "step: 200, loss: 0.02250088006258011\n",
            "step: 210, loss: 0.015305657871067524\n",
            "step: 220, loss: 0.0037792471703141928\n",
            "step: 230, loss: 0.014236421324312687\n",
            "step: 240, loss: 0.01829446293413639\n",
            "step: 250, loss: 0.06194530054926872\n",
            "step: 260, loss: 0.0011366767575964332\n",
            "step: 270, loss: 0.00771399075165391\n",
            "step: 280, loss: 0.0039055643137544394\n",
            "step: 290, loss: 0.005733022931963205\n",
            "step: 300, loss: 0.13112467527389526\n",
            "step: 310, loss: 0.05461474880576134\n",
            "step: 320, loss: 0.0005030310712754726\n",
            "step: 330, loss: 0.05615857616066933\n",
            "step: 340, loss: 0.0006900735315866768\n",
            "step: 350, loss: 0.0006109054083935916\n",
            "step: 360, loss: 0.20822666585445404\n",
            "step: 370, loss: 0.0049751317128539085\n",
            "step: 380, loss: 0.005973485764116049\n",
            "step: 390, loss: 0.000886106863617897\n",
            "step: 400, loss: 0.004203359130769968\n",
            "step: 410, loss: 7.298494165297598e-05\n",
            "step: 420, loss: 0.0035460288636386395\n",
            "step: 430, loss: 0.0004250782949384302\n",
            "step: 440, loss: 0.0016250911867246032\n",
            "step: 450, loss: 0.09673330187797546\n",
            "step: 460, loss: 0.0012061805464327335\n",
            "step: 470, loss: 0.0014653595862910151\n",
            "step: 480, loss: 0.0031273080967366695\n",
            "step: 490, loss: 0.032933201640844345\n",
            "step: 500, loss: 0.0037084086798131466\n",
            "step: 510, loss: 0.12587055563926697\n",
            "step: 520, loss: 0.0005671812104992568\n",
            "step: 530, loss: 0.029968073591589928\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9405255878284925, f1=0.9297197978870005, best_f1=0.9387186629526463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04969286546111107\n",
            "step: 10, loss: 0.0014475365169346333\n",
            "step: 20, loss: 0.04314536228775978\n",
            "step: 30, loss: 0.01202332228422165\n",
            "step: 40, loss: 0.007086253259330988\n",
            "step: 50, loss: 0.007627982646226883\n",
            "step: 60, loss: 0.0018362922128289938\n",
            "step: 70, loss: 0.014482775703072548\n",
            "step: 80, loss: 0.0019323044689372182\n",
            "step: 90, loss: 0.008305399678647518\n",
            "step: 100, loss: 0.0015955225098878145\n",
            "step: 110, loss: 0.000749966420698911\n",
            "step: 120, loss: 0.0037090752739459276\n",
            "step: 130, loss: 0.00011629953951342031\n",
            "step: 140, loss: 0.0022220781538635492\n",
            "step: 150, loss: 0.0013095370959490538\n",
            "step: 160, loss: 0.00018421180720906705\n",
            "step: 170, loss: 0.005179708823561668\n",
            "step: 180, loss: 0.10952174663543701\n",
            "step: 190, loss: 0.03948710113763809\n",
            "step: 200, loss: 0.001589589985087514\n",
            "step: 210, loss: 0.06165158003568649\n",
            "step: 220, loss: 0.0009356860537081957\n",
            "step: 230, loss: 0.071540467441082\n",
            "step: 240, loss: 0.017157528549432755\n",
            "step: 250, loss: 0.004004678688943386\n",
            "step: 260, loss: 0.042231447994709015\n",
            "step: 270, loss: 0.05979641526937485\n",
            "step: 280, loss: 0.00978123676031828\n",
            "step: 290, loss: 0.0008144705789163709\n",
            "step: 300, loss: 0.00025338990963064134\n",
            "step: 310, loss: 0.0014514746144413948\n",
            "step: 320, loss: 0.0184024665504694\n",
            "step: 330, loss: 0.0023921802639961243\n",
            "step: 340, loss: 0.024266688153147697\n",
            "step: 350, loss: 0.0037876025307923555\n",
            "step: 360, loss: 0.010986345820128918\n",
            "step: 370, loss: 0.10628094524145126\n",
            "step: 380, loss: 0.4395420253276825\n",
            "step: 390, loss: 0.017672734335064888\n",
            "step: 400, loss: 0.07414772361516953\n",
            "step: 410, loss: 0.009710468351840973\n",
            "step: 420, loss: 0.023204850032925606\n",
            "step: 430, loss: 0.014657208696007729\n",
            "step: 440, loss: 0.000631182745564729\n",
            "step: 450, loss: 0.0007865637307986617\n",
            "step: 460, loss: 0.0076106879860162735\n",
            "step: 470, loss: 0.14120464026927948\n",
            "step: 480, loss: 0.0023430834989994764\n",
            "step: 490, loss: 0.03811946138739586\n",
            "step: 500, loss: 0.0004244732845108956\n",
            "step: 510, loss: 0.0018391278572380543\n",
            "step: 520, loss: 0.0006144606159068644\n",
            "step: 530, loss: 0.002639038721099496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9506057781919852, f1=0.9340813464235624, best_f1=0.9340813464235624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002132877561962232\n",
            "step: 10, loss: 0.0009755277424119413\n",
            "step: 20, loss: 0.002163355238735676\n",
            "step: 30, loss: 0.004684266168624163\n",
            "step: 40, loss: 4.7670560888946056e-05\n",
            "step: 50, loss: 0.00034147006226703525\n",
            "step: 60, loss: 0.0013168886071071029\n",
            "step: 70, loss: 0.0004509449645411223\n",
            "step: 80, loss: 0.0016122572124004364\n",
            "step: 90, loss: 0.0019663905259221792\n",
            "step: 100, loss: 0.0028919668402522802\n",
            "step: 110, loss: 0.0005764457164332271\n",
            "step: 120, loss: 0.001484600710682571\n",
            "step: 130, loss: 0.0007208209717646241\n",
            "step: 140, loss: 0.0003678470675367862\n",
            "step: 150, loss: 0.010386061854660511\n",
            "step: 160, loss: 0.0005670481477864087\n",
            "step: 170, loss: 0.13340993225574493\n",
            "step: 180, loss: 0.00023885091650299728\n",
            "step: 190, loss: 0.0005311642889864743\n",
            "step: 200, loss: 0.0015890449285507202\n",
            "step: 210, loss: 0.20128972828388214\n",
            "step: 220, loss: 0.005530728958547115\n",
            "step: 230, loss: 0.034681420773267746\n",
            "step: 240, loss: 0.05187563598155975\n",
            "step: 250, loss: 0.005673292558640242\n",
            "step: 260, loss: 0.004524268209934235\n",
            "step: 270, loss: 0.0022943750955164433\n",
            "step: 280, loss: 0.0005796749610453844\n",
            "step: 290, loss: 0.026383861899375916\n",
            "step: 300, loss: 0.00018618206377141178\n",
            "step: 310, loss: 0.0014433974865823984\n",
            "step: 320, loss: 6.800903793191537e-05\n",
            "step: 330, loss: 0.00046692066825926304\n",
            "step: 340, loss: 0.005032409448176622\n",
            "step: 350, loss: 0.023281650617718697\n",
            "step: 360, loss: 0.01597648486495018\n",
            "step: 370, loss: 0.057688962668180466\n",
            "step: 380, loss: 0.00033958599669858813\n",
            "step: 390, loss: 0.004515867214649916\n",
            "step: 400, loss: 0.036615900695323944\n",
            "step: 410, loss: 0.027852389961481094\n",
            "step: 420, loss: 0.00028429957455955446\n",
            "step: 430, loss: 0.0008091387571766973\n",
            "step: 440, loss: 0.0003003249003086239\n",
            "step: 450, loss: 0.00014718469174113125\n",
            "step: 460, loss: 0.002585948444902897\n",
            "step: 470, loss: 0.05825360491871834\n",
            "step: 480, loss: 0.02887088805437088\n",
            "step: 490, loss: 0.0017492541810497642\n",
            "step: 500, loss: 0.0038616631645709276\n",
            "step: 510, loss: 0.017022114247083664\n",
            "step: 520, loss: 0.0016969849821180105\n",
            "step: 530, loss: 0.030093906447291374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9471715755025713, f1=0.9416126042632066, best_f1=0.9340813464235624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007431962061673403\n",
            "step: 10, loss: 0.006061313208192587\n",
            "step: 20, loss: 0.00021020795975346118\n",
            "step: 30, loss: 0.04775696247816086\n",
            "step: 40, loss: 5.037080336478539e-05\n",
            "step: 50, loss: 9.729204612085596e-05\n",
            "step: 60, loss: 0.0001154470955953002\n",
            "step: 70, loss: 0.015308587811887264\n",
            "step: 80, loss: 0.0005100471316836774\n",
            "step: 90, loss: 0.11156589537858963\n",
            "step: 100, loss: 0.0002471848565619439\n",
            "step: 110, loss: 0.013644066639244556\n",
            "step: 120, loss: 0.01748546212911606\n",
            "step: 130, loss: 0.0010945475660264492\n",
            "step: 140, loss: 0.0007004266954027116\n",
            "step: 150, loss: 0.001771922456100583\n",
            "step: 160, loss: 0.011550264433026314\n",
            "step: 170, loss: 0.006125576328486204\n",
            "step: 180, loss: 0.04854150861501694\n",
            "step: 190, loss: 0.026689261198043823\n",
            "step: 200, loss: 0.00029114363132975996\n",
            "step: 210, loss: 0.007602598052471876\n",
            "step: 220, loss: 0.0010491784196346998\n",
            "step: 230, loss: 9.012540976982564e-05\n",
            "step: 240, loss: 0.0025189078878611326\n",
            "step: 250, loss: 0.01014406606554985\n",
            "step: 260, loss: 0.00012184240767965093\n",
            "step: 270, loss: 0.012542981654405594\n",
            "step: 280, loss: 0.00649716192856431\n",
            "step: 290, loss: 0.0015753997722640634\n",
            "step: 300, loss: 0.0005192464450374246\n",
            "step: 310, loss: 0.026507368311285973\n",
            "step: 320, loss: 0.00031774831586517394\n",
            "step: 330, loss: 0.0010258262045681477\n",
            "step: 340, loss: 0.0045887259766459465\n",
            "step: 350, loss: 0.10749781876802444\n",
            "step: 360, loss: 0.023883331567049026\n",
            "step: 370, loss: 0.0018842399585992098\n",
            "step: 380, loss: 0.043480709195137024\n",
            "step: 390, loss: 0.00590889248996973\n",
            "step: 400, loss: 0.030372444540262222\n",
            "step: 410, loss: 0.0009592183632776141\n",
            "step: 420, loss: 0.0005239195306785405\n",
            "step: 430, loss: 0.026765871793031693\n",
            "step: 440, loss: 4.191350672044791e-05\n",
            "step: 450, loss: 0.0004383880877867341\n",
            "step: 460, loss: 9.912654059007764e-05\n",
            "step: 470, loss: 7.459695189027116e-05\n",
            "step: 480, loss: 0.000347844761563465\n",
            "step: 490, loss: 0.0009544034255668521\n",
            "step: 500, loss: 0.00265506561845541\n",
            "step: 510, loss: 0.0014572748914361\n",
            "step: 520, loss: 0.005029766354709864\n",
            "step: 530, loss: 0.03718110918998718\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.950668510834486, f1=0.9380449747590638, best_f1=0.9380449747590638\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000634077878203243\n",
            "step: 10, loss: 0.0007678703404963017\n",
            "step: 20, loss: 0.0034366301260888577\n",
            "step: 30, loss: 0.0006709540612064302\n",
            "step: 40, loss: 0.00013371629756875336\n",
            "step: 50, loss: 0.0001442816574126482\n",
            "step: 60, loss: 0.00076869415352121\n",
            "step: 70, loss: 0.0003219509089831263\n",
            "step: 80, loss: 0.0030750734731554985\n",
            "step: 90, loss: 0.0014365717070177197\n",
            "step: 100, loss: 0.000801276764832437\n",
            "step: 110, loss: 0.005925323348492384\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.19300587475299835\n",
            "step: 130, loss: 0.001928783836774528\n",
            "step: 140, loss: 0.001240733079612255\n",
            "step: 150, loss: 0.0021802030969411135\n",
            "step: 160, loss: 0.03711390495300293\n",
            "step: 170, loss: 0.0003156749880872667\n",
            "step: 180, loss: 0.010088055394589901\n",
            "step: 190, loss: 0.00019770915969274938\n",
            "step: 200, loss: 0.00036592819378711283\n",
            "step: 210, loss: 0.0007621193071827292\n",
            "step: 220, loss: 0.0009250015136785805\n",
            "step: 230, loss: 0.0012746861902996898\n",
            "step: 240, loss: 0.00023518914531450719\n",
            "step: 250, loss: 0.0014338516630232334\n",
            "step: 260, loss: 0.0012665257090702653\n",
            "step: 270, loss: 0.00034381638397462666\n",
            "step: 280, loss: 0.0014943305868655443\n",
            "step: 290, loss: 0.0005398361827246845\n",
            "step: 300, loss: 0.002645441796630621\n",
            "step: 310, loss: 0.03742494061589241\n",
            "step: 320, loss: 0.0018871568609029055\n",
            "step: 330, loss: 0.0007573999464511871\n",
            "step: 340, loss: 0.0004039780469611287\n",
            "step: 350, loss: 0.000786574266385287\n",
            "step: 360, loss: 2.9034143153694458e-05\n",
            "step: 370, loss: 0.002856515347957611\n",
            "step: 380, loss: 0.0022138357162475586\n",
            "step: 390, loss: 0.0005546686588786542\n",
            "step: 400, loss: 0.00027392100309953094\n",
            "step: 410, loss: 0.0028409603983163834\n",
            "step: 420, loss: 0.0004655096272472292\n",
            "step: 430, loss: 0.0005322038778103888\n",
            "step: 440, loss: 9.268067515222356e-05\n",
            "step: 450, loss: 0.0008085679146461189\n",
            "step: 460, loss: 0.0019620233215391636\n",
            "step: 470, loss: 0.03079114481806755\n",
            "step: 480, loss: 0.001441243221051991\n",
            "step: 490, loss: 0.0415210947394371\n",
            "step: 500, loss: 0.002275102771818638\n",
            "step: 510, loss: 0.00026693817926570773\n",
            "step: 520, loss: 0.04473176598548889\n",
            "step: 530, loss: 0.01282251626253128\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9530450953045096, f1=0.9444444444444445, best_f1=0.9444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016518362099304795\n",
            "step: 10, loss: 0.00019152104505337775\n",
            "step: 20, loss: 0.00035366031806916\n",
            "step: 30, loss: 0.0009505244088359177\n",
            "step: 40, loss: 0.00042955452227033675\n",
            "step: 50, loss: 0.0002327277761651203\n",
            "step: 60, loss: 0.003900860669091344\n",
            "step: 70, loss: 0.00014490995090454817\n",
            "step: 80, loss: 0.003489074297249317\n",
            "step: 90, loss: 0.002306508831679821\n",
            "step: 100, loss: 0.0008105012821033597\n",
            "step: 110, loss: 0.001698292326182127\n",
            "step: 120, loss: 0.005200750194489956\n",
            "step: 130, loss: 0.0007551565067842603\n",
            "step: 140, loss: 0.0010997502831742167\n",
            "step: 150, loss: 0.004254885949194431\n",
            "step: 160, loss: 0.0029742619954049587\n",
            "step: 170, loss: 0.0004929943243041635\n",
            "step: 180, loss: 0.00016519465134479105\n",
            "step: 190, loss: 0.001645370153710246\n",
            "step: 200, loss: 0.0002863496192730963\n",
            "step: 210, loss: 0.00032479967921972275\n",
            "step: 220, loss: 0.015772158280014992\n",
            "step: 230, loss: 6.316410872386768e-05\n",
            "step: 240, loss: 0.0011894999770447612\n",
            "step: 250, loss: 9.034208778757602e-05\n",
            "step: 260, loss: 0.0018747596768662333\n",
            "step: 270, loss: 0.0029922404792159796\n",
            "step: 280, loss: 0.0006958087324164808\n",
            "step: 290, loss: 0.00015087724023032933\n",
            "step: 300, loss: 0.0012666789116337895\n",
            "step: 310, loss: 0.0033642700873315334\n",
            "step: 320, loss: 0.0021870487835258245\n",
            "step: 330, loss: 3.0336659619933926e-05\n",
            "step: 340, loss: 0.0031102292705327272\n",
            "step: 350, loss: 0.0017288713715970516\n",
            "step: 360, loss: 0.0001625916047487408\n",
            "step: 370, loss: 0.0004020347259938717\n",
            "step: 380, loss: 0.00020599467097781599\n",
            "step: 390, loss: 0.0016329006757587194\n",
            "step: 400, loss: 0.014262910932302475\n",
            "step: 410, loss: 0.005766941234469414\n",
            "step: 420, loss: 0.0021485157776623964\n",
            "step: 430, loss: 0.0032361806370317936\n",
            "step: 440, loss: 0.001606883597560227\n",
            "step: 450, loss: 0.0020681985188275576\n",
            "step: 460, loss: 0.002509994665160775\n",
            "step: 470, loss: 0.0019948307890444994\n",
            "step: 480, loss: 0.00038612785283476114\n",
            "step: 490, loss: 0.0010529012652114034\n",
            "step: 500, loss: 0.0020684467162936926\n",
            "step: 510, loss: 0.12190890312194824\n",
            "step: 520, loss: 0.0015155471628531814\n",
            "step: 530, loss: 0.012204539962112904\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.951501154734411, f1=0.9410138248847926, best_f1=0.9444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016229255124926567\n",
            "step: 10, loss: 0.006343742832541466\n",
            "step: 20, loss: 0.00834632571786642\n",
            "step: 30, loss: 0.030465658754110336\n",
            "step: 40, loss: 0.0029280143789947033\n",
            "step: 50, loss: 0.04381750524044037\n",
            "step: 60, loss: 0.0018338720547035336\n",
            "step: 70, loss: 0.011594177223742008\n",
            "step: 80, loss: 0.0005921798292547464\n",
            "step: 90, loss: 0.005054776091128588\n",
            "step: 100, loss: 0.004837649408727884\n",
            "step: 110, loss: 0.0012757846852764487\n",
            "step: 120, loss: 0.0004572017933242023\n",
            "step: 130, loss: 0.006465218961238861\n",
            "step: 140, loss: 0.0008961822604760528\n",
            "step: 150, loss: 5.730338307330385e-05\n",
            "step: 160, loss: 0.0002728812978602946\n",
            "step: 170, loss: 0.00011497143714223057\n",
            "step: 180, loss: 0.0001298597053391859\n",
            "step: 190, loss: 0.000351826281985268\n",
            "step: 200, loss: 0.001874408801086247\n",
            "step: 210, loss: 7.499246567022055e-05\n",
            "step: 220, loss: 0.00011571315553737804\n",
            "step: 230, loss: 0.0007019918411970139\n",
            "step: 240, loss: 0.0031666401773691177\n",
            "step: 250, loss: 5.6634580687386915e-05\n",
            "step: 260, loss: 0.00036496814573183656\n",
            "step: 270, loss: 0.023342369124293327\n",
            "step: 280, loss: 0.00016928957484196872\n",
            "step: 290, loss: 0.00028202502289786935\n",
            "step: 300, loss: 0.0005667543155141175\n",
            "step: 310, loss: 0.003216532524675131\n",
            "step: 320, loss: 0.0001997118815779686\n",
            "step: 330, loss: 0.020161785185337067\n",
            "step: 340, loss: 0.002490856684744358\n",
            "step: 350, loss: 0.00028792256489396095\n",
            "step: 360, loss: 5.3506038966588676e-05\n",
            "step: 370, loss: 0.0019524061353877187\n",
            "step: 380, loss: 0.0004478069895412773\n",
            "step: 390, loss: 0.0026799533516168594\n",
            "step: 400, loss: 0.00022886636725161225\n",
            "step: 410, loss: 0.0013927598483860493\n",
            "step: 420, loss: 0.004086476285010576\n",
            "step: 430, loss: 0.0007713983068242669\n",
            "step: 440, loss: 0.000344862142810598\n",
            "step: 450, loss: 0.006634116172790527\n",
            "step: 460, loss: 2.3729173335595988e-05\n",
            "step: 470, loss: 0.002262897090986371\n",
            "step: 480, loss: 0.05414436757564545\n",
            "step: 490, loss: 4.322417589719407e-05\n",
            "step: 500, loss: 5.20563407917507e-05\n",
            "step: 510, loss: 0.0013697422109544277\n",
            "step: 520, loss: 0.00036458944668993354\n",
            "step: 530, loss: 0.0012088080402463675\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9526462395543175, f1=0.9365446966188051, best_f1=0.9444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014264110941439867\n",
            "step: 10, loss: 0.006087945774197578\n",
            "step: 20, loss: 0.003391778562217951\n",
            "step: 30, loss: 0.000408013176638633\n",
            "step: 40, loss: 0.000500185415148735\n",
            "step: 50, loss: 0.0012857945403084159\n",
            "step: 60, loss: 0.00027166021754965186\n",
            "step: 70, loss: 0.01954314112663269\n",
            "step: 80, loss: 0.01879965327680111\n",
            "step: 90, loss: 0.00011153166997246444\n",
            "step: 100, loss: 0.0009683422977104783\n",
            "step: 110, loss: 0.0012800248805433512\n",
            "step: 120, loss: 0.005289692431688309\n",
            "step: 130, loss: 0.0006655930774286389\n",
            "step: 140, loss: 0.00021607983217108995\n",
            "step: 150, loss: 0.001241170335561037\n",
            "step: 160, loss: 0.0025299203116446733\n",
            "step: 170, loss: 0.003245402593165636\n",
            "step: 180, loss: 0.0002187907521147281\n",
            "step: 190, loss: 0.0001551575114717707\n",
            "step: 200, loss: 0.001931200735270977\n",
            "step: 210, loss: 1.8190115952165797e-05\n",
            "step: 220, loss: 3.216946424799971e-05\n",
            "step: 230, loss: 0.0006406032480299473\n",
            "step: 240, loss: 0.000497330620419234\n",
            "step: 250, loss: 0.0003685545816551894\n",
            "step: 260, loss: 0.07824977487325668\n",
            "step: 270, loss: 0.031091617420315742\n",
            "step: 280, loss: 0.00032860456849448383\n",
            "step: 290, loss: 0.00015065597835928202\n",
            "step: 300, loss: 1.4837379239907023e-05\n",
            "step: 310, loss: 0.00020110775949433446\n",
            "step: 320, loss: 5.974302621325478e-05\n",
            "step: 330, loss: 0.0011482246918603778\n",
            "step: 340, loss: 0.004447094164788723\n",
            "step: 350, loss: 1.5932650057948194e-05\n",
            "step: 360, loss: 0.16465403139591217\n",
            "step: 370, loss: 0.00026422415976412594\n",
            "step: 380, loss: 0.00017880393716041\n",
            "step: 390, loss: 0.00025748711777850986\n",
            "step: 400, loss: 0.0002837704960256815\n",
            "step: 410, loss: 6.83379330439493e-05\n",
            "step: 420, loss: 0.0012675740290433168\n",
            "step: 430, loss: 4.947320121573284e-05\n",
            "step: 440, loss: 0.0001539126824354753\n",
            "step: 450, loss: 0.2397109717130661\n",
            "step: 460, loss: 0.050735924392938614\n",
            "step: 470, loss: 0.004090490750968456\n",
            "step: 480, loss: 6.297365325735882e-05\n",
            "step: 490, loss: 0.00036912772338837385\n",
            "step: 500, loss: 0.001392032252624631\n",
            "step: 510, loss: 0.0001809524546843022\n",
            "step: 520, loss: 0.0004521885421127081\n",
            "step: 530, loss: 0.00010989701695507392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.950943396226415, f1=0.9384687646782527, best_f1=0.9444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006767346058040857\n",
            "step: 10, loss: 0.0036711962893605232\n",
            "step: 20, loss: 0.0009905119659379125\n",
            "step: 30, loss: 0.001599328825250268\n",
            "step: 40, loss: 0.0013924466911703348\n",
            "step: 50, loss: 0.0006745065911673009\n",
            "step: 60, loss: 0.0007601968827657402\n",
            "step: 70, loss: 0.0002985617029480636\n",
            "step: 80, loss: 0.00011465429270174354\n",
            "step: 90, loss: 2.5602268578950316e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.000838143692817539\n",
            "step: 110, loss: 0.0010234435321763158\n",
            "step: 120, loss: 4.1940227674786e-05\n",
            "step: 130, loss: 0.0003602292272262275\n",
            "step: 140, loss: 0.04622070491313934\n",
            "step: 150, loss: 0.000559343839995563\n",
            "step: 160, loss: 5.9897305618505925e-05\n",
            "step: 170, loss: 0.00015161816554609686\n",
            "step: 180, loss: 8.605478797107935e-05\n",
            "step: 190, loss: 0.0001649943587835878\n",
            "step: 200, loss: 0.0001793291448848322\n",
            "step: 210, loss: 0.0001787720393622294\n",
            "step: 220, loss: 0.00034597795456647873\n",
            "step: 230, loss: 8.58730127220042e-05\n",
            "step: 240, loss: 0.0009446758776903152\n",
            "step: 250, loss: 0.0023282822221517563\n",
            "step: 260, loss: 0.0016065583331510425\n",
            "step: 270, loss: 0.034661535173654556\n",
            "step: 280, loss: 0.0013543760869652033\n",
            "step: 290, loss: 0.0003477366699371487\n",
            "step: 300, loss: 0.00038565241266041994\n",
            "step: 310, loss: 0.011614833027124405\n",
            "step: 320, loss: 0.00024471955839544535\n",
            "step: 330, loss: 0.0007168235024437308\n",
            "step: 340, loss: 0.0008755055605433881\n",
            "step: 350, loss: 0.00028111462597735226\n",
            "step: 360, loss: 8.026081195566803e-05\n",
            "step: 370, loss: 0.00026464497204869986\n",
            "step: 380, loss: 0.02519017830491066\n",
            "step: 390, loss: 0.0007336859125643969\n",
            "step: 400, loss: 0.002555044600740075\n",
            "step: 410, loss: 3.6300985811976716e-05\n",
            "step: 420, loss: 0.00011496153456391767\n",
            "step: 430, loss: 0.0006671060691587627\n",
            "step: 440, loss: 0.00025964638916775584\n",
            "step: 450, loss: 4.813914347323589e-05\n",
            "step: 460, loss: 0.0004107909044250846\n",
            "step: 470, loss: 1.1957946298934985e-05\n",
            "step: 480, loss: 0.0004260833957232535\n",
            "step: 490, loss: 0.00042598502477630973\n",
            "step: 500, loss: 0.007846334017813206\n",
            "step: 510, loss: 0.002401969628408551\n",
            "step: 520, loss: 0.00023308073286898434\n",
            "step: 530, loss: 0.0021612723357975483\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9521597770552717, f1=0.9425925925925925, best_f1=0.9444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.370756530785002e-05\n",
            "step: 10, loss: 0.00048340845387429\n",
            "step: 20, loss: 0.0014788978733122349\n",
            "step: 30, loss: 0.00030144694028422236\n",
            "step: 40, loss: 4.6220855438150465e-05\n",
            "step: 50, loss: 0.00046139484038576484\n",
            "step: 60, loss: 0.00013359445438254625\n",
            "step: 70, loss: 3.7283596611814573e-05\n",
            "step: 80, loss: 0.00025278606335632503\n",
            "step: 90, loss: 8.652130782138556e-05\n",
            "step: 100, loss: 0.00014421755622606725\n",
            "step: 110, loss: 0.0003426856710575521\n",
            "step: 120, loss: 0.00020936956570949405\n",
            "step: 130, loss: 0.00909795518964529\n",
            "step: 140, loss: 5.191103628021665e-05\n",
            "step: 150, loss: 0.001079176552593708\n",
            "step: 160, loss: 8.266639633802697e-05\n",
            "step: 170, loss: 3.0454555599135347e-05\n",
            "step: 180, loss: 0.0018350580940023065\n",
            "step: 190, loss: 0.0014902190305292606\n",
            "step: 200, loss: 0.026424376294016838\n",
            "step: 210, loss: 0.012111296877264977\n",
            "step: 220, loss: 0.0008345473324880004\n",
            "step: 230, loss: 0.00010579688387224451\n",
            "step: 240, loss: 0.0005329950363375247\n",
            "step: 250, loss: 7.464743248419836e-05\n",
            "step: 260, loss: 2.324713750567753e-05\n",
            "step: 270, loss: 3.219397331122309e-05\n",
            "step: 280, loss: 3.5760222090175375e-05\n",
            "step: 290, loss: 0.000836177496239543\n",
            "step: 300, loss: 0.005184963811188936\n",
            "step: 310, loss: 0.015639694407582283\n",
            "step: 320, loss: 0.00022996218467596918\n",
            "step: 330, loss: 6.195496098371223e-05\n",
            "step: 340, loss: 3.12152951664757e-05\n",
            "step: 350, loss: 0.0037667674478143454\n",
            "step: 360, loss: 0.0003154030709993094\n",
            "step: 370, loss: 0.0014340004418045282\n",
            "step: 380, loss: 0.0009246104746125638\n",
            "step: 390, loss: 1.7817341358750127e-05\n",
            "step: 400, loss: 0.014460649341344833\n",
            "step: 410, loss: 4.8233207053272054e-05\n",
            "step: 420, loss: 0.00015975507267285138\n",
            "step: 430, loss: 2.0059298549313098e-05\n",
            "step: 440, loss: 0.0004395870491862297\n",
            "step: 450, loss: 0.0016460237093269825\n",
            "step: 460, loss: 0.00048098101979121566\n",
            "step: 470, loss: 1.0676578312995844e-05\n",
            "step: 480, loss: 0.0018032500520348549\n",
            "step: 490, loss: 0.006646408699452877\n",
            "step: 500, loss: 0.00021377291704993695\n",
            "step: 510, loss: 0.00493669556453824\n",
            "step: 520, loss: 0.0003357178356964141\n",
            "step: 530, loss: 0.0004996138741262257\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9524697110904008, f1=0.9422180801491147, best_f1=0.9444444444444445\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:33, 169.65it/s]\n",
            "load_f1 = 0.9523809523809523\n",
            "real_f1 = 0.9503940658321743\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 144.36it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM",
        "pw03GW7dmkqy",
        "jeDvm9a1dIlo",
        "NJ3ExOzkeDVk",
        "tb_EWW7DgNFL",
        "NC7Q_ekTgNFN",
        "Zbv_H8sHgw8C",
        "nXvTChDGgw8D",
        "SSCCmtSggw8E",
        "CrVM9KP9hstt",
        "nyGyaWAphstt",
        "dL0eWrGYhstu",
        "zW6LV4zMhstv",
        "VngEb4vfhstw"
      ],
      "name": "CMedium_30_3_5_roberta.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}