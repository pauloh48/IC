{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSgSb9vUtCyX",
        "outputId": "e3c4ea01-255f-4962-fc9c-15e9a226d4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 8.54 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.6 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 19.6 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 64.8 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 69.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 23.6 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 17.62 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 26.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 7.0 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 64.9 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 63.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Collecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 72.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 68.5 MB/s \n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 66.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=3b7e38f0fa692305fdd947b8f6016c9441f7d6f4ed2981d10d1f153dd343d1c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=b4e6a17d3ac7d71baca0a9b56eda8d9574705a5aaab051829ec2fe0ffe589c73\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_hfO6D_uLby",
        "outputId": "4b913e3f-53f2-4693-c756-5e706b0fbb0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10091, done.\u001b[K\n",
            "remote: Counting objects: 100% (207/207), done.\u001b[K\n",
            "remote: Compressing objects: 100% (144/144), done.\u001b[K\n",
            "remote: Total 10091 (delta 100), reused 131 (delta 59), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10091/10091), 14.96 MiB | 1.74 MiB/s, done.\n",
            "Resolving deltas: 100% (6908/6908), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-gvzxkpz3\n",
            "Created temporary directory: /tmp/pip-req-tracker-9w6c3awy\n",
            "Initialized build tracking at /tmp/pip-req-tracker-9w6c3awy\n",
            "Created build tracker: /tmp/pip-req-tracker-9w6c3awy\n",
            "Entered build tracker: /tmp/pip-req-tracker-9w6c3awy\n",
            "Created temporary directory: /tmp/pip-install-durxbajx\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-0s163tny\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-9w6c3awy'\n",
            "    Running setup.py (path:/tmp/pip-req-build-0s163tny/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-tnb20hf6\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-tnb20hf6/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-tnb20hf6/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-tnb20hf6/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-tnb20hf6/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-tnb20hf6/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-tnb20hf6/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-0s163tny has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-9w6c3awy'\n",
            "Created temporary directory: /tmp/pip-unpack-sotcm81e\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-a49jb_o1\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-a49jb_o1\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-0s163tny/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-0s163tny/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-a49jb_o1\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-a49jb_o1/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298114 sha256=b0c1c837dd6e515375ad25629070eaa487a999a993be2113a5671e921c429301\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gvzxkpz3/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-9w6c3awy'\n",
            "/content/ditto\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKSZwBG_uyzV",
        "outputId": "845b82e8-5339-45ca-a90a-ef36d9fcbe42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 32.7 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.48-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting botocore==1.27.48\n",
            "  Downloading botocore-1.27.48-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 54.9 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.48->awscli) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.48->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.48 botocore-1.27.48 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ],
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR84V9pFRkw8",
        "outputId": "4b2bee87-fbf8-45b0-c2b3-b0ff5cee670c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.7.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAyGpUo9ifJM"
      },
      "source": [
        "## importa config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xawOMn6icU7",
        "outputId": "ae153c3f-97e6-4501-8365-ca51f869bbf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 985, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 985 (delta 16), reused 7 (delta 0), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (985/985), 252.10 MiB | 17.53 MiB/s, done.\n",
            "Resolving deltas: 100% (600/600), done.\n",
            "Checking out files: 100% (1274/1274), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw03GW7dmkqy"
      },
      "source": [
        "## remove e move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQVym9vwmx-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "690613a7-0b16-4a6d-bcf2-0d71ac9547dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "outputs": [],
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/ADirty_90_3_5/configs.json /content/ditto/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm4nohJxf9bD"
      },
      "source": [
        "# DA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDvm9a1dIlo"
      },
      "source": [
        "## DA STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qxLFPNvcGgH",
        "outputId": "3f785f94-d724-47fe-a483-6bf75feff982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 417kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 811kB/s]\n",
            "Downloading: 100% 456k/456k [00:01<00:00, 416kB/s]\n",
            "Downloading: 100% 501M/501M [00:07<00:00, 68.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.41138964891433716\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3076923076923077, f1=0.24444444444444444, best_f1=0.24444444444444444\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.43859201669692993\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.3373493975903615, f1=0.25316455696202533, best_f1=0.25316455696202533\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4355158805847168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.3076923076923077, f1=0.2823529411764706, best_f1=0.25316455696202533\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24790231883525848\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.37333333333333335, f1=0.30000000000000004, best_f1=0.30000000000000004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2856398820877075\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.2916666666666667, f1=0.3010752688172043, best_f1=0.30000000000000004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.33419474959373474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.3783783783783784, f1=0.3225806451612903, best_f1=0.3225806451612903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5935828685760498\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.3111111111111111, f1=0.2758620689655173, best_f1=0.3225806451612903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5075299143791199\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.3684210526315789, f1=0.36923076923076925, best_f1=0.3225806451612903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3876578211784363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.4444444444444445, f1=0.4090909090909091, best_f1=0.4090909090909091\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.42664986848831177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.4745762711864407, f1=0.42553191489361697, best_f1=0.42553191489361697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4746597707271576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.49122807017543857, f1=0.42553191489361697, best_f1=0.42553191489361697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25290536880493164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.45833333333333326, f1=0.5789473684210527, best_f1=0.42553191489361697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2970203161239624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.5185185185185185, f1=0.4761904761904762, best_f1=0.4761904761904762\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20660606026649475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5185185185185185, f1=0.43137254901960786, best_f1=0.4761904761904762\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2977107763290405\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5185185185185185, f1=0.43137254901960786, best_f1=0.4761904761904762\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 107606.90it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.5116279069767441\n",
            "real_f1 = 0.5283018867924528\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:29, 147.19it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjO-q4GLeCE1"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ7mparQevgM",
        "outputId": "16596322-1203-46ac-b52e-1a71a6fe957a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5594528317451477\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.426152765750885\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.4639163911342621\n",
            "step: 30, loss: 0.28728801012039185\n",
            "step: 40, loss: 0.32928267121315\n",
            "step: 50, loss: 0.6241447329521179\n",
            "step: 60, loss: 0.4424308240413666\n",
            "step: 70, loss: 0.25717243552207947\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.502198338508606\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 90, loss: 0.3603366017341614\n",
            "step: 100, loss: 0.10199589282274246\n",
            "step: 110, loss: 0.2527065873146057\n",
            "step: 120, loss: 0.19465531408786774\n",
            "step: 130, loss: 0.016640830785036087\n",
            "step: 140, loss: 0.07389167696237564\n",
            "step: 150, loss: 0.1039348915219307\n",
            "step: 160, loss: 0.051416635513305664\n",
            "step: 170, loss: 0.22984512150287628\n",
            "step: 180, loss: 0.02081911824643612\n",
            "step: 190, loss: 0.13898733258247375\n",
            "step: 200, loss: 0.07074031978845596\n",
            "step: 210, loss: 0.01985827088356018\n",
            "step: 220, loss: 0.037789300084114075\n",
            "step: 230, loss: 0.03189815580844879\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9751131221719457, f1=0.9680365296803655, best_f1=0.9680365296803655\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0029032211750745773\n",
            "step: 10, loss: 0.10406063497066498\n",
            "step: 20, loss: 0.020126042887568474\n",
            "step: 30, loss: 0.01611391268670559\n",
            "step: 40, loss: 0.015947800129652023\n",
            "step: 50, loss: 0.004891406279057264\n",
            "step: 60, loss: 0.0067612421698868275\n",
            "step: 70, loss: 0.021271973848342896\n",
            "step: 80, loss: 0.08145947009325027\n",
            "step: 90, loss: 0.010733840987086296\n",
            "step: 100, loss: 0.007137634791433811\n",
            "step: 110, loss: 0.17646963894367218\n",
            "step: 120, loss: 0.0016096564941108227\n",
            "step: 130, loss: 0.0027878712862730026\n",
            "step: 140, loss: 0.0076788137666881084\n",
            "step: 150, loss: 0.11563393473625183\n",
            "step: 160, loss: 0.007876909337937832\n",
            "step: 170, loss: 0.08905985951423645\n",
            "step: 180, loss: 0.008960344828665257\n",
            "step: 190, loss: 0.11518072336912155\n",
            "step: 200, loss: 0.009689787402749062\n",
            "step: 210, loss: 0.11641671508550644\n",
            "step: 220, loss: 0.02188892476260662\n",
            "step: 230, loss: 0.14674246311187744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9820627802690582, f1=0.9773755656108598, best_f1=0.9773755656108598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005520571488887072\n",
            "step: 10, loss: 0.033481717109680176\n",
            "step: 20, loss: 0.10210629552602768\n",
            "step: 30, loss: 0.0019659230019897223\n",
            "step: 40, loss: 0.008607838302850723\n",
            "step: 50, loss: 0.04264243692159653\n",
            "step: 60, loss: 0.011702976189553738\n",
            "step: 70, loss: 0.0019984757527709007\n",
            "step: 80, loss: 0.0008596020052209496\n",
            "step: 90, loss: 0.005943822208791971\n",
            "step: 100, loss: 0.0027064955793321133\n",
            "step: 110, loss: 0.0035992946941405535\n",
            "step: 120, loss: 0.0006418082630261779\n",
            "step: 130, loss: 0.002146809594705701\n",
            "step: 140, loss: 0.00046450516674667597\n",
            "step: 150, loss: 0.057408738881349564\n",
            "step: 160, loss: 0.0020453904289752245\n",
            "step: 170, loss: 0.0018004290759563446\n",
            "step: 180, loss: 0.08300425857305527\n",
            "step: 190, loss: 0.010702645406126976\n",
            "step: 200, loss: 0.00787993986159563\n",
            "step: 210, loss: 0.0021690744906663895\n",
            "step: 220, loss: 0.006581088528037071\n",
            "step: 230, loss: 0.025018360465765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9783845278725825, f1=0.9759999999999999, best_f1=0.9773755656108598\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012682436965405941\n",
            "step: 10, loss: 0.0011268671369180083\n",
            "step: 20, loss: 0.0012164213694632053\n",
            "step: 30, loss: 0.003400753950700164\n",
            "step: 40, loss: 0.02341885305941105\n",
            "step: 50, loss: 0.005095299798995256\n",
            "step: 60, loss: 0.0011905863648280501\n",
            "step: 70, loss: 0.0015707946149632335\n",
            "step: 80, loss: 0.0013853221898898482\n",
            "step: 90, loss: 0.002246077638119459\n",
            "step: 100, loss: 0.0073193134739995\n",
            "step: 110, loss: 0.002807039301842451\n",
            "step: 120, loss: 0.0017775368178263307\n",
            "step: 130, loss: 0.011494860053062439\n",
            "step: 140, loss: 0.0003693613689392805\n",
            "step: 150, loss: 0.0003195127646904439\n",
            "step: 160, loss: 0.0005670480313710868\n",
            "step: 170, loss: 0.005471720825880766\n",
            "step: 180, loss: 0.09682486206293106\n",
            "step: 190, loss: 0.004555656109005213\n",
            "step: 200, loss: 0.017059743404388428\n",
            "step: 210, loss: 0.0009029322536662221\n",
            "step: 220, loss: 0.049264129251241684\n",
            "step: 230, loss: 0.007244246546179056\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9854096520763187, f1=0.9820224719101124, best_f1=0.9820224719101124\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02619859203696251\n",
            "step: 10, loss: 0.002304235240444541\n",
            "step: 20, loss: 0.001323841162957251\n",
            "step: 30, loss: 0.0015646599931642413\n",
            "step: 40, loss: 0.0008431965834461153\n",
            "step: 50, loss: 0.0006622375803999603\n",
            "step: 60, loss: 0.0008432514150626957\n",
            "step: 70, loss: 0.001037242473103106\n",
            "step: 80, loss: 0.03463221713900566\n",
            "step: 90, loss: 0.016542337834835052\n",
            "step: 100, loss: 0.00022278103278949857\n",
            "step: 110, loss: 0.001418804400600493\n",
            "step: 120, loss: 0.21143901348114014\n",
            "step: 130, loss: 0.0005831983289681375\n",
            "step: 140, loss: 0.00105130928568542\n",
            "step: 150, loss: 0.06033499166369438\n",
            "step: 160, loss: 0.0009175627492368221\n",
            "step: 170, loss: 0.0020632471423596144\n",
            "step: 180, loss: 0.02343612350523472\n",
            "step: 190, loss: 0.006931660696864128\n",
            "step: 200, loss: 0.03379898890852928\n",
            "step: 210, loss: 0.021391648799180984\n",
            "step: 220, loss: 0.0027216284070163965\n",
            "step: 230, loss: 0.005947648547589779\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.990990990990991, f1=0.9909706546275394, best_f1=0.9909706546275394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012401659041643143\n",
            "step: 10, loss: 0.0041468762792646885\n",
            "step: 20, loss: 0.002925332635641098\n",
            "step: 30, loss: 0.0045775240287184715\n",
            "step: 40, loss: 0.0006006951443850994\n",
            "step: 50, loss: 0.0005530991475097835\n",
            "step: 60, loss: 0.21601849794387817\n",
            "step: 70, loss: 0.0017968651372939348\n",
            "step: 80, loss: 0.054895661771297455\n",
            "step: 90, loss: 0.010730482637882233\n",
            "step: 100, loss: 0.001978188520297408\n",
            "step: 110, loss: 0.000945572683122009\n",
            "step: 120, loss: 0.0003612501604948193\n",
            "step: 130, loss: 0.0006637598853558302\n",
            "step: 140, loss: 0.0014650062657892704\n",
            "step: 150, loss: 0.0005185747286304832\n",
            "step: 160, loss: 0.043115030974149704\n",
            "step: 170, loss: 0.004472588654607534\n",
            "step: 180, loss: 0.00017103897698689252\n",
            "step: 190, loss: 0.0005409217556007206\n",
            "step: 200, loss: 0.014420310966670513\n",
            "step: 210, loss: 0.00147503730840981\n",
            "step: 220, loss: 0.015035044401884079\n",
            "step: 230, loss: 0.0007823532214388251\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9876819708846584, f1=0.9797752808988766, best_f1=0.9909706546275394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009342344710603356\n",
            "step: 10, loss: 0.0005284327198751271\n",
            "step: 20, loss: 0.0020071968901902437\n",
            "step: 30, loss: 0.00020591144857462496\n",
            "step: 40, loss: 0.0003810326161328703\n",
            "step: 50, loss: 0.0004949304857291281\n",
            "step: 60, loss: 0.0006708185537718236\n",
            "step: 70, loss: 0.00028071997803635895\n",
            "step: 80, loss: 0.0012372353812679648\n",
            "step: 90, loss: 0.0008579103159718215\n",
            "step: 100, loss: 0.00039129555807448924\n",
            "step: 110, loss: 0.0016003830824047327\n",
            "step: 120, loss: 0.0008676068973727524\n",
            "step: 130, loss: 0.0004212207277305424\n",
            "step: 140, loss: 0.00011508754687383771\n",
            "step: 150, loss: 0.0003610964340623468\n",
            "step: 160, loss: 0.00022388403885997832\n",
            "step: 170, loss: 0.00029179328703321517\n",
            "step: 180, loss: 0.03461304306983948\n",
            "step: 190, loss: 0.00022872694535180926\n",
            "step: 200, loss: 0.004119702614843845\n",
            "step: 210, loss: 0.007798140402883291\n",
            "step: 220, loss: 0.0004366521316114813\n",
            "step: 230, loss: 0.001008384395390749\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9876543209876544, f1=0.9831649831649831, best_f1=0.9909706546275394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007590728346258402\n",
            "step: 10, loss: 0.00671972893178463\n",
            "step: 20, loss: 0.007098160218447447\n",
            "step: 30, loss: 0.002171957166865468\n",
            "step: 40, loss: 0.0013649447355419397\n",
            "step: 50, loss: 0.001670626807026565\n",
            "step: 60, loss: 0.004750428255647421\n",
            "step: 70, loss: 0.00020986419986002147\n",
            "step: 80, loss: 0.001622675103135407\n",
            "step: 90, loss: 0.00017865611880552024\n",
            "step: 100, loss: 0.00021758349612355232\n",
            "step: 110, loss: 0.0006946335779502988\n",
            "step: 120, loss: 0.00010319525608792901\n",
            "step: 130, loss: 0.0002971123030874878\n",
            "step: 140, loss: 0.00026923793484456837\n",
            "step: 150, loss: 0.008807668462395668\n",
            "step: 160, loss: 0.000633751624263823\n",
            "step: 170, loss: 0.08422993123531342\n",
            "step: 180, loss: 0.0008898893720470369\n",
            "step: 190, loss: 0.0004692889051511884\n",
            "step: 200, loss: 0.00031224131816998124\n",
            "step: 210, loss: 0.0012246964033693075\n",
            "step: 220, loss: 0.0029312586411833763\n",
            "step: 230, loss: 0.0025388621725142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9910112359550561, f1=0.9831649831649831, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007624350255355239\n",
            "step: 10, loss: 0.0023321642074733973\n",
            "step: 20, loss: 0.015668055042624474\n",
            "step: 30, loss: 0.0004852791898883879\n",
            "step: 40, loss: 0.00022163828543853015\n",
            "step: 50, loss: 0.00033880164846777916\n",
            "step: 60, loss: 0.00017195298278238624\n",
            "step: 70, loss: 0.033761437982320786\n",
            "step: 80, loss: 0.0015297499485313892\n",
            "step: 90, loss: 0.01397309172898531\n",
            "step: 100, loss: 0.002087402855977416\n",
            "step: 110, loss: 0.0003333617059979588\n",
            "step: 120, loss: 0.0002675863215699792\n",
            "step: 130, loss: 0.0008090949268080294\n",
            "step: 140, loss: 0.003252160269767046\n",
            "step: 150, loss: 0.0011368909617885947\n",
            "step: 160, loss: 0.08157844096422195\n",
            "step: 170, loss: 0.0018952219979837537\n",
            "step: 180, loss: 0.010100231505930424\n",
            "step: 190, loss: 0.0003028852224815637\n",
            "step: 200, loss: 0.0005937201785854995\n",
            "step: 210, loss: 0.0028841898310929537\n",
            "step: 220, loss: 0.0002490011975169182\n",
            "step: 230, loss: 7.384144555544481e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9863636363636363, f1=0.9829738933030647, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020241799938958138\n",
            "step: 10, loss: 0.0002114108792738989\n",
            "step: 20, loss: 0.00039399502566084266\n",
            "step: 30, loss: 0.00045309419510886073\n",
            "step: 40, loss: 0.0070190271362662315\n",
            "step: 50, loss: 0.0004046616959385574\n",
            "step: 60, loss: 0.0008885483839549124\n",
            "step: 70, loss: 0.0006034501711837947\n",
            "step: 80, loss: 0.0001969316363101825\n",
            "step: 90, loss: 0.00026698861620388925\n",
            "step: 100, loss: 0.0002005032147280872\n",
            "step: 110, loss: 0.001284239930100739\n",
            "step: 120, loss: 0.00019558638450689614\n",
            "step: 130, loss: 0.00029641471337527037\n",
            "step: 140, loss: 0.0001369398960378021\n",
            "step: 150, loss: 0.00036872801138088107\n",
            "step: 160, loss: 0.00010732188820838928\n",
            "step: 170, loss: 0.00017600294086150825\n",
            "step: 180, loss: 0.004459569230675697\n",
            "step: 190, loss: 0.0001950790756382048\n",
            "step: 200, loss: 0.000518154411111027\n",
            "step: 210, loss: 0.002999034011736512\n",
            "step: 220, loss: 0.003229474416002631\n",
            "step: 230, loss: 7.494715828215703e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9910313901345291, f1=0.9865168539325843, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.0892507715616375e-05\n",
            "step: 10, loss: 0.00012038202839903533\n",
            "step: 20, loss: 9.653306915424764e-05\n",
            "step: 30, loss: 7.404074858641252e-05\n",
            "step: 40, loss: 3.07720220007468e-05\n",
            "step: 50, loss: 4.5689361286349595e-05\n",
            "step: 60, loss: 0.09002037346363068\n",
            "step: 70, loss: 0.018080605193972588\n",
            "step: 80, loss: 0.0003767946909647435\n",
            "step: 90, loss: 0.0002918145328294486\n",
            "step: 100, loss: 0.00018504432227928191\n",
            "step: 110, loss: 0.0008259626338258386\n",
            "step: 120, loss: 9.013589442474768e-05\n",
            "step: 130, loss: 8.87203641468659e-05\n",
            "step: 140, loss: 0.0003739367239177227\n",
            "step: 150, loss: 0.0018496226985007524\n",
            "step: 160, loss: 0.007907778955996037\n",
            "step: 170, loss: 0.000190995866432786\n",
            "step: 180, loss: 0.00012158403842477128\n",
            "step: 190, loss: 0.010987343266606331\n",
            "step: 200, loss: 0.00032150078914128244\n",
            "step: 210, loss: 0.00019303348381072283\n",
            "step: 220, loss: 0.00113511027302593\n",
            "step: 230, loss: 0.00010570452286629006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.987709497206704, f1=0.9821029082774049, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010557177301961929\n",
            "step: 10, loss: 0.00010122002277057618\n",
            "step: 20, loss: 0.005081820301711559\n",
            "step: 30, loss: 0.031203050166368484\n",
            "step: 40, loss: 0.0009209828567691147\n",
            "step: 50, loss: 0.000844803114887327\n",
            "step: 60, loss: 0.0007604074780829251\n",
            "step: 70, loss: 0.00024774103076197207\n",
            "step: 80, loss: 0.00012747812434099615\n",
            "step: 90, loss: 0.0007871088455431163\n",
            "step: 100, loss: 0.00021726716659031808\n",
            "step: 110, loss: 0.00013501386274583638\n",
            "step: 120, loss: 0.0001232087379321456\n",
            "step: 130, loss: 8.67171329446137e-05\n",
            "step: 140, loss: 0.00011169586650794372\n",
            "step: 150, loss: 9.115792636293918e-05\n",
            "step: 160, loss: 0.000600694096647203\n",
            "step: 170, loss: 0.000703939818777144\n",
            "step: 180, loss: 9.087631769943982e-05\n",
            "step: 190, loss: 0.0005576623370870948\n",
            "step: 200, loss: 0.00017533850041218102\n",
            "step: 210, loss: 0.00014008361904416233\n",
            "step: 220, loss: 5.644539851346053e-05\n",
            "step: 230, loss: 0.00035540293902158737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9853768278965129, f1=0.984304932735426, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.74084697291255e-05\n",
            "step: 10, loss: 0.00012886915646959096\n",
            "step: 20, loss: 9.616361057851464e-05\n",
            "step: 30, loss: 0.0014257762813940644\n",
            "step: 40, loss: 0.00018049831851385534\n",
            "step: 50, loss: 0.0003888139617629349\n",
            "step: 60, loss: 0.0002726053062360734\n",
            "step: 70, loss: 0.00017314814613200724\n",
            "step: 80, loss: 0.00028615043265745044\n",
            "step: 90, loss: 7.797110447427258e-05\n",
            "step: 100, loss: 0.0002917555102612823\n",
            "step: 110, loss: 0.0002410035376669839\n",
            "step: 120, loss: 0.0001687566691543907\n",
            "step: 130, loss: 0.00022166708367876709\n",
            "step: 140, loss: 8.098284888546914e-05\n",
            "step: 150, loss: 4.910533607471734e-05\n",
            "step: 160, loss: 0.0018039002316072583\n",
            "step: 170, loss: 0.0001656301465118304\n",
            "step: 180, loss: 0.0018093022517859936\n",
            "step: 190, loss: 0.00010552906314842403\n",
            "step: 200, loss: 6.584631773876026e-05\n",
            "step: 210, loss: 0.00014314329018816352\n",
            "step: 220, loss: 0.00021916895639151335\n",
            "step: 230, loss: 6.30016002105549e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9876819708846584, f1=0.9832026875699889, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.347344606183469e-05\n",
            "step: 10, loss: 7.933519373182207e-05\n",
            "step: 20, loss: 5.3676350944442675e-05\n",
            "step: 30, loss: 0.00012113719276385382\n",
            "step: 40, loss: 0.00016146511188708246\n",
            "step: 50, loss: 4.549498771666549e-05\n",
            "step: 60, loss: 8.729195542400703e-05\n",
            "step: 70, loss: 0.00019652668561320752\n",
            "step: 80, loss: 5.274168142932467e-05\n",
            "step: 90, loss: 0.00014842583914287388\n",
            "step: 100, loss: 0.00010168866720050573\n",
            "step: 110, loss: 7.135736814234406e-05\n",
            "step: 120, loss: 1.809648711059708e-05\n",
            "step: 130, loss: 0.00014514957729261369\n",
            "step: 140, loss: 8.108558540698141e-05\n",
            "step: 150, loss: 6.761275290045887e-05\n",
            "step: 160, loss: 0.0011290640104562044\n",
            "step: 170, loss: 9.14677293621935e-05\n",
            "step: 180, loss: 0.00015548712690360844\n",
            "step: 190, loss: 0.00013112569286022335\n",
            "step: 200, loss: 0.00048414795310236514\n",
            "step: 210, loss: 4.7458110202569515e-05\n",
            "step: 220, loss: 0.0001286850165342912\n",
            "step: 230, loss: 5.460292231873609e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9887640449438202, f1=0.9854096520763187, best_f1=0.9865168539325843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020373391453176737\n",
            "step: 10, loss: 5.702055568690412e-05\n",
            "step: 20, loss: 0.0008444447303190827\n",
            "step: 30, loss: 6.0059435782022774e-05\n",
            "step: 40, loss: 4.1487321141175926e-05\n",
            "step: 50, loss: 5.056126246927306e-05\n",
            "step: 60, loss: 0.04105804115533829\n",
            "step: 70, loss: 5.9942263760603964e-05\n",
            "step: 80, loss: 5.89656010561157e-05\n",
            "step: 90, loss: 7.35834619263187e-05\n",
            "step: 100, loss: 3.483234468149021e-05\n",
            "step: 110, loss: 0.00022670128964819014\n",
            "step: 120, loss: 0.0002774225431494415\n",
            "step: 130, loss: 8.948156755650416e-05\n",
            "step: 140, loss: 0.004575501196086407\n",
            "step: 150, loss: 0.00044725966290570796\n",
            "step: 160, loss: 0.00037151912692934275\n",
            "step: 170, loss: 2.8779262720490806e-05\n",
            "step: 180, loss: 0.00011273184645688161\n",
            "step: 190, loss: 4.94705636810977e-05\n",
            "step: 200, loss: 0.00015538488514721394\n",
            "step: 210, loss: 0.00013019415200687945\n",
            "step: 220, loss: 0.00014397522318176925\n",
            "step: 230, loss: 9.1987443738617e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9887640449438202, f1=0.9865168539325843, best_f1=0.9865168539325843\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:16, 149.88it/s]\n",
            "load_f1 = 0.9898534385569334\n",
            "real_f1 = 0.9865168539325843\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 143.66it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_G0OicNeCnd"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkIRgx40ezP8",
        "outputId": "aad28a42-9239-4778-dea1-a9f489ab2d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6351011395454407\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.38612601161003113\n",
            "step: 20, loss: 0.3133067190647125\n",
            "step: 30, loss: 0.36976903676986694\n",
            "step: 40, loss: 0.37451326847076416\n",
            "step: 50, loss: 0.6486261487007141\n",
            "step: 60, loss: 0.33434930443763733\n",
            "step: 70, loss: 0.4712095856666565\n",
            "step: 80, loss: 0.4840647578239441\n",
            "step: 90, loss: 0.42971599102020264\n",
            "step: 100, loss: 0.5336258411407471\n",
            "step: 110, loss: 0.48330163955688477\n",
            "step: 120, loss: 0.5849539041519165\n",
            "step: 130, loss: 0.5600461363792419\n",
            "step: 140, loss: 0.369144082069397\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 150, loss: 0.40193066000938416\n",
            "step: 160, loss: 0.49846264719963074\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 170, loss: 0.19392763078212738\n",
            "step: 180, loss: 0.20836257934570312\n",
            "step: 190, loss: 0.25089001655578613\n",
            "step: 200, loss: 0.5663240551948547\n",
            "step: 210, loss: 0.15442998707294464\n",
            "step: 220, loss: 0.1682528853416443\n",
            "step: 230, loss: 0.36979812383651733\n",
            "step: 240, loss: 0.08767421543598175\n",
            "step: 250, loss: 0.08031388372182846\n",
            "step: 260, loss: 0.47808197140693665\n",
            "step: 270, loss: 0.2268693596124649\n",
            "step: 280, loss: 0.11248474568128586\n",
            "step: 290, loss: 0.14444176852703094\n",
            "step: 300, loss: 0.19852089881896973\n",
            "step: 310, loss: 0.3766535222530365\n",
            "step: 320, loss: 0.13476771116256714\n",
            "step: 330, loss: 0.1244756430387497\n",
            "step: 340, loss: 0.49138346314430237\n",
            "step: 350, loss: 0.2982802391052246\n",
            "step: 360, loss: 0.0627615749835968\n",
            "step: 370, loss: 0.022944042459130287\n",
            "step: 380, loss: 0.14215002954006195\n",
            "step: 390, loss: 0.03650803491473198\n",
            "step: 400, loss: 0.01734234020113945\n",
            "step: 410, loss: 0.2714202404022217\n",
            "step: 420, loss: 0.10159015655517578\n",
            "step: 430, loss: 0.10040714591741562\n",
            "step: 440, loss: 0.17616361379623413\n",
            "step: 450, loss: 0.10390278697013855\n",
            "step: 460, loss: 0.10853704810142517\n",
            "step: 470, loss: 0.09111225605010986\n",
            "step: 480, loss: 0.1851404756307602\n",
            "step: 490, loss: 0.1374576985836029\n",
            "step: 500, loss: 0.13309070467948914\n",
            "step: 510, loss: 0.11793189495801926\n",
            "step: 520, loss: 0.7279350161552429\n",
            "step: 530, loss: 0.10063080489635468\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8838916934373566, f1=0.8821362799263353, best_f1=0.8821362799263353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14847244322299957\n",
            "step: 10, loss: 0.12544135749340057\n",
            "step: 20, loss: 0.19450081884860992\n",
            "step: 30, loss: 0.1987629383802414\n",
            "step: 40, loss: 0.10997682064771652\n",
            "step: 50, loss: 0.18963319063186646\n",
            "step: 60, loss: 0.09831631183624268\n",
            "step: 70, loss: 0.11352141201496124\n",
            "step: 80, loss: 0.11711686849594116\n",
            "step: 90, loss: 0.0455399714410305\n",
            "step: 100, loss: 0.18694248795509338\n",
            "step: 110, loss: 0.1352463960647583\n",
            "step: 120, loss: 0.16131041944026947\n",
            "step: 130, loss: 0.03046775422990322\n",
            "step: 140, loss: 0.1611226350069046\n",
            "step: 150, loss: 0.08304721117019653\n",
            "step: 160, loss: 0.031019030138850212\n",
            "step: 170, loss: 0.11304184049367905\n",
            "step: 180, loss: 0.06660997122526169\n",
            "step: 190, loss: 0.057459309697151184\n",
            "step: 200, loss: 0.24896420538425446\n",
            "step: 210, loss: 0.20394110679626465\n",
            "step: 220, loss: 0.009946306236088276\n",
            "step: 230, loss: 0.10693851113319397\n",
            "step: 240, loss: 0.06275536119937897\n",
            "step: 250, loss: 0.15281665325164795\n",
            "step: 260, loss: 0.2625812888145447\n",
            "step: 270, loss: 0.05904730409383774\n",
            "step: 280, loss: 0.14349530637264252\n",
            "step: 290, loss: 0.08465474098920822\n",
            "step: 300, loss: 0.18484243750572205\n",
            "step: 310, loss: 0.10333912819623947\n",
            "step: 320, loss: 0.1947673261165619\n",
            "step: 330, loss: 0.11911077797412872\n",
            "step: 340, loss: 0.2228899449110031\n",
            "step: 350, loss: 0.02836088091135025\n",
            "step: 360, loss: 0.08518368750810623\n",
            "step: 370, loss: 0.013317195698618889\n",
            "step: 380, loss: 0.1502322405576706\n",
            "step: 390, loss: 0.0706862360239029\n",
            "step: 400, loss: 0.07863053679466248\n",
            "step: 410, loss: 0.07792431116104126\n",
            "step: 420, loss: 0.04500935971736908\n",
            "step: 430, loss: 0.3473028838634491\n",
            "step: 440, loss: 0.048050351440906525\n",
            "step: 450, loss: 0.0707370713353157\n",
            "step: 460, loss: 0.06472722440958023\n",
            "step: 470, loss: 0.17852507531642914\n",
            "step: 480, loss: 0.09556399285793304\n",
            "step: 490, loss: 0.0717795342206955\n",
            "step: 500, loss: 0.016396308317780495\n",
            "step: 510, loss: 0.1361425518989563\n",
            "step: 520, loss: 0.4722483456134796\n",
            "step: 530, loss: 0.11091696470975876\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.8866171003717472, f1=0.8973418881759854, best_f1=0.8973418881759854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22811955213546753\n",
            "step: 10, loss: 0.21165207028388977\n",
            "step: 20, loss: 0.03481403365731239\n",
            "step: 30, loss: 0.14381353557109833\n",
            "step: 40, loss: 0.25117161870002747\n",
            "step: 50, loss: 0.07211744040250778\n",
            "step: 60, loss: 0.06654778122901917\n",
            "step: 70, loss: 0.127228781580925\n",
            "step: 80, loss: 0.06278495490550995\n",
            "step: 90, loss: 0.04213269054889679\n",
            "step: 100, loss: 0.09881757944822311\n",
            "step: 110, loss: 0.09184791147708893\n",
            "step: 120, loss: 0.20226092636585236\n",
            "step: 130, loss: 0.1100887656211853\n",
            "step: 140, loss: 0.0474117211997509\n",
            "step: 150, loss: 0.08832366019487381\n",
            "step: 160, loss: 0.08886843919754028\n",
            "step: 170, loss: 0.012309521436691284\n",
            "step: 180, loss: 0.06767550110816956\n",
            "step: 190, loss: 0.12314288318157196\n",
            "step: 200, loss: 0.029904361814260483\n",
            "step: 210, loss: 0.18387414515018463\n",
            "step: 220, loss: 0.04390004277229309\n",
            "step: 230, loss: 0.058404017239809036\n",
            "step: 240, loss: 0.11391881853342056\n",
            "step: 250, loss: 0.20566126704216003\n",
            "step: 260, loss: 0.026706982403993607\n",
            "step: 270, loss: 0.05882973596453667\n",
            "step: 280, loss: 0.010946234688162804\n",
            "step: 290, loss: 0.05028095841407776\n",
            "step: 300, loss: 0.10829918086528778\n",
            "step: 310, loss: 0.23936814069747925\n",
            "step: 320, loss: 0.08068688213825226\n",
            "step: 330, loss: 0.042075015604496\n",
            "step: 340, loss: 0.0174017995595932\n",
            "step: 350, loss: 0.1449311226606369\n",
            "step: 360, loss: 0.05544956773519516\n",
            "step: 370, loss: 0.11404254287481308\n",
            "step: 380, loss: 0.07844426482915878\n",
            "step: 390, loss: 0.037922345101833344\n",
            "step: 400, loss: 0.1806066781282425\n",
            "step: 410, loss: 0.02471070922911167\n",
            "step: 420, loss: 0.17063257098197937\n",
            "step: 430, loss: 0.07564198970794678\n",
            "step: 440, loss: 0.27632656693458557\n",
            "step: 450, loss: 0.1497832089662552\n",
            "step: 460, loss: 0.11089959740638733\n",
            "step: 470, loss: 0.036705195903778076\n",
            "step: 480, loss: 0.13683074712753296\n",
            "step: 490, loss: 0.03289215639233589\n",
            "step: 500, loss: 0.04977305233478546\n",
            "step: 510, loss: 0.018474865704774857\n",
            "step: 520, loss: 0.008796379901468754\n",
            "step: 530, loss: 0.07201908528804779\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9093432007400555, f1=0.9148642429820525, best_f1=0.9148642429820525\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05245935916900635\n",
            "step: 10, loss: 0.02824772708117962\n",
            "step: 20, loss: 0.13919541239738464\n",
            "step: 30, loss: 0.0591355599462986\n",
            "step: 40, loss: 0.07408786565065384\n",
            "step: 50, loss: 0.06578163802623749\n",
            "step: 60, loss: 0.033786043524742126\n",
            "step: 70, loss: 0.10769366472959518\n",
            "step: 80, loss: 0.15578103065490723\n",
            "step: 90, loss: 0.1830172836780548\n",
            "step: 100, loss: 0.010826620273292065\n",
            "step: 110, loss: 0.12243637442588806\n",
            "step: 120, loss: 0.06441392004489899\n",
            "step: 130, loss: 0.10451457649469376\n",
            "step: 140, loss: 0.12075409293174744\n",
            "step: 150, loss: 0.037376370280981064\n",
            "step: 160, loss: 0.05559299886226654\n",
            "step: 170, loss: 0.10726892948150635\n",
            "step: 180, loss: 0.13734321296215057\n",
            "step: 190, loss: 0.2520022988319397\n",
            "step: 200, loss: 0.03347938880324364\n",
            "step: 210, loss: 0.03522009029984474\n",
            "step: 220, loss: 0.059637442231178284\n",
            "step: 230, loss: 0.012157713063061237\n",
            "step: 240, loss: 0.031465355306863785\n",
            "step: 250, loss: 0.1268928498029709\n",
            "step: 260, loss: 0.10228607058525085\n",
            "step: 270, loss: 0.18526417016983032\n",
            "step: 280, loss: 0.05470636114478111\n",
            "step: 290, loss: 0.05034111812710762\n",
            "step: 300, loss: 0.04994826018810272\n",
            "step: 310, loss: 0.012103640474379063\n",
            "step: 320, loss: 0.17352239787578583\n",
            "step: 330, loss: 0.037681519985198975\n",
            "step: 340, loss: 0.005946212448179722\n",
            "step: 350, loss: 0.21464702486991882\n",
            "step: 360, loss: 0.1007341593503952\n",
            "step: 370, loss: 0.012872946448624134\n",
            "step: 380, loss: 0.011543791741132736\n",
            "step: 390, loss: 0.011076614260673523\n",
            "step: 400, loss: 0.0162592064589262\n",
            "step: 410, loss: 0.007909908890724182\n",
            "step: 420, loss: 0.08768895268440247\n",
            "step: 430, loss: 0.023157037794589996\n",
            "step: 440, loss: 0.028019804507493973\n",
            "step: 450, loss: 0.05359445512294769\n",
            "step: 460, loss: 0.15735699236392975\n",
            "step: 470, loss: 0.03426113724708557\n",
            "step: 480, loss: 0.020692570134997368\n",
            "step: 490, loss: 0.04787531495094299\n",
            "step: 500, loss: 0.12325131893157959\n",
            "step: 510, loss: 0.13307785987854004\n",
            "step: 520, loss: 0.05611758306622505\n",
            "step: 530, loss: 0.14107175171375275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9178338001867413, f1=0.9182624941616068, best_f1=0.9182624941616068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007968500256538391\n",
            "step: 10, loss: 0.01027492806315422\n",
            "step: 20, loss: 0.019204219803214073\n",
            "step: 30, loss: 0.133038729429245\n",
            "step: 40, loss: 0.06580730527639389\n",
            "step: 50, loss: 0.10366249084472656\n",
            "step: 60, loss: 0.017007512971758842\n",
            "step: 70, loss: 0.016540059819817543\n",
            "step: 80, loss: 0.008497867733240128\n",
            "step: 90, loss: 0.013056902214884758\n",
            "step: 100, loss: 0.11167081445455551\n",
            "step: 110, loss: 0.09875364601612091\n",
            "step: 120, loss: 0.12625162303447723\n",
            "step: 130, loss: 0.020596042275428772\n",
            "step: 140, loss: 0.01999702677130699\n",
            "step: 150, loss: 0.02961566485464573\n",
            "step: 160, loss: 0.08131398260593414\n",
            "step: 170, loss: 0.04857851192355156\n",
            "step: 180, loss: 0.0058292970061302185\n",
            "step: 190, loss: 0.012386539950966835\n",
            "step: 200, loss: 0.006401561200618744\n",
            "step: 210, loss: 0.14196732640266418\n",
            "step: 220, loss: 0.033282212913036346\n",
            "step: 230, loss: 0.04085564985871315\n",
            "step: 240, loss: 0.015318794175982475\n",
            "step: 250, loss: 0.08338861912488937\n",
            "step: 260, loss: 0.0020017530769109726\n",
            "step: 270, loss: 0.009806566871702671\n",
            "step: 280, loss: 0.04637272655963898\n",
            "step: 290, loss: 0.02051803283393383\n",
            "step: 300, loss: 0.0676451250910759\n",
            "step: 310, loss: 0.11709047853946686\n",
            "step: 320, loss: 0.17348837852478027\n",
            "step: 330, loss: 0.026497818529605865\n",
            "step: 340, loss: 0.03998910263180733\n",
            "step: 350, loss: 0.016538865864276886\n",
            "step: 360, loss: 0.0012428724439814687\n",
            "step: 370, loss: 0.01726483181118965\n",
            "step: 380, loss: 0.012271610088646412\n",
            "step: 390, loss: 0.2376711666584015\n",
            "step: 400, loss: 0.038892727345228195\n",
            "step: 410, loss: 0.16916389763355255\n",
            "step: 420, loss: 0.14851273596286774\n",
            "step: 430, loss: 0.1474739909172058\n",
            "step: 440, loss: 0.014888681471347809\n",
            "step: 450, loss: 0.01605718955397606\n",
            "step: 460, loss: 0.05502030625939369\n",
            "step: 470, loss: 0.0455644316971302\n",
            "step: 480, loss: 0.08203010261058807\n",
            "step: 490, loss: 0.09451986104249954\n",
            "step: 500, loss: 0.01985687017440796\n",
            "step: 510, loss: 0.013070452027022839\n",
            "step: 520, loss: 0.23676557838916779\n",
            "step: 530, loss: 0.024433841928839684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9132893496701224, f1=0.912, best_f1=0.9182624941616068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04616393893957138\n",
            "step: 10, loss: 0.005283580161631107\n",
            "step: 20, loss: 0.014713317155838013\n",
            "step: 30, loss: 0.005029469262808561\n",
            "step: 40, loss: 0.0371866337954998\n",
            "step: 50, loss: 0.00238743401132524\n",
            "step: 60, loss: 0.006975526921451092\n",
            "step: 70, loss: 0.0016796945128589869\n",
            "step: 80, loss: 0.01799045130610466\n",
            "step: 90, loss: 0.03376137465238571\n",
            "step: 100, loss: 0.28636252880096436\n",
            "step: 110, loss: 0.02454954758286476\n",
            "step: 120, loss: 0.05036143213510513\n",
            "step: 130, loss: 0.005504448898136616\n",
            "step: 140, loss: 0.005276423413306475\n",
            "step: 150, loss: 0.006164176855236292\n",
            "step: 160, loss: 0.028516534715890884\n",
            "step: 170, loss: 0.003669972298666835\n",
            "step: 180, loss: 0.03503444790840149\n",
            "step: 190, loss: 0.29769033193588257\n",
            "step: 200, loss: 0.05725831538438797\n",
            "step: 210, loss: 0.0416690967977047\n",
            "step: 220, loss: 0.009449665434658527\n",
            "step: 230, loss: 0.007787223905324936\n",
            "step: 240, loss: 0.045777201652526855\n",
            "step: 250, loss: 0.19583937525749207\n",
            "step: 260, loss: 0.009511456824839115\n",
            "step: 270, loss: 0.016454022377729416\n",
            "step: 280, loss: 0.011760985478758812\n",
            "step: 290, loss: 0.0036420042160898447\n",
            "step: 300, loss: 0.0052647097036242485\n",
            "step: 310, loss: 0.046893078833818436\n",
            "step: 320, loss: 0.005053083412349224\n",
            "step: 330, loss: 0.009030308574438095\n",
            "step: 340, loss: 0.059837453067302704\n",
            "step: 350, loss: 0.025057530030608177\n",
            "step: 360, loss: 0.06780587136745453\n",
            "step: 370, loss: 0.004759653005748987\n",
            "step: 380, loss: 0.006626870483160019\n",
            "step: 390, loss: 0.004992565605789423\n",
            "step: 400, loss: 0.035105518996715546\n",
            "step: 410, loss: 0.0027090057265013456\n",
            "step: 420, loss: 0.0009753339691087604\n",
            "step: 430, loss: 0.017801212146878242\n",
            "step: 440, loss: 0.0009980102768167853\n",
            "step: 450, loss: 0.1379779428243637\n",
            "step: 460, loss: 0.002115562791004777\n",
            "step: 470, loss: 0.006441256031394005\n",
            "step: 480, loss: 0.007757776882499456\n",
            "step: 490, loss: 0.015103660523891449\n",
            "step: 500, loss: 0.07736295461654663\n",
            "step: 510, loss: 0.16256630420684814\n",
            "step: 520, loss: 0.002398537937551737\n",
            "step: 530, loss: 0.02884935960173607\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9238673517048108, f1=0.9232914923291492, best_f1=0.9232914923291492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10501015186309814\n",
            "step: 10, loss: 0.02207767777144909\n",
            "step: 20, loss: 0.0038011674769222736\n",
            "step: 30, loss: 0.09509924054145813\n",
            "step: 40, loss: 0.011400713585317135\n",
            "step: 50, loss: 0.016228633001446724\n",
            "step: 60, loss: 0.006177045404911041\n",
            "step: 70, loss: 0.010879465378820896\n",
            "step: 80, loss: 0.0035539199598133564\n",
            "step: 90, loss: 0.0009259554208256304\n",
            "step: 100, loss: 0.0048835184425115585\n",
            "step: 110, loss: 0.002209028694778681\n",
            "step: 120, loss: 0.01522222999483347\n",
            "step: 130, loss: 0.0012930551311001182\n",
            "step: 140, loss: 0.004833132028579712\n",
            "step: 150, loss: 0.010881110094487667\n",
            "step: 160, loss: 0.0030484984163194895\n",
            "step: 170, loss: 0.02089509181678295\n",
            "step: 180, loss: 0.17789921164512634\n",
            "step: 190, loss: 0.0033127155620604753\n",
            "step: 200, loss: 0.00973569881170988\n",
            "step: 210, loss: 0.0019784148316830397\n",
            "step: 220, loss: 0.004577151499688625\n",
            "step: 230, loss: 0.0009747868170961738\n",
            "step: 240, loss: 0.08039645850658417\n",
            "step: 250, loss: 0.007913777604699135\n",
            "step: 260, loss: 0.07740479707717896\n",
            "step: 270, loss: 0.001622293726541102\n",
            "step: 280, loss: 0.007190435193479061\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 290, loss: 0.20281237363815308\n",
            "step: 300, loss: 0.0053489431738853455\n",
            "step: 310, loss: 0.002891476033255458\n",
            "step: 320, loss: 0.01316593587398529\n",
            "step: 330, loss: 0.03278180956840515\n",
            "step: 340, loss: 0.16700169444084167\n",
            "step: 350, loss: 0.033616963773965836\n",
            "step: 360, loss: 0.004128642380237579\n",
            "step: 370, loss: 0.041178736835718155\n",
            "step: 380, loss: 0.005752590950578451\n",
            "step: 390, loss: 0.0051356153562664986\n",
            "step: 400, loss: 0.28911492228507996\n",
            "step: 410, loss: 0.035610735416412354\n",
            "step: 420, loss: 0.002873928053304553\n",
            "step: 430, loss: 0.008111400529742241\n",
            "step: 440, loss: 0.007176954299211502\n",
            "step: 450, loss: 0.013701566495001316\n",
            "step: 460, loss: 0.005585273262113333\n",
            "step: 470, loss: 0.14043648540973663\n",
            "step: 480, loss: 0.005964913871139288\n",
            "step: 490, loss: 0.08469241112470627\n",
            "step: 500, loss: 0.019702095538377762\n",
            "step: 510, loss: 0.0010489409323781729\n",
            "step: 520, loss: 0.0017738559981808066\n",
            "step: 530, loss: 0.0026496390346437693\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9216043755697356, f1=0.9125683060109289, best_f1=0.9232914923291492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004012977704405785\n",
            "step: 10, loss: 0.0985226184129715\n",
            "step: 20, loss: 0.015622572042047977\n",
            "step: 30, loss: 0.025683004409074783\n",
            "step: 40, loss: 0.006950364913791418\n",
            "step: 50, loss: 0.005830494686961174\n",
            "step: 60, loss: 0.07673219591379166\n",
            "step: 70, loss: 0.0027386893052607775\n",
            "step: 80, loss: 0.0015306684654206038\n",
            "step: 90, loss: 0.009311988949775696\n",
            "step: 100, loss: 0.025752047076821327\n",
            "step: 110, loss: 0.0013349559158086777\n",
            "step: 120, loss: 0.00319209904409945\n",
            "step: 130, loss: 0.02932463213801384\n",
            "step: 140, loss: 0.2258920967578888\n",
            "step: 150, loss: 0.004268335644155741\n",
            "step: 160, loss: 0.0028441878966987133\n",
            "step: 170, loss: 0.05479341745376587\n",
            "step: 180, loss: 0.0038359917234629393\n",
            "step: 190, loss: 0.09724009782075882\n",
            "step: 200, loss: 0.005253995768725872\n",
            "step: 210, loss: 0.2047569304704666\n",
            "step: 220, loss: 0.0015345580177381635\n",
            "step: 230, loss: 0.05508190393447876\n",
            "step: 240, loss: 0.040642403066158295\n",
            "step: 250, loss: 0.003808635286986828\n",
            "step: 260, loss: 0.0013783914037048817\n",
            "step: 270, loss: 0.02492378279566765\n",
            "step: 280, loss: 0.010311906225979328\n",
            "step: 290, loss: 0.04919526353478432\n",
            "step: 300, loss: 0.0017262650653719902\n",
            "step: 310, loss: 0.015141579322516918\n",
            "step: 320, loss: 0.0049286638386547565\n",
            "step: 330, loss: 0.10132134705781937\n",
            "step: 340, loss: 0.035868726670742035\n",
            "step: 350, loss: 0.0007314517861232162\n",
            "step: 360, loss: 0.022731022909283638\n",
            "step: 370, loss: 0.11587131768465042\n",
            "step: 380, loss: 0.0026036230847239494\n",
            "step: 390, loss: 0.0033268628176301718\n",
            "step: 400, loss: 0.009251156821846962\n",
            "step: 410, loss: 0.03747495263814926\n",
            "step: 420, loss: 0.006303034722805023\n",
            "step: 430, loss: 0.0010071491124108434\n",
            "step: 440, loss: 0.020210135728120804\n",
            "step: 450, loss: 0.011574364267289639\n",
            "step: 460, loss: 0.014356336556375027\n",
            "step: 470, loss: 0.0790020301938057\n",
            "step: 480, loss: 0.015409826301038265\n",
            "step: 490, loss: 0.037560705095529556\n",
            "step: 500, loss: 0.0010831007966771722\n",
            "step: 510, loss: 0.1544981300830841\n",
            "step: 520, loss: 0.001768853748217225\n",
            "step: 530, loss: 0.009600410237908363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9257722452743199, f1=0.9130434782608695, best_f1=0.9130434782608695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015163797652348876\n",
            "step: 10, loss: 0.01659390702843666\n",
            "step: 20, loss: 0.016154251992702484\n",
            "step: 30, loss: 0.07912939041852951\n",
            "step: 40, loss: 0.002970188157632947\n",
            "step: 50, loss: 0.02704489603638649\n",
            "step: 60, loss: 0.002572939032688737\n",
            "step: 70, loss: 0.010081132873892784\n",
            "step: 80, loss: 0.18791097402572632\n",
            "step: 90, loss: 0.0412900485098362\n",
            "step: 100, loss: 0.0020864016842097044\n",
            "step: 110, loss: 0.17503143846988678\n",
            "step: 120, loss: 0.0507887601852417\n",
            "step: 130, loss: 0.01187089178711176\n",
            "step: 140, loss: 0.010608943179249763\n",
            "step: 150, loss: 0.01396444346755743\n",
            "step: 160, loss: 0.004205103497952223\n",
            "step: 170, loss: 0.026084251701831818\n",
            "step: 180, loss: 0.20834234356880188\n",
            "step: 190, loss: 0.003672792576253414\n",
            "step: 200, loss: 0.00840406771749258\n",
            "step: 210, loss: 0.19714149832725525\n",
            "step: 220, loss: 0.0021318045910447836\n",
            "step: 230, loss: 0.030214346945285797\n",
            "step: 240, loss: 0.03767986595630646\n",
            "step: 250, loss: 0.04798346385359764\n",
            "step: 260, loss: 0.0029067164286971092\n",
            "step: 270, loss: 0.0034761414863169193\n",
            "step: 280, loss: 0.0022663038689643145\n",
            "step: 290, loss: 0.006923946551978588\n",
            "step: 300, loss: 0.02205900475382805\n",
            "step: 310, loss: 0.008356927894055843\n",
            "step: 320, loss: 0.0017449064180254936\n",
            "step: 330, loss: 0.012744028121232986\n",
            "step: 340, loss: 0.1126837506890297\n",
            "step: 350, loss: 0.020594701170921326\n",
            "step: 360, loss: 0.0012399459956213832\n",
            "step: 370, loss: 0.001360153197310865\n",
            "step: 380, loss: 0.004158307332545519\n",
            "step: 390, loss: 0.0015682230005040765\n",
            "step: 400, loss: 0.15446846187114716\n",
            "step: 410, loss: 0.008747043088078499\n",
            "step: 420, loss: 0.0047064973041415215\n",
            "step: 430, loss: 0.020939424633979797\n",
            "step: 440, loss: 0.0012210042914375663\n",
            "step: 450, loss: 0.018608756363391876\n",
            "step: 460, loss: 0.0011678199516609311\n",
            "step: 470, loss: 0.015468657948076725\n",
            "step: 480, loss: 0.001060259761288762\n",
            "step: 490, loss: 0.0025018120650202036\n",
            "step: 500, loss: 0.004551149904727936\n",
            "step: 510, loss: 0.013017961755394936\n",
            "step: 520, loss: 0.18548667430877686\n",
            "step: 530, loss: 0.004543748218566179\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9205328433624255, f1=0.9186733303044072, best_f1=0.9130434782608695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0036567479837685823\n",
            "step: 10, loss: 0.002251212950795889\n",
            "step: 20, loss: 0.0015706279082223773\n",
            "step: 30, loss: 0.0023645805194973946\n",
            "step: 40, loss: 0.0009794748621061444\n",
            "step: 50, loss: 0.0007037596078589559\n",
            "step: 60, loss: 0.0014986684545874596\n",
            "step: 70, loss: 0.002694555092602968\n",
            "step: 80, loss: 0.03988974913954735\n",
            "step: 90, loss: 0.008473369292914867\n",
            "step: 100, loss: 0.03364599123597145\n",
            "step: 110, loss: 0.009783929213881493\n",
            "step: 120, loss: 0.0007856017909944057\n",
            "step: 130, loss: 0.10580138117074966\n",
            "step: 140, loss: 0.0037035555578768253\n",
            "step: 150, loss: 0.007887299172580242\n",
            "step: 160, loss: 0.10933302342891693\n",
            "step: 170, loss: 0.00986415520310402\n",
            "step: 180, loss: 0.0036022942513227463\n",
            "step: 190, loss: 0.0019499697955325246\n",
            "step: 200, loss: 0.001222729915753007\n",
            "step: 210, loss: 0.0019901571795344353\n",
            "step: 220, loss: 0.0047682481817901134\n",
            "step: 230, loss: 0.0007458811742253602\n",
            "step: 240, loss: 0.0016404452035203576\n",
            "step: 250, loss: 0.005635388195514679\n",
            "step: 260, loss: 0.015194381587207317\n",
            "step: 270, loss: 0.0008700048783794045\n",
            "step: 280, loss: 0.06652333587408066\n",
            "step: 290, loss: 0.0012459177523851395\n",
            "step: 300, loss: 0.03633175045251846\n",
            "step: 310, loss: 0.06548359990119934\n",
            "step: 320, loss: 0.012010066770017147\n",
            "step: 330, loss: 0.0026208420749753714\n",
            "step: 340, loss: 0.0016380222514271736\n",
            "step: 350, loss: 0.0006561008049175143\n",
            "step: 360, loss: 0.0003287906583864242\n",
            "step: 370, loss: 0.0011079973774030805\n",
            "step: 380, loss: 0.000591972260735929\n",
            "step: 390, loss: 0.0026066857390105724\n",
            "step: 400, loss: 0.006370049901306629\n",
            "step: 410, loss: 0.0018013442168012261\n",
            "step: 420, loss: 0.0011760811321437359\n",
            "step: 430, loss: 0.0007921199430711567\n",
            "step: 440, loss: 0.0038201971910893917\n",
            "step: 450, loss: 0.06317632645368576\n",
            "step: 460, loss: 0.0016202405095100403\n",
            "step: 470, loss: 0.015186609700322151\n",
            "step: 480, loss: 0.006342931184917688\n",
            "step: 490, loss: 0.04835737496614456\n",
            "step: 500, loss: 0.009976787492632866\n",
            "step: 510, loss: 0.04686089977622032\n",
            "step: 520, loss: 0.23728260397911072\n",
            "step: 530, loss: 0.015155879780650139\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9246823956442831, f1=0.9151157512482978, best_f1=0.9130434782608695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004691645503044128\n",
            "step: 10, loss: 0.0265961941331625\n",
            "step: 20, loss: 0.030887421220541\n",
            "step: 30, loss: 0.0017487769946455956\n",
            "step: 40, loss: 0.0004988955333828926\n",
            "step: 50, loss: 0.0016658138483762741\n",
            "step: 60, loss: 0.0006401002174243331\n",
            "step: 70, loss: 0.005013283807784319\n",
            "step: 80, loss: 0.0005511431954801083\n",
            "step: 90, loss: 0.000937897537369281\n",
            "step: 100, loss: 0.0035937426146119833\n",
            "step: 110, loss: 0.0004886435344815254\n",
            "step: 120, loss: 0.0005697210435755551\n",
            "step: 130, loss: 0.0004334079276304692\n",
            "step: 140, loss: 0.0010330491932108998\n",
            "step: 150, loss: 0.007776985410600901\n",
            "step: 160, loss: 0.00041327139479108155\n",
            "step: 170, loss: 0.0051018111407756805\n",
            "step: 180, loss: 0.00630735419690609\n",
            "step: 190, loss: 0.00563550041988492\n",
            "step: 200, loss: 0.0005984751041978598\n",
            "step: 210, loss: 0.005397078115493059\n",
            "step: 220, loss: 0.0013402157928794622\n",
            "step: 230, loss: 0.013451017439365387\n",
            "step: 240, loss: 0.00254154484719038\n",
            "step: 250, loss: 0.03395452722907066\n",
            "step: 260, loss: 0.0036126456689089537\n",
            "step: 270, loss: 0.007790683768689632\n",
            "step: 280, loss: 0.0032086200080811977\n",
            "step: 290, loss: 0.000360988691681996\n",
            "step: 300, loss: 0.12921807169914246\n",
            "step: 310, loss: 0.03421911969780922\n",
            "step: 320, loss: 0.008983093313872814\n",
            "step: 330, loss: 0.0006895131664350629\n",
            "step: 340, loss: 0.029261719435453415\n",
            "step: 350, loss: 0.020682338625192642\n",
            "step: 360, loss: 0.009830488823354244\n",
            "step: 370, loss: 0.0038620547857135534\n",
            "step: 380, loss: 0.0014512869529426098\n",
            "step: 390, loss: 0.0007721766596660018\n",
            "step: 400, loss: 0.0032302450854331255\n",
            "step: 410, loss: 0.010747838765382767\n",
            "step: 420, loss: 0.0035302285104990005\n",
            "step: 430, loss: 0.0018517569405958056\n",
            "step: 440, loss: 0.0023376806639134884\n",
            "step: 450, loss: 0.0014837830094620585\n",
            "step: 460, loss: 0.003163191257044673\n",
            "step: 470, loss: 0.001881306990981102\n",
            "step: 480, loss: 0.0031347908079624176\n",
            "step: 490, loss: 0.002632132265716791\n",
            "step: 500, loss: 0.0016785489860922098\n",
            "step: 510, loss: 0.002844605129212141\n",
            "step: 520, loss: 0.001574427355080843\n",
            "step: 530, loss: 0.004092956893146038\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9189189189189189, f1=0.9143897996357012, best_f1=0.9130434782608695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009150452679023147\n",
            "step: 10, loss: 0.0014096691738814116\n",
            "step: 20, loss: 0.17969825863838196\n",
            "step: 30, loss: 0.0007243262371048331\n",
            "step: 40, loss: 0.0014026880962774158\n",
            "step: 50, loss: 0.0015134032582864165\n",
            "step: 60, loss: 0.006840801797807217\n",
            "step: 70, loss: 0.0008777836337685585\n",
            "step: 80, loss: 0.0009846326429396868\n",
            "step: 90, loss: 0.0034589588176459074\n",
            "step: 100, loss: 0.002700434299185872\n",
            "step: 110, loss: 0.0008381738443858922\n",
            "step: 120, loss: 0.0014708969974890351\n",
            "step: 130, loss: 0.0016707169124856591\n",
            "step: 140, loss: 0.007902368903160095\n",
            "step: 150, loss: 0.0006314184283837676\n",
            "step: 160, loss: 0.05621441826224327\n",
            "step: 170, loss: 0.0012318645603954792\n",
            "step: 180, loss: 0.0008037746883928776\n",
            "step: 190, loss: 0.0010862494818866253\n",
            "step: 200, loss: 0.0010483120568096638\n",
            "step: 210, loss: 0.19801536202430725\n",
            "step: 220, loss: 0.001158258761279285\n",
            "step: 230, loss: 0.012947685085237026\n",
            "step: 240, loss: 0.0033683404326438904\n",
            "step: 250, loss: 0.005209184717386961\n",
            "step: 260, loss: 0.0015906081534922123\n",
            "step: 270, loss: 0.0025895629078149796\n",
            "step: 280, loss: 0.0031487918458878994\n",
            "step: 290, loss: 0.0034981579519808292\n",
            "step: 300, loss: 0.011741465888917446\n",
            "step: 310, loss: 0.0025048665702342987\n",
            "step: 320, loss: 0.002531621605157852\n",
            "step: 330, loss: 0.003538500051945448\n",
            "step: 340, loss: 0.0005084144650027156\n",
            "step: 350, loss: 0.0003558180760592222\n",
            "step: 360, loss: 0.0006113371346145868\n",
            "step: 370, loss: 0.00029899488436058164\n",
            "step: 380, loss: 0.0004632028576452285\n",
            "step: 390, loss: 0.014787690714001656\n",
            "step: 400, loss: 0.00037155457539483905\n",
            "step: 410, loss: 0.04817451909184456\n",
            "step: 420, loss: 0.0003002102894242853\n",
            "step: 430, loss: 0.008048463612794876\n",
            "step: 440, loss: 0.02615337073802948\n",
            "step: 450, loss: 0.0007803601329214871\n",
            "step: 460, loss: 0.0006218502530828118\n",
            "step: 470, loss: 0.002761293202638626\n",
            "step: 480, loss: 0.005948053672909737\n",
            "step: 490, loss: 0.0045326584950089455\n",
            "step: 500, loss: 0.0034872102551162243\n",
            "step: 510, loss: 0.0025775611866265535\n",
            "step: 520, loss: 0.0038979605305939913\n",
            "step: 530, loss: 0.004244874697178602\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9189189189189189, f1=0.9137697516930023, best_f1=0.9130434782608695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011406268458813429\n",
            "step: 10, loss: 0.005292908754199743\n",
            "step: 20, loss: 0.009033527225255966\n",
            "step: 30, loss: 0.0018019620329141617\n",
            "step: 40, loss: 0.0023066415451467037\n",
            "step: 50, loss: 0.008561034686863422\n",
            "step: 60, loss: 0.000979105127044022\n",
            "step: 70, loss: 0.003067555371671915\n",
            "step: 80, loss: 0.047696564346551895\n",
            "step: 90, loss: 0.009290033020079136\n",
            "step: 100, loss: 0.0005921151023358107\n",
            "step: 110, loss: 0.0030332414899021387\n",
            "step: 120, loss: 0.0017853158060461283\n",
            "step: 130, loss: 0.00031311833299696445\n",
            "step: 140, loss: 0.000756744178943336\n",
            "step: 150, loss: 0.001676490530371666\n",
            "step: 160, loss: 0.19387243688106537\n",
            "step: 170, loss: 0.018412642180919647\n",
            "step: 180, loss: 0.0017423885874450207\n",
            "step: 190, loss: 0.0010798489674925804\n",
            "step: 200, loss: 0.0007955605397000909\n",
            "step: 210, loss: 0.001884011086076498\n",
            "step: 220, loss: 0.0006206681136973202\n",
            "step: 230, loss: 0.003783195512369275\n",
            "step: 240, loss: 0.0017603242304176092\n",
            "step: 250, loss: 0.0049663023091852665\n",
            "step: 260, loss: 0.000683958234731108\n",
            "step: 270, loss: 0.0036724978126585484\n",
            "step: 280, loss: 0.10866707563400269\n",
            "step: 290, loss: 0.0007166583091020584\n",
            "step: 300, loss: 0.002208842197433114\n",
            "step: 310, loss: 0.0006809354526922107\n",
            "step: 320, loss: 0.0005737656028941274\n",
            "step: 330, loss: 0.0011745308293029666\n",
            "step: 340, loss: 0.014402462169528008\n",
            "step: 350, loss: 0.00046257703797891736\n",
            "step: 360, loss: 0.11535166203975677\n",
            "step: 370, loss: 0.0041632382199168205\n",
            "step: 380, loss: 0.001633987994864583\n",
            "step: 390, loss: 0.0007936495239846408\n",
            "step: 400, loss: 0.0010804540943354368\n",
            "step: 410, loss: 0.0008431979222223163\n",
            "step: 420, loss: 0.001810406451113522\n",
            "step: 430, loss: 0.0242938045412302\n",
            "step: 440, loss: 0.0006058414001017809\n",
            "step: 450, loss: 0.000641107268165797\n",
            "step: 460, loss: 0.0011184219038113952\n",
            "step: 470, loss: 0.001885362551547587\n",
            "step: 480, loss: 0.0019416327122598886\n",
            "step: 490, loss: 0.012223012745380402\n",
            "step: 500, loss: 0.00026096386136487126\n",
            "step: 510, loss: 0.0008804411045275629\n",
            "step: 520, loss: 0.0006605254602618515\n",
            "step: 530, loss: 0.0014459743397310376\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9203539823008848, f1=0.9194444444444445, best_f1=0.9130434782608695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020873630419373512\n",
            "step: 10, loss: 0.0012613923754543066\n",
            "step: 20, loss: 0.0004583942936733365\n",
            "step: 30, loss: 0.0022074186708778143\n",
            "step: 40, loss: 0.00046738641685806215\n",
            "step: 50, loss: 0.000568790128454566\n",
            "step: 60, loss: 0.0010613470803946257\n",
            "step: 70, loss: 0.0007946642581373453\n",
            "step: 80, loss: 0.0006122337654232979\n",
            "step: 90, loss: 0.00033192537375725806\n",
            "step: 100, loss: 0.0007361328462138772\n",
            "step: 110, loss: 0.00044317528954707086\n",
            "step: 120, loss: 0.0004352094838395715\n",
            "step: 130, loss: 0.0003870263753924519\n",
            "step: 140, loss: 0.004387493245303631\n",
            "step: 150, loss: 0.00044863048242405057\n",
            "step: 160, loss: 0.0013719775015488267\n",
            "step: 170, loss: 0.0030535710975527763\n",
            "step: 180, loss: 0.00042174156988039613\n",
            "step: 190, loss: 0.0005055476212874055\n",
            "step: 200, loss: 0.0006175029557198286\n",
            "step: 210, loss: 0.0005698434542864561\n",
            "step: 220, loss: 0.000480829447042197\n",
            "step: 230, loss: 0.0004235242959111929\n",
            "step: 240, loss: 0.00023546034935861826\n",
            "step: 250, loss: 0.009064515121281147\n",
            "step: 260, loss: 0.0004121811653021723\n",
            "step: 270, loss: 0.0005295826122164726\n",
            "step: 280, loss: 0.005964611191302538\n",
            "step: 290, loss: 0.0007313134265132248\n",
            "step: 300, loss: 0.0004595088539645076\n",
            "step: 310, loss: 0.0098929638043046\n",
            "step: 320, loss: 0.004491677042096853\n",
            "step: 330, loss: 0.0013515790924429893\n",
            "step: 340, loss: 0.002245261799544096\n",
            "step: 350, loss: 0.00039360413211397827\n",
            "step: 360, loss: 0.000439281458966434\n",
            "step: 370, loss: 0.0005070788902230561\n",
            "step: 380, loss: 0.0008012718753889203\n",
            "step: 390, loss: 0.0023645306937396526\n",
            "step: 400, loss: 0.021091675385832787\n",
            "step: 410, loss: 0.00023155577946454287\n",
            "step: 420, loss: 0.005995175801217556\n",
            "step: 430, loss: 0.0004960451042279601\n",
            "step: 440, loss: 0.022029360756278038\n",
            "step: 450, loss: 0.00044227830949239433\n",
            "step: 460, loss: 0.017770428210496902\n",
            "step: 470, loss: 0.0006563760107383132\n",
            "step: 480, loss: 0.0005035363137722015\n",
            "step: 490, loss: 0.003887646598741412\n",
            "step: 500, loss: 0.1983850598335266\n",
            "step: 510, loss: 0.02826749160885811\n",
            "step: 520, loss: 0.00026039427029900253\n",
            "step: 530, loss: 0.0006570414989255369\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9235757295044, f1=0.9205909510618652, best_f1=0.9130434782608695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000685620354488492\n",
            "step: 10, loss: 0.0020957994274795055\n",
            "step: 20, loss: 0.001452335505746305\n",
            "step: 30, loss: 0.02140270173549652\n",
            "step: 40, loss: 0.0005848715081810951\n",
            "step: 50, loss: 0.0015297731151804328\n",
            "step: 60, loss: 0.0008107298635877669\n",
            "step: 70, loss: 0.0007561798556707799\n",
            "step: 80, loss: 0.00045278004836291075\n",
            "step: 90, loss: 0.000367650791304186\n",
            "step: 100, loss: 0.0002122376172337681\n",
            "step: 110, loss: 0.14868929982185364\n",
            "step: 120, loss: 0.0005801466759294271\n",
            "step: 130, loss: 0.0016229747561737895\n",
            "step: 140, loss: 0.001531061832793057\n",
            "step: 150, loss: 0.0007574281771667302\n",
            "step: 160, loss: 0.0006967707886360586\n",
            "step: 170, loss: 0.0002584295580163598\n",
            "step: 180, loss: 0.002704012906178832\n",
            "step: 190, loss: 0.009394517168402672\n",
            "step: 200, loss: 0.0005220242310315371\n",
            "step: 210, loss: 0.001082940143533051\n",
            "step: 220, loss: 0.0005869147134944797\n",
            "step: 230, loss: 0.0010986273409798741\n",
            "step: 240, loss: 0.0034582328516989946\n",
            "step: 250, loss: 0.0010584791889414191\n",
            "step: 260, loss: 0.0003900118754245341\n",
            "step: 270, loss: 0.0005982285365462303\n",
            "step: 280, loss: 0.000511002610437572\n",
            "step: 290, loss: 0.007243099622428417\n",
            "step: 300, loss: 0.00036729214480146766\n",
            "step: 310, loss: 0.041408102959394455\n",
            "step: 320, loss: 0.0010249100159853697\n",
            "step: 330, loss: 0.0013091614237055182\n",
            "step: 340, loss: 0.004806044511497021\n",
            "step: 350, loss: 0.0034369854256510735\n",
            "step: 360, loss: 0.0005120699643157423\n",
            "step: 370, loss: 0.0008455016650259495\n",
            "step: 380, loss: 0.0008612275123596191\n",
            "step: 390, loss: 0.000502693117596209\n",
            "step: 400, loss: 0.0010986081324517727\n",
            "step: 410, loss: 0.0019210614264011383\n",
            "step: 420, loss: 0.0020384013187140226\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 430, loss: 0.00027446020976640284\n",
            "step: 440, loss: 0.18959514796733856\n",
            "step: 450, loss: 0.0008807061240077019\n",
            "step: 460, loss: 0.00044351289398036897\n",
            "step: 470, loss: 0.0004656514502130449\n",
            "step: 480, loss: 0.0009682451491244137\n",
            "step: 490, loss: 0.0006480729207396507\n",
            "step: 500, loss: 0.0003596169117372483\n",
            "step: 510, loss: 0.0005702492780983448\n",
            "step: 520, loss: 0.0005674765561707318\n",
            "step: 530, loss: 0.00035922107053920627\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.92243381328379, f1=0.920796665122742, best_f1=0.9130434782608695\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:34, 164.41it/s]\n",
            "load_f1 = 0.9213793103448276\n",
            "real_f1 = 0.9170549860205033\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:29, 147.32it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdkSbOTQeC3W"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4XWqpo1e0O0",
        "outputId": "e619dbf4-3006-4135-bc67-be41bde39ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5065819621086121\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.49555039405822754\n",
            "step: 20, loss: 0.5027462244033813\n",
            "step: 30, loss: 0.29124391078948975\n",
            "step: 40, loss: 0.31876257061958313\n",
            "step: 50, loss: 0.43278881907463074\n",
            "step: 60, loss: 0.4955633282661438\n",
            "step: 70, loss: 0.3042401373386383\n",
            "step: 80, loss: 0.3719005286693573\n",
            "step: 90, loss: 0.2751767039299011\n",
            "step: 100, loss: 0.22545351088047028\n",
            "step: 110, loss: 0.2822023034095764\n",
            "step: 120, loss: 0.38511574268341064\n",
            "step: 130, loss: 0.25948479771614075\n",
            "step: 140, loss: 0.47530362010002136\n",
            "step: 150, loss: 0.37561458349227905\n",
            "step: 160, loss: 0.5102120041847229\n",
            "step: 170, loss: 0.22757142782211304\n",
            "step: 180, loss: 0.320095032453537\n",
            "step: 190, loss: 0.4974554181098938\n",
            "step: 200, loss: 0.3054679036140442\n",
            "step: 210, loss: 0.3119015693664551\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3055555555555556, f1=0.30561797752808983, best_f1=0.30561797752808983\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.33213019371032715\n",
            "step: 10, loss: 0.15780100226402283\n",
            "step: 20, loss: 0.5023573637008667\n",
            "step: 30, loss: 0.5443429946899414\n",
            "step: 40, loss: 0.44649413228034973\n",
            "step: 50, loss: 0.26675158739089966\n",
            "step: 60, loss: 0.3253406286239624\n",
            "step: 70, loss: 0.44081681966781616\n",
            "step: 80, loss: 0.3076709806919098\n",
            "step: 90, loss: 0.36771920323371887\n",
            "step: 100, loss: 0.5077871084213257\n",
            "step: 110, loss: 0.3715517222881317\n",
            "step: 120, loss: 0.2281557321548462\n",
            "step: 130, loss: 0.17745822668075562\n",
            "step: 140, loss: 0.24348647892475128\n",
            "step: 150, loss: 0.4239214062690735\n",
            "step: 160, loss: 0.18244795501232147\n",
            "step: 170, loss: 0.5681547522544861\n",
            "step: 180, loss: 0.3221178948879242\n",
            "step: 190, loss: 0.2925656735897064\n",
            "step: 200, loss: 0.1702505350112915\n",
            "step: 210, loss: 0.3171185553073883\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.1852731591448931, f1=0.18519984170953702, best_f1=0.30561797752808983\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25398433208465576\n",
            "step: 10, loss: 0.26919251680374146\n",
            "step: 20, loss: 0.4791133403778076\n",
            "step: 30, loss: 0.2715141177177429\n",
            "step: 40, loss: 0.45798879861831665\n",
            "step: 50, loss: 0.48420822620391846\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.5333207249641418\n",
            "step: 70, loss: 0.1938079446554184\n",
            "step: 80, loss: 0.44735482335090637\n",
            "step: 90, loss: 0.24294739961624146\n",
            "step: 100, loss: 0.3450112044811249\n",
            "step: 110, loss: 0.24099892377853394\n",
            "step: 120, loss: 0.24695149064064026\n",
            "step: 130, loss: 0.15687936544418335\n",
            "step: 140, loss: 0.3959440588951111\n",
            "step: 150, loss: 0.31796470284461975\n",
            "step: 160, loss: 0.2071683257818222\n",
            "step: 170, loss: 0.3816579580307007\n",
            "step: 180, loss: 0.24725496768951416\n",
            "step: 190, loss: 0.16550230979919434\n",
            "step: 200, loss: 0.2555943429470062\n",
            "step: 210, loss: 0.27832263708114624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.30561797752808983\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2829914689064026\n",
            "step: 10, loss: 0.2811829447746277\n",
            "step: 20, loss: 0.3338456451892853\n",
            "step: 30, loss: 0.2518210709095001\n",
            "step: 40, loss: 0.25160592794418335\n",
            "step: 50, loss: 0.24989177286624908\n",
            "step: 60, loss: 0.4900056719779968\n",
            "step: 70, loss: 0.2521796226501465\n",
            "step: 80, loss: 0.24904510378837585\n",
            "step: 90, loss: 0.45277848839759827\n",
            "step: 100, loss: 0.38018468022346497\n",
            "step: 110, loss: 0.574101448059082\n",
            "step: 120, loss: 0.38805586099624634\n",
            "step: 130, loss: 0.6942037343978882\n",
            "step: 140, loss: 0.5077409148216248\n",
            "step: 150, loss: 0.3803161680698395\n",
            "step: 160, loss: 0.31201720237731934\n",
            "step: 170, loss: 0.17997628450393677\n",
            "step: 180, loss: 0.08676380664110184\n",
            "step: 190, loss: 0.16596779227256775\n",
            "step: 200, loss: 0.27736207842826843\n",
            "step: 210, loss: 0.39292776584625244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.30561797752808983\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3688429892063141\n",
            "step: 10, loss: 0.3204520642757416\n",
            "step: 20, loss: 0.3118830621242523\n",
            "step: 30, loss: 0.23533868789672852\n",
            "step: 40, loss: 0.43399038910865784\n",
            "step: 50, loss: 0.3744618892669678\n",
            "step: 60, loss: 0.38819563388824463\n",
            "step: 70, loss: 0.22943098843097687\n",
            "step: 80, loss: 0.4271084666252136\n",
            "step: 90, loss: 0.45880040526390076\n",
            "step: 100, loss: 0.24574096500873566\n",
            "step: 110, loss: 0.16516192257404327\n",
            "step: 120, loss: 0.24789780378341675\n",
            "step: 130, loss: 0.2826957106590271\n",
            "step: 140, loss: 0.5292657613754272\n",
            "step: 150, loss: 0.34256768226623535\n",
            "step: 160, loss: 0.2335994392633438\n",
            "step: 170, loss: 0.36919403076171875\n",
            "step: 180, loss: 0.24129554629325867\n",
            "step: 190, loss: 0.5207574367523193\n",
            "step: 200, loss: 0.4055531620979309\n",
            "step: 210, loss: 0.21476352214813232\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.23902439024390246, f1=0.2473684210526316, best_f1=0.30561797752808983\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1333063691854477\n",
            "step: 10, loss: 0.38769373297691345\n",
            "step: 20, loss: 0.3081243634223938\n",
            "step: 30, loss: 0.2625757157802582\n",
            "step: 40, loss: 0.2783297002315521\n",
            "step: 50, loss: 0.5030919909477234\n",
            "step: 60, loss: 0.2252502143383026\n",
            "step: 70, loss: 0.34827205538749695\n",
            "step: 80, loss: 0.2244080752134323\n",
            "step: 90, loss: 0.2564213275909424\n",
            "step: 100, loss: 0.23919008672237396\n",
            "step: 110, loss: 0.25458383560180664\n",
            "step: 120, loss: 0.17664596438407898\n",
            "step: 130, loss: 0.39128732681274414\n",
            "step: 140, loss: 0.318776398897171\n",
            "step: 150, loss: 0.13356110453605652\n",
            "step: 160, loss: 0.14964479207992554\n",
            "step: 170, loss: 0.26696595549583435\n",
            "step: 180, loss: 0.2821234166622162\n",
            "step: 190, loss: 0.1951409876346588\n",
            "step: 200, loss: 0.29206565022468567\n",
            "step: 210, loss: 0.44061926007270813\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.25386996904024767, f1=0.2990654205607477, best_f1=0.30561797752808983\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.45984742045402527\n",
            "step: 10, loss: 0.2673366963863373\n",
            "step: 20, loss: 0.2883480191230774\n",
            "step: 30, loss: 0.223985493183136\n",
            "step: 40, loss: 0.10177639871835709\n",
            "step: 50, loss: 0.2546747028827667\n",
            "step: 60, loss: 0.2331138551235199\n",
            "step: 70, loss: 0.20482118427753448\n",
            "step: 80, loss: 0.20412947237491608\n",
            "step: 90, loss: 0.306633859872818\n",
            "step: 100, loss: 0.2730729877948761\n",
            "step: 110, loss: 0.291764497756958\n",
            "step: 120, loss: 0.25013500452041626\n",
            "step: 130, loss: 0.40603017807006836\n",
            "step: 140, loss: 0.2395932674407959\n",
            "step: 150, loss: 0.23244275152683258\n",
            "step: 160, loss: 0.3835136294364929\n",
            "step: 170, loss: 0.5696312189102173\n",
            "step: 180, loss: 0.1781913936138153\n",
            "step: 190, loss: 0.22178325057029724\n",
            "step: 200, loss: 0.21187593042850494\n",
            "step: 210, loss: 0.2995346784591675\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.49914529914529915, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30750706791877747\n",
            "step: 10, loss: 0.20194511115550995\n",
            "step: 20, loss: 0.16911791265010834\n",
            "step: 30, loss: 0.34724050760269165\n",
            "step: 40, loss: 0.1732526421546936\n",
            "step: 50, loss: 0.08372803777456284\n",
            "step: 60, loss: 0.13521891832351685\n",
            "step: 70, loss: 0.3633383810520172\n",
            "step: 80, loss: 0.2124808430671692\n",
            "step: 90, loss: 0.3580867350101471\n",
            "step: 100, loss: 0.23321110010147095\n",
            "step: 110, loss: 0.18804334104061127\n",
            "step: 120, loss: 0.2705872356891632\n",
            "step: 130, loss: 0.05917583033442497\n",
            "step: 140, loss: 0.23120525479316711\n",
            "step: 150, loss: 0.3558219075202942\n",
            "step: 160, loss: 0.23279419541358948\n",
            "step: 170, loss: 0.30315932631492615\n",
            "step: 180, loss: 0.24486394226551056\n",
            "step: 190, loss: 0.12324802577495575\n",
            "step: 200, loss: 0.1882697194814682\n",
            "step: 210, loss: 0.2838694453239441\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.5205479452054794, f1=0.5308219178082191, best_f1=0.5308219178082191\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.264043390750885\n",
            "step: 10, loss: 0.24249373376369476\n",
            "step: 20, loss: 0.14704087376594543\n",
            "step: 30, loss: 0.10017545521259308\n",
            "step: 40, loss: 0.08500445634126663\n",
            "step: 50, loss: 0.4289775788784027\n",
            "step: 60, loss: 0.10483985394239426\n",
            "step: 70, loss: 0.3085933327674866\n",
            "step: 80, loss: 0.15304934978485107\n",
            "step: 90, loss: 0.30083996057510376\n",
            "step: 100, loss: 0.16598625481128693\n",
            "step: 110, loss: 0.23102878034114838\n",
            "step: 120, loss: 0.4273487329483032\n",
            "step: 130, loss: 0.18230365216732025\n",
            "step: 140, loss: 0.29915285110473633\n",
            "step: 150, loss: 0.13024233281612396\n",
            "step: 160, loss: 0.16693635284900665\n",
            "step: 170, loss: 0.2088109850883484\n",
            "step: 180, loss: 0.22826242446899414\n",
            "step: 190, loss: 0.17900249361991882\n",
            "step: 200, loss: 0.0854855552315712\n",
            "step: 210, loss: 0.18814575672149658\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.5349794238683128, f1=0.5330578512396694, best_f1=0.5330578512396694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2029745876789093\n",
            "step: 10, loss: 0.18028314411640167\n",
            "step: 20, loss: 0.16303081810474396\n",
            "step: 30, loss: 0.035686250776052475\n",
            "step: 40, loss: 0.16796109080314636\n",
            "step: 50, loss: 0.18492422997951508\n",
            "step: 60, loss: 0.05897346884012222\n",
            "step: 70, loss: 0.12670700252056122\n",
            "step: 80, loss: 0.0857561007142067\n",
            "step: 90, loss: 0.11107026040554047\n",
            "step: 100, loss: 0.2470582276582718\n",
            "step: 110, loss: 0.05381181091070175\n",
            "step: 120, loss: 0.2899174392223358\n",
            "step: 130, loss: 0.02288675308227539\n",
            "step: 140, loss: 0.0805114135146141\n",
            "step: 150, loss: 0.07252947986125946\n",
            "step: 160, loss: 0.10479419678449631\n",
            "step: 170, loss: 0.044612426310777664\n",
            "step: 180, loss: 0.09795096516609192\n",
            "step: 190, loss: 0.08415444195270538\n",
            "step: 200, loss: 0.28096774220466614\n",
            "step: 210, loss: 0.11323501914739609\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5318471337579618, f1=0.54281098546042, best_f1=0.5330578512396694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1450858861207962\n",
            "step: 10, loss: 0.08217116445302963\n",
            "step: 20, loss: 0.02624645084142685\n",
            "step: 30, loss: 0.03313416987657547\n",
            "step: 40, loss: 0.12248758971691132\n",
            "step: 50, loss: 0.17248652875423431\n",
            "step: 60, loss: 0.11512826383113861\n",
            "step: 70, loss: 0.04687308892607689\n",
            "step: 80, loss: 0.25571075081825256\n",
            "step: 90, loss: 0.29135552048683167\n",
            "step: 100, loss: 0.34516221284866333\n",
            "step: 110, loss: 0.11340992897748947\n",
            "step: 120, loss: 0.08529014140367508\n",
            "step: 130, loss: 0.024884231388568878\n",
            "step: 140, loss: 0.04209569841623306\n",
            "step: 150, loss: 0.24697698652744293\n",
            "step: 160, loss: 0.010779780335724354\n",
            "step: 170, loss: 0.07810559868812561\n",
            "step: 180, loss: 0.06346392631530762\n",
            "step: 190, loss: 0.20351332426071167\n",
            "step: 200, loss: 0.06188318878412247\n",
            "step: 210, loss: 0.20884555578231812\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.525, f1=0.5539033457249071, best_f1=0.5330578512396694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04249080643057823\n",
            "step: 10, loss: 0.18806129693984985\n",
            "step: 20, loss: 0.3089653253555298\n",
            "step: 30, loss: 0.07189373672008514\n",
            "step: 40, loss: 0.02473921701312065\n",
            "step: 50, loss: 0.2986468970775604\n",
            "step: 60, loss: 0.028413791209459305\n",
            "step: 70, loss: 0.14558196067810059\n",
            "step: 80, loss: 0.08965042233467102\n",
            "step: 90, loss: 0.24180255830287933\n",
            "step: 100, loss: 0.013772975653409958\n",
            "step: 110, loss: 0.17000170052051544\n",
            "step: 120, loss: 0.01800685189664364\n",
            "step: 130, loss: 0.29954224824905396\n",
            "step: 140, loss: 0.1297782063484192\n",
            "step: 150, loss: 0.022460419684648514\n",
            "step: 160, loss: 0.047492824494838715\n",
            "step: 170, loss: 0.14762920141220093\n",
            "step: 180, loss: 0.08005271106958389\n",
            "step: 190, loss: 0.01940738782286644\n",
            "step: 200, loss: 0.009337091818451881\n",
            "step: 210, loss: 0.0740518793463707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.5354058721934368, f1=0.5317324185248713, best_f1=0.5317324185248713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02492845430970192\n",
            "step: 10, loss: 0.03039163164794445\n",
            "step: 20, loss: 0.03930874168872833\n",
            "step: 30, loss: 0.031709618866443634\n",
            "step: 40, loss: 0.08282869309186935\n",
            "step: 50, loss: 0.10373611748218536\n",
            "step: 60, loss: 0.1241961345076561\n",
            "step: 70, loss: 0.20110656321048737\n",
            "step: 80, loss: 0.04200339689850807\n",
            "step: 90, loss: 0.1846109926700592\n",
            "step: 100, loss: 0.024959541857242584\n",
            "step: 110, loss: 0.06342066824436188\n",
            "step: 120, loss: 0.09828800708055496\n",
            "step: 130, loss: 0.05554359406232834\n",
            "step: 140, loss: 0.0754702091217041\n",
            "step: 150, loss: 0.10248152911663055\n",
            "step: 160, loss: 0.17237983644008636\n",
            "step: 170, loss: 0.04363808408379555\n",
            "step: 180, loss: 0.056336089968681335\n",
            "step: 190, loss: 0.1914200782775879\n",
            "step: 200, loss: 0.0815994143486023\n",
            "step: 210, loss: 0.021527202799916267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5304659498207884, f1=0.5335753176043557, best_f1=0.5317324185248713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02209438756108284\n",
            "step: 10, loss: 0.06517577171325684\n",
            "step: 20, loss: 0.13495831191539764\n",
            "step: 30, loss: 0.030706321820616722\n",
            "step: 40, loss: 0.023430529981851578\n",
            "step: 50, loss: 0.028690006583929062\n",
            "step: 60, loss: 0.05294524505734444\n",
            "step: 70, loss: 0.030955733731389046\n",
            "step: 80, loss: 0.047308895736932755\n",
            "step: 90, loss: 0.015081712044775486\n",
            "step: 100, loss: 0.07538734376430511\n",
            "step: 110, loss: 0.09955113381147385\n",
            "step: 120, loss: 0.11396002769470215\n",
            "step: 130, loss: 0.06573940068483353\n",
            "step: 140, loss: 0.06196126714348793\n",
            "step: 150, loss: 0.11683379858732224\n",
            "step: 160, loss: 0.06113802641630173\n",
            "step: 170, loss: 0.032069429755210876\n",
            "step: 180, loss: 0.020615197718143463\n",
            "step: 190, loss: 0.05892046168446541\n",
            "step: 200, loss: 0.03861774131655693\n",
            "step: 210, loss: 0.012795483693480492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5194805194805194, f1=0.5418060200668896, best_f1=0.5317324185248713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04855883866548538\n",
            "step: 10, loss: 0.034218717366456985\n",
            "step: 20, loss: 0.159603551030159\n",
            "step: 30, loss: 0.03114980272948742\n",
            "step: 40, loss: 0.13628056645393372\n",
            "step: 50, loss: 0.02140801027417183\n",
            "step: 60, loss: 0.22241556644439697\n",
            "step: 70, loss: 0.11336888372898102\n",
            "step: 80, loss: 0.05861866846680641\n",
            "step: 90, loss: 0.012606680393218994\n",
            "step: 100, loss: 0.020165111869573593\n",
            "step: 110, loss: 0.26237332820892334\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.07491306960582733\n",
            "step: 130, loss: 0.02448955364525318\n",
            "step: 140, loss: 0.09363394975662231\n",
            "step: 150, loss: 0.050403252243995667\n",
            "step: 160, loss: 0.17744304239749908\n",
            "step: 170, loss: 0.031423795968294144\n",
            "step: 180, loss: 0.033867593854665756\n",
            "step: 190, loss: 0.04957297816872597\n",
            "step: 200, loss: 0.02840203046798706\n",
            "step: 210, loss: 0.10210806876420975\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5272108843537415, f1=0.5342706502636204, best_f1=0.5317324185248713\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:09, 231.18it/s]\n",
            "load_f1 = 0.5250431778929189\n",
            "real_f1 = 0.5217391304347826\n",
            "267it [00:00, 1211.54it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 144.50it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewoOK8t9eDFM"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJDquM2Oe05D",
        "outputId": "a4d970ff-c010-4afa-8396-0e508a56c2fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4422927796840668\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.41409310698509216\n",
            "step: 20, loss: 0.32544437050819397\n",
            "step: 30, loss: 0.40906378626823425\n",
            "step: 40, loss: 0.24761825799942017\n",
            "step: 50, loss: 0.31688305735588074\n",
            "step: 60, loss: 0.4866802990436554\n",
            "step: 70, loss: 0.4059370756149292\n",
            "step: 80, loss: 0.18210797011852264\n",
            "step: 90, loss: 0.3153109550476074\n",
            "step: 100, loss: 0.43451833724975586\n",
            "step: 110, loss: 0.28668680787086487\n",
            "step: 120, loss: 0.3464924395084381\n",
            "step: 130, loss: 0.3472649157047272\n",
            "step: 140, loss: 0.17474211752414703\n",
            "step: 150, loss: 0.3111874461174011\n",
            "step: 160, loss: 0.23043999075889587\n",
            "step: 170, loss: 0.35315608978271484\n",
            "step: 180, loss: 0.14174070954322815\n",
            "step: 190, loss: 0.16247199475765228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.39999999999999997, f1=0.405458089668616, best_f1=0.405458089668616\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.35986989736557007\n",
            "step: 10, loss: 0.34668806195259094\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.7110645771026611\n",
            "step: 30, loss: 0.24589204788208008\n",
            "step: 40, loss: 0.31077539920806885\n",
            "step: 50, loss: 0.4048060178756714\n",
            "step: 60, loss: 0.42475640773773193\n",
            "step: 70, loss: 0.3224136233329773\n",
            "step: 80, loss: 0.19063366949558258\n",
            "step: 90, loss: 0.29442620277404785\n",
            "step: 100, loss: 0.24961301684379578\n",
            "step: 110, loss: 0.37375056743621826\n",
            "step: 120, loss: 0.23218320310115814\n",
            "step: 130, loss: 0.46910005807876587\n",
            "step: 140, loss: 0.3113897144794464\n",
            "step: 150, loss: 0.3519757390022278\n",
            "step: 160, loss: 0.2502190172672272\n",
            "step: 170, loss: 0.19273990392684937\n",
            "step: 180, loss: 0.15336520969867706\n",
            "step: 190, loss: 0.10529036819934845\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.4195121951219512, f1=0.460093896713615, best_f1=0.460093896713615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4220387041568756\n",
            "step: 10, loss: 0.29722389578819275\n",
            "step: 20, loss: 0.27691972255706787\n",
            "step: 30, loss: 0.17167600989341736\n",
            "step: 40, loss: 0.09400293976068497\n",
            "step: 50, loss: 0.24306637048721313\n",
            "step: 60, loss: 0.13951432704925537\n",
            "step: 70, loss: 0.25234079360961914\n",
            "step: 80, loss: 0.14888954162597656\n",
            "step: 90, loss: 0.08500877767801285\n",
            "step: 100, loss: 0.1613772213459015\n",
            "step: 110, loss: 0.4575992524623871\n",
            "step: 120, loss: 0.29300248622894287\n",
            "step: 130, loss: 0.09385479241609573\n",
            "step: 140, loss: 0.057957254350185394\n",
            "step: 150, loss: 0.4309006333351135\n",
            "step: 160, loss: 0.20312298834323883\n",
            "step: 170, loss: 0.48355454206466675\n",
            "step: 180, loss: 0.18041406571865082\n",
            "step: 190, loss: 0.1289614588022232\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7513227513227512, f1=0.7724867724867726, best_f1=0.7724867724867726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07140780985355377\n",
            "step: 10, loss: 0.2034432291984558\n",
            "step: 20, loss: 0.18512868881225586\n",
            "step: 30, loss: 0.03261328488588333\n",
            "step: 40, loss: 0.10274912416934967\n",
            "step: 50, loss: 0.060377366840839386\n",
            "step: 60, loss: 0.14623773097991943\n",
            "step: 70, loss: 0.031918272376060486\n",
            "step: 80, loss: 0.06516040116548538\n",
            "step: 90, loss: 0.024382898584008217\n",
            "step: 100, loss: 0.21663112938404083\n",
            "step: 110, loss: 0.27006903290748596\n",
            "step: 120, loss: 0.09533444792032242\n",
            "step: 130, loss: 0.1639796644449234\n",
            "step: 140, loss: 0.16995210945606232\n",
            "step: 150, loss: 0.1607610434293747\n",
            "step: 160, loss: 0.06679418683052063\n",
            "step: 170, loss: 0.052404653280973434\n",
            "step: 180, loss: 0.07460153102874756\n",
            "step: 190, loss: 0.022547459229826927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8021390374331551, f1=0.8225806451612903, best_f1=0.8225806451612903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15385565161705017\n",
            "step: 10, loss: 0.1621655523777008\n",
            "step: 20, loss: 0.01902157813310623\n",
            "step: 30, loss: 0.07840847969055176\n",
            "step: 40, loss: 0.09860822558403015\n",
            "step: 50, loss: 0.06318444013595581\n",
            "step: 60, loss: 0.004941849038004875\n",
            "step: 70, loss: 0.24780358374118805\n",
            "step: 80, loss: 0.013822120614349842\n",
            "step: 90, loss: 0.06372038275003433\n",
            "step: 100, loss: 0.036353904753923416\n",
            "step: 110, loss: 0.1368672400712967\n",
            "step: 120, loss: 0.24289143085479736\n",
            "step: 130, loss: 0.19530470669269562\n",
            "step: 140, loss: 0.0677742063999176\n",
            "step: 150, loss: 0.11469512432813644\n",
            "step: 160, loss: 0.06566904485225677\n",
            "step: 170, loss: 0.0061753978952765465\n",
            "step: 180, loss: 0.0555865578353405\n",
            "step: 190, loss: 0.12352076917886734\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8184281842818427, f1=0.8346883468834688, best_f1=0.8346883468834688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07568302005529404\n",
            "step: 10, loss: 0.04024016857147217\n",
            "step: 20, loss: 0.07030551880598068\n",
            "step: 30, loss: 0.12585316598415375\n",
            "step: 40, loss: 0.06028800830245018\n",
            "step: 50, loss: 0.22236433625221252\n",
            "step: 60, loss: 0.05161904916167259\n",
            "step: 70, loss: 0.07725097984075546\n",
            "step: 80, loss: 0.14765025675296783\n",
            "step: 90, loss: 0.12054220587015152\n",
            "step: 100, loss: 0.14079606533050537\n",
            "step: 110, loss: 0.004207131452858448\n",
            "step: 120, loss: 0.011158633977174759\n",
            "step: 130, loss: 0.07788905501365662\n",
            "step: 140, loss: 0.0038105607964098454\n",
            "step: 150, loss: 0.015883473679423332\n",
            "step: 160, loss: 0.2287253588438034\n",
            "step: 170, loss: 0.21728813648223877\n",
            "step: 180, loss: 0.035189248621463776\n",
            "step: 190, loss: 0.11514163017272949\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8096514745308311, f1=0.8167539267015705, best_f1=0.8346883468834688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012467917054891586\n",
            "step: 10, loss: 0.09524158388376236\n",
            "step: 20, loss: 0.006351935677230358\n",
            "step: 30, loss: 0.01908287964761257\n",
            "step: 40, loss: 0.015899933874607086\n",
            "step: 50, loss: 0.0018725156551226974\n",
            "step: 60, loss: 0.18724325299263\n",
            "step: 70, loss: 0.0056974864564836025\n",
            "step: 80, loss: 0.005995457526296377\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 90, loss: 0.07211531698703766\n",
            "step: 100, loss: 0.14356911182403564\n",
            "step: 110, loss: 0.23787711560726166\n",
            "step: 120, loss: 0.007029885426163673\n",
            "step: 130, loss: 0.1295914500951767\n",
            "step: 140, loss: 0.07789154350757599\n",
            "step: 150, loss: 0.041053276509046555\n",
            "step: 160, loss: 0.09590721130371094\n",
            "step: 170, loss: 0.0838974267244339\n",
            "step: 180, loss: 0.1171867847442627\n",
            "step: 190, loss: 0.1642819195985794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7869674185463658, f1=0.8153846153846155, best_f1=0.8346883468834688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10871706157922745\n",
            "step: 10, loss: 0.02430316060781479\n",
            "step: 20, loss: 0.01094183698296547\n",
            "step: 30, loss: 0.16032782196998596\n",
            "step: 40, loss: 0.05893300101161003\n",
            "step: 50, loss: 0.04453577473759651\n",
            "step: 60, loss: 0.043273985385894775\n",
            "step: 70, loss: 0.017044885084033012\n",
            "step: 80, loss: 0.20851218700408936\n",
            "step: 90, loss: 0.047314856201410294\n",
            "step: 100, loss: 0.011095613241195679\n",
            "step: 110, loss: 0.21190455555915833\n",
            "step: 120, loss: 0.0922723263502121\n",
            "step: 130, loss: 0.013334646821022034\n",
            "step: 140, loss: 0.01972431316971779\n",
            "step: 150, loss: 0.0370931476354599\n",
            "step: 160, loss: 0.001504286308772862\n",
            "step: 170, loss: 0.21040870249271393\n",
            "step: 180, loss: 0.03785346448421478\n",
            "step: 190, loss: 0.04098917916417122\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.773067331670823, f1=0.7979539641943734, best_f1=0.8346883468834688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004622331820428371\n",
            "step: 10, loss: 0.0014756574528291821\n",
            "step: 20, loss: 0.02787819877266884\n",
            "step: 30, loss: 0.017538610845804214\n",
            "step: 40, loss: 0.09332991391420364\n",
            "step: 50, loss: 0.009741660207509995\n",
            "step: 60, loss: 0.01520139817148447\n",
            "step: 70, loss: 0.13682110607624054\n",
            "step: 80, loss: 0.0031182230450212955\n",
            "step: 90, loss: 0.06829529255628586\n",
            "step: 100, loss: 0.005257747136056423\n",
            "step: 110, loss: 0.030822718515992165\n",
            "step: 120, loss: 0.002975975628942251\n",
            "step: 130, loss: 0.07077882438898087\n",
            "step: 140, loss: 0.206937775015831\n",
            "step: 150, loss: 0.011869892477989197\n",
            "step: 160, loss: 0.013742850162088871\n",
            "step: 170, loss: 0.008577234111726284\n",
            "step: 180, loss: 0.006597996223717928\n",
            "step: 190, loss: 0.002933636773377657\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7919799498746868, f1=0.8102564102564102, best_f1=0.8346883468834688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0037466431967914104\n",
            "step: 10, loss: 0.038845039904117584\n",
            "step: 20, loss: 0.016598911955952644\n",
            "step: 30, loss: 0.10876806825399399\n",
            "step: 40, loss: 0.006768432445824146\n",
            "step: 50, loss: 0.0020321072079241276\n",
            "step: 60, loss: 0.0017500682733953\n",
            "step: 70, loss: 0.0036165642086416483\n",
            "step: 80, loss: 0.00907049234956503\n",
            "step: 90, loss: 0.006749528460204601\n",
            "step: 100, loss: 0.005654171574860811\n",
            "step: 110, loss: 0.06147634983062744\n",
            "step: 120, loss: 0.0045119295828044415\n",
            "step: 130, loss: 0.0017243691254407167\n",
            "step: 140, loss: 0.0025894527789205313\n",
            "step: 150, loss: 0.014479683712124825\n",
            "step: 160, loss: 0.0019843997433781624\n",
            "step: 170, loss: 0.0008840336813591421\n",
            "step: 180, loss: 0.006510570179671049\n",
            "step: 190, loss: 0.001753023942001164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7967914438502673, f1=0.8266666666666667, best_f1=0.8346883468834688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013846231624484062\n",
            "step: 10, loss: 0.0007782697794027627\n",
            "step: 20, loss: 0.0028479972388595343\n",
            "step: 30, loss: 0.17238472402095795\n",
            "step: 40, loss: 0.0023527462035417557\n",
            "step: 50, loss: 0.001000315765850246\n",
            "step: 60, loss: 0.004634345881640911\n",
            "step: 70, loss: 0.02691614255309105\n",
            "step: 80, loss: 0.07293510437011719\n",
            "step: 90, loss: 0.0005022595287300646\n",
            "step: 100, loss: 0.006856773514300585\n",
            "step: 110, loss: 0.017416780814528465\n",
            "step: 120, loss: 0.0015412714565172791\n",
            "step: 130, loss: 0.0059960754588246346\n",
            "step: 140, loss: 0.001860815566033125\n",
            "step: 150, loss: 0.0038337286096066236\n",
            "step: 160, loss: 0.0008916427032090724\n",
            "step: 170, loss: 0.0008527648169547319\n",
            "step: 180, loss: 0.18274015188217163\n",
            "step: 190, loss: 0.00162572565022856\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7989821882951654, f1=0.8082901554404145, best_f1=0.8346883468834688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021235773339867592\n",
            "step: 10, loss: 0.0018341972026973963\n",
            "step: 20, loss: 0.00047729327343404293\n",
            "step: 30, loss: 0.02423781156539917\n",
            "step: 40, loss: 0.0011873898329213262\n",
            "step: 50, loss: 0.011660690419375896\n",
            "step: 60, loss: 0.003914512228220701\n",
            "step: 70, loss: 0.001386126852594316\n",
            "step: 80, loss: 0.002265702001750469\n",
            "step: 90, loss: 0.1496557742357254\n",
            "step: 100, loss: 0.0014980320120230317\n",
            "step: 110, loss: 0.013311675749719143\n",
            "step: 120, loss: 0.0101112499833107\n",
            "step: 130, loss: 0.010119706392288208\n",
            "step: 140, loss: 0.0051437499932944775\n",
            "step: 150, loss: 0.00896557979285717\n",
            "step: 160, loss: 0.01920558698475361\n",
            "step: 170, loss: 0.003471375908702612\n",
            "step: 180, loss: 0.002873901277780533\n",
            "step: 190, loss: 0.001517430180683732\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7835616438356164, f1=0.8279569892473119, best_f1=0.8346883468834688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020920580253005028\n",
            "step: 10, loss: 0.021181579679250717\n",
            "step: 20, loss: 0.009583834558725357\n",
            "step: 30, loss: 0.09481120854616165\n",
            "step: 40, loss: 0.0021839344408363104\n",
            "step: 50, loss: 0.006871088873594999\n",
            "step: 60, loss: 0.0011605279287323356\n",
            "step: 70, loss: 0.000808277924079448\n",
            "step: 80, loss: 0.000646025815512985\n",
            "step: 90, loss: 0.0012245166581124067\n",
            "step: 100, loss: 0.000573089171666652\n",
            "step: 110, loss: 0.0152945127338171\n",
            "step: 120, loss: 0.002188538433983922\n",
            "step: 130, loss: 0.0045098052360117435\n",
            "step: 140, loss: 0.00385781261138618\n",
            "step: 150, loss: 0.05207080766558647\n",
            "step: 160, loss: 0.036667317152023315\n",
            "step: 170, loss: 0.00272530410438776\n",
            "step: 180, loss: 0.007953735068440437\n",
            "step: 190, loss: 0.04741779342293739\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8085106382978724, f1=0.8213333333333334, best_f1=0.8346883468834688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02418673224747181\n",
            "step: 10, loss: 0.0011584277963265777\n",
            "step: 20, loss: 0.0005950785125605762\n",
            "step: 30, loss: 0.0007228407193906605\n",
            "step: 40, loss: 0.007938901893794537\n",
            "step: 50, loss: 0.0025574197061359882\n",
            "step: 60, loss: 0.06156151369214058\n",
            "step: 70, loss: 0.0017860213993117213\n",
            "step: 80, loss: 0.0005719165783375502\n",
            "step: 90, loss: 0.0018519412260502577\n",
            "step: 100, loss: 0.004791425075381994\n",
            "step: 110, loss: 0.014746167697012424\n",
            "step: 120, loss: 0.03566556051373482\n",
            "step: 130, loss: 0.0010636416263878345\n",
            "step: 140, loss: 0.009127047844231129\n",
            "step: 150, loss: 0.01326728519052267\n",
            "step: 160, loss: 0.0006958436570130289\n",
            "step: 170, loss: 0.0009233799064531922\n",
            "step: 180, loss: 0.0012343230191618204\n",
            "step: 190, loss: 0.002348574809730053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8085106382978724, f1=0.8213333333333334, best_f1=0.8346883468834688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011898830998688936\n",
            "step: 10, loss: 0.0014948770403862\n",
            "step: 20, loss: 0.03716828301548958\n",
            "step: 30, loss: 0.0008018446387723088\n",
            "step: 40, loss: 0.048574455082416534\n",
            "step: 50, loss: 0.0022076584864407778\n",
            "step: 60, loss: 0.0013282527215778828\n",
            "step: 70, loss: 0.005780709441751242\n",
            "step: 80, loss: 0.003941587172448635\n",
            "step: 90, loss: 0.020133126527071\n",
            "step: 100, loss: 0.0004997252253815532\n",
            "step: 110, loss: 0.04008844122290611\n",
            "step: 120, loss: 0.005750284995883703\n",
            "step: 130, loss: 0.0012994854478165507\n",
            "step: 140, loss: 0.0008093419019132853\n",
            "step: 150, loss: 0.002237556502223015\n",
            "step: 160, loss: 0.0010223676217719913\n",
            "step: 170, loss: 0.016734635457396507\n",
            "step: 180, loss: 0.0007122833631001413\n",
            "step: 190, loss: 0.0012910082004964352\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8106666666666666, f1=0.8213333333333334, best_f1=0.8346883468834688\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:13, 150.75it/s]\n",
            "load_f1 = 0.8102564102564102\n",
            "real_f1 = 0.8\n",
            "733it [00:00, 3294.47it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 143.97it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      },
      "source": [
        "## DA TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1O9a5RjeDtU"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2WpDwuee1mM",
        "outputId": "75dbfada-f8f9-4b62-cb6e-984a71a126b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.49235254526138306\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5098289251327515\n",
            "step: 20, loss: 0.29689550399780273\n",
            "step: 30, loss: 0.39665302634239197\n",
            "step: 40, loss: 0.5697903037071228\n",
            "step: 50, loss: 0.30461689829826355\n",
            "step: 60, loss: 0.5367575287818909\n",
            "step: 70, loss: 0.3520621657371521\n",
            "step: 80, loss: 0.23777510225772858\n",
            "step: 90, loss: 0.26740512251853943\n",
            "step: 100, loss: 0.16569991409778595\n",
            "step: 110, loss: 0.42603716254234314\n",
            "step: 120, loss: 0.3442416787147522\n",
            "step: 130, loss: 0.3018963038921356\n",
            "step: 140, loss: 0.36068612337112427\n",
            "step: 150, loss: 0.30384042859077454\n",
            "step: 160, loss: 0.3982252776622772\n",
            "step: 170, loss: 0.31325003504753113\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.20744081172491546, f1=0.2169222032935832, best_f1=0.2169222032935832\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2948252558708191\n",
            "step: 10, loss: 0.46320536732673645\n",
            "step: 20, loss: 0.31385859847068787\n",
            "step: 30, loss: 0.2863704562187195\n",
            "step: 40, loss: 0.08380362391471863\n",
            "step: 50, loss: 0.3892049491405487\n",
            "step: 60, loss: 0.19973202049732208\n",
            "step: 70, loss: 0.470720112323761\n",
            "step: 80, loss: 0.19658270478248596\n",
            "step: 90, loss: 0.24735011160373688\n",
            "step: 100, loss: 0.4814050793647766\n",
            "step: 110, loss: 0.23632027208805084\n",
            "step: 120, loss: 0.12785808742046356\n",
            "step: 130, loss: 0.33999449014663696\n",
            "step: 140, loss: 0.30530494451522827\n",
            "step: 150, loss: 0.1816920042037964\n",
            "step: 160, loss: 0.20218393206596375\n",
            "step: 170, loss: 0.13093549013137817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7115384615384616, f1=0.6808510638297873, best_f1=0.6808510638297873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.41847485303878784\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 10, loss: 0.1848270148038864\n",
            "step: 20, loss: 0.23522669076919556\n",
            "step: 30, loss: 0.02429567277431488\n",
            "step: 40, loss: 0.05595700442790985\n",
            "step: 50, loss: 0.3433971703052521\n",
            "step: 60, loss: 0.07859610766172409\n",
            "step: 70, loss: 0.21774831414222717\n",
            "step: 80, loss: 0.2217555046081543\n",
            "step: 90, loss: 0.2660749554634094\n",
            "step: 100, loss: 0.04817015677690506\n",
            "step: 110, loss: 0.04534030333161354\n",
            "step: 120, loss: 0.25696542859077454\n",
            "step: 130, loss: 0.24972710013389587\n",
            "step: 140, loss: 0.12346900254487991\n",
            "step: 150, loss: 0.0915837287902832\n",
            "step: 160, loss: 0.0651705339550972\n",
            "step: 170, loss: 0.11424975097179413\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7727272727272727, f1=0.7121212121212122, best_f1=0.7121212121212122\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06227083504199982\n",
            "step: 10, loss: 0.2539524435997009\n",
            "step: 20, loss: 0.052208539098501205\n",
            "step: 30, loss: 0.27970120310783386\n",
            "step: 40, loss: 0.09276780486106873\n",
            "step: 50, loss: 0.08464747667312622\n",
            "step: 60, loss: 0.26282984018325806\n",
            "step: 70, loss: 0.02077663689851761\n",
            "step: 80, loss: 0.4334898889064789\n",
            "step: 90, loss: 0.22710749506950378\n",
            "step: 100, loss: 0.1277080476284027\n",
            "step: 110, loss: 0.2607320249080658\n",
            "step: 120, loss: 0.4069218337535858\n",
            "step: 130, loss: 0.1449524164199829\n",
            "step: 140, loss: 0.18057143688201904\n",
            "step: 150, loss: 0.21944858133792877\n",
            "step: 160, loss: 0.028035620227456093\n",
            "step: 170, loss: 0.027984196320176125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8088235294117646, f1=0.7570093457943925, best_f1=0.7570093457943925\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1112307459115982\n",
            "step: 10, loss: 0.09245765954256058\n",
            "step: 20, loss: 0.12726226449012756\n",
            "step: 30, loss: 0.018103860318660736\n",
            "step: 40, loss: 0.019678901880979538\n",
            "step: 50, loss: 0.04801221191883087\n",
            "step: 60, loss: 0.07169615477323532\n",
            "step: 70, loss: 0.1294727623462677\n",
            "step: 80, loss: 0.025788284838199615\n",
            "step: 90, loss: 0.02719702199101448\n",
            "step: 100, loss: 0.007815835066139698\n",
            "step: 110, loss: 0.013477136380970478\n",
            "step: 120, loss: 0.05185035988688469\n",
            "step: 130, loss: 0.006297420710325241\n",
            "step: 140, loss: 0.1221737340092659\n",
            "step: 150, loss: 0.05755538493394852\n",
            "step: 160, loss: 0.14785262942314148\n",
            "step: 170, loss: 0.004289326258003712\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8205128205128205, f1=0.7843137254901961, best_f1=0.7843137254901961\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14761824905872345\n",
            "step: 10, loss: 0.047271162271499634\n",
            "step: 20, loss: 0.17530712485313416\n",
            "step: 30, loss: 0.035412974655628204\n",
            "step: 40, loss: 0.0021724626421928406\n",
            "step: 50, loss: 0.01433294266462326\n",
            "step: 60, loss: 0.022649817168712616\n",
            "step: 70, loss: 0.02569042518734932\n",
            "step: 80, loss: 0.024494873359799385\n",
            "step: 90, loss: 0.09421252459287643\n",
            "step: 100, loss: 0.02220999263226986\n",
            "step: 110, loss: 0.04902447760105133\n",
            "step: 120, loss: 0.010168012231588364\n",
            "step: 130, loss: 0.19636861979961395\n",
            "step: 140, loss: 0.13185663521289825\n",
            "step: 150, loss: 0.15862837433815002\n",
            "step: 160, loss: 0.01391275692731142\n",
            "step: 170, loss: 0.006605606526136398\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8284313725490197, f1=0.7801418439716311, best_f1=0.7801418439716311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016514429822564125\n",
            "step: 10, loss: 0.021366072818636894\n",
            "step: 20, loss: 0.01774064637720585\n",
            "step: 30, loss: 0.3536234200000763\n",
            "step: 40, loss: 0.00894930213689804\n",
            "step: 50, loss: 0.0035356886219233274\n",
            "step: 60, loss: 0.06433667242527008\n",
            "step: 70, loss: 0.01078890636563301\n",
            "step: 80, loss: 0.0074262917041778564\n",
            "step: 90, loss: 0.01387022901326418\n",
            "step: 100, loss: 0.0055985115468502045\n",
            "step: 110, loss: 0.1387646496295929\n",
            "step: 120, loss: 0.004272830672562122\n",
            "step: 130, loss: 0.06882971525192261\n",
            "step: 140, loss: 0.04064992070198059\n",
            "step: 150, loss: 0.08564121276140213\n",
            "step: 160, loss: 0.020920267328619957\n",
            "step: 170, loss: 0.09326106309890747\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8325123152709358, f1=0.7754137115839244, best_f1=0.7754137115839244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02936648018658161\n",
            "step: 10, loss: 0.046474892646074295\n",
            "step: 20, loss: 0.032125674188137054\n",
            "step: 30, loss: 0.0002490428159944713\n",
            "step: 40, loss: 0.004474342800676823\n",
            "step: 50, loss: 0.0432635098695755\n",
            "step: 60, loss: 0.007080600131303072\n",
            "step: 70, loss: 0.021608108654618263\n",
            "step: 80, loss: 0.011001075617969036\n",
            "step: 90, loss: 0.05072018876671791\n",
            "step: 100, loss: 0.0007494384190067649\n",
            "step: 110, loss: 0.09391949325799942\n",
            "step: 120, loss: 0.002974834991618991\n",
            "step: 130, loss: 0.003673334140330553\n",
            "step: 140, loss: 0.023403828963637352\n",
            "step: 150, loss: 0.00973766203969717\n",
            "step: 160, loss: 0.02468654327094555\n",
            "step: 170, loss: 0.08590700477361679\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.834567901234568, f1=0.7895981087470448, best_f1=0.7895981087470448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.053671758621931076\n",
            "step: 10, loss: 0.0038165391888469458\n",
            "step: 20, loss: 0.005136832594871521\n",
            "step: 30, loss: 0.06998128443956375\n",
            "step: 40, loss: 0.023995842784643173\n",
            "step: 50, loss: 0.0011700565228238702\n",
            "step: 60, loss: 0.03506031632423401\n",
            "step: 70, loss: 0.26223403215408325\n",
            "step: 80, loss: 0.0021228373516350985\n",
            "step: 90, loss: 0.01263468898832798\n",
            "step: 100, loss: 0.045981355011463165\n",
            "step: 110, loss: 0.013752954080700874\n",
            "step: 120, loss: 0.0029198552947491407\n",
            "step: 130, loss: 0.009915086440742016\n",
            "step: 140, loss: 0.010303942486643791\n",
            "step: 150, loss: 0.22934271395206451\n",
            "step: 160, loss: 0.01718793623149395\n",
            "step: 170, loss: 0.018628962337970734\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.802919708029197, f1=0.7647058823529412, best_f1=0.7895981087470448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031855665147304535\n",
            "step: 10, loss: 0.1262202262878418\n",
            "step: 20, loss: 0.16969981789588928\n",
            "step: 30, loss: 0.061377428472042084\n",
            "step: 40, loss: 0.005908862687647343\n",
            "step: 50, loss: 0.055816445499658585\n",
            "step: 60, loss: 0.00839067529886961\n",
            "step: 70, loss: 0.017213864251971245\n",
            "step: 80, loss: 0.024231962859630585\n",
            "step: 90, loss: 0.053947992622852325\n",
            "step: 100, loss: 0.0014773006550967693\n",
            "step: 110, loss: 0.008117358200252056\n",
            "step: 120, loss: 0.0008015169878490269\n",
            "step: 130, loss: 0.0016008717939257622\n",
            "step: 140, loss: 0.0012454584939405322\n",
            "step: 150, loss: 0.1652512550354004\n",
            "step: 160, loss: 0.019276907667517662\n",
            "step: 170, loss: 0.04283568263053894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8238213399503721, f1=0.7692307692307693, best_f1=0.7895981087470448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06437047570943832\n",
            "step: 10, loss: 0.007048470433801413\n",
            "step: 20, loss: 0.0001925508986460045\n",
            "step: 30, loss: 0.004177096765488386\n",
            "step: 40, loss: 0.015721667557954788\n",
            "step: 50, loss: 0.0016567904967814684\n",
            "step: 60, loss: 0.006914779543876648\n",
            "step: 70, loss: 0.004825785756111145\n",
            "step: 80, loss: 0.12259458005428314\n",
            "step: 90, loss: 0.008878654800355434\n",
            "step: 100, loss: 0.022134924307465553\n",
            "step: 110, loss: 0.009970585815608501\n",
            "step: 120, loss: 0.0006582450005225837\n",
            "step: 130, loss: 0.005372012034058571\n",
            "step: 140, loss: 0.008172184228897095\n",
            "step: 150, loss: 0.005258761812001467\n",
            "step: 160, loss: 0.04881008341908455\n",
            "step: 170, loss: 0.0021910294890403748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8256410256410256, f1=0.7990074441687345, best_f1=0.7895981087470448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028251383919268847\n",
            "step: 10, loss: 0.0006908312207087874\n",
            "step: 20, loss: 0.003268349915742874\n",
            "step: 30, loss: 0.03772815316915512\n",
            "step: 40, loss: 0.010768122971057892\n",
            "step: 50, loss: 0.004754107911139727\n",
            "step: 60, loss: 0.0019108927808701992\n",
            "step: 70, loss: 0.033018745481967926\n",
            "step: 80, loss: 0.00015879000420682132\n",
            "step: 90, loss: 0.012411192990839481\n",
            "step: 100, loss: 0.0005580217111855745\n",
            "step: 110, loss: 0.0025041031185537577\n",
            "step: 120, loss: 0.03572998568415642\n",
            "step: 130, loss: 0.0019361970480531454\n",
            "step: 140, loss: 0.00014929278404451907\n",
            "step: 150, loss: 0.00042022517300210893\n",
            "step: 160, loss: 0.00036371522583067417\n",
            "step: 170, loss: 0.0020858640782535076\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8072916666666666, f1=0.7889447236180904, best_f1=0.7895981087470448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0065927887335419655\n",
            "step: 10, loss: 0.004416411276906729\n",
            "step: 20, loss: 0.0004597783845383674\n",
            "step: 30, loss: 0.008411243557929993\n",
            "step: 40, loss: 0.001081477734260261\n",
            "step: 50, loss: 0.02471449226140976\n",
            "step: 60, loss: 0.001299968222156167\n",
            "step: 70, loss: 0.07788458466529846\n",
            "step: 80, loss: 0.00013141980161890388\n",
            "step: 90, loss: 0.0005957631510682404\n",
            "step: 100, loss: 0.03395865857601166\n",
            "step: 110, loss: 0.005768731702119112\n",
            "step: 120, loss: 0.005893555004149675\n",
            "step: 130, loss: 0.00016175053315237164\n",
            "step: 140, loss: 0.04277295991778374\n",
            "step: 150, loss: 0.00027490119100548327\n",
            "step: 160, loss: 0.006785823963582516\n",
            "step: 170, loss: 0.00015763197734486312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8211586901763224, f1=0.7796610169491525, best_f1=0.7895981087470448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028879977762699127\n",
            "step: 10, loss: 0.0019367298809811473\n",
            "step: 20, loss: 0.004484019707888365\n",
            "step: 30, loss: 0.0042213331907987595\n",
            "step: 40, loss: 0.00065524538513273\n",
            "step: 50, loss: 0.00030734832398593426\n",
            "step: 60, loss: 0.0019389602821320295\n",
            "step: 70, loss: 0.006947537884116173\n",
            "step: 80, loss: 0.019079996272921562\n",
            "step: 90, loss: 0.000225709707592614\n",
            "step: 100, loss: 0.007859482429921627\n",
            "step: 110, loss: 0.004939975216984749\n",
            "step: 120, loss: 0.001256493735127151\n",
            "step: 130, loss: 0.061224017292261124\n",
            "step: 140, loss: 0.018133951351046562\n",
            "step: 150, loss: 0.016490748152136803\n",
            "step: 160, loss: 0.0009042039746418595\n",
            "step: 170, loss: 0.07551202923059464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8140703517587939, f1=0.7815533980582524, best_f1=0.7895981087470448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010101227089762688\n",
            "step: 10, loss: 0.0014237097930163145\n",
            "step: 20, loss: 0.028051257133483887\n",
            "step: 30, loss: 0.00042951753130182624\n",
            "step: 40, loss: 0.0003905567864421755\n",
            "step: 50, loss: 0.00014542760618496686\n",
            "step: 60, loss: 0.00038055848563089967\n",
            "step: 70, loss: 0.008486546576023102\n",
            "step: 80, loss: 0.003254023613408208\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.007853282615542412\n",
            "step: 100, loss: 0.008859360590577126\n",
            "step: 110, loss: 0.00024549910449422896\n",
            "step: 120, loss: 9.442191367270425e-05\n",
            "step: 130, loss: 0.02507689781486988\n",
            "step: 140, loss: 0.030768102034926414\n",
            "step: 150, loss: 9.864967432804406e-05\n",
            "step: 160, loss: 0.016578909009695053\n",
            "step: 170, loss: 0.0001195978038595058\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8089330024813897, f1=0.7761904761904763, best_f1=0.7895981087470448\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 231.52it/s]\n",
            "load_f1 = 0.7405405405405405\n",
            "real_f1 = 0.7335092348284961\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 145.06it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pmKonkXeD7k"
      },
      "source": [
        "## DA DIRTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRxHd3j2eEH8"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lnLoRSEe2fE",
        "outputId": "61b9ac15-e358-49c5-86ae-f21e5a043dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 417kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.08MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 4.16MB/s]\n",
            "Downloading: 100% 501M/501M [00:06<00:00, 72.2MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5850075483322144\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.484156996011734\n",
            "step: 20, loss: 0.5720878839492798\n",
            "step: 30, loss: 0.2402663230895996\n",
            "step: 40, loss: 0.35405758023262024\n",
            "step: 50, loss: 0.5646131038665771\n",
            "step: 60, loss: 0.44379445910453796\n",
            "step: 70, loss: 0.2025277018547058\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.28884145617485046\n",
            "step: 90, loss: 0.2155524492263794\n",
            "step: 100, loss: 0.1586422622203827\n",
            "step: 110, loss: 0.19123078882694244\n",
            "step: 120, loss: 0.217472106218338\n",
            "step: 130, loss: 0.03961076959967613\n",
            "step: 140, loss: 0.10698439180850983\n",
            "step: 150, loss: 0.1279735416173935\n",
            "step: 160, loss: 0.06075297296047211\n",
            "step: 170, loss: 0.1217728704214096\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 180, loss: 0.22966298460960388\n",
            "step: 190, loss: 0.017154064029455185\n",
            "step: 200, loss: 0.10645948350429535\n",
            "step: 210, loss: 0.04203534126281738\n",
            "step: 220, loss: 0.11592482030391693\n",
            "step: 230, loss: 0.03994778171181679\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9537444933920706, f1=0.9578713968957872, best_f1=0.9578713968957872\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009035427123308182\n",
            "step: 10, loss: 0.17645780742168427\n",
            "step: 20, loss: 0.014681139960885048\n",
            "step: 30, loss: 0.06525406241416931\n",
            "step: 40, loss: 0.03980020433664322\n",
            "step: 50, loss: 0.006776215508580208\n",
            "step: 60, loss: 0.038745079189538956\n",
            "step: 70, loss: 0.10968749970197678\n",
            "step: 80, loss: 0.02718541771173477\n",
            "step: 90, loss: 0.01826278306543827\n",
            "step: 100, loss: 0.01655390113592148\n",
            "step: 110, loss: 0.03709663078188896\n",
            "step: 120, loss: 0.10244651883840561\n",
            "step: 130, loss: 0.017832806333899498\n",
            "step: 140, loss: 0.06511400640010834\n",
            "step: 150, loss: 0.132195845246315\n",
            "step: 160, loss: 0.0160610843449831\n",
            "step: 170, loss: 0.0016882000491023064\n",
            "step: 180, loss: 0.014063436537981033\n",
            "step: 190, loss: 0.017840884625911713\n",
            "step: 200, loss: 0.024665117263793945\n",
            "step: 210, loss: 0.025970112532377243\n",
            "step: 220, loss: 0.05258557200431824\n",
            "step: 230, loss: 0.0014680515741929412\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9582863585118377, f1=0.9517241379310345, best_f1=0.9517241379310345\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012736041098833084\n",
            "step: 10, loss: 0.012421488761901855\n",
            "step: 20, loss: 0.07649245113134384\n",
            "step: 30, loss: 0.05132708325982094\n",
            "step: 40, loss: 0.006023343186825514\n",
            "step: 50, loss: 0.020911717787384987\n",
            "step: 60, loss: 0.018510911613702774\n",
            "step: 70, loss: 0.1109013557434082\n",
            "step: 80, loss: 0.0363314226269722\n",
            "step: 90, loss: 0.005809234455227852\n",
            "step: 100, loss: 0.014506781473755836\n",
            "step: 110, loss: 0.043632201850414276\n",
            "step: 120, loss: 0.004808720666915178\n",
            "step: 130, loss: 0.006011276040226221\n",
            "step: 140, loss: 0.0027150388341397047\n",
            "step: 150, loss: 0.1116485670208931\n",
            "step: 160, loss: 0.025192543864250183\n",
            "step: 170, loss: 0.02152390405535698\n",
            "step: 180, loss: 0.05534455180168152\n",
            "step: 190, loss: 0.05548907816410065\n",
            "step: 200, loss: 0.011921389028429985\n",
            "step: 210, loss: 0.0027869022451341152\n",
            "step: 220, loss: 0.07550419121980667\n",
            "step: 230, loss: 0.03653300181031227\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9662162162162162, f1=0.9671574178935448, best_f1=0.9671574178935448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023674657568335533\n",
            "step: 10, loss: 0.01527270395308733\n",
            "step: 20, loss: 0.0032508312724530697\n",
            "step: 30, loss: 0.0016594667686149478\n",
            "step: 40, loss: 0.07947637885808945\n",
            "step: 50, loss: 0.012757294811308384\n",
            "step: 60, loss: 0.02381238527595997\n",
            "step: 70, loss: 0.11371422559022903\n",
            "step: 80, loss: 0.11891257762908936\n",
            "step: 90, loss: 0.09764615446329117\n",
            "step: 100, loss: 0.013970177620649338\n",
            "step: 110, loss: 0.004251571372151375\n",
            "step: 120, loss: 0.020122574642300606\n",
            "step: 130, loss: 0.02795739844441414\n",
            "step: 140, loss: 0.0158880352973938\n",
            "step: 150, loss: 0.008644734509289265\n",
            "step: 160, loss: 0.0028054174035787582\n",
            "step: 170, loss: 0.004306952003389597\n",
            "step: 180, loss: 0.03503376245498657\n",
            "step: 190, loss: 0.005496075842529535\n",
            "step: 200, loss: 0.048899538815021515\n",
            "step: 210, loss: 0.01410241611301899\n",
            "step: 220, loss: 0.0009899038122966886\n",
            "step: 230, loss: 0.001620929571799934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9692132269099202, f1=0.9502890173410403, best_f1=0.9502890173410403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00738714262843132\n",
            "step: 10, loss: 0.023497579619288445\n",
            "step: 20, loss: 0.02096370980143547\n",
            "step: 30, loss: 0.004669730085879564\n",
            "step: 40, loss: 0.010800698772072792\n",
            "step: 50, loss: 0.02542324922978878\n",
            "step: 60, loss: 0.018115615472197533\n",
            "step: 70, loss: 0.0011941005941480398\n",
            "step: 80, loss: 0.05051407217979431\n",
            "step: 90, loss: 0.05465855076909065\n",
            "step: 100, loss: 0.0005495318328030407\n",
            "step: 110, loss: 0.0012089692754670978\n",
            "step: 120, loss: 0.0016580312512814999\n",
            "step: 130, loss: 0.006188681349158287\n",
            "step: 140, loss: 0.04767724499106407\n",
            "step: 150, loss: 0.022124402225017548\n",
            "step: 160, loss: 0.005575126502662897\n",
            "step: 170, loss: 0.003574963193386793\n",
            "step: 180, loss: 0.03629503399133682\n",
            "step: 190, loss: 0.009942063130438328\n",
            "step: 200, loss: 0.008681409060955048\n",
            "step: 210, loss: 0.015661969780921936\n",
            "step: 220, loss: 0.005374344065785408\n",
            "step: 230, loss: 0.10821506381034851\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9719416386083053, f1=0.9513023782559457, best_f1=0.9513023782559457\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002273818012326956\n",
            "step: 10, loss: 0.0008620147127658129\n",
            "step: 20, loss: 0.0029615960083901882\n",
            "step: 30, loss: 0.011437949724495411\n",
            "step: 40, loss: 0.000817171239759773\n",
            "step: 50, loss: 0.0006691793096251786\n",
            "step: 60, loss: 0.006391379050910473\n",
            "step: 70, loss: 0.17428207397460938\n",
            "step: 80, loss: 0.011056025512516499\n",
            "step: 90, loss: 0.001991491997614503\n",
            "step: 100, loss: 0.0005913437926210463\n",
            "step: 110, loss: 0.01251537911593914\n",
            "step: 120, loss: 0.0005778261111117899\n",
            "step: 130, loss: 0.006430257577449083\n",
            "step: 140, loss: 0.0020592708606272936\n",
            "step: 150, loss: 0.01267568115144968\n",
            "step: 160, loss: 0.004222383722662926\n",
            "step: 170, loss: 0.0004467711260076612\n",
            "step: 180, loss: 0.004597908351570368\n",
            "step: 190, loss: 0.001430472475476563\n",
            "step: 200, loss: 0.008722356520593166\n",
            "step: 210, loss: 0.0145291518419981\n",
            "step: 220, loss: 0.015649553388357162\n",
            "step: 230, loss: 0.001951441285200417\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9711111111111111, f1=0.9654403567447045, best_f1=0.9513023782559457\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002047439804300666\n",
            "step: 10, loss: 0.003530814778059721\n",
            "step: 20, loss: 0.0005020053358748555\n",
            "step: 30, loss: 0.00047516843187622726\n",
            "step: 40, loss: 0.0007454418810084462\n",
            "step: 50, loss: 0.000945655454415828\n",
            "step: 60, loss: 0.0013467719545587897\n",
            "step: 70, loss: 0.0012551106046885252\n",
            "step: 80, loss: 0.003911688458174467\n",
            "step: 90, loss: 0.0009887408232316375\n",
            "step: 100, loss: 0.0007380563183687627\n",
            "step: 110, loss: 0.0032440139912068844\n",
            "step: 120, loss: 0.0031551397405564785\n",
            "step: 130, loss: 0.005049690138548613\n",
            "step: 140, loss: 0.0007850852562114596\n",
            "step: 150, loss: 0.03958795219659805\n",
            "step: 160, loss: 0.0006361969863064587\n",
            "step: 170, loss: 0.012093433178961277\n",
            "step: 180, loss: 0.007171406410634518\n",
            "step: 190, loss: 0.02448847144842148\n",
            "step: 200, loss: 0.0015042823506519198\n",
            "step: 210, loss: 0.013813537545502186\n",
            "step: 220, loss: 0.0006820220150984824\n",
            "step: 230, loss: 0.0006742729456163943\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9772727272727272, f1=0.9611872146118722, best_f1=0.9611872146118722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006521005416288972\n",
            "step: 10, loss: 0.0012788008898496628\n",
            "step: 20, loss: 0.0009307658183388412\n",
            "step: 30, loss: 0.003737750696018338\n",
            "step: 40, loss: 0.005400712136179209\n",
            "step: 50, loss: 0.004040514584630728\n",
            "step: 60, loss: 0.0010678237304091454\n",
            "step: 70, loss: 0.00026963985874317586\n",
            "step: 80, loss: 0.025013018399477005\n",
            "step: 90, loss: 0.0006234640022739768\n",
            "step: 100, loss: 0.0006887138588353992\n",
            "step: 110, loss: 0.002689995104447007\n",
            "step: 120, loss: 0.0006460452568717301\n",
            "step: 130, loss: 0.03605359047651291\n",
            "step: 140, loss: 0.0005976286483928561\n",
            "step: 150, loss: 0.16787469387054443\n",
            "step: 160, loss: 0.0022724969312548637\n",
            "step: 170, loss: 0.055069632828235626\n",
            "step: 180, loss: 0.0036548341158777475\n",
            "step: 190, loss: 0.011985325254499912\n",
            "step: 200, loss: 0.13327530026435852\n",
            "step: 210, loss: 0.002752432134002447\n",
            "step: 220, loss: 0.0012432645307853818\n",
            "step: 230, loss: 0.0007777552818879485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9752808988764046, f1=0.963963963963964, best_f1=0.9611872146118722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005061274860054255\n",
            "step: 10, loss: 0.0006863146554678679\n",
            "step: 20, loss: 0.001231155707500875\n",
            "step: 30, loss: 0.0014811073197051883\n",
            "step: 40, loss: 0.002060921862721443\n",
            "step: 50, loss: 0.01829294115304947\n",
            "step: 60, loss: 0.0007117573404684663\n",
            "step: 70, loss: 0.018163669854402542\n",
            "step: 80, loss: 0.003492768155410886\n",
            "step: 90, loss: 0.16351047158241272\n",
            "step: 100, loss: 0.00030104792676866055\n",
            "step: 110, loss: 0.0003183303924743086\n",
            "step: 120, loss: 0.07799991220235825\n",
            "step: 130, loss: 0.011628732085227966\n",
            "step: 140, loss: 0.0004974417388439178\n",
            "step: 150, loss: 0.0002412226895103231\n",
            "step: 160, loss: 0.0008629766525700688\n",
            "step: 170, loss: 0.0002456303918734193\n",
            "step: 180, loss: 0.00396139407530427\n",
            "step: 190, loss: 8.918378443922848e-05\n",
            "step: 200, loss: 0.0006211867439560592\n",
            "step: 210, loss: 0.0008217755821533501\n",
            "step: 220, loss: 0.01565036177635193\n",
            "step: 230, loss: 0.00021160666074138135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9741863075196409, f1=0.9638009049773756, best_f1=0.9611872146118722\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00036116319824941456\n",
            "step: 10, loss: 0.000607621215749532\n",
            "step: 20, loss: 0.005182304419577122\n",
            "step: 30, loss: 0.00012311873433645815\n",
            "step: 40, loss: 0.00792614370584488\n",
            "step: 50, loss: 0.00016351982776541263\n",
            "step: 60, loss: 0.00020231102826073766\n",
            "step: 70, loss: 0.005731641314923763\n",
            "step: 80, loss: 0.004548089113086462\n",
            "step: 90, loss: 0.0005733675789088011\n",
            "step: 100, loss: 0.001001800294034183\n",
            "step: 110, loss: 0.0030825070571154356\n",
            "step: 120, loss: 0.0005982252769172192\n",
            "step: 130, loss: 0.0007704953895881772\n",
            "step: 140, loss: 0.00020620286522898823\n",
            "step: 150, loss: 0.005786876659840345\n",
            "step: 160, loss: 9.188867988996208e-05\n",
            "step: 170, loss: 0.0001840086915763095\n",
            "step: 180, loss: 0.004947856534272432\n",
            "step: 190, loss: 0.0007262722938321531\n",
            "step: 200, loss: 0.0009321649558842182\n",
            "step: 210, loss: 0.006013920530676842\n",
            "step: 220, loss: 0.0002166048507206142\n",
            "step: 230, loss: 0.00021357531659305096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9807474518686297, f1=0.963718820861678, best_f1=0.963718820861678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017555431986693293\n",
            "step: 10, loss: 0.0006842633592896163\n",
            "step: 20, loss: 0.00023457813949789852\n",
            "step: 30, loss: 0.00016504299128428102\n",
            "step: 40, loss: 7.428186654578894e-05\n",
            "step: 50, loss: 0.00021970612579025328\n",
            "step: 60, loss: 0.01085524633526802\n",
            "step: 70, loss: 0.00023215208784677088\n",
            "step: 80, loss: 0.0005101553397253156\n",
            "step: 90, loss: 0.19367016851902008\n",
            "step: 100, loss: 0.0004114776966162026\n",
            "step: 110, loss: 0.0054990206845104694\n",
            "step: 120, loss: 0.0005309933912940323\n",
            "step: 130, loss: 0.00027011078782379627\n",
            "step: 140, loss: 0.0013312968658283353\n",
            "step: 150, loss: 0.0004413897986523807\n",
            "step: 160, loss: 0.0007705691386945546\n",
            "step: 170, loss: 0.00758472690358758\n",
            "step: 180, loss: 0.0004602769622579217\n",
            "step: 190, loss: 0.00038238344131968915\n",
            "step: 200, loss: 0.0015542850596830249\n",
            "step: 210, loss: 0.00023946484725456685\n",
            "step: 220, loss: 0.00031835745903663337\n",
            "step: 230, loss: 0.0004929449642077088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9796380090497738, f1=0.9683972911963882, best_f1=0.963718820861678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004643940774258226\n",
            "step: 10, loss: 0.00026026423438452184\n",
            "step: 20, loss: 0.001981467241421342\n",
            "step: 30, loss: 0.007200699765235186\n",
            "step: 40, loss: 0.0002239397872472182\n",
            "step: 50, loss: 0.0021689871791750193\n",
            "step: 60, loss: 0.0005861124373041093\n",
            "step: 70, loss: 0.00019211611652281135\n",
            "step: 80, loss: 6.793371721869335e-05\n",
            "step: 90, loss: 0.0011578060220927\n",
            "step: 100, loss: 8.080551924649626e-05\n",
            "step: 110, loss: 8.709450776223093e-05\n",
            "step: 120, loss: 0.0002590655058156699\n",
            "step: 130, loss: 0.00022967855329625309\n",
            "step: 140, loss: 0.0006959949969314039\n",
            "step: 150, loss: 0.00033293821616098285\n",
            "step: 160, loss: 0.006475898902863264\n",
            "step: 170, loss: 0.00035406355164013803\n",
            "step: 180, loss: 0.0015987132210284472\n",
            "step: 190, loss: 0.00037620370858348906\n",
            "step: 200, loss: 0.0004955226904712617\n",
            "step: 210, loss: 0.00314123323187232\n",
            "step: 220, loss: 0.03420831635594368\n",
            "step: 230, loss: 0.00033711371361278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9785794813979707, f1=0.9685393258426966, best_f1=0.963718820861678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005327956168912351\n",
            "step: 10, loss: 0.0004490526916924864\n",
            "step: 20, loss: 0.00044629626790992916\n",
            "step: 30, loss: 0.00024252616276498884\n",
            "step: 40, loss: 0.0008366873953491449\n",
            "step: 50, loss: 0.003525296924635768\n",
            "step: 60, loss: 0.0005832270835526288\n",
            "step: 70, loss: 0.0005956381210125983\n",
            "step: 80, loss: 0.006309507880359888\n",
            "step: 90, loss: 0.00023212558880914003\n",
            "step: 100, loss: 0.0005467826267704368\n",
            "step: 110, loss: 0.0005579552380368114\n",
            "step: 120, loss: 0.0002998782729264349\n",
            "step: 130, loss: 0.0005510238115675747\n",
            "step: 140, loss: 8.759072079556063e-05\n",
            "step: 150, loss: 0.00019740939023904502\n",
            "step: 160, loss: 0.0008912305347621441\n",
            "step: 170, loss: 0.0002889930037781596\n",
            "step: 180, loss: 0.032950326800346375\n",
            "step: 190, loss: 0.0002837429929058999\n",
            "step: 200, loss: 3.6557557905325666e-05\n",
            "step: 210, loss: 0.0010231365449726582\n",
            "step: 220, loss: 0.00041298248106613755\n",
            "step: 230, loss: 0.00038595549995079637\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9808773903262092, f1=0.9652076318742986, best_f1=0.9652076318742986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001764976914273575\n",
            "step: 10, loss: 0.0002117412514053285\n",
            "step: 20, loss: 0.0003892107051797211\n",
            "step: 30, loss: 0.0013447507517412305\n",
            "step: 40, loss: 0.00018194603035226464\n",
            "step: 50, loss: 6.0781843785662204e-05\n",
            "step: 60, loss: 0.0001556318165967241\n",
            "step: 70, loss: 0.0002099411067320034\n",
            "step: 80, loss: 0.000286593014607206\n",
            "step: 90, loss: 0.0037779826670885086\n",
            "step: 100, loss: 0.0017054929630830884\n",
            "step: 110, loss: 0.0009010019130073488\n",
            "step: 120, loss: 6.614124140469357e-05\n",
            "step: 130, loss: 0.0005115754320286214\n",
            "step: 140, loss: 0.0009047744679264724\n",
            "step: 150, loss: 0.00047842503408901393\n",
            "step: 160, loss: 0.0002619950973894447\n",
            "step: 170, loss: 0.00013377550931181759\n",
            "step: 180, loss: 0.010627007111907005\n",
            "step: 190, loss: 0.00019120072829537094\n",
            "step: 200, loss: 0.0006157051539048553\n",
            "step: 210, loss: 0.0002469994651619345\n",
            "step: 220, loss: 0.0014003097312524915\n",
            "step: 230, loss: 0.00025665474822744727\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9764309764309763, f1=0.9675977653631285, best_f1=0.9652076318742986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04352060332894325\n",
            "step: 10, loss: 0.0002381288359174505\n",
            "step: 20, loss: 0.0014263620832934976\n",
            "step: 30, loss: 0.0008856107597239316\n",
            "step: 40, loss: 0.00029007942066527903\n",
            "step: 50, loss: 0.00025933151482604444\n",
            "step: 60, loss: 0.008786112070083618\n",
            "step: 70, loss: 0.0016709580086171627\n",
            "step: 80, loss: 0.00040850977529771626\n",
            "step: 90, loss: 0.00021402222046162933\n",
            "step: 100, loss: 0.00012027379852952436\n",
            "step: 110, loss: 0.00010958637722069398\n",
            "step: 120, loss: 0.05864306166768074\n",
            "step: 130, loss: 0.00010806990030687302\n",
            "step: 140, loss: 0.0102815181016922\n",
            "step: 150, loss: 0.00012744627019856125\n",
            "step: 160, loss: 0.0007813902338966727\n",
            "step: 170, loss: 8.139592682709917e-05\n",
            "step: 180, loss: 0.00029721984174102545\n",
            "step: 190, loss: 0.0038179182447493076\n",
            "step: 200, loss: 0.0025678849779069424\n",
            "step: 210, loss: 0.0009580273763276637\n",
            "step: 220, loss: 0.00019082137441728264\n",
            "step: 230, loss: 0.0005557229742407799\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.976271186440678, f1=0.9660633484162895, best_f1=0.9652076318742986\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 196.02it/s]\n",
            "load_f1 = 0.9820224719101124\n",
            "real_f1 = 0.9765363128491621\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 183.79it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW78AaaneEUs"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hwn5WkZe3Kb",
        "outputId": "b2c4f47e-51e2-4e56-cc70-dd1705bfe2f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6021271347999573\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.48927000164985657\n",
            "step: 20, loss: 0.28137579560279846\n",
            "step: 30, loss: 0.3575286865234375\n",
            "step: 40, loss: 0.32774513959884644\n",
            "step: 50, loss: 0.4312056303024292\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.2407153993844986\n",
            "step: 70, loss: 0.30559852719306946\n",
            "step: 80, loss: 0.217124342918396\n",
            "step: 90, loss: 0.21193501353263855\n",
            "step: 100, loss: 0.2576051950454712\n",
            "step: 110, loss: 0.2475411742925644\n",
            "step: 120, loss: 0.15867742896080017\n",
            "step: 130, loss: 0.12336186319589615\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 140, loss: 0.2521691918373108\n",
            "step: 150, loss: 0.24412395060062408\n",
            "step: 160, loss: 0.13285748660564423\n",
            "step: 170, loss: 0.18323275446891785\n",
            "step: 180, loss: 0.20185911655426025\n",
            "step: 190, loss: 0.18593519926071167\n",
            "step: 200, loss: 0.16255688667297363\n",
            "step: 210, loss: 0.05090378224849701\n",
            "step: 220, loss: 0.03489828482270241\n",
            "step: 230, loss: 0.30177515745162964\n",
            "step: 240, loss: 0.05433971807360649\n",
            "step: 250, loss: 0.0330197848379612\n",
            "step: 260, loss: 0.2743716537952423\n",
            "step: 270, loss: 0.35621941089630127\n",
            "step: 280, loss: 0.05251535028219223\n",
            "step: 290, loss: 0.0627850741147995\n",
            "step: 300, loss: 0.1559806913137436\n",
            "step: 310, loss: 0.1465369015932083\n",
            "step: 320, loss: 0.15394383668899536\n",
            "step: 330, loss: 0.1402025818824768\n",
            "step: 340, loss: 0.3821689188480377\n",
            "step: 350, loss: 0.20290139317512512\n",
            "step: 360, loss: 0.03662355616688728\n",
            "step: 370, loss: 0.09656445682048798\n",
            "step: 380, loss: 0.15540984272956848\n",
            "step: 390, loss: 0.10217973589897156\n",
            "step: 400, loss: 0.04591572284698486\n",
            "step: 410, loss: 0.278754860162735\n",
            "step: 420, loss: 0.021489422768354416\n",
            "step: 430, loss: 0.041315607726573944\n",
            "step: 440, loss: 0.030172375962138176\n",
            "step: 450, loss: 0.18416619300842285\n",
            "step: 460, loss: 0.08546731621026993\n",
            "step: 470, loss: 0.04200410842895508\n",
            "step: 480, loss: 0.11571329832077026\n",
            "step: 490, loss: 0.14761732518672943\n",
            "step: 500, loss: 0.1178479939699173\n",
            "step: 510, loss: 0.2040894329547882\n",
            "step: 520, loss: 0.38892021775245667\n",
            "step: 530, loss: 0.09004048258066177\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9041474654377881, f1=0.9005524861878452, best_f1=0.9005524861878452\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10381624847650528\n",
            "step: 10, loss: 0.14179623126983643\n",
            "step: 20, loss: 0.1949073225259781\n",
            "step: 30, loss: 0.059853650629520416\n",
            "step: 40, loss: 0.19580356776714325\n",
            "step: 50, loss: 0.2509353756904602\n",
            "step: 60, loss: 0.03562203049659729\n",
            "step: 70, loss: 0.0410626120865345\n",
            "step: 80, loss: 0.1461990773677826\n",
            "step: 90, loss: 0.055674728006124496\n",
            "step: 100, loss: 0.15537574887275696\n",
            "step: 110, loss: 0.0392434224486351\n",
            "step: 120, loss: 0.09558281302452087\n",
            "step: 130, loss: 0.02751762978732586\n",
            "step: 140, loss: 0.0791824534535408\n",
            "step: 150, loss: 0.053813111037015915\n",
            "step: 160, loss: 0.018685828894376755\n",
            "step: 170, loss: 0.2128654569387436\n",
            "step: 180, loss: 0.19536761939525604\n",
            "step: 190, loss: 0.057176701724529266\n",
            "step: 200, loss: 0.22321532666683197\n",
            "step: 210, loss: 0.0357600674033165\n",
            "step: 220, loss: 0.006225903052836657\n",
            "step: 230, loss: 0.12016916275024414\n",
            "step: 240, loss: 0.12966719269752502\n",
            "step: 250, loss: 0.0843336433172226\n",
            "step: 260, loss: 0.08021166175603867\n",
            "step: 270, loss: 0.03307768702507019\n",
            "step: 280, loss: 0.039629630744457245\n",
            "step: 290, loss: 0.042494628578424454\n",
            "step: 300, loss: 0.15113162994384766\n",
            "step: 310, loss: 0.041911520063877106\n",
            "step: 320, loss: 0.18417659401893616\n",
            "step: 330, loss: 0.09656403958797455\n",
            "step: 340, loss: 0.14465685188770294\n",
            "step: 350, loss: 0.006920136045664549\n",
            "step: 360, loss: 0.07558457553386688\n",
            "step: 370, loss: 0.06431883573532104\n",
            "step: 380, loss: 0.17166177928447723\n",
            "step: 390, loss: 0.0216812901198864\n",
            "step: 400, loss: 0.12795014679431915\n",
            "step: 410, loss: 0.19118836522102356\n",
            "step: 420, loss: 0.08130518347024918\n",
            "step: 430, loss: 0.101955346763134\n",
            "step: 440, loss: 0.015658140182495117\n",
            "step: 450, loss: 0.11181028187274933\n",
            "step: 460, loss: 0.10854510217905045\n",
            "step: 470, loss: 0.09498029947280884\n",
            "step: 480, loss: 0.02972794696688652\n",
            "step: 490, loss: 0.04405564069747925\n",
            "step: 500, loss: 0.0227005984634161\n",
            "step: 510, loss: 0.19244424998760223\n",
            "step: 520, loss: 0.3794453740119934\n",
            "step: 530, loss: 0.10961294919252396\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.929368029739777, f1=0.9230769230769231, best_f1=0.9230769230769231\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13766154646873474\n",
            "step: 10, loss: 0.23226284980773926\n",
            "step: 20, loss: 0.017370279878377914\n",
            "step: 30, loss: 0.06101859733462334\n",
            "step: 40, loss: 0.07651188224554062\n",
            "step: 50, loss: 0.12042830884456635\n",
            "step: 60, loss: 0.03611094132065773\n",
            "step: 70, loss: 0.039589375257492065\n",
            "step: 80, loss: 0.1483336091041565\n",
            "step: 90, loss: 0.0323101170361042\n",
            "step: 100, loss: 0.15278975665569305\n",
            "step: 110, loss: 0.10441923141479492\n",
            "step: 120, loss: 0.1408575028181076\n",
            "step: 130, loss: 0.04817638546228409\n",
            "step: 140, loss: 0.024588342756032944\n",
            "step: 150, loss: 0.04674936830997467\n",
            "step: 160, loss: 0.037655871361494064\n",
            "step: 170, loss: 0.02514350228011608\n",
            "step: 180, loss: 0.03278575837612152\n",
            "step: 190, loss: 0.03935021907091141\n",
            "step: 200, loss: 0.046699728816747665\n",
            "step: 210, loss: 0.06856601685285568\n",
            "step: 220, loss: 0.11200110614299774\n",
            "step: 230, loss: 0.06406103819608688\n",
            "step: 240, loss: 0.21842694282531738\n",
            "step: 250, loss: 0.1319742351770401\n",
            "step: 260, loss: 0.11526448279619217\n",
            "step: 270, loss: 0.04213925451040268\n",
            "step: 280, loss: 0.051270030438899994\n",
            "step: 290, loss: 0.00315583567135036\n",
            "step: 300, loss: 0.3229296803474426\n",
            "step: 310, loss: 0.05423345789313316\n",
            "step: 320, loss: 0.08636312186717987\n",
            "step: 330, loss: 0.004904761910438538\n",
            "step: 340, loss: 0.02581176906824112\n",
            "step: 350, loss: 0.1068231463432312\n",
            "step: 360, loss: 0.017048027366399765\n",
            "step: 370, loss: 0.047542523592710495\n",
            "step: 380, loss: 0.008642377331852913\n",
            "step: 390, loss: 0.03711691126227379\n",
            "step: 400, loss: 0.18638063967227936\n",
            "step: 410, loss: 0.027552513405680656\n",
            "step: 420, loss: 0.022535935044288635\n",
            "step: 430, loss: 0.11152851581573486\n",
            "step: 440, loss: 0.3678845763206482\n",
            "step: 450, loss: 0.03414585068821907\n",
            "step: 460, loss: 0.1742609739303589\n",
            "step: 470, loss: 0.00974488165229559\n",
            "step: 480, loss: 0.1804620623588562\n",
            "step: 490, loss: 0.10786823183298111\n",
            "step: 500, loss: 0.14911320805549622\n",
            "step: 510, loss: 0.08688095957040787\n",
            "step: 520, loss: 0.003099879715591669\n",
            "step: 530, loss: 0.03239227831363678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.929784304726939, f1=0.9144434222631096, best_f1=0.9144434222631096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.029294800013303757\n",
            "step: 10, loss: 0.008641844615340233\n",
            "step: 20, loss: 0.18510885536670685\n",
            "step: 30, loss: 0.06187906488776207\n",
            "step: 40, loss: 0.033923353999853134\n",
            "step: 50, loss: 0.02228126861155033\n",
            "step: 60, loss: 0.01766025274991989\n",
            "step: 70, loss: 0.09794598072767258\n",
            "step: 80, loss: 0.14240875840187073\n",
            "step: 90, loss: 0.07564151287078857\n",
            "step: 100, loss: 0.0028763869777321815\n",
            "step: 110, loss: 0.14325737953186035\n",
            "step: 120, loss: 0.0038432118017226458\n",
            "step: 130, loss: 0.01304568350315094\n",
            "step: 140, loss: 0.053687684237957\n",
            "step: 150, loss: 0.014715554192662239\n",
            "step: 160, loss: 0.019038410857319832\n",
            "step: 170, loss: 0.01066505629569292\n",
            "step: 180, loss: 0.08490083366632462\n",
            "step: 190, loss: 0.0449758842587471\n",
            "step: 200, loss: 0.037272512912750244\n",
            "step: 210, loss: 0.0029867347329854965\n",
            "step: 220, loss: 0.016164930537343025\n",
            "step: 230, loss: 0.04373384267091751\n",
            "step: 240, loss: 0.006161415483802557\n",
            "step: 250, loss: 0.08538873493671417\n",
            "step: 260, loss: 0.0057889665476977825\n",
            "step: 270, loss: 0.02113775908946991\n",
            "step: 280, loss: 0.010244578123092651\n",
            "step: 290, loss: 0.1684872955083847\n",
            "step: 300, loss: 0.008358447812497616\n",
            "step: 310, loss: 0.008144109509885311\n",
            "step: 320, loss: 0.006458647549152374\n",
            "step: 330, loss: 0.020441723987460136\n",
            "step: 340, loss: 0.06920942664146423\n",
            "step: 350, loss: 0.026912806555628777\n",
            "step: 360, loss: 0.1063561886548996\n",
            "step: 370, loss: 0.005702035501599312\n",
            "step: 380, loss: 0.0776008814573288\n",
            "step: 390, loss: 0.005586352664977312\n",
            "step: 400, loss: 0.021316256374120712\n",
            "step: 410, loss: 0.002851000754162669\n",
            "step: 420, loss: 0.016986966133117676\n",
            "step: 430, loss: 0.0019328331109136343\n",
            "step: 440, loss: 0.004689862951636314\n",
            "step: 450, loss: 0.11684533208608627\n",
            "step: 460, loss: 0.09055396914482117\n",
            "step: 470, loss: 0.002395639894530177\n",
            "step: 480, loss: 0.025210272520780563\n",
            "step: 490, loss: 0.0013335166731849313\n",
            "step: 500, loss: 0.056699465960264206\n",
            "step: 510, loss: 0.058556728065013885\n",
            "step: 520, loss: 0.01882915571331978\n",
            "step: 530, loss: 0.1708500236272812\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9372997711670481, f1=0.9288664525011474, best_f1=0.9288664525011474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016921034548431635\n",
            "step: 10, loss: 0.014089605771005154\n",
            "step: 20, loss: 0.008025983348488808\n",
            "step: 30, loss: 0.013129192404448986\n",
            "step: 40, loss: 0.05321604013442993\n",
            "step: 50, loss: 0.045059531927108765\n",
            "step: 60, loss: 0.014983884058892727\n",
            "step: 70, loss: 0.06685131043195724\n",
            "step: 80, loss: 0.0009884759783744812\n",
            "step: 90, loss: 0.16580845415592194\n",
            "step: 100, loss: 0.015609601512551308\n",
            "step: 110, loss: 0.056010618805885315\n",
            "step: 120, loss: 0.12020819634199142\n",
            "step: 130, loss: 0.009086444042623043\n",
            "step: 140, loss: 0.00521048391237855\n",
            "step: 150, loss: 0.010167153552174568\n",
            "step: 160, loss: 0.02282230369746685\n",
            "step: 170, loss: 0.01825261116027832\n",
            "step: 180, loss: 0.002138679614290595\n",
            "step: 190, loss: 0.01257923524826765\n",
            "step: 200, loss: 0.035842448472976685\n",
            "step: 210, loss: 0.005132411140948534\n",
            "step: 220, loss: 0.03918416425585747\n",
            "step: 230, loss: 0.04311414435505867\n",
            "step: 240, loss: 0.044667601585388184\n",
            "step: 250, loss: 0.09206278622150421\n",
            "step: 260, loss: 0.001610989449545741\n",
            "step: 270, loss: 0.012542682699859142\n",
            "step: 280, loss: 0.00905867200344801\n",
            "step: 290, loss: 0.05140280723571777\n",
            "step: 300, loss: 0.0785491093993187\n",
            "step: 310, loss: 0.046047601848840714\n",
            "step: 320, loss: 0.15891729295253754\n",
            "step: 330, loss: 0.005801382474601269\n",
            "step: 340, loss: 0.01797856017947197\n",
            "step: 350, loss: 0.0019962394144386053\n",
            "step: 360, loss: 0.002237877808511257\n",
            "step: 370, loss: 0.0037727304734289646\n",
            "step: 380, loss: 0.00364652369171381\n",
            "step: 390, loss: 0.003518258221447468\n",
            "step: 400, loss: 0.021674279123544693\n",
            "step: 410, loss: 0.0342731848359108\n",
            "step: 420, loss: 0.18357323110103607\n",
            "step: 430, loss: 0.02028162032365799\n",
            "step: 440, loss: 0.13803927600383759\n",
            "step: 450, loss: 0.212399423122406\n",
            "step: 460, loss: 0.09104660153388977\n",
            "step: 470, loss: 0.03889746591448784\n",
            "step: 480, loss: 0.006817690096795559\n",
            "step: 490, loss: 0.0332193560898304\n",
            "step: 500, loss: 0.009161967784166336\n",
            "step: 510, loss: 0.0024661861825734377\n",
            "step: 520, loss: 0.014234182424843311\n",
            "step: 530, loss: 0.015023387968540192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9344337459880789, f1=0.9295380307979467, best_f1=0.9288664525011474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02543627843260765\n",
            "step: 10, loss: 0.007895926013588905\n",
            "step: 20, loss: 0.004163545090705156\n",
            "step: 30, loss: 0.004550545942038298\n",
            "step: 40, loss: 0.001282573095522821\n",
            "step: 50, loss: 0.005025175400078297\n",
            "step: 60, loss: 0.09254316240549088\n",
            "step: 70, loss: 0.002112705260515213\n",
            "step: 80, loss: 0.025633448734879494\n",
            "step: 90, loss: 0.04866908863186836\n",
            "step: 100, loss: 0.010381752625107765\n",
            "step: 110, loss: 0.024721913039684296\n",
            "step: 120, loss: 0.04300244152545929\n",
            "step: 130, loss: 0.014074713923037052\n",
            "step: 140, loss: 0.002798115834593773\n",
            "step: 150, loss: 0.0009436198743060231\n",
            "step: 160, loss: 0.06615009158849716\n",
            "step: 170, loss: 0.004571512341499329\n",
            "step: 180, loss: 0.06582914292812347\n",
            "step: 190, loss: 0.22292016446590424\n",
            "step: 200, loss: 0.02437349036335945\n",
            "step: 210, loss: 0.05212430655956268\n",
            "step: 220, loss: 0.015516535378992558\n",
            "step: 230, loss: 0.05192519351840019\n",
            "step: 240, loss: 0.0033310800790786743\n",
            "step: 250, loss: 0.010705908760428429\n",
            "step: 260, loss: 0.023934733122587204\n",
            "step: 270, loss: 0.014135511592030525\n",
            "step: 280, loss: 0.0022637604270130396\n",
            "step: 290, loss: 0.004628363065421581\n",
            "step: 300, loss: 0.003681726288050413\n",
            "step: 310, loss: 0.19951778650283813\n",
            "step: 320, loss: 0.0018719421932473779\n",
            "step: 330, loss: 0.008933614008128643\n",
            "step: 340, loss: 0.004546693526208401\n",
            "step: 350, loss: 0.00489721680060029\n",
            "step: 360, loss: 0.005437348037958145\n",
            "step: 370, loss: 0.019695919007062912\n",
            "step: 380, loss: 0.1031794548034668\n",
            "step: 390, loss: 0.017162248492240906\n",
            "step: 400, loss: 0.037092845886945724\n",
            "step: 410, loss: 0.009044804610311985\n",
            "step: 420, loss: 0.020571302622556686\n",
            "step: 430, loss: 0.002165383193641901\n",
            "step: 440, loss: 0.0008278750465251505\n",
            "step: 450, loss: 0.2183479368686676\n",
            "step: 460, loss: 0.007779781706631184\n",
            "step: 470, loss: 0.0056286342442035675\n",
            "step: 480, loss: 0.022093497216701508\n",
            "step: 490, loss: 0.003143079113215208\n",
            "step: 500, loss: 0.06030775234103203\n",
            "step: 510, loss: 0.13739104568958282\n",
            "step: 520, loss: 0.0012227605329826474\n",
            "step: 530, loss: 0.021909160539507866\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9313815187557181, f1=0.925925925925926, best_f1=0.9288664525011474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017792920116335154\n",
            "step: 10, loss: 0.002102920552715659\n",
            "step: 20, loss: 0.0041181850247085094\n",
            "step: 30, loss: 0.03450154513120651\n",
            "step: 40, loss: 0.0024164023343473673\n",
            "step: 50, loss: 0.02935618907213211\n",
            "step: 60, loss: 0.0016345734475180507\n",
            "step: 70, loss: 0.0004341187886893749\n",
            "step: 80, loss: 0.008161685429513454\n",
            "step: 90, loss: 0.004945521242916584\n",
            "step: 100, loss: 0.0011239363811910152\n",
            "step: 110, loss: 0.0007200157851912081\n",
            "step: 120, loss: 0.1079743281006813\n",
            "step: 130, loss: 0.00048131096991710365\n",
            "step: 140, loss: 0.009125837124884129\n",
            "step: 150, loss: 0.02200760506093502\n",
            "step: 160, loss: 0.0009193685255013406\n",
            "step: 170, loss: 0.04170534014701843\n",
            "step: 180, loss: 0.040061235427856445\n",
            "step: 190, loss: 0.007478517480194569\n",
            "step: 200, loss: 0.0011200743028894067\n",
            "step: 210, loss: 0.00022609072038903832\n",
            "step: 220, loss: 0.020101098343729973\n",
            "step: 230, loss: 0.0007055198075249791\n",
            "step: 240, loss: 0.05177328735589981\n",
            "step: 250, loss: 0.07281693071126938\n",
            "step: 260, loss: 0.014269497245550156\n",
            "step: 270, loss: 0.0022992955055087805\n",
            "step: 280, loss: 0.006246650591492653\n",
            "step: 290, loss: 0.0025513439904898405\n",
            "step: 300, loss: 0.00041740352753549814\n",
            "step: 310, loss: 0.0012088377261534333\n",
            "step: 320, loss: 0.04237521067261696\n",
            "step: 330, loss: 0.0013671584893018007\n",
            "step: 340, loss: 0.00017359759658575058\n",
            "step: 350, loss: 0.0021774915512651205\n",
            "step: 360, loss: 0.000228030068683438\n",
            "step: 370, loss: 0.0195525661110878\n",
            "step: 380, loss: 0.07582211494445801\n",
            "step: 390, loss: 0.02748693898320198\n",
            "step: 400, loss: 0.04000873118638992\n",
            "step: 410, loss: 0.0022018318995833397\n",
            "step: 420, loss: 0.005486351437866688\n",
            "step: 430, loss: 0.012260430492460728\n",
            "step: 440, loss: 0.021848587319254875\n",
            "step: 450, loss: 0.027237260714173317\n",
            "step: 460, loss: 0.002495324471965432\n",
            "step: 470, loss: 0.16546055674552917\n",
            "step: 480, loss: 0.007945035584270954\n",
            "step: 490, loss: 0.0029760103207081556\n",
            "step: 500, loss: 0.0175702515989542\n",
            "step: 510, loss: 0.004830487072467804\n",
            "step: 520, loss: 0.00028079995536245406\n",
            "step: 530, loss: 0.005022471770644188\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9357374017568193, f1=0.9319129226493746, best_f1=0.9288664525011474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032543351408094168\n",
            "step: 10, loss: 0.0007758802967146039\n",
            "step: 20, loss: 0.0006304754060693085\n",
            "step: 30, loss: 0.021944014355540276\n",
            "step: 40, loss: 0.0052810730412602425\n",
            "step: 50, loss: 0.0009342639823444188\n",
            "step: 60, loss: 0.01532079093158245\n",
            "step: 70, loss: 0.028955040499567986\n",
            "step: 80, loss: 0.005348785314708948\n",
            "step: 90, loss: 0.01316872425377369\n",
            "step: 100, loss: 0.011087772436439991\n",
            "step: 110, loss: 0.0006838872795924544\n",
            "step: 120, loss: 0.0008560656569898129\n",
            "step: 130, loss: 0.002935072174295783\n",
            "step: 140, loss: 0.10654205083847046\n",
            "step: 150, loss: 0.0017998652765527368\n",
            "step: 160, loss: 0.022664586082100868\n",
            "step: 170, loss: 0.10904897749423981\n",
            "step: 180, loss: 0.003947189077734947\n",
            "step: 190, loss: 0.12775427103042603\n",
            "step: 200, loss: 0.015153573825955391\n",
            "step: 210, loss: 0.10159645229578018\n",
            "step: 220, loss: 0.007313641719520092\n",
            "step: 230, loss: 0.06525719165802002\n",
            "step: 240, loss: 0.2104329615831375\n",
            "step: 250, loss: 0.0054368507117033005\n",
            "step: 260, loss: 0.0012861313298344612\n",
            "step: 270, loss: 0.009582238271832466\n",
            "step: 280, loss: 0.04163568094372749\n",
            "step: 290, loss: 0.006659362930804491\n",
            "step: 300, loss: 0.002084288513287902\n",
            "step: 310, loss: 0.0018715556943789124\n",
            "step: 320, loss: 0.011740499176084995\n",
            "step: 330, loss: 0.0013578010257333517\n",
            "step: 340, loss: 0.01367335394024849\n",
            "step: 350, loss: 0.0006839080015197396\n",
            "step: 360, loss: 0.010455263778567314\n",
            "step: 370, loss: 0.24377687275409698\n",
            "step: 380, loss: 0.0016606093849986792\n",
            "step: 390, loss: 0.08632919192314148\n",
            "step: 400, loss: 0.005730229429900646\n",
            "step: 410, loss: 0.005184752866625786\n",
            "step: 420, loss: 0.004878136795014143\n",
            "step: 430, loss: 0.01247997023165226\n",
            "step: 440, loss: 0.08612404018640518\n",
            "step: 450, loss: 0.004977701231837273\n",
            "step: 460, loss: 0.003931726794689894\n",
            "step: 470, loss: 0.07593853026628494\n",
            "step: 480, loss: 0.03616431728005409\n",
            "step: 490, loss: 0.0016352023230865598\n",
            "step: 500, loss: 0.11569756269454956\n",
            "step: 510, loss: 0.027433140203356743\n",
            "step: 520, loss: 0.0053259991109371185\n",
            "step: 530, loss: 0.003593201283365488\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9330232558139535, f1=0.9272641952135147, best_f1=0.9288664525011474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009230218711309135\n",
            "step: 10, loss: 0.006502000615000725\n",
            "step: 20, loss: 0.00017660853336565197\n",
            "step: 30, loss: 0.10369863361120224\n",
            "step: 40, loss: 0.0036889705806970596\n",
            "step: 50, loss: 0.002247895346954465\n",
            "step: 60, loss: 0.0007602301775477827\n",
            "step: 70, loss: 0.007409462705254555\n",
            "step: 80, loss: 0.005737386178225279\n",
            "step: 90, loss: 0.007122059818357229\n",
            "step: 100, loss: 0.05324344336986542\n",
            "step: 110, loss: 0.007488677278161049\n",
            "step: 120, loss: 0.0038652485236525536\n",
            "step: 130, loss: 0.0017510614125058055\n",
            "step: 140, loss: 0.023816946893930435\n",
            "step: 150, loss: 0.007216922007501125\n",
            "step: 160, loss: 0.006600687745958567\n",
            "step: 170, loss: 0.0013261234853416681\n",
            "step: 180, loss: 0.01630353182554245\n",
            "step: 190, loss: 0.000561145949177444\n",
            "step: 200, loss: 0.0010969401337206364\n",
            "step: 210, loss: 0.010053911246359348\n",
            "step: 220, loss: 0.0008229310042224824\n",
            "step: 230, loss: 0.0026157028041779995\n",
            "step: 240, loss: 0.0006076306453906\n",
            "step: 250, loss: 0.004408820997923613\n",
            "step: 260, loss: 0.002611488802358508\n",
            "step: 270, loss: 0.0009119197493419051\n",
            "step: 280, loss: 0.0014557165559381247\n",
            "step: 290, loss: 0.0009737854707054794\n",
            "step: 300, loss: 0.014442305080592632\n",
            "step: 310, loss: 0.008031229488551617\n",
            "step: 320, loss: 0.0037156229373067617\n",
            "step: 330, loss: 0.004428910091519356\n",
            "step: 340, loss: 0.046731311827898026\n",
            "step: 350, loss: 0.20491397380828857\n",
            "step: 360, loss: 0.043532803654670715\n",
            "step: 370, loss: 0.0002763580996543169\n",
            "step: 380, loss: 0.003683590330183506\n",
            "step: 390, loss: 0.0008981875726021826\n",
            "step: 400, loss: 0.007332648616284132\n",
            "step: 410, loss: 0.003608172293752432\n",
            "step: 420, loss: 0.0015710457228124142\n",
            "step: 430, loss: 0.0029462631791830063\n",
            "step: 440, loss: 0.0005870398017577827\n",
            "step: 450, loss: 0.0011707281228154898\n",
            "step: 460, loss: 0.012724538333714008\n",
            "step: 470, loss: 0.0008610282675363123\n",
            "step: 480, loss: 0.009001852944493294\n",
            "step: 490, loss: 0.0014163326704874635\n",
            "step: 500, loss: 0.01833421178162098\n",
            "step: 510, loss: 0.008889159187674522\n",
            "step: 520, loss: 0.005718624219298363\n",
            "step: 530, loss: 0.016195738688111305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9360709286047598, f1=0.9313264346190029, best_f1=0.9288664525011474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0036183572374284267\n",
            "step: 10, loss: 0.0013667060993611813\n",
            "step: 20, loss: 0.008106927387416363\n",
            "step: 30, loss: 0.005471321288496256\n",
            "step: 40, loss: 0.03908894211053848\n",
            "step: 50, loss: 0.0016764140455052257\n",
            "step: 60, loss: 0.00039488085894845426\n",
            "step: 70, loss: 0.000365017622243613\n",
            "step: 80, loss: 0.0014208696084097028\n",
            "step: 90, loss: 0.00202162261120975\n",
            "step: 100, loss: 0.002209308324381709\n",
            "step: 110, loss: 0.006611153017729521\n",
            "step: 120, loss: 0.004054723307490349\n",
            "step: 130, loss: 0.009013688191771507\n",
            "step: 140, loss: 0.004395270720124245\n",
            "step: 150, loss: 0.0022092140279710293\n",
            "step: 160, loss: 0.03088958188891411\n",
            "step: 170, loss: 0.0013146671699360013\n",
            "step: 180, loss: 0.007726968266069889\n",
            "step: 190, loss: 0.0003690905577968806\n",
            "step: 200, loss: 0.0011341985082253814\n",
            "step: 210, loss: 0.015698427334427834\n",
            "step: 220, loss: 0.1047629565000534\n",
            "step: 230, loss: 0.0013831929536536336\n",
            "step: 240, loss: 0.0026448089629411697\n",
            "step: 250, loss: 0.059804484248161316\n",
            "step: 260, loss: 0.021290669217705727\n",
            "step: 270, loss: 0.04755224287509918\n",
            "step: 280, loss: 0.001276828465051949\n",
            "step: 290, loss: 0.004321693908423185\n",
            "step: 300, loss: 0.005605428013950586\n",
            "step: 310, loss: 0.013560120016336441\n",
            "step: 320, loss: 0.0035357968881726265\n",
            "step: 330, loss: 0.046418629586696625\n",
            "step: 340, loss: 0.00015353340131696314\n",
            "step: 350, loss: 0.09236089885234833\n",
            "step: 360, loss: 0.0013610331807285547\n",
            "step: 370, loss: 9.03607506188564e-05\n",
            "step: 380, loss: 0.003365136682987213\n",
            "step: 390, loss: 0.001176066230982542\n",
            "step: 400, loss: 0.0007000644109211862\n",
            "step: 410, loss: 0.00031687552109360695\n",
            "step: 420, loss: 0.0028277456294745207\n",
            "step: 430, loss: 0.0022288423497229815\n",
            "step: 440, loss: 0.0005956141976639628\n",
            "step: 450, loss: 0.004611152224242687\n",
            "step: 460, loss: 0.002675495343282819\n",
            "step: 470, loss: 0.004068596754223108\n",
            "step: 480, loss: 0.002137131290510297\n",
            "step: 490, loss: 0.0007766523631289601\n",
            "step: 500, loss: 0.00424261623993516\n",
            "step: 510, loss: 0.001428383169695735\n",
            "step: 520, loss: 0.004592460580170155\n",
            "step: 530, loss: 0.04049052298069\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9346497414198403, f1=0.9262564584311883, best_f1=0.9288664525011474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00020427550771273673\n",
            "step: 10, loss: 0.0014476985670626163\n",
            "step: 20, loss: 0.002810215577483177\n",
            "step: 30, loss: 0.0008752995636314154\n",
            "step: 40, loss: 0.0012543199118226767\n",
            "step: 50, loss: 0.01181510929018259\n",
            "step: 60, loss: 0.0009829485788941383\n",
            "step: 70, loss: 0.0007639721734449267\n",
            "step: 80, loss: 0.0008271416299976408\n",
            "step: 90, loss: 0.0009156232699751854\n",
            "step: 100, loss: 0.0010928952833637595\n",
            "step: 110, loss: 0.004076366778463125\n",
            "step: 120, loss: 0.00033474076190032065\n",
            "step: 130, loss: 0.025657907128334045\n",
            "step: 140, loss: 0.01785808801651001\n",
            "step: 150, loss: 0.0023428453132510185\n",
            "step: 160, loss: 0.0003977542801294476\n",
            "step: 170, loss: 0.0007523559615947306\n",
            "step: 180, loss: 0.00032526295399293303\n",
            "step: 190, loss: 0.0003768807218875736\n",
            "step: 200, loss: 0.0012011559447273612\n",
            "step: 210, loss: 0.004116297699511051\n",
            "step: 220, loss: 0.008744061924517155\n",
            "step: 230, loss: 0.0004353329713921994\n",
            "step: 240, loss: 0.005398523062467575\n",
            "step: 250, loss: 0.0014456266071647406\n",
            "step: 260, loss: 0.005637768190354109\n",
            "step: 270, loss: 0.0006657946505583823\n",
            "step: 280, loss: 0.0007807389483787119\n",
            "step: 290, loss: 0.0022538688499480486\n",
            "step: 300, loss: 0.0006679610232822597\n",
            "step: 310, loss: 0.023074757307767868\n",
            "step: 320, loss: 0.0004893758450634778\n",
            "step: 330, loss: 9.616243187338114e-05\n",
            "step: 340, loss: 0.005563965067267418\n",
            "step: 350, loss: 0.0007004905492067337\n",
            "step: 360, loss: 0.025554435327649117\n",
            "step: 370, loss: 0.00498721981421113\n",
            "step: 380, loss: 0.0031774400267750025\n",
            "step: 390, loss: 0.001823842409066856\n",
            "step: 400, loss: 0.0056471191346645355\n",
            "step: 410, loss: 0.0009770026663318276\n",
            "step: 420, loss: 0.012685094960033894\n",
            "step: 430, loss: 0.0014699576422572136\n",
            "step: 440, loss: 0.0005353084998205304\n",
            "step: 450, loss: 0.00879866536706686\n",
            "step: 460, loss: 0.022784367203712463\n",
            "step: 470, loss: 0.00048049932229332626\n",
            "step: 480, loss: 5.97483194724191e-05\n",
            "step: 490, loss: 0.00701029971241951\n",
            "step: 500, loss: 0.00017188598576467484\n",
            "step: 510, loss: 0.0010348324431106448\n",
            "step: 520, loss: 0.008227905258536339\n",
            "step: 530, loss: 0.01293687429279089\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9335180055401663, f1=0.9291338582677167, best_f1=0.9288664525011474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003505278844386339\n",
            "step: 10, loss: 0.00337662803940475\n",
            "step: 20, loss: 0.01355853769928217\n",
            "step: 30, loss: 0.00012171889102319255\n",
            "step: 40, loss: 0.013292371295392513\n",
            "step: 50, loss: 0.0037972277496010065\n",
            "step: 60, loss: 0.01940314657986164\n",
            "step: 70, loss: 0.0003634584427345544\n",
            "step: 80, loss: 0.00014259052113629878\n",
            "step: 90, loss: 0.00026602353318594396\n",
            "step: 100, loss: 0.023186199367046356\n",
            "step: 110, loss: 7.982304668985307e-05\n",
            "step: 120, loss: 0.0004933247691951692\n",
            "step: 130, loss: 0.0005827456479892135\n",
            "step: 140, loss: 0.00020417293126229197\n",
            "step: 150, loss: 0.005080288741737604\n",
            "step: 160, loss: 0.0005313549772836268\n",
            "step: 170, loss: 0.0010804341873154044\n",
            "step: 180, loss: 0.0036357049830257893\n",
            "step: 190, loss: 0.001448947936296463\n",
            "step: 200, loss: 0.001395135186612606\n",
            "step: 210, loss: 0.00035978714004158974\n",
            "step: 220, loss: 0.0008323414949700236\n",
            "step: 230, loss: 0.0007243268773891032\n",
            "step: 240, loss: 0.0016037641325965524\n",
            "step: 250, loss: 0.16614004969596863\n",
            "step: 260, loss: 0.01475249882787466\n",
            "step: 270, loss: 0.001102876616641879\n",
            "step: 280, loss: 0.0006275522173382342\n",
            "step: 290, loss: 0.0035195511300116777\n",
            "step: 300, loss: 0.002045743865892291\n",
            "step: 310, loss: 0.01697416976094246\n",
            "step: 320, loss: 0.03143361583352089\n",
            "step: 330, loss: 0.0023141277488321066\n",
            "step: 340, loss: 0.0008734888979233801\n",
            "step: 350, loss: 0.00029551313491538167\n",
            "step: 360, loss: 0.0035744013730436563\n",
            "step: 370, loss: 0.0019134856993332505\n",
            "step: 380, loss: 0.007881473749876022\n",
            "step: 390, loss: 0.008702069520950317\n",
            "step: 400, loss: 0.00020491672330535948\n",
            "step: 410, loss: 0.02102666161954403\n",
            "step: 420, loss: 0.0011943705612793565\n",
            "step: 430, loss: 0.04131095111370087\n",
            "step: 440, loss: 0.0033626649528741837\n",
            "step: 450, loss: 0.09794166684150696\n",
            "step: 460, loss: 0.0019933811854571104\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 470, loss: 0.0036225372459739447\n",
            "step: 480, loss: 0.025773225352168083\n",
            "step: 490, loss: 0.001356636406853795\n",
            "step: 500, loss: 0.020604213699698448\n",
            "step: 510, loss: 0.018803419545292854\n",
            "step: 520, loss: 0.008267750963568687\n",
            "step: 530, loss: 0.006365531589835882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9373246024321795, f1=0.9315838800374883, best_f1=0.9315838800374883\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018377596279606223\n",
            "step: 10, loss: 0.0004883133224211633\n",
            "step: 20, loss: 0.0026669041253626347\n",
            "step: 30, loss: 7.245273445732892e-05\n",
            "step: 40, loss: 0.0035212193615734577\n",
            "step: 50, loss: 0.013475008308887482\n",
            "step: 60, loss: 0.014412999153137207\n",
            "step: 70, loss: 0.005062179174274206\n",
            "step: 80, loss: 0.008025445975363255\n",
            "step: 90, loss: 0.0003232705930713564\n",
            "step: 100, loss: 0.007601145189255476\n",
            "step: 110, loss: 0.0026336577720940113\n",
            "step: 120, loss: 0.0028938171453773975\n",
            "step: 130, loss: 0.006478547118604183\n",
            "step: 140, loss: 0.001160553190857172\n",
            "step: 150, loss: 0.0016225532162934542\n",
            "step: 160, loss: 0.025890706107020378\n",
            "step: 170, loss: 0.0007352612446993589\n",
            "step: 180, loss: 0.0013397295260801911\n",
            "step: 190, loss: 0.0005043904529884458\n",
            "step: 200, loss: 0.00011341738718328997\n",
            "step: 210, loss: 0.00595637783408165\n",
            "step: 220, loss: 0.036426760256290436\n",
            "step: 230, loss: 0.010070830583572388\n",
            "step: 240, loss: 0.0008686622022651136\n",
            "step: 250, loss: 0.0005072771455161273\n",
            "step: 260, loss: 0.0009473328245803714\n",
            "step: 270, loss: 0.17270348966121674\n",
            "step: 280, loss: 0.003807180095463991\n",
            "step: 290, loss: 0.011776904575526714\n",
            "step: 300, loss: 9.994326683226973e-05\n",
            "step: 310, loss: 0.00107255217153579\n",
            "step: 320, loss: 0.002581903710961342\n",
            "step: 330, loss: 0.0009580028126947582\n",
            "step: 340, loss: 0.01146735344082117\n",
            "step: 350, loss: 0.00012344360584393144\n",
            "step: 360, loss: 0.12640663981437683\n",
            "step: 370, loss: 0.010212793946266174\n",
            "step: 380, loss: 0.00045933181536383927\n",
            "step: 390, loss: 0.0005255842115730047\n",
            "step: 400, loss: 0.0003264894476160407\n",
            "step: 410, loss: 0.00017671272507868707\n",
            "step: 420, loss: 0.0010116543853655457\n",
            "step: 430, loss: 0.00039567824569530785\n",
            "step: 440, loss: 0.0006040507578290999\n",
            "step: 450, loss: 0.03238028660416603\n",
            "step: 460, loss: 0.0005062703276053071\n",
            "step: 470, loss: 0.0013256451347842813\n",
            "step: 480, loss: 0.0005113255465403199\n",
            "step: 490, loss: 0.0005785905523225665\n",
            "step: 500, loss: 0.007307484280318022\n",
            "step: 510, loss: 0.003853969508782029\n",
            "step: 520, loss: 0.0016352537786588073\n",
            "step: 530, loss: 0.00016071686695795506\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9375854214123007, f1=0.9285714285714285, best_f1=0.9285714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01781112514436245\n",
            "step: 10, loss: 0.0014026411809027195\n",
            "step: 20, loss: 3.2068790460471064e-05\n",
            "step: 30, loss: 0.00027536790003068745\n",
            "step: 40, loss: 0.0004596763465087861\n",
            "step: 50, loss: 0.09773945063352585\n",
            "step: 60, loss: 0.00042737636249512434\n",
            "step: 70, loss: 0.014949873089790344\n",
            "step: 80, loss: 4.925778557662852e-05\n",
            "step: 90, loss: 8.085162698989734e-05\n",
            "step: 100, loss: 0.00015752350736875087\n",
            "step: 110, loss: 0.00026980185066349804\n",
            "step: 120, loss: 0.00200975826010108\n",
            "step: 130, loss: 7.904045924078673e-05\n",
            "step: 140, loss: 0.0008983080624602735\n",
            "step: 150, loss: 0.0008312642457894981\n",
            "step: 160, loss: 0.0005077594541944563\n",
            "step: 170, loss: 0.0009875206742435694\n",
            "step: 180, loss: 5.76547572563868e-05\n",
            "step: 190, loss: 0.0011490782490000129\n",
            "step: 200, loss: 0.036937277764081955\n",
            "step: 210, loss: 0.013656212948262691\n",
            "step: 220, loss: 0.00022465481015387923\n",
            "step: 230, loss: 0.00023242815223056823\n",
            "step: 240, loss: 0.0004887585528194904\n",
            "step: 250, loss: 0.0016569230938330293\n",
            "step: 260, loss: 0.006768590770661831\n",
            "step: 270, loss: 0.0006787653546780348\n",
            "step: 280, loss: 0.00020997844694647938\n",
            "step: 290, loss: 0.0005163763416931033\n",
            "step: 300, loss: 2.8969747290830128e-05\n",
            "step: 310, loss: 7.686371827730909e-05\n",
            "step: 320, loss: 3.459786603343673e-05\n",
            "step: 330, loss: 0.0007596353534609079\n",
            "step: 340, loss: 0.0002955097588710487\n",
            "step: 350, loss: 0.0003149946278426796\n",
            "step: 360, loss: 0.06680188328027725\n",
            "step: 370, loss: 0.013365935534238815\n",
            "step: 380, loss: 0.002355885459110141\n",
            "step: 390, loss: 0.00024144267081283033\n",
            "step: 400, loss: 0.008928489871323109\n",
            "step: 410, loss: 0.00131730898283422\n",
            "step: 420, loss: 8.201503806049004e-05\n",
            "step: 430, loss: 0.0008981192950159311\n",
            "step: 440, loss: 0.0011177656706422567\n",
            "step: 450, loss: 0.0008324099471792579\n",
            "step: 460, loss: 0.0029804883524775505\n",
            "step: 470, loss: 2.662056613189634e-05\n",
            "step: 480, loss: 6.312497134786099e-05\n",
            "step: 490, loss: 0.0014502127887681127\n",
            "step: 500, loss: 0.02719845436513424\n",
            "step: 510, loss: 0.07118108123540878\n",
            "step: 520, loss: 0.00021438916155602783\n",
            "step: 530, loss: 0.0008017323561944067\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9345622119815669, f1=0.9295644114921224, best_f1=0.9285714285714285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00024465747992508113\n",
            "step: 10, loss: 0.0010972125455737114\n",
            "step: 20, loss: 0.0008176579722203314\n",
            "step: 30, loss: 0.004318741615861654\n",
            "step: 40, loss: 9.364553261548281e-05\n",
            "step: 50, loss: 0.0009738208027556539\n",
            "step: 60, loss: 0.00033560709562152624\n",
            "step: 70, loss: 0.00010108618153026327\n",
            "step: 80, loss: 0.0005230996757745743\n",
            "step: 90, loss: 0.0047906544059515\n",
            "step: 100, loss: 0.00010667349124560133\n",
            "step: 110, loss: 0.01586259715259075\n",
            "step: 120, loss: 0.000494497362524271\n",
            "step: 130, loss: 0.004473199602216482\n",
            "step: 140, loss: 2.445058271405287e-05\n",
            "step: 150, loss: 0.00011385582911316305\n",
            "step: 160, loss: 0.0013846845831722021\n",
            "step: 170, loss: 4.976492709829472e-05\n",
            "step: 180, loss: 0.009037848562002182\n",
            "step: 190, loss: 0.0010237941751256585\n",
            "step: 200, loss: 0.0021266506519168615\n",
            "step: 210, loss: 0.00045238915481604636\n",
            "step: 220, loss: 5.2556559239747e-05\n",
            "step: 230, loss: 0.00016932415019255131\n",
            "step: 240, loss: 5.7005683629540727e-05\n",
            "step: 250, loss: 0.0001566333812661469\n",
            "step: 260, loss: 4.225858356221579e-05\n",
            "step: 270, loss: 0.0007947084377519786\n",
            "step: 280, loss: 7.10362583049573e-05\n",
            "step: 290, loss: 0.000729748047888279\n",
            "step: 300, loss: 0.0001847274979809299\n",
            "step: 310, loss: 0.00034712222986854613\n",
            "step: 320, loss: 9.481490997131914e-05\n",
            "step: 330, loss: 0.009078329429030418\n",
            "step: 340, loss: 0.00025283044669777155\n",
            "step: 350, loss: 0.0032281142193824053\n",
            "step: 360, loss: 0.001468396163545549\n",
            "step: 370, loss: 0.00037576176691800356\n",
            "step: 380, loss: 0.00037407176569104195\n",
            "step: 390, loss: 0.002070062793791294\n",
            "step: 400, loss: 0.01147549506276846\n",
            "step: 410, loss: 0.007691649720072746\n",
            "step: 420, loss: 0.0003641662187874317\n",
            "step: 430, loss: 3.476767233223654e-05\n",
            "step: 440, loss: 0.0017180141294375062\n",
            "step: 450, loss: 0.0008193021058104932\n",
            "step: 460, loss: 0.001909807906486094\n",
            "step: 470, loss: 0.00011875862401211634\n",
            "step: 480, loss: 0.002669180743396282\n",
            "step: 490, loss: 0.00012022402370348573\n",
            "step: 500, loss: 0.008801541291177273\n",
            "step: 510, loss: 7.965114491526037e-05\n",
            "step: 520, loss: 1.0128843314305414e-05\n",
            "step: 530, loss: 0.00038923360989429057\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9361900326036331, f1=0.9296037296037295, best_f1=0.9285714285714285\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:25, 224.14it/s]\n",
            "load_f1 = 0.9352319706017456\n",
            "real_f1 = 0.936953520478601\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 195.59it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6rVRw-HgNFH"
      },
      "source": [
        "# BASELINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ifpsOJMgNFH"
      },
      "source": [
        "## BASELINE STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE7bDM3EgNFI",
        "outputId": "b7d2382a-b5cc-426b-f82a-cf45d71db2a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.441062867641449\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.2828282828282828, f1=0.2947368421052632, best_f1=0.2947368421052632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.44913971424102783\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.34146341463414637, f1=0.32352941176470584, best_f1=0.32352941176470584\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.40065884590148926\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.3529411764705882, f1=0.4615384615384615, best_f1=0.4615384615384615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24683749675750732\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.3636363636363636, f1=0.3728813559322034, best_f1=0.3728813559322034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.27311110496520996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.4242424242424242, f1=0.4444444444444445, best_f1=0.4444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21405752003192902\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.3010752688172043, f1=0.29213483146067415, best_f1=0.4444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5001158118247986\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.3835616438356164, f1=0.4705882352941177, best_f1=0.4444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.44739460945129395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.3943661971830986, f1=0.4705882352941177, best_f1=0.4444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2879963517189026\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.39999999999999997, f1=0.4761904761904762, best_f1=0.4444444444444445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3988233208656311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.5365853658536585, f1=0.5454545454545454, best_f1=0.5454545454545454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4699839949607849\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.4571428571428571, f1=0.6206896551724138, best_f1=0.5454545454545454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2629753649234772\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5238095238095237, f1=0.6285714285714286, best_f1=0.5454545454545454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23949657380580902\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.5555555555555556, f1=0.6451612903225806, best_f1=0.6451612903225806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1767512708902359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5555555555555556, f1=0.6451612903225806, best_f1=0.6451612903225806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30019500851631165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5555555555555556, f1=0.6451612903225806, best_f1=0.6451612903225806\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 122177.23it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.5106382978723404\n",
            "real_f1 = 0.46428571428571436\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 139.42it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyjgIIwdgNFK",
        "outputId": "6fe7b171-86b3-4144-8c2a-6cbb4a7f7cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5938526391983032\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.48562103509902954\n",
            "step: 20, loss: 0.6333747506141663\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.3459849953651428\n",
            "step: 40, loss: 0.3323812186717987\n",
            "step: 50, loss: 0.5452267527580261\n",
            "step: 60, loss: 0.47154197096824646\n",
            "step: 70, loss: 0.43257904052734375\n",
            "step: 80, loss: 0.5814096927642822\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.07252690941095352\n",
            "step: 100, loss: 0.33503180742263794\n",
            "step: 110, loss: 0.3852945864200592\n",
            "step: 120, loss: 0.1114940196275711\n",
            "step: 130, loss: 0.009514975361526012\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 140, loss: 0.27434593439102173\n",
            "step: 150, loss: 0.16252738237380981\n",
            "step: 160, loss: 0.16562223434448242\n",
            "step: 170, loss: 0.4892139136791229\n",
            "step: 180, loss: 0.3461964726448059\n",
            "step: 190, loss: 0.04931934177875519\n",
            "step: 200, loss: 0.09193266928195953\n",
            "step: 210, loss: 0.020676637068390846\n",
            "step: 220, loss: 0.025267530232667923\n",
            "step: 230, loss: 0.008619076572358608\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9647577092511014, f1=0.9678135405105438, best_f1=0.9678135405105438\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07440400123596191\n",
            "step: 10, loss: 0.06152735650539398\n",
            "step: 20, loss: 0.04666152969002724\n",
            "step: 30, loss: 0.009391325525939465\n",
            "step: 40, loss: 0.05100557208061218\n",
            "step: 50, loss: 0.010302886366844177\n",
            "step: 60, loss: 0.010826312005519867\n",
            "step: 70, loss: 0.04302038997411728\n",
            "step: 80, loss: 0.0031378427520394325\n",
            "step: 90, loss: 0.08385016024112701\n",
            "step: 100, loss: 0.0058509656228125095\n",
            "step: 110, loss: 0.13018319010734558\n",
            "step: 120, loss: 0.00585900666192174\n",
            "step: 130, loss: 0.08506608754396439\n",
            "step: 140, loss: 0.004952284973114729\n",
            "step: 150, loss: 0.23084203898906708\n",
            "step: 160, loss: 0.08085712790489197\n",
            "step: 170, loss: 0.11089438199996948\n",
            "step: 180, loss: 0.025805726647377014\n",
            "step: 190, loss: 0.20386505126953125\n",
            "step: 200, loss: 0.02985413558781147\n",
            "step: 210, loss: 0.017250224947929382\n",
            "step: 220, loss: 0.0017987008905038238\n",
            "step: 230, loss: 0.0017120074480772018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9707865168539327, f1=0.9694224235560589, best_f1=0.9694224235560589\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04583348333835602\n",
            "step: 10, loss: 0.02133730612695217\n",
            "step: 20, loss: 0.04529983550310135\n",
            "step: 30, loss: 0.006853187922388315\n",
            "step: 40, loss: 0.006692719645798206\n",
            "step: 50, loss: 0.036768339574337006\n",
            "step: 60, loss: 0.003625725395977497\n",
            "step: 70, loss: 0.02437812089920044\n",
            "step: 80, loss: 0.0005373246967792511\n",
            "step: 90, loss: 0.0416354201734066\n",
            "step: 100, loss: 0.013044782914221287\n",
            "step: 110, loss: 0.0052366554737091064\n",
            "step: 120, loss: 0.0016216435469686985\n",
            "step: 130, loss: 0.006635958794504404\n",
            "step: 140, loss: 0.025638090446591377\n",
            "step: 150, loss: 0.14262984693050385\n",
            "step: 160, loss: 0.005493388511240482\n",
            "step: 170, loss: 0.012542244046926498\n",
            "step: 180, loss: 0.0406901054084301\n",
            "step: 190, loss: 0.009869452565908432\n",
            "step: 200, loss: 0.04750300943851471\n",
            "step: 210, loss: 0.02468492090702057\n",
            "step: 220, loss: 0.007924798876047134\n",
            "step: 230, loss: 0.011605417355895042\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9655172413793103, f1=0.9796380090497738, best_f1=0.9694224235560589\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01441898476332426\n",
            "step: 10, loss: 0.011146421544253826\n",
            "step: 20, loss: 0.006373291369527578\n",
            "step: 30, loss: 0.00148735661059618\n",
            "step: 40, loss: 0.1557079702615738\n",
            "step: 50, loss: 0.04148606210947037\n",
            "step: 60, loss: 0.0030208416283130646\n",
            "step: 70, loss: 0.013582823798060417\n",
            "step: 80, loss: 0.001978776417672634\n",
            "step: 90, loss: 0.025033632293343544\n",
            "step: 100, loss: 0.002396622207015753\n",
            "step: 110, loss: 0.0037354191299527884\n",
            "step: 120, loss: 0.003983047790825367\n",
            "step: 130, loss: 0.03493601083755493\n",
            "step: 140, loss: 0.0008264645584858954\n",
            "step: 150, loss: 0.0005122970324009657\n",
            "step: 160, loss: 0.0009274812182411551\n",
            "step: 170, loss: 0.011129608377814293\n",
            "step: 180, loss: 0.010648279450833797\n",
            "step: 190, loss: 0.0016634612111374736\n",
            "step: 200, loss: 0.024359749630093575\n",
            "step: 210, loss: 0.00790405459702015\n",
            "step: 220, loss: 0.08957234025001526\n",
            "step: 230, loss: 0.005463816691190004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.967525195968645, f1=0.9752808988764046, best_f1=0.9694224235560589\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10688533633947372\n",
            "step: 10, loss: 0.011288609355688095\n",
            "step: 20, loss: 0.040032584220170975\n",
            "step: 30, loss: 0.11199728399515152\n",
            "step: 40, loss: 0.012127633206546307\n",
            "step: 50, loss: 0.05533739551901817\n",
            "step: 60, loss: 0.017913270741701126\n",
            "step: 70, loss: 0.005926681216806173\n",
            "step: 80, loss: 0.011351799592375755\n",
            "step: 90, loss: 0.03774731978774071\n",
            "step: 100, loss: 0.0004126751737203449\n",
            "step: 110, loss: 0.05606576055288315\n",
            "step: 120, loss: 0.046321019530296326\n",
            "step: 130, loss: 0.00117921968922019\n",
            "step: 140, loss: 0.0007618131348863244\n",
            "step: 150, loss: 0.007205943111330271\n",
            "step: 160, loss: 0.06658142060041428\n",
            "step: 170, loss: 0.017257103696465492\n",
            "step: 180, loss: 0.01527467742562294\n",
            "step: 190, loss: 0.1411631852388382\n",
            "step: 200, loss: 0.007597208023071289\n",
            "step: 210, loss: 0.0028288105968385935\n",
            "step: 220, loss: 0.0012273446191102266\n",
            "step: 230, loss: 0.01653279922902584\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9774266365688488, f1=0.9808773903262092, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006522138719446957\n",
            "step: 10, loss: 0.0016581331146880984\n",
            "step: 20, loss: 0.0032140163239091635\n",
            "step: 30, loss: 0.0010925107635557652\n",
            "step: 40, loss: 0.0017818816704675555\n",
            "step: 50, loss: 0.0024088609497994184\n",
            "step: 60, loss: 0.005603129975497723\n",
            "step: 70, loss: 0.004805885720998049\n",
            "step: 80, loss: 0.004340043291449547\n",
            "step: 90, loss: 0.018218912184238434\n",
            "step: 100, loss: 0.0055126831866800785\n",
            "step: 110, loss: 0.002260128501802683\n",
            "step: 120, loss: 0.0003397281689103693\n",
            "step: 130, loss: 0.010390058159828186\n",
            "step: 140, loss: 0.004406701307743788\n",
            "step: 150, loss: 0.008397446013987064\n",
            "step: 160, loss: 0.001289120758883655\n",
            "step: 170, loss: 0.00026617993717081845\n",
            "step: 180, loss: 0.00014477618969976902\n",
            "step: 190, loss: 0.0002866294817067683\n",
            "step: 200, loss: 0.0057721324265003204\n",
            "step: 210, loss: 0.00039975077379494905\n",
            "step: 220, loss: 0.006931490730494261\n",
            "step: 230, loss: 0.0006662625819444656\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9821428571428571, f1=0.9798657718120806, best_f1=0.9798657718120806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007846711087040603\n",
            "step: 10, loss: 0.00040572983562014997\n",
            "step: 20, loss: 0.00029023404931649566\n",
            "step: 30, loss: 0.00039074523374438286\n",
            "step: 40, loss: 0.0006361599662341177\n",
            "step: 50, loss: 0.0004270822973921895\n",
            "step: 60, loss: 0.0005432607140392065\n",
            "step: 70, loss: 0.0004036369500681758\n",
            "step: 80, loss: 0.0003189498675055802\n",
            "step: 90, loss: 0.0002844003611244261\n",
            "step: 100, loss: 0.0003140360931865871\n",
            "step: 110, loss: 0.00027684870292432606\n",
            "step: 120, loss: 0.0003468646318651736\n",
            "step: 130, loss: 0.00022905458172317594\n",
            "step: 140, loss: 0.00018452052609063685\n",
            "step: 150, loss: 0.0006285668932832778\n",
            "step: 160, loss: 0.0003802869759965688\n",
            "step: 170, loss: 0.011521594598889351\n",
            "step: 180, loss: 0.0007365313940681517\n",
            "step: 190, loss: 0.002512479666620493\n",
            "step: 200, loss: 0.007209653966128826\n",
            "step: 210, loss: 0.0023914561606943607\n",
            "step: 220, loss: 0.0011875564232468605\n",
            "step: 230, loss: 0.05240686610341072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9810055865921787, f1=0.9842342342342343, best_f1=0.9798657718120806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00752011826261878\n",
            "step: 10, loss: 0.0030739977955818176\n",
            "step: 20, loss: 0.0009172834688797593\n",
            "step: 30, loss: 0.0005402707611210644\n",
            "step: 40, loss: 0.0006064899498596787\n",
            "step: 50, loss: 0.0003634823951870203\n",
            "step: 60, loss: 0.13788209855556488\n",
            "step: 70, loss: 0.0014262925833463669\n",
            "step: 80, loss: 0.04880150035023689\n",
            "step: 90, loss: 0.0004771807580254972\n",
            "step: 100, loss: 0.0032821090426295996\n",
            "step: 110, loss: 0.002207112265750766\n",
            "step: 120, loss: 0.0014656824059784412\n",
            "step: 130, loss: 0.0006125461659394205\n",
            "step: 140, loss: 0.00041506264824420214\n",
            "step: 150, loss: 0.007384235970675945\n",
            "step: 160, loss: 0.0008938332321122289\n",
            "step: 170, loss: 0.043737709522247314\n",
            "step: 180, loss: 0.0010104079265147448\n",
            "step: 190, loss: 0.06086602061986923\n",
            "step: 200, loss: 0.006020913366228342\n",
            "step: 210, loss: 0.0017920290119946003\n",
            "step: 220, loss: 0.0009266135166399181\n",
            "step: 230, loss: 0.0014651952078565955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9854423292273236, f1=0.984304932735426, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00043562453356571496\n",
            "step: 10, loss: 0.00043169857235625386\n",
            "step: 20, loss: 0.0005011096363887191\n",
            "step: 30, loss: 0.0052239540964365005\n",
            "step: 40, loss: 0.00028876078431494534\n",
            "step: 50, loss: 0.0031968161929398775\n",
            "step: 60, loss: 0.00028994851163588464\n",
            "step: 70, loss: 0.003945299424231052\n",
            "step: 80, loss: 0.00046493910485878587\n",
            "step: 90, loss: 0.0071648601442575455\n",
            "step: 100, loss: 0.0002216530847363174\n",
            "step: 110, loss: 0.002970176748931408\n",
            "step: 120, loss: 0.0029049876611679792\n",
            "step: 130, loss: 0.0005279594333842397\n",
            "step: 140, loss: 0.0006539154564961791\n",
            "step: 150, loss: 0.015003843232989311\n",
            "step: 160, loss: 0.0009324262500740588\n",
            "step: 170, loss: 0.00028157129418104887\n",
            "step: 180, loss: 0.0005178634310141206\n",
            "step: 190, loss: 0.00030248588882386684\n",
            "step: 200, loss: 0.00032683523022569716\n",
            "step: 210, loss: 0.003941940143704414\n",
            "step: 220, loss: 0.0004904085071757436\n",
            "step: 230, loss: 0.0004538496141321957\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9841269841269841, f1=0.9898305084745763, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003860544820781797\n",
            "step: 10, loss: 0.0006416024407371879\n",
            "step: 20, loss: 0.0004023470974061638\n",
            "step: 30, loss: 0.00019442591292317957\n",
            "step: 40, loss: 0.005896559450775385\n",
            "step: 50, loss: 0.00014363403897732496\n",
            "step: 60, loss: 0.00036162816104479134\n",
            "step: 70, loss: 0.004156539682298899\n",
            "step: 80, loss: 0.0005575005779974163\n",
            "step: 90, loss: 0.0003768513852264732\n",
            "step: 100, loss: 0.0009303194237872958\n",
            "step: 110, loss: 0.0025942407082766294\n",
            "step: 120, loss: 0.00013771136582363397\n",
            "step: 130, loss: 0.00029373710276558995\n",
            "step: 140, loss: 0.0001630053884582594\n",
            "step: 150, loss: 0.0006868263008072972\n",
            "step: 160, loss: 0.00012848214828409255\n",
            "step: 170, loss: 0.00021655880846083164\n",
            "step: 180, loss: 0.0015735370106995106\n",
            "step: 190, loss: 0.00032815386657603085\n",
            "step: 200, loss: 0.0004162569239269942\n",
            "step: 210, loss: 0.002785316202789545\n",
            "step: 220, loss: 0.0001751650415826589\n",
            "step: 230, loss: 0.0002907566668000072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9840546697038726, f1=0.9829351535836178, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014116255624685436\n",
            "step: 10, loss: 0.0003339835675433278\n",
            "step: 20, loss: 0.00020543599384836853\n",
            "step: 30, loss: 0.0003020930744241923\n",
            "step: 40, loss: 4.196958980173804e-05\n",
            "step: 50, loss: 0.0001126006172853522\n",
            "step: 60, loss: 0.00024417118402197957\n",
            "step: 70, loss: 0.0002492843195796013\n",
            "step: 80, loss: 0.0009658181807026267\n",
            "step: 90, loss: 0.01749914512038231\n",
            "step: 100, loss: 0.0003958838642574847\n",
            "step: 110, loss: 0.003406773554161191\n",
            "step: 120, loss: 0.00015309237642213702\n",
            "step: 130, loss: 0.00021187915990594774\n",
            "step: 140, loss: 0.00015639662160538137\n",
            "step: 150, loss: 0.00019029249961022288\n",
            "step: 160, loss: 0.20853757858276367\n",
            "step: 170, loss: 0.019206320866942406\n",
            "step: 180, loss: 0.001155993901193142\n",
            "step: 190, loss: 0.0012593016726896167\n",
            "step: 200, loss: 0.001793174771592021\n",
            "step: 210, loss: 0.00021524530893657357\n",
            "step: 220, loss: 0.00017384484817739576\n",
            "step: 230, loss: 0.0001509020512457937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9818181818181818, f1=0.9864253393665158, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003904259647242725\n",
            "step: 10, loss: 0.0001932278973981738\n",
            "step: 20, loss: 0.003988394979387522\n",
            "step: 30, loss: 0.011612806469202042\n",
            "step: 40, loss: 0.0004978320794180036\n",
            "step: 50, loss: 0.0006217342452146113\n",
            "step: 60, loss: 0.0010496187023818493\n",
            "step: 70, loss: 0.001280471682548523\n",
            "step: 80, loss: 0.0022960505448281765\n",
            "step: 90, loss: 0.006038527935743332\n",
            "step: 100, loss: 0.00011685074423439801\n",
            "step: 110, loss: 9.947604849003255e-05\n",
            "step: 120, loss: 0.0020342052448540926\n",
            "step: 130, loss: 0.00028009992092847824\n",
            "step: 140, loss: 0.0005118577391840518\n",
            "step: 150, loss: 0.0003010753425769508\n",
            "step: 160, loss: 0.0035787662491202354\n",
            "step: 170, loss: 0.0031522370409220457\n",
            "step: 180, loss: 0.00014434251352213323\n",
            "step: 190, loss: 0.002723670331761241\n",
            "step: 200, loss: 0.0010627801530063152\n",
            "step: 210, loss: 0.001224437728524208\n",
            "step: 220, loss: 0.0088764987885952\n",
            "step: 230, loss: 0.0013648371677845716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9797297297297298, f1=0.987598647125141, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007183703128248453\n",
            "step: 10, loss: 0.00027008456527255476\n",
            "step: 20, loss: 0.00035223105805926025\n",
            "step: 30, loss: 5.138126289239153e-05\n",
            "step: 40, loss: 0.00022314206580631435\n",
            "step: 50, loss: 0.003908944316208363\n",
            "step: 60, loss: 0.00010816259600687772\n",
            "step: 70, loss: 0.00046603637747466564\n",
            "step: 80, loss: 0.0006485520279966295\n",
            "step: 90, loss: 0.000328767579048872\n",
            "step: 100, loss: 0.002021492924541235\n",
            "step: 110, loss: 0.000874347286298871\n",
            "step: 120, loss: 0.00021527479111682624\n",
            "step: 130, loss: 0.00018399101099930704\n",
            "step: 140, loss: 0.00020993981161154807\n",
            "step: 150, loss: 0.00023829842393752187\n",
            "step: 160, loss: 0.020882701501250267\n",
            "step: 170, loss: 0.00011049041495425627\n",
            "step: 180, loss: 0.0041602598503232\n",
            "step: 190, loss: 0.00016600106027908623\n",
            "step: 200, loss: 6.94645059411414e-05\n",
            "step: 210, loss: 0.00017572996148373932\n",
            "step: 220, loss: 0.00014678132720291615\n",
            "step: 230, loss: 0.00013137958012521267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9797752808988766, f1=0.9820627802690582, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.212619286496192e-05\n",
            "step: 10, loss: 5.703007263946347e-05\n",
            "step: 20, loss: 0.00010730259964475408\n",
            "step: 30, loss: 0.00010977705824188888\n",
            "step: 40, loss: 0.00021776757785119116\n",
            "step: 50, loss: 8.339035412063822e-05\n",
            "step: 60, loss: 0.00011840133083751425\n",
            "step: 70, loss: 0.006662969943135977\n",
            "step: 80, loss: 7.800775347277522e-05\n",
            "step: 90, loss: 0.00018947769422084093\n",
            "step: 100, loss: 8.573298691771924e-05\n",
            "step: 110, loss: 9.352356573799625e-05\n",
            "step: 120, loss: 3.54822441295255e-05\n",
            "step: 130, loss: 0.0006350176990963519\n",
            "step: 140, loss: 8.940324914874509e-05\n",
            "step: 150, loss: 4.313972021918744e-05\n",
            "step: 160, loss: 6.75059127388522e-05\n",
            "step: 170, loss: 6.542647315654904e-05\n",
            "step: 180, loss: 8.189469372155145e-05\n",
            "step: 190, loss: 0.0008324403315782547\n",
            "step: 200, loss: 0.0003122072957921773\n",
            "step: 210, loss: 0.0001267645857296884\n",
            "step: 220, loss: 0.00011879367957590148\n",
            "step: 230, loss: 5.167825293028727e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9830890642615557, f1=0.9864559819413092, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017469908925704658\n",
            "step: 10, loss: 0.00014470572932623327\n",
            "step: 20, loss: 0.002603573724627495\n",
            "step: 30, loss: 0.0001004577788989991\n",
            "step: 40, loss: 9.141235932474956e-05\n",
            "step: 50, loss: 0.00016185594722628593\n",
            "step: 60, loss: 0.032939814031124115\n",
            "step: 70, loss: 0.00015657702169846743\n",
            "step: 80, loss: 7.768510840833187e-05\n",
            "step: 90, loss: 6.618574116146192e-05\n",
            "step: 100, loss: 4.79082009405829e-05\n",
            "step: 110, loss: 0.00017061250400729477\n",
            "step: 120, loss: 0.0010037440806627274\n",
            "step: 130, loss: 0.00015031409566290677\n",
            "step: 140, loss: 0.0024686024989932775\n",
            "step: 150, loss: 0.0004430816334206611\n",
            "step: 160, loss: 0.00013885246880818158\n",
            "step: 170, loss: 3.7993006117176265e-05\n",
            "step: 180, loss: 8.610275835962966e-05\n",
            "step: 190, loss: 0.00010386539361206815\n",
            "step: 200, loss: 0.00016247938037849963\n",
            "step: 210, loss: 0.006296322215348482\n",
            "step: 220, loss: 0.0001723758177831769\n",
            "step: 230, loss: 0.00015021975559648126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9841269841269841, f1=0.9875706214689265, best_f1=0.984304932735426\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:17, 145.10it/s]\n",
            "load_f1 = 0.9865771812080537\n",
            "real_f1 = 0.9843749999999999\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 136.40it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck7uL6uPgNFK"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyyxG2qpgNFL",
        "outputId": "7c20ac07-60b3-46c2-daf4-f956fd292bea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.629680335521698\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.43175941705703735\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.3114365339279175\n",
            "step: 30, loss: 0.3141498565673828\n",
            "step: 40, loss: 0.3172353506088257\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.5808732509613037\n",
            "step: 60, loss: 0.164769247174263\n",
            "step: 70, loss: 0.17624874413013458\n",
            "step: 80, loss: 0.2139173150062561\n",
            "step: 90, loss: 0.12726716697216034\n",
            "step: 100, loss: 0.33401089906692505\n",
            "step: 110, loss: 0.14215819537639618\n",
            "step: 120, loss: 0.15814858675003052\n",
            "step: 130, loss: 0.29725322127342224\n",
            "step: 140, loss: 0.17667295038700104\n",
            "step: 150, loss: 0.12264479696750641\n",
            "step: 160, loss: 0.16256290674209595\n",
            "step: 170, loss: 0.11906136572360992\n",
            "step: 180, loss: 0.1393405795097351\n",
            "step: 190, loss: 0.13497178256511688\n",
            "step: 200, loss: 0.10492784529924393\n",
            "step: 210, loss: 0.04792417585849762\n",
            "step: 220, loss: 0.0836707353591919\n",
            "step: 230, loss: 0.20006729662418365\n",
            "step: 240, loss: 0.06162654608488083\n",
            "step: 250, loss: 0.09375179558992386\n",
            "step: 260, loss: 0.4345814287662506\n",
            "step: 270, loss: 0.27628207206726074\n",
            "step: 280, loss: 0.07775543630123138\n",
            "step: 290, loss: 0.14721234142780304\n",
            "step: 300, loss: 0.12682917714118958\n",
            "step: 310, loss: 0.19645623862743378\n",
            "step: 320, loss: 0.11628822237253189\n",
            "step: 330, loss: 0.09139484167098999\n",
            "step: 340, loss: 0.6751250624656677\n",
            "step: 350, loss: 0.21702203154563904\n",
            "step: 360, loss: 0.04478643834590912\n",
            "step: 370, loss: 0.007595019415020943\n",
            "step: 380, loss: 0.31458520889282227\n",
            "step: 390, loss: 0.05240287259221077\n",
            "step: 400, loss: 0.03738105297088623\n",
            "step: 410, loss: 0.24271635711193085\n",
            "step: 420, loss: 0.088417649269104\n",
            "step: 430, loss: 0.010109161026775837\n",
            "step: 440, loss: 0.06085675582289696\n",
            "step: 450, loss: 0.13612860441207886\n",
            "step: 460, loss: 0.027313554659485817\n",
            "step: 470, loss: 0.07523215562105179\n",
            "step: 480, loss: 0.10194891691207886\n",
            "step: 490, loss: 0.12896791100502014\n",
            "step: 500, loss: 0.04108947142958641\n",
            "step: 510, loss: 0.027824928984045982\n",
            "step: 520, loss: 0.11346544325351715\n",
            "step: 530, loss: 0.09527228027582169\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9166666666666666, f1=0.9150507848568791, best_f1=0.9150507848568791\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11982619017362595\n",
            "step: 10, loss: 0.0931345671415329\n",
            "step: 20, loss: 0.05803162232041359\n",
            "step: 30, loss: 0.06155838072299957\n",
            "step: 40, loss: 0.02871089242398739\n",
            "step: 50, loss: 0.08321170508861542\n",
            "step: 60, loss: 0.05103663355112076\n",
            "step: 70, loss: 0.04026106745004654\n",
            "step: 80, loss: 0.06575610488653183\n",
            "step: 90, loss: 0.25028541684150696\n",
            "step: 100, loss: 0.17229235172271729\n",
            "step: 110, loss: 0.052205588668584824\n",
            "step: 120, loss: 0.09621263295412064\n",
            "step: 130, loss: 0.013427119702100754\n",
            "step: 140, loss: 0.09291041642427444\n",
            "step: 150, loss: 0.10348770022392273\n",
            "step: 160, loss: 0.07038123905658722\n",
            "step: 170, loss: 0.036126021295785904\n",
            "step: 180, loss: 0.025622976943850517\n",
            "step: 190, loss: 0.05638252571225166\n",
            "step: 200, loss: 0.10015246272087097\n",
            "step: 210, loss: 0.02452709525823593\n",
            "step: 220, loss: 0.010032973252236843\n",
            "step: 230, loss: 0.28006142377853394\n",
            "step: 240, loss: 0.08966798335313797\n",
            "step: 250, loss: 0.021270956844091415\n",
            "step: 260, loss: 0.08263510465621948\n",
            "step: 270, loss: 0.010080794803798199\n",
            "step: 280, loss: 0.09169990569353104\n",
            "step: 290, loss: 0.08845221996307373\n",
            "step: 300, loss: 0.04984324425458908\n",
            "step: 310, loss: 0.12109357863664627\n",
            "step: 320, loss: 0.2539539635181427\n",
            "step: 330, loss: 0.023224351927638054\n",
            "step: 340, loss: 0.19416488707065582\n",
            "step: 350, loss: 0.026232430711388588\n",
            "step: 360, loss: 0.04343106970191002\n",
            "step: 370, loss: 0.015620467253029346\n",
            "step: 380, loss: 0.2139187902212143\n",
            "step: 390, loss: 0.010273735038936138\n",
            "step: 400, loss: 0.1314421445131302\n",
            "step: 410, loss: 0.0559769906103611\n",
            "step: 420, loss: 0.08841246366500854\n",
            "step: 430, loss: 0.21161171793937683\n",
            "step: 440, loss: 0.011045346967875957\n",
            "step: 450, loss: 0.0349871963262558\n",
            "step: 460, loss: 0.024622049182653427\n",
            "step: 470, loss: 0.18505162000656128\n",
            "step: 480, loss: 0.027182897552847862\n",
            "step: 490, loss: 0.054943520575761795\n",
            "step: 500, loss: 0.003097415203228593\n",
            "step: 510, loss: 0.06046804413199425\n",
            "step: 520, loss: 0.4317646622657776\n",
            "step: 530, loss: 0.11884469538927078\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9274563820018366, f1=0.9305492510213346, best_f1=0.9305492510213346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13948920369148254\n",
            "step: 10, loss: 0.04884772747755051\n",
            "step: 20, loss: 0.005324604455381632\n",
            "step: 30, loss: 0.054531145840883255\n",
            "step: 40, loss: 0.021364878863096237\n",
            "step: 50, loss: 0.1250738501548767\n",
            "step: 60, loss: 0.06266967952251434\n",
            "step: 70, loss: 0.022289399057626724\n",
            "step: 80, loss: 0.026159878820180893\n",
            "step: 90, loss: 0.03504432737827301\n",
            "step: 100, loss: 0.21271854639053345\n",
            "step: 110, loss: 0.11461307108402252\n",
            "step: 120, loss: 0.06803617626428604\n",
            "step: 130, loss: 0.04191787913441658\n",
            "step: 140, loss: 0.012765977531671524\n",
            "step: 150, loss: 0.05366409942507744\n",
            "step: 160, loss: 0.027216864749789238\n",
            "step: 170, loss: 0.06726095825433731\n",
            "step: 180, loss: 0.017621180042624474\n",
            "step: 190, loss: 0.00556514086201787\n",
            "step: 200, loss: 0.06145872548222542\n",
            "step: 210, loss: 0.0548914298415184\n",
            "step: 220, loss: 0.029523909091949463\n",
            "step: 230, loss: 0.048098284751176834\n",
            "step: 240, loss: 0.017675410956144333\n",
            "step: 250, loss: 0.09265591949224472\n",
            "step: 260, loss: 0.017129043117165565\n",
            "step: 270, loss: 0.048690445721149445\n",
            "step: 280, loss: 0.002819737186655402\n",
            "step: 290, loss: 0.14290279150009155\n",
            "step: 300, loss: 0.1783103346824646\n",
            "step: 310, loss: 0.08049929887056351\n",
            "step: 320, loss: 0.1799211949110031\n",
            "step: 330, loss: 0.006874922197312117\n",
            "step: 340, loss: 0.01734117977321148\n",
            "step: 350, loss: 0.0344221256673336\n",
            "step: 360, loss: 0.011758668348193169\n",
            "step: 370, loss: 0.1692887246608734\n",
            "step: 380, loss: 0.041850849986076355\n",
            "step: 390, loss: 0.017205998301506042\n",
            "step: 400, loss: 0.2860925495624542\n",
            "step: 410, loss: 0.13197818398475647\n",
            "step: 420, loss: 0.034276243299245834\n",
            "step: 430, loss: 0.03538311645388603\n",
            "step: 440, loss: 0.17488279938697815\n",
            "step: 450, loss: 0.06430917978286743\n",
            "step: 460, loss: 0.15020082890987396\n",
            "step: 470, loss: 0.0038180879782885313\n",
            "step: 480, loss: 0.11265254020690918\n",
            "step: 490, loss: 0.0072954026982188225\n",
            "step: 500, loss: 0.055973898619413376\n",
            "step: 510, loss: 0.026306306943297386\n",
            "step: 520, loss: 0.0029277147259563208\n",
            "step: 530, loss: 0.010993215255439281\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9254426840633737, f1=0.9227906976744187, best_f1=0.9305492510213346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03301657363772392\n",
            "step: 10, loss: 0.007326907478272915\n",
            "step: 20, loss: 0.20568591356277466\n",
            "step: 30, loss: 0.03164760768413544\n",
            "step: 40, loss: 0.015300288796424866\n",
            "step: 50, loss: 0.024897266179323196\n",
            "step: 60, loss: 0.007034568581730127\n",
            "step: 70, loss: 0.06444208323955536\n",
            "step: 80, loss: 0.19308751821517944\n",
            "step: 90, loss: 0.12215981632471085\n",
            "step: 100, loss: 0.0023580545093864202\n",
            "step: 110, loss: 0.17240601778030396\n",
            "step: 120, loss: 0.010591224767267704\n",
            "step: 130, loss: 0.043879132717847824\n",
            "step: 140, loss: 0.00943259708583355\n",
            "step: 150, loss: 0.01214094553142786\n",
            "step: 160, loss: 0.005643244832754135\n",
            "step: 170, loss: 0.0072900960221886635\n",
            "step: 180, loss: 0.11389496177434921\n",
            "step: 190, loss: 0.1266767978668213\n",
            "step: 200, loss: 0.023490577936172485\n",
            "step: 210, loss: 0.0062323459424078465\n",
            "step: 220, loss: 0.02788694016635418\n",
            "step: 230, loss: 0.032669197767972946\n",
            "step: 240, loss: 0.025789543986320496\n",
            "step: 250, loss: 0.04843505099415779\n",
            "step: 260, loss: 0.012255621142685413\n",
            "step: 270, loss: 0.05958543345332146\n",
            "step: 280, loss: 0.058479294180870056\n",
            "step: 290, loss: 0.012399320490658283\n",
            "step: 300, loss: 0.005856127478182316\n",
            "step: 310, loss: 0.03304709494113922\n",
            "step: 320, loss: 0.19704462587833405\n",
            "step: 330, loss: 0.019253037869930267\n",
            "step: 340, loss: 0.016709014773368835\n",
            "step: 350, loss: 0.02400650829076767\n",
            "step: 360, loss: 0.12310384958982468\n",
            "step: 370, loss: 0.013060852885246277\n",
            "step: 380, loss: 0.003944440744817257\n",
            "step: 390, loss: 0.009186454117298126\n",
            "step: 400, loss: 0.03461157903075218\n",
            "step: 410, loss: 0.007338410243391991\n",
            "step: 420, loss: 0.02195519581437111\n",
            "step: 430, loss: 0.05311669036746025\n",
            "step: 440, loss: 0.004142853897064924\n",
            "step: 450, loss: 0.053425133228302\n",
            "step: 460, loss: 0.036596182733774185\n",
            "step: 470, loss: 0.0055281417444348335\n",
            "step: 480, loss: 0.1919451802968979\n",
            "step: 490, loss: 0.010945295915007591\n",
            "step: 500, loss: 0.31365180015563965\n",
            "step: 510, loss: 0.07643971592187881\n",
            "step: 520, loss: 0.01100990641862154\n",
            "step: 530, loss: 0.1736065149307251\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9322671683913453, f1=0.9257683215130024, best_f1=0.9257683215130024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002572464058175683\n",
            "step: 10, loss: 0.0140226436778903\n",
            "step: 20, loss: 0.06484180688858032\n",
            "step: 30, loss: 0.19269324839115143\n",
            "step: 40, loss: 0.004143074154853821\n",
            "step: 50, loss: 0.12802422046661377\n",
            "step: 60, loss: 0.012447649613022804\n",
            "step: 70, loss: 0.0030825159046798944\n",
            "step: 80, loss: 0.002635106211528182\n",
            "step: 90, loss: 0.002633033087477088\n",
            "step: 100, loss: 0.10956881195306778\n",
            "step: 110, loss: 0.02352232113480568\n",
            "step: 120, loss: 0.1766478419303894\n",
            "step: 130, loss: 0.014576088637113571\n",
            "step: 140, loss: 0.005929882638156414\n",
            "step: 150, loss: 0.008844430558383465\n",
            "step: 160, loss: 0.008799230679869652\n",
            "step: 170, loss: 0.01874133199453354\n",
            "step: 180, loss: 0.0018626826349645853\n",
            "step: 190, loss: 0.006171423476189375\n",
            "step: 200, loss: 0.004912331700325012\n",
            "step: 210, loss: 0.0016239344840869308\n",
            "step: 220, loss: 0.039661336690187454\n",
            "step: 230, loss: 0.004136991687119007\n",
            "step: 240, loss: 0.006119880825281143\n",
            "step: 250, loss: 0.15711934864521027\n",
            "step: 260, loss: 0.010936273261904716\n",
            "step: 270, loss: 0.009327058680355549\n",
            "step: 280, loss: 0.004093049094080925\n",
            "step: 290, loss: 0.09156054258346558\n",
            "step: 300, loss: 0.21820537745952606\n",
            "step: 310, loss: 0.021249903365969658\n",
            "step: 320, loss: 0.07006686180830002\n",
            "step: 330, loss: 0.002402074169367552\n",
            "step: 340, loss: 0.008761836215853691\n",
            "step: 350, loss: 0.11817622929811478\n",
            "step: 360, loss: 0.0016862807096913457\n",
            "step: 370, loss: 0.0019462648779153824\n",
            "step: 380, loss: 0.003551474306732416\n",
            "step: 390, loss: 0.004013270139694214\n",
            "step: 400, loss: 0.012776031158864498\n",
            "step: 410, loss: 0.00879279151558876\n",
            "step: 420, loss: 0.08786216378211975\n",
            "step: 430, loss: 0.014706517569720745\n",
            "step: 440, loss: 0.000697198323905468\n",
            "step: 450, loss: 0.001955314539372921\n",
            "step: 460, loss: 0.02999928593635559\n",
            "step: 470, loss: 0.02585727721452713\n",
            "step: 480, loss: 0.0030410755425691605\n",
            "step: 490, loss: 0.014846544712781906\n",
            "step: 500, loss: 0.09733512997627258\n",
            "step: 510, loss: 0.032935258001089096\n",
            "step: 520, loss: 0.3106621503829956\n",
            "step: 530, loss: 0.004235659260302782\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9324817518248174, f1=0.9230769230769231, best_f1=0.9230769230769231\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027654124423861504\n",
            "step: 10, loss: 0.001140695414505899\n",
            "step: 20, loss: 0.014146311208605766\n",
            "step: 30, loss: 0.008123229257762432\n",
            "step: 40, loss: 0.06868819147348404\n",
            "step: 50, loss: 0.0769839808344841\n",
            "step: 60, loss: 0.03311145678162575\n",
            "step: 70, loss: 0.03915689140558243\n",
            "step: 80, loss: 0.004384187050163746\n",
            "step: 90, loss: 0.0006568501703441143\n",
            "step: 100, loss: 0.34359270334243774\n",
            "step: 110, loss: 0.00344346696510911\n",
            "step: 120, loss: 0.00908522680401802\n",
            "step: 130, loss: 0.0018734189216047525\n",
            "step: 140, loss: 0.001078462926670909\n",
            "step: 150, loss: 0.0013834593119099736\n",
            "step: 160, loss: 0.08099190890789032\n",
            "step: 170, loss: 0.00931699387729168\n",
            "step: 180, loss: 0.0014127466129139066\n",
            "step: 190, loss: 0.06263530999422073\n",
            "step: 200, loss: 0.017343275249004364\n",
            "step: 210, loss: 0.012001263909041882\n",
            "step: 220, loss: 0.027164852246642113\n",
            "step: 230, loss: 0.0319710448384285\n",
            "step: 240, loss: 0.03500097990036011\n",
            "step: 250, loss: 0.1980077177286148\n",
            "step: 260, loss: 0.03707345202565193\n",
            "step: 270, loss: 0.0030181892216205597\n",
            "step: 280, loss: 0.0042432076297700405\n",
            "step: 290, loss: 0.001693011145107448\n",
            "step: 300, loss: 0.001333201420493424\n",
            "step: 310, loss: 0.0141260651871562\n",
            "step: 320, loss: 0.0001508362329332158\n",
            "step: 330, loss: 0.0217172522097826\n",
            "step: 340, loss: 0.00047770075616426766\n",
            "step: 350, loss: 0.029022572562098503\n",
            "step: 360, loss: 0.08943935483694077\n",
            "step: 370, loss: 0.004000859800726175\n",
            "step: 380, loss: 0.005511286202818155\n",
            "step: 390, loss: 0.01867753639817238\n",
            "step: 400, loss: 0.009883249178528786\n",
            "step: 410, loss: 0.005600467789918184\n",
            "step: 420, loss: 0.01644296385347843\n",
            "step: 430, loss: 0.00017543771537020802\n",
            "step: 440, loss: 0.0017064149724319577\n",
            "step: 450, loss: 0.05015953257679939\n",
            "step: 460, loss: 0.0715038850903511\n",
            "step: 470, loss: 0.007554801646620035\n",
            "step: 480, loss: 0.11457246541976929\n",
            "step: 490, loss: 0.07369835674762726\n",
            "step: 500, loss: 0.005708020646125078\n",
            "step: 510, loss: 0.22516348958015442\n",
            "step: 520, loss: 0.004711732268333435\n",
            "step: 530, loss: 0.06755684316158295\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9353233830845772, f1=0.9276285844333181, best_f1=0.9276285844333181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01390145719051361\n",
            "step: 10, loss: 0.002785701537504792\n",
            "step: 20, loss: 0.002032115589827299\n",
            "step: 30, loss: 0.023734163492918015\n",
            "step: 40, loss: 0.0012060145381838083\n",
            "step: 50, loss: 0.010164277628064156\n",
            "step: 60, loss: 0.0039795516058802605\n",
            "step: 70, loss: 0.0070814467035233974\n",
            "step: 80, loss: 0.000881667947396636\n",
            "step: 90, loss: 0.00025944216758944094\n",
            "step: 100, loss: 0.0003486165660433471\n",
            "step: 110, loss: 0.0005724957445636392\n",
            "step: 120, loss: 0.0015747197903692722\n",
            "step: 130, loss: 0.00021095202828291804\n",
            "step: 140, loss: 0.0003671112353913486\n",
            "step: 150, loss: 0.010206182487308979\n",
            "step: 160, loss: 0.004664258100092411\n",
            "step: 170, loss: 0.008610139600932598\n",
            "step: 180, loss: 0.06310689449310303\n",
            "step: 190, loss: 0.0013889153487980366\n",
            "step: 200, loss: 0.0015903160674497485\n",
            "step: 210, loss: 0.18015450239181519\n",
            "step: 220, loss: 0.05579695478081703\n",
            "step: 230, loss: 0.0006810504128225148\n",
            "step: 240, loss: 0.0008808057173155248\n",
            "step: 250, loss: 0.0013751272344961762\n",
            "step: 260, loss: 0.012419505044817924\n",
            "step: 270, loss: 0.0002894254575949162\n",
            "step: 280, loss: 0.0010982801904901862\n",
            "step: 290, loss: 0.003571548964828253\n",
            "step: 300, loss: 0.0009217915358021855\n",
            "step: 310, loss: 0.0008437392534688115\n",
            "step: 320, loss: 0.018662523478269577\n",
            "step: 330, loss: 0.0042294468730688095\n",
            "step: 340, loss: 0.0014449672307819128\n",
            "step: 350, loss: 0.027630720287561417\n",
            "step: 360, loss: 0.001372880069538951\n",
            "step: 370, loss: 0.004483519122004509\n",
            "step: 380, loss: 0.0026510946918278933\n",
            "step: 390, loss: 0.035990260541439056\n",
            "step: 400, loss: 0.13416755199432373\n",
            "step: 410, loss: 0.002907323418185115\n",
            "step: 420, loss: 0.004823458846658468\n",
            "step: 430, loss: 0.015850527212023735\n",
            "step: 440, loss: 0.0007233255309984088\n",
            "step: 450, loss: 0.0045258477330207825\n",
            "step: 460, loss: 0.07981184870004654\n",
            "step: 470, loss: 0.136990487575531\n",
            "step: 480, loss: 0.0005697853048332036\n",
            "step: 490, loss: 0.006618824787437916\n",
            "step: 500, loss: 0.002339256927371025\n",
            "step: 510, loss: 0.00011387075210222974\n",
            "step: 520, loss: 0.0060467179864645\n",
            "step: 530, loss: 0.0008406029082834721\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9271217712177123, f1=0.9222222222222223, best_f1=0.9276285844333181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018198990437667817\n",
            "step: 10, loss: 0.00027575617423281074\n",
            "step: 20, loss: 6.642167136305943e-05\n",
            "step: 30, loss: 0.00031711251358501613\n",
            "step: 40, loss: 0.0003063795738853514\n",
            "step: 50, loss: 0.0024500174913555384\n",
            "step: 60, loss: 0.00026073423214256763\n",
            "step: 70, loss: 0.0032186219468712807\n",
            "step: 80, loss: 0.01452922448515892\n",
            "step: 90, loss: 0.0003915579291060567\n",
            "step: 100, loss: 0.002798357978463173\n",
            "step: 110, loss: 0.0012633127626031637\n",
            "step: 120, loss: 0.0058888322673738\n",
            "step: 130, loss: 0.0007888861000537872\n",
            "step: 140, loss: 0.012801758013665676\n",
            "step: 150, loss: 0.00038041206425987184\n",
            "step: 160, loss: 0.11782947182655334\n",
            "step: 170, loss: 0.019421564415097237\n",
            "step: 180, loss: 0.014479480683803558\n",
            "step: 190, loss: 0.008545000106096268\n",
            "step: 200, loss: 0.002792560262605548\n",
            "step: 210, loss: 0.03430136665701866\n",
            "step: 220, loss: 0.011023357510566711\n",
            "step: 230, loss: 0.14374759793281555\n",
            "step: 240, loss: 0.04527280852198601\n",
            "step: 250, loss: 0.0008525883313268423\n",
            "step: 260, loss: 0.0002246765943709761\n",
            "step: 270, loss: 0.05842183530330658\n",
            "step: 280, loss: 0.00018192263087257743\n",
            "step: 290, loss: 0.0012964651687070727\n",
            "step: 300, loss: 8.91543022589758e-05\n",
            "step: 310, loss: 0.0002812313032336533\n",
            "step: 320, loss: 0.0005671947146765888\n",
            "step: 330, loss: 0.0003124113427475095\n",
            "step: 340, loss: 0.017799807712435722\n",
            "step: 350, loss: 0.006486350204795599\n",
            "step: 360, loss: 0.013518177904188633\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 370, loss: 0.13877475261688232\n",
            "step: 380, loss: 0.0009718420915305614\n",
            "step: 390, loss: 0.013689447194337845\n",
            "step: 400, loss: 0.0003272646863479167\n",
            "step: 410, loss: 0.0017785237869247794\n",
            "step: 420, loss: 0.0002731347631197423\n",
            "step: 430, loss: 0.0002774264430627227\n",
            "step: 440, loss: 0.0015118424780666828\n",
            "step: 450, loss: 0.001155554666183889\n",
            "step: 460, loss: 0.007251799572259188\n",
            "step: 470, loss: 0.05374271795153618\n",
            "step: 480, loss: 0.003367780474945903\n",
            "step: 490, loss: 0.0019903325010091066\n",
            "step: 500, loss: 0.0017816643230617046\n",
            "step: 510, loss: 0.12376514077186584\n",
            "step: 520, loss: 0.0002901339903473854\n",
            "step: 530, loss: 0.003823414910584688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9293504030346136, f1=0.9246901811248809, best_f1=0.9276285844333181\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002054545795544982\n",
            "step: 10, loss: 0.0053407493978738785\n",
            "step: 20, loss: 0.0005148958298377693\n",
            "step: 30, loss: 0.0856022909283638\n",
            "step: 40, loss: 0.00042664946522563696\n",
            "step: 50, loss: 0.015413272194564342\n",
            "step: 60, loss: 0.000474046275485307\n",
            "step: 70, loss: 0.0003770636103581637\n",
            "step: 80, loss: 0.03222255781292915\n",
            "step: 90, loss: 0.03281306475400925\n",
            "step: 100, loss: 0.00027435977244749665\n",
            "step: 110, loss: 0.005108838900923729\n",
            "step: 120, loss: 0.0004660889389924705\n",
            "step: 130, loss: 0.0010150981834158301\n",
            "step: 140, loss: 0.0003449978830758482\n",
            "step: 150, loss: 0.00017609127098694444\n",
            "step: 160, loss: 0.26260069012641907\n",
            "step: 170, loss: 0.010313811711966991\n",
            "step: 180, loss: 0.01179942861199379\n",
            "step: 190, loss: 0.0009462814196012914\n",
            "step: 200, loss: 0.01601625233888626\n",
            "step: 210, loss: 0.1058863177895546\n",
            "step: 220, loss: 0.007108369842171669\n",
            "step: 230, loss: 0.0019244066206738353\n",
            "step: 240, loss: 0.0008907892042770982\n",
            "step: 250, loss: 0.0026847142726182938\n",
            "step: 260, loss: 0.08940631151199341\n",
            "step: 270, loss: 0.001300258096307516\n",
            "step: 280, loss: 0.0027262119110673666\n",
            "step: 290, loss: 0.0003399818961042911\n",
            "step: 300, loss: 0.028862399980425835\n",
            "step: 310, loss: 0.041045498102903366\n",
            "step: 320, loss: 0.0044095744378864765\n",
            "step: 330, loss: 0.029832903295755386\n",
            "step: 340, loss: 0.0033136142883449793\n",
            "step: 350, loss: 0.010368488729000092\n",
            "step: 360, loss: 0.0003796473320107907\n",
            "step: 370, loss: 0.017045196145772934\n",
            "step: 380, loss: 0.0007122300448827446\n",
            "step: 390, loss: 0.00030534653342328966\n",
            "step: 400, loss: 0.005096752196550369\n",
            "step: 410, loss: 0.0006113793351687491\n",
            "step: 420, loss: 0.00022431851539295167\n",
            "step: 430, loss: 0.008875364437699318\n",
            "step: 440, loss: 0.0009604044025763869\n",
            "step: 450, loss: 0.0029622248839586973\n",
            "step: 460, loss: 0.00020752966520376503\n",
            "step: 470, loss: 0.0002838623768184334\n",
            "step: 480, loss: 0.0001333390100626275\n",
            "step: 490, loss: 0.006025656126439571\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 500, loss: 0.0003266028070356697\n",
            "step: 510, loss: 0.016771133989095688\n",
            "step: 520, loss: 0.011359707452356815\n",
            "step: 530, loss: 0.036190494894981384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9362677670793214, f1=0.9259088817303267, best_f1=0.9259088817303267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015440539456903934\n",
            "step: 10, loss: 0.0011044725542888045\n",
            "step: 20, loss: 0.0003321695839986205\n",
            "step: 30, loss: 0.005973921623080969\n",
            "step: 40, loss: 0.00020897132344543934\n",
            "step: 50, loss: 0.00010642661800375208\n",
            "step: 60, loss: 0.009908880107104778\n",
            "step: 70, loss: 0.0007165849092416465\n",
            "step: 80, loss: 0.0006328907329589128\n",
            "step: 90, loss: 0.0021525027696043253\n",
            "step: 100, loss: 0.003646726254373789\n",
            "step: 110, loss: 0.0016768831992521882\n",
            "step: 120, loss: 0.001036955276504159\n",
            "step: 130, loss: 0.0002127356274286285\n",
            "step: 140, loss: 0.00024289725115522742\n",
            "step: 150, loss: 0.0012488639913499355\n",
            "step: 160, loss: 0.003928112331777811\n",
            "step: 170, loss: 0.00011000486847478896\n",
            "step: 180, loss: 0.0011233395198360085\n",
            "step: 190, loss: 0.00029067907598800957\n",
            "step: 200, loss: 0.00021353396004997194\n",
            "step: 210, loss: 0.00039323794771917164\n",
            "step: 220, loss: 0.00023610405332874507\n",
            "step: 230, loss: 0.00017519286484457552\n",
            "step: 240, loss: 0.06852412223815918\n",
            "step: 250, loss: 0.0013159875525161624\n",
            "step: 260, loss: 0.0018622762290760875\n",
            "step: 270, loss: 0.005812837742269039\n",
            "step: 280, loss: 0.008067648857831955\n",
            "step: 290, loss: 0.001101729809306562\n",
            "step: 300, loss: 0.0018348312005400658\n",
            "step: 310, loss: 0.007047594990581274\n",
            "step: 320, loss: 0.0035710069350898266\n",
            "step: 330, loss: 0.0006617330946028233\n",
            "step: 340, loss: 0.0009721763199195266\n",
            "step: 350, loss: 0.0003900434821844101\n",
            "step: 360, loss: 0.0017884951084852219\n",
            "step: 370, loss: 0.0014509963802993298\n",
            "step: 380, loss: 0.0007405334617942572\n",
            "step: 390, loss: 0.0001289093925151974\n",
            "step: 400, loss: 0.0004486065008677542\n",
            "step: 410, loss: 0.00015747631550766528\n",
            "step: 420, loss: 0.0002560762222856283\n",
            "step: 430, loss: 6.0589380154851824e-05\n",
            "step: 440, loss: 0.0001382002083119005\n",
            "step: 450, loss: 0.08789735287427902\n",
            "step: 460, loss: 0.0019354312680661678\n",
            "step: 470, loss: 0.003078971989452839\n",
            "step: 480, loss: 0.0001511320733698085\n",
            "step: 490, loss: 0.0030372971668839455\n",
            "step: 500, loss: 0.00038049605791457\n",
            "step: 510, loss: 0.0003804028092417866\n",
            "step: 520, loss: 0.00032840532367117703\n",
            "step: 530, loss: 0.0006227563717402518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9327231121281464, f1=0.9293302540415704, best_f1=0.9259088817303267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00035390406264923513\n",
            "step: 10, loss: 8.379959035664797e-05\n",
            "step: 20, loss: 7.30970423319377e-05\n",
            "step: 30, loss: 0.0011312839342281222\n",
            "step: 40, loss: 0.00013523505185730755\n",
            "step: 50, loss: 0.00022601228556595743\n",
            "step: 60, loss: 0.0005304781370796263\n",
            "step: 70, loss: 0.0005904525169171393\n",
            "step: 80, loss: 0.004434897564351559\n",
            "step: 90, loss: 0.002092203125357628\n",
            "step: 100, loss: 0.004205185454338789\n",
            "step: 110, loss: 0.0003314864879939705\n",
            "step: 120, loss: 0.00020576656970661134\n",
            "step: 130, loss: 0.0005161768640391529\n",
            "step: 140, loss: 0.00019522396905813366\n",
            "step: 150, loss: 0.00037042051553726196\n",
            "step: 160, loss: 0.0007233366486616433\n",
            "step: 170, loss: 7.22648692317307e-05\n",
            "step: 180, loss: 0.0017271980177611113\n",
            "step: 190, loss: 0.00010802015458466485\n",
            "step: 200, loss: 0.00013387089711613953\n",
            "step: 210, loss: 0.00010832558473339304\n",
            "step: 220, loss: 0.00014021170500200242\n",
            "step: 230, loss: 0.003926129080355167\n",
            "step: 240, loss: 0.0007356039714068174\n",
            "step: 250, loss: 0.006651950068771839\n",
            "step: 260, loss: 0.00017336464952677488\n",
            "step: 270, loss: 0.0010453024879097939\n",
            "step: 280, loss: 0.000116850329504814\n",
            "step: 290, loss: 6.820773705840111e-05\n",
            "step: 300, loss: 7.142317917896435e-05\n",
            "step: 310, loss: 0.0033540534786880016\n",
            "step: 320, loss: 0.00012023474118905142\n",
            "step: 330, loss: 4.8859619710128754e-05\n",
            "step: 340, loss: 0.02898534946143627\n",
            "step: 350, loss: 8.595141844125465e-05\n",
            "step: 360, loss: 0.00018183926295023412\n",
            "step: 370, loss: 9.531990508548915e-05\n",
            "step: 380, loss: 8.931040792958811e-05\n",
            "step: 390, loss: 3.876482878695242e-05\n",
            "step: 400, loss: 0.00022347772028297186\n",
            "step: 410, loss: 0.0002144345489796251\n",
            "step: 420, loss: 0.0016645106952637434\n",
            "step: 430, loss: 9.826544555835426e-05\n",
            "step: 440, loss: 0.00020720595784951001\n",
            "step: 450, loss: 0.0001503115490777418\n",
            "step: 460, loss: 0.00019554859318304807\n",
            "step: 470, loss: 0.00015897002595011145\n",
            "step: 480, loss: 0.0002554327074903995\n",
            "step: 490, loss: 0.00021405497682280838\n",
            "step: 500, loss: 0.00016864600183907896\n",
            "step: 510, loss: 0.00011103812721557915\n",
            "step: 520, loss: 0.0014382416848093271\n",
            "step: 530, loss: 0.0004113998147659004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9285389167045972, f1=0.9258753979081401, best_f1=0.9259088817303267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019888958195224404\n",
            "step: 10, loss: 0.0001267176994588226\n",
            "step: 20, loss: 0.0007098838104866445\n",
            "step: 30, loss: 0.00025224938872270286\n",
            "step: 40, loss: 0.001958019332960248\n",
            "step: 50, loss: 0.00011936207010876387\n",
            "step: 60, loss: 0.001714699319563806\n",
            "step: 70, loss: 0.000139984447741881\n",
            "step: 80, loss: 0.2414475381374359\n",
            "step: 90, loss: 0.0003743777342606336\n",
            "step: 100, loss: 0.03079436719417572\n",
            "step: 110, loss: 0.0007942226366139948\n",
            "step: 120, loss: 0.00026140132104046643\n",
            "step: 130, loss: 0.0005109406192786992\n",
            "step: 140, loss: 0.0065711503848433495\n",
            "step: 150, loss: 0.0006559110479429364\n",
            "step: 160, loss: 0.00023261918977368623\n",
            "step: 170, loss: 0.0012629989068955183\n",
            "step: 180, loss: 0.00018708172137849033\n",
            "step: 190, loss: 0.0003163266519550234\n",
            "step: 200, loss: 0.004267554730176926\n",
            "step: 210, loss: 0.012384366244077682\n",
            "step: 220, loss: 0.0001523498649476096\n",
            "step: 230, loss: 9.437319386051968e-05\n",
            "step: 240, loss: 0.0017059330129995942\n",
            "step: 250, loss: 0.00011010841262759641\n",
            "step: 260, loss: 0.00013970410509500653\n",
            "step: 270, loss: 0.00581360561773181\n",
            "step: 280, loss: 0.0005750980926677585\n",
            "step: 290, loss: 6.874301470816135e-05\n",
            "step: 300, loss: 0.00031703300192020833\n",
            "step: 310, loss: 0.0003673978499136865\n",
            "step: 320, loss: 0.002130653243511915\n",
            "step: 330, loss: 0.00030832711490802467\n",
            "step: 340, loss: 7.357646973105147e-05\n",
            "step: 350, loss: 0.00030208821408450603\n",
            "step: 360, loss: 0.02932494878768921\n",
            "step: 370, loss: 0.00012550552492029965\n",
            "step: 380, loss: 5.8747278671944514e-05\n",
            "step: 390, loss: 0.00017904341802932322\n",
            "step: 400, loss: 4.233346408000216e-05\n",
            "step: 410, loss: 0.0018969988450407982\n",
            "step: 420, loss: 0.00041690326179377735\n",
            "step: 430, loss: 7.481456850655377e-05\n",
            "step: 440, loss: 8.673613774590194e-05\n",
            "step: 450, loss: 0.023318126797676086\n",
            "step: 460, loss: 0.00010195787763223052\n",
            "step: 470, loss: 0.018831325694918633\n",
            "step: 480, loss: 0.07057234644889832\n",
            "step: 490, loss: 0.0028973277658224106\n",
            "step: 500, loss: 4.5241024054121226e-05\n",
            "step: 510, loss: 0.0002163470780942589\n",
            "step: 520, loss: 0.014123985543847084\n",
            "step: 530, loss: 0.0001907268160721287\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9315693430656934, f1=0.9337568058076225, best_f1=0.9259088817303267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000496270542498678\n",
            "step: 10, loss: 0.0001807117514545098\n",
            "step: 20, loss: 0.00032145919976755977\n",
            "step: 30, loss: 0.0001057453264365904\n",
            "step: 40, loss: 0.0001950684527400881\n",
            "step: 50, loss: 0.0024732989259064198\n",
            "step: 60, loss: 0.00012736202916130424\n",
            "step: 70, loss: 0.0007271224167197943\n",
            "step: 80, loss: 0.00022832323156762868\n",
            "step: 90, loss: 0.0002941954298876226\n",
            "step: 100, loss: 9.697068890091032e-05\n",
            "step: 110, loss: 4.002750210929662e-05\n",
            "step: 120, loss: 7.999161607585847e-05\n",
            "step: 130, loss: 5.9919158957200125e-05\n",
            "step: 140, loss: 0.0001955080369953066\n",
            "step: 150, loss: 0.0001680971181485802\n",
            "step: 160, loss: 0.00022944629017729312\n",
            "step: 170, loss: 0.0006074936245568097\n",
            "step: 180, loss: 0.0016425808425992727\n",
            "step: 190, loss: 0.00012465320469345897\n",
            "step: 200, loss: 0.0025890229735523462\n",
            "step: 210, loss: 0.00010616982035571709\n",
            "step: 220, loss: 5.704180875909515e-05\n",
            "step: 230, loss: 0.00013745957403443754\n",
            "step: 240, loss: 0.0003512621915433556\n",
            "step: 250, loss: 0.007265718188136816\n",
            "step: 260, loss: 5.96654717810452e-05\n",
            "step: 270, loss: 0.00030214007711037993\n",
            "step: 280, loss: 8.039870590437204e-05\n",
            "step: 290, loss: 5.795833931188099e-05\n",
            "step: 300, loss: 0.0003704381815623492\n",
            "step: 310, loss: 6.610876880586147e-05\n",
            "step: 320, loss: 4.648297544918023e-05\n",
            "step: 330, loss: 0.018533555790781975\n",
            "step: 340, loss: 7.121107773855329e-05\n",
            "step: 350, loss: 8.096508827293292e-05\n",
            "step: 360, loss: 0.01551082544028759\n",
            "step: 370, loss: 4.668417386710644e-05\n",
            "step: 380, loss: 4.5398512156680226e-05\n",
            "step: 390, loss: 0.00017274045967496932\n",
            "step: 400, loss: 0.0001817491283873096\n",
            "step: 410, loss: 0.000625659478828311\n",
            "step: 420, loss: 0.0003086747310589999\n",
            "step: 430, loss: 0.00011530660412972793\n",
            "step: 440, loss: 4.565032941172831e-05\n",
            "step: 450, loss: 9.167464304482564e-05\n",
            "step: 460, loss: 0.07564923167228699\n",
            "step: 470, loss: 0.001983547816053033\n",
            "step: 480, loss: 2.7311085432302207e-05\n",
            "step: 490, loss: 9.877424599835649e-05\n",
            "step: 500, loss: 2.8871887479908764e-05\n",
            "step: 510, loss: 7.411933620460331e-05\n",
            "step: 520, loss: 0.00010896822641370818\n",
            "step: 530, loss: 0.0002949864137917757\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9338919925512105, f1=0.9306466729147141, best_f1=0.9259088817303267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013228881871327758\n",
            "step: 10, loss: 0.0002313986187800765\n",
            "step: 20, loss: 5.918269380345009e-05\n",
            "step: 30, loss: 0.0001139300293289125\n",
            "step: 40, loss: 0.00012739682279061526\n",
            "step: 50, loss: 0.1022888720035553\n",
            "step: 60, loss: 8.46331677166745e-05\n",
            "step: 70, loss: 8.591072401031852e-05\n",
            "step: 80, loss: 0.00029852805892005563\n",
            "step: 90, loss: 6.434969691326842e-05\n",
            "step: 100, loss: 6.43156236037612e-05\n",
            "step: 110, loss: 0.0015446225879713893\n",
            "step: 120, loss: 6.0934853536309674e-05\n",
            "step: 130, loss: 5.495732693816535e-05\n",
            "step: 140, loss: 0.00512182991951704\n",
            "step: 150, loss: 7.477123290300369e-05\n",
            "step: 160, loss: 0.008753906935453415\n",
            "step: 170, loss: 7.158060179790482e-05\n",
            "step: 180, loss: 5.3614592616213486e-05\n",
            "step: 190, loss: 7.794410339556634e-05\n",
            "step: 200, loss: 0.00015942983736749738\n",
            "step: 210, loss: 0.00010119451326318085\n",
            "step: 220, loss: 5.7605378970038146e-05\n",
            "step: 230, loss: 5.5307260481640697e-05\n",
            "step: 240, loss: 0.004442162346094847\n",
            "step: 250, loss: 0.0013818012084811926\n",
            "step: 260, loss: 4.143272235523909e-05\n",
            "step: 270, loss: 7.774063851684332e-05\n",
            "step: 280, loss: 0.0001058164780260995\n",
            "step: 290, loss: 3.906477286363952e-05\n",
            "step: 300, loss: 0.0002236540021840483\n",
            "step: 310, loss: 7.488634582841769e-05\n",
            "step: 320, loss: 0.00011063790589105338\n",
            "step: 330, loss: 0.0006930183153599501\n",
            "step: 340, loss: 6.402521103154868e-05\n",
            "step: 350, loss: 5.150521610630676e-05\n",
            "step: 360, loss: 7.867306703701615e-05\n",
            "step: 370, loss: 0.004141377750784159\n",
            "step: 380, loss: 0.0004178885428700596\n",
            "step: 390, loss: 0.00023572756617795676\n",
            "step: 400, loss: 0.001089574070647359\n",
            "step: 410, loss: 2.0800875063287094e-05\n",
            "step: 420, loss: 4.814854764845222e-05\n",
            "step: 430, loss: 0.0007572854519821703\n",
            "step: 440, loss: 4.0896320570027456e-05\n",
            "step: 450, loss: 0.0006041480810381472\n",
            "step: 460, loss: 0.022441893815994263\n",
            "step: 470, loss: 8.586591138737276e-05\n",
            "step: 480, loss: 0.00014946071314625442\n",
            "step: 490, loss: 0.00010925615788437426\n",
            "step: 500, loss: 9.241319639841095e-05\n",
            "step: 510, loss: 0.06278534978628159\n",
            "step: 520, loss: 6.487542850663885e-05\n",
            "step: 530, loss: 0.0007334593683481216\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9362677670793214, f1=0.9308755760368663, best_f1=0.9259088817303267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.266851025633514e-05\n",
            "step: 10, loss: 8.272752165794373e-05\n",
            "step: 20, loss: 0.0002794316387735307\n",
            "step: 30, loss: 8.242473268182948e-05\n",
            "step: 40, loss: 9.538480662740767e-05\n",
            "step: 50, loss: 9.054369729710743e-05\n",
            "step: 60, loss: 0.00019979331409558654\n",
            "step: 70, loss: 0.00013891975686419755\n",
            "step: 80, loss: 6.594962906092405e-05\n",
            "step: 90, loss: 4.1710907680680975e-05\n",
            "step: 100, loss: 3.373419167473912e-05\n",
            "step: 110, loss: 0.00010918680345639586\n",
            "step: 120, loss: 4.423322752700187e-05\n",
            "step: 130, loss: 4.970522059011273e-05\n",
            "step: 140, loss: 5.209200389799662e-05\n",
            "step: 150, loss: 6.219816714292392e-05\n",
            "step: 160, loss: 8.173240348696709e-05\n",
            "step: 170, loss: 3.6502082366496325e-05\n",
            "step: 180, loss: 0.00011171281948918477\n",
            "step: 190, loss: 0.0015375040238723159\n",
            "step: 200, loss: 4.287693809601478e-05\n",
            "step: 210, loss: 0.00014849517901893705\n",
            "step: 220, loss: 6.904893962200731e-05\n",
            "step: 230, loss: 0.2222173660993576\n",
            "step: 240, loss: 0.001549765351228416\n",
            "step: 250, loss: 3.3192962291650474e-05\n",
            "step: 260, loss: 4.245625314069912e-05\n",
            "step: 270, loss: 0.00012047795462422073\n",
            "step: 280, loss: 4.577423169394024e-05\n",
            "step: 290, loss: 3.457821730989963e-05\n",
            "step: 300, loss: 4.0851162339095026e-05\n",
            "step: 310, loss: 7.121434464352205e-05\n",
            "step: 320, loss: 0.0049459622241556644\n",
            "step: 330, loss: 8.265711949206889e-05\n",
            "step: 340, loss: 7.05920610926114e-05\n",
            "step: 350, loss: 0.04684591293334961\n",
            "step: 360, loss: 9.562411287333816e-05\n",
            "step: 370, loss: 9.249311551684514e-05\n",
            "step: 380, loss: 0.00016047077951952815\n",
            "step: 390, loss: 6.239896902116016e-05\n",
            "step: 400, loss: 0.00012651389988604933\n",
            "step: 410, loss: 0.0008546349708922207\n",
            "step: 420, loss: 4.800245733349584e-05\n",
            "step: 430, loss: 2.963902079500258e-05\n",
            "step: 440, loss: 0.0005451733595691621\n",
            "step: 450, loss: 0.005008205305784941\n",
            "step: 460, loss: 6.285251583904028e-05\n",
            "step: 470, loss: 4.726892075268552e-05\n",
            "step: 480, loss: 0.0010399610036984086\n",
            "step: 490, loss: 6.429854693124071e-05\n",
            "step: 500, loss: 0.00012040587171213701\n",
            "step: 510, loss: 8.003642869880423e-05\n",
            "step: 520, loss: 0.0005819337675347924\n",
            "step: 530, loss: 3.7166373658692464e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9350411710887466, f1=0.9323515876668201, best_f1=0.9259088817303267\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:36, 155.54it/s]\n",
            "load_f1 = 0.9365808823529411\n",
            "real_f1 = 0.9353507565337\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 138.32it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb_EWW7DgNFL"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oQ7ANLogNFM",
        "outputId": "05b45577-9b7c-4bcb-ee23-eee3cbecc30e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.47057685256004333\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.44022688269615173\n",
            "step: 20, loss: 0.398640900850296\n",
            "step: 30, loss: 0.2861962616443634\n",
            "step: 40, loss: 0.3886429965496063\n",
            "step: 50, loss: 0.40738990902900696\n",
            "step: 60, loss: 0.40930142998695374\n",
            "step: 70, loss: 0.36534908413887024\n",
            "step: 80, loss: 0.4007253348827362\n",
            "step: 90, loss: 0.23915813863277435\n",
            "step: 100, loss: 0.24649623036384583\n",
            "step: 110, loss: 0.2797473669052124\n",
            "step: 120, loss: 0.3492371737957001\n",
            "step: 130, loss: 0.24358147382736206\n",
            "step: 140, loss: 0.47904351353645325\n",
            "step: 150, loss: 0.33665376901626587\n",
            "step: 160, loss: 0.4535064101219177\n",
            "step: 170, loss: 0.20035699009895325\n",
            "step: 180, loss: 0.2889772057533264\n",
            "step: 190, loss: 0.3997660279273987\n",
            "step: 200, loss: 0.2717917561531067\n",
            "step: 210, loss: 0.41316720843315125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3414634146341463, f1=0.421455938697318, best_f1=0.421455938697318\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3301513195037842\n",
            "step: 10, loss: 0.10639730095863342\n",
            "step: 20, loss: 0.3723852336406708\n",
            "step: 30, loss: 0.5049834251403809\n",
            "step: 40, loss: 0.4261194169521332\n",
            "step: 50, loss: 0.20247656106948853\n",
            "step: 60, loss: 0.3137248158454895\n",
            "step: 70, loss: 0.32034456729888916\n",
            "step: 80, loss: 0.21514977514743805\n",
            "step: 90, loss: 0.28681719303131104\n",
            "step: 100, loss: 0.46909281611442566\n",
            "step: 110, loss: 0.27773162722587585\n",
            "step: 120, loss: 0.11756698042154312\n",
            "step: 130, loss: 0.15547122061252594\n",
            "step: 140, loss: 0.1047646626830101\n",
            "step: 150, loss: 0.4442128539085388\n",
            "step: 160, loss: 0.09338098764419556\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 0.38864991068840027\n",
            "step: 180, loss: 0.25050365924835205\n",
            "step: 190, loss: 0.3426584303379059\n",
            "step: 200, loss: 0.10370062291622162\n",
            "step: 210, loss: 0.2936111390590668\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.22910216718266255, f1=0.19108280254777069, best_f1=0.421455938697318\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2673249840736389\n",
            "step: 10, loss: 0.1912701278924942\n",
            "step: 20, loss: 0.41207802295684814\n",
            "step: 30, loss: 0.1904943585395813\n",
            "step: 40, loss: 0.3420887887477875\n",
            "step: 50, loss: 0.39855310320854187\n",
            "step: 60, loss: 0.5102981328964233\n",
            "step: 70, loss: 0.24098879098892212\n",
            "step: 80, loss: 0.3690958619117737\n",
            "step: 90, loss: 0.21288812160491943\n",
            "step: 100, loss: 0.25694984197616577\n",
            "step: 110, loss: 0.21709349751472473\n",
            "step: 120, loss: 0.12235189229249954\n",
            "step: 130, loss: 0.22941982746124268\n",
            "step: 140, loss: 0.2192024141550064\n",
            "step: 150, loss: 0.13872356712818146\n",
            "step: 160, loss: 0.20644626021385193\n",
            "step: 170, loss: 0.19063812494277954\n",
            "step: 180, loss: 0.13821102678775787\n",
            "step: 190, loss: 0.11106958985328674\n",
            "step: 200, loss: 0.17342796921730042\n",
            "step: 210, loss: 0.2556306719779968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.4928716904276986, f1=0.496, best_f1=0.496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10709405690431595\n",
            "step: 10, loss: 0.18018198013305664\n",
            "step: 20, loss: 0.11947031319141388\n",
            "step: 30, loss: 0.11683034151792526\n",
            "step: 40, loss: 0.1918284147977829\n",
            "step: 50, loss: 0.2341548651456833\n",
            "step: 60, loss: 0.2620646059513092\n",
            "step: 70, loss: 0.17270582914352417\n",
            "step: 80, loss: 0.12131622433662415\n",
            "step: 90, loss: 0.15781639516353607\n",
            "step: 100, loss: 0.21975520253181458\n",
            "step: 110, loss: 0.46440234780311584\n",
            "step: 120, loss: 0.19759561121463776\n",
            "step: 130, loss: 0.29947271943092346\n",
            "step: 140, loss: 0.518985390663147\n",
            "step: 150, loss: 0.2890082001686096\n",
            "step: 160, loss: 0.24420596659183502\n",
            "step: 170, loss: 0.2281530797481537\n",
            "step: 180, loss: 0.01988767832517624\n",
            "step: 190, loss: 0.25109782814979553\n",
            "step: 200, loss: 0.07978051155805588\n",
            "step: 210, loss: 0.3075094223022461\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5514223194748359, f1=0.501138952164009, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1652311533689499\n",
            "step: 10, loss: 0.03901650011539459\n",
            "step: 20, loss: 0.11579795926809311\n",
            "step: 30, loss: 0.06315837800502777\n",
            "step: 40, loss: 0.2048652023077011\n",
            "step: 50, loss: 0.26353585720062256\n",
            "step: 60, loss: 0.11511317640542984\n",
            "step: 70, loss: 0.41361215710639954\n",
            "step: 80, loss: 0.12631700932979584\n",
            "step: 90, loss: 0.0808449238538742\n",
            "step: 100, loss: 0.04091925919055939\n",
            "step: 110, loss: 0.06808255612850189\n",
            "step: 120, loss: 0.092070072889328\n",
            "step: 130, loss: 0.12346461415290833\n",
            "step: 140, loss: 0.10860294103622437\n",
            "step: 150, loss: 0.18166585266590118\n",
            "step: 160, loss: 0.08917096257209778\n",
            "step: 170, loss: 0.33394405245780945\n",
            "step: 180, loss: 0.17635808885097504\n",
            "step: 190, loss: 0.1974513977766037\n",
            "step: 200, loss: 0.19451329112052917\n",
            "step: 210, loss: 0.1387922763824463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.5392491467576791, f1=0.5507246376811594, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03379688411951065\n",
            "step: 10, loss: 0.10711834579706192\n",
            "step: 20, loss: 0.08800775557756424\n",
            "step: 30, loss: 0.19060401618480682\n",
            "step: 40, loss: 0.11963289976119995\n",
            "step: 50, loss: 0.21711328625679016\n",
            "step: 60, loss: 0.09515660256147385\n",
            "step: 70, loss: 0.028228972107172012\n",
            "step: 80, loss: 0.15631809830665588\n",
            "step: 90, loss: 0.17689137160778046\n",
            "step: 100, loss: 0.20765048265457153\n",
            "step: 110, loss: 0.04456058144569397\n",
            "step: 120, loss: 0.042772259563207626\n",
            "step: 130, loss: 0.09509594738483429\n",
            "step: 140, loss: 0.11223773658275604\n",
            "step: 150, loss: 0.03343861177563667\n",
            "step: 160, loss: 0.10208284854888916\n",
            "step: 170, loss: 0.10856091976165771\n",
            "step: 180, loss: 0.10906998068094254\n",
            "step: 190, loss: 0.07345253229141235\n",
            "step: 200, loss: 0.11473946273326874\n",
            "step: 210, loss: 0.07323312759399414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.5295238095238096, f1=0.5346938775510204, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12923932075500488\n",
            "step: 10, loss: 0.07607994973659515\n",
            "step: 20, loss: 0.038453925400972366\n",
            "step: 30, loss: 0.02171974442899227\n",
            "step: 40, loss: 0.006569000892341137\n",
            "step: 50, loss: 0.09373444318771362\n",
            "step: 60, loss: 0.1459779590368271\n",
            "step: 70, loss: 0.011428261175751686\n",
            "step: 80, loss: 0.1206008642911911\n",
            "step: 90, loss: 0.12418156117200851\n",
            "step: 100, loss: 0.12221723794937134\n",
            "step: 110, loss: 0.015352763235569\n",
            "step: 120, loss: 0.06289944052696228\n",
            "step: 130, loss: 0.163813054561615\n",
            "step: 140, loss: 0.026976367458701134\n",
            "step: 150, loss: 0.05725480988621712\n",
            "step: 160, loss: 0.13330133259296417\n",
            "step: 170, loss: 0.07363133132457733\n",
            "step: 180, loss: 0.08322969824075699\n",
            "step: 190, loss: 0.016224730759859085\n",
            "step: 200, loss: 0.0386897474527359\n",
            "step: 210, loss: 0.14906767010688782\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.5093632958801498, f1=0.5393258426966293, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1483946442604065\n",
            "step: 10, loss: 0.05223635956645012\n",
            "step: 20, loss: 0.020746050402522087\n",
            "step: 30, loss: 0.0355597585439682\n",
            "step: 40, loss: 0.1824563890695572\n",
            "step: 50, loss: 0.13441625237464905\n",
            "step: 60, loss: 0.11753232032060623\n",
            "step: 70, loss: 0.17182520031929016\n",
            "step: 80, loss: 0.07581929117441177\n",
            "step: 90, loss: 0.11523384600877762\n",
            "step: 100, loss: 0.08647292852401733\n",
            "step: 110, loss: 0.02665085531771183\n",
            "step: 120, loss: 0.06211115047335625\n",
            "step: 130, loss: 0.07920731604099274\n",
            "step: 140, loss: 0.14410270750522614\n",
            "step: 150, loss: 0.27377116680145264\n",
            "step: 160, loss: 0.21131491661071777\n",
            "step: 170, loss: 0.04429056867957115\n",
            "step: 180, loss: 0.0575488805770874\n",
            "step: 190, loss: 0.1420467644929886\n",
            "step: 200, loss: 0.06341280788183212\n",
            "step: 210, loss: 0.08957144618034363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.5482625482625482, f1=0.5245283018867924, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05630604922771454\n",
            "step: 10, loss: 0.01887933909893036\n",
            "step: 20, loss: 0.02309660241007805\n",
            "step: 30, loss: 0.004032495431602001\n",
            "step: 40, loss: 0.002077554352581501\n",
            "step: 50, loss: 0.10243944078683853\n",
            "step: 60, loss: 0.018821414560079575\n",
            "step: 70, loss: 0.18741054832935333\n",
            "step: 80, loss: 0.0625896230340004\n",
            "step: 90, loss: 0.12112221866846085\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 100, loss: 0.08218575268983841\n",
            "step: 110, loss: 0.0945076197385788\n",
            "step: 120, loss: 0.03928019478917122\n",
            "step: 130, loss: 0.06366614997386932\n",
            "step: 140, loss: 0.24015967547893524\n",
            "step: 150, loss: 0.06625840812921524\n",
            "step: 160, loss: 0.03350147232413292\n",
            "step: 170, loss: 0.1545037180185318\n",
            "step: 180, loss: 0.06227089464664459\n",
            "step: 190, loss: 0.0026503868866711855\n",
            "step: 200, loss: 0.007555628661066294\n",
            "step: 210, loss: 0.04146617278456688\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.5419354838709678, f1=0.5296610169491526, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017531488090753555\n",
            "step: 10, loss: 0.0197527464479208\n",
            "step: 20, loss: 0.020056230947375298\n",
            "step: 30, loss: 0.012134202755987644\n",
            "step: 40, loss: 0.14515835046768188\n",
            "step: 50, loss: 0.01356012187898159\n",
            "step: 60, loss: 0.003949158359318972\n",
            "step: 70, loss: 0.010251497849822044\n",
            "step: 80, loss: 0.033448975533246994\n",
            "step: 90, loss: 0.06563766300678253\n",
            "step: 100, loss: 0.08967777341604233\n",
            "step: 110, loss: 0.010036122053861618\n",
            "step: 120, loss: 0.09615640342235565\n",
            "step: 130, loss: 0.010838952846825123\n",
            "step: 140, loss: 0.008103078231215477\n",
            "step: 150, loss: 0.022884489968419075\n",
            "step: 160, loss: 0.028789203613996506\n",
            "step: 170, loss: 0.022323492914438248\n",
            "step: 180, loss: 0.0240032821893692\n",
            "step: 190, loss: 0.0717868059873581\n",
            "step: 200, loss: 0.13190369307994843\n",
            "step: 210, loss: 0.015317214652895927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5128205128205128, f1=0.5306122448979591, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02526705339550972\n",
            "step: 10, loss: 0.023814348503947258\n",
            "step: 20, loss: 0.005903994664549828\n",
            "step: 30, loss: 0.017847472801804543\n",
            "step: 40, loss: 0.0035406097304075956\n",
            "step: 50, loss: 0.012703903019428253\n",
            "step: 60, loss: 0.010120199993252754\n",
            "step: 70, loss: 0.00804691482335329\n",
            "step: 80, loss: 0.007026770152151585\n",
            "step: 90, loss: 0.09626755118370056\n",
            "step: 100, loss: 0.022044191136956215\n",
            "step: 110, loss: 0.006488374900072813\n",
            "step: 120, loss: 0.005074964836239815\n",
            "step: 130, loss: 0.005440716166049242\n",
            "step: 140, loss: 0.0029291457030922174\n",
            "step: 150, loss: 0.0369836799800396\n",
            "step: 160, loss: 0.000637088727671653\n",
            "step: 170, loss: 0.00990633387118578\n",
            "step: 180, loss: 0.013054060749709606\n",
            "step: 190, loss: 0.10148080438375473\n",
            "step: 200, loss: 0.003183140652254224\n",
            "step: 210, loss: 0.11049973219633102\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.5135623869801085, f1=0.5369369369369369, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001324945711530745\n",
            "step: 10, loss: 0.025609459728002548\n",
            "step: 20, loss: 0.09105542302131653\n",
            "step: 30, loss: 0.008014517836272717\n",
            "step: 40, loss: 0.0003010148066096008\n",
            "step: 50, loss: 0.0017354594310745597\n",
            "step: 60, loss: 0.0005346747348085046\n",
            "step: 70, loss: 0.0023710287641733885\n",
            "step: 80, loss: 0.005288045387715101\n",
            "step: 90, loss: 0.013014500960707664\n",
            "step: 100, loss: 0.006034354213625193\n",
            "step: 110, loss: 0.017347626388072968\n",
            "step: 120, loss: 0.004246900789439678\n",
            "step: 130, loss: 0.03188389539718628\n",
            "step: 140, loss: 0.009081107564270496\n",
            "step: 150, loss: 0.0002049483300652355\n",
            "step: 160, loss: 0.022991806268692017\n",
            "step: 170, loss: 0.0013255911180749536\n",
            "step: 180, loss: 0.05255982652306557\n",
            "step: 190, loss: 0.0010092565789818764\n",
            "step: 200, loss: 0.0012571299448609352\n",
            "step: 210, loss: 0.03861300274729729\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.5384615384615384, f1=0.5482041587901701, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0132059371098876\n",
            "step: 10, loss: 0.022747663781046867\n",
            "step: 20, loss: 0.00023941094696056098\n",
            "step: 30, loss: 0.0007851270493119955\n",
            "step: 40, loss: 0.008608534000813961\n",
            "step: 50, loss: 0.020207200199365616\n",
            "step: 60, loss: 0.08565198630094528\n",
            "step: 70, loss: 0.0025555461179465055\n",
            "step: 80, loss: 0.0026775833684951067\n",
            "step: 90, loss: 0.005288212094455957\n",
            "step: 100, loss: 0.002925093984231353\n",
            "step: 110, loss: 0.04432830214500427\n",
            "step: 120, loss: 0.035289835184812546\n",
            "step: 130, loss: 0.005520145408809185\n",
            "step: 140, loss: 0.00257607433013618\n",
            "step: 150, loss: 0.0022211282048374414\n",
            "step: 160, loss: 0.015271141193807125\n",
            "step: 170, loss: 0.00716851931065321\n",
            "step: 180, loss: 0.0018016532994806767\n",
            "step: 190, loss: 0.12021900713443756\n",
            "step: 200, loss: 0.011032648384571075\n",
            "step: 210, loss: 0.000615430879406631\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.5107296137339056, f1=0.5305263157894737, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001067854231223464\n",
            "step: 10, loss: 0.006010159850120544\n",
            "step: 20, loss: 0.00631955498829484\n",
            "step: 30, loss: 0.0017635003896430135\n",
            "step: 40, loss: 0.0018784207059070468\n",
            "step: 50, loss: 0.024431778118014336\n",
            "step: 60, loss: 0.02166070230305195\n",
            "step: 70, loss: 0.00031135682365857065\n",
            "step: 80, loss: 0.014051804319024086\n",
            "step: 90, loss: 0.0011052308836951852\n",
            "step: 100, loss: 0.00021569253294728696\n",
            "step: 110, loss: 0.006588434800505638\n",
            "step: 120, loss: 0.0041179233230650425\n",
            "step: 130, loss: 0.03028508462011814\n",
            "step: 140, loss: 0.01364207360893488\n",
            "step: 150, loss: 0.0039269388653337955\n",
            "step: 160, loss: 0.0005800789804197848\n",
            "step: 170, loss: 0.0059949494898319244\n",
            "step: 180, loss: 0.001461973413825035\n",
            "step: 190, loss: 0.015728704631328583\n",
            "step: 200, loss: 0.02566668950021267\n",
            "step: 210, loss: 0.008528560400009155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5235173824130879, f1=0.5237113402061856, best_f1=0.501138952164009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004458356183022261\n",
            "step: 10, loss: 0.0460464283823967\n",
            "step: 20, loss: 0.13075023889541626\n",
            "step: 30, loss: 0.0013850259128957987\n",
            "step: 40, loss: 0.01767604798078537\n",
            "step: 50, loss: 0.0008547450415790081\n",
            "step: 60, loss: 0.0013259651605039835\n",
            "step: 70, loss: 0.0006349296309053898\n",
            "step: 80, loss: 0.0018121832981705666\n",
            "step: 90, loss: 0.001311911386437714\n",
            "step: 100, loss: 0.0026618982665240765\n",
            "step: 110, loss: 0.04391893371939659\n",
            "step: 120, loss: 0.0018708904972299933\n",
            "step: 130, loss: 0.007082193624228239\n",
            "step: 140, loss: 0.002446397440508008\n",
            "step: 150, loss: 0.05567275732755661\n",
            "step: 160, loss: 0.021022789180278778\n",
            "step: 170, loss: 0.019703274592757225\n",
            "step: 180, loss: 0.0009262463427148759\n",
            "step: 190, loss: 0.006678009871393442\n",
            "step: 200, loss: 0.0004009292460978031\n",
            "step: 210, loss: 0.00435854634270072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.5200845665961945, f1=0.5210084033613446, best_f1=0.501138952164009\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 463.93it/s]\n",
            "load_f1 = 0.5493562231759657\n",
            "real_f1 = 0.5511111111111111\n",
            "267it [00:00, 1245.56it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 203.55it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIIoASlugNFN",
        "outputId": "5386b691-e25a-4cef-cc8e-e9bd329d9a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rDownloading:   0% 0.00/481 [00:00<?, ?B/s]\rDownloading: 100% 481/481 [00:00<00:00, 619kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 35.3MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 28.4MB/s]\n",
            "Downloading: 100% 501M/501M [00:10<00:00, 50.1MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4512113332748413\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.40071263909339905\n",
            "step: 20, loss: 0.2611177861690521\n",
            "step: 30, loss: 0.36583060026168823\n",
            "step: 40, loss: 0.23873446881771088\n",
            "step: 50, loss: 0.3057631850242615\n",
            "step: 60, loss: 0.4374149739742279\n",
            "step: 70, loss: 0.4442554712295532\n",
            "step: 80, loss: 0.15269428491592407\n",
            "step: 90, loss: 0.3096036911010742\n",
            "step: 100, loss: 0.42504453659057617\n",
            "step: 110, loss: 0.23296961188316345\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.38019582629203796\n",
            "step: 130, loss: 0.3042263388633728\n",
            "step: 140, loss: 0.199650838971138\n",
            "step: 150, loss: 0.30961912870407104\n",
            "step: 160, loss: 0.2492312788963318\n",
            "step: 170, loss: 0.36791399121284485\n",
            "step: 180, loss: 0.16620618104934692\n",
            "step: 190, loss: 0.16056863963603973\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.17255252570406795, f1=0.17165847116674118, best_f1=0.17165847116674118\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.41196292638778687\n",
            "step: 10, loss: 0.2930580973625183\n",
            "step: 20, loss: 0.6274330615997314\n",
            "step: 30, loss: 0.24216675758361816\n",
            "step: 40, loss: 0.563340425491333\n",
            "step: 50, loss: 0.33554893732070923\n",
            "step: 60, loss: 0.4800533354282379\n",
            "step: 70, loss: 0.31294554471969604\n",
            "step: 80, loss: 0.1561821848154068\n",
            "step: 90, loss: 0.33851486444473267\n",
            "step: 100, loss: 0.2613258361816406\n",
            "step: 110, loss: 0.37431228160858154\n",
            "step: 120, loss: 0.23577766120433807\n",
            "step: 130, loss: 0.4989562928676605\n",
            "step: 140, loss: 0.33381709456443787\n",
            "step: 150, loss: 0.321774959564209\n",
            "step: 160, loss: 0.30783531069755554\n",
            "step: 170, loss: 0.23905570805072784\n",
            "step: 180, loss: 0.16909779608249664\n",
            "step: 190, loss: 0.23969274759292603\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17165847116674118\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3942989408969879\n",
            "step: 10, loss: 0.3729674220085144\n",
            "step: 20, loss: 0.48591774702072144\n",
            "step: 30, loss: 0.30101558566093445\n",
            "step: 40, loss: 0.08963991701602936\n",
            "step: 50, loss: 0.3571828305721283\n",
            "step: 60, loss: 0.18615339696407318\n",
            "step: 70, loss: 0.37721577286720276\n",
            "step: 80, loss: 0.3133876621723175\n",
            "step: 90, loss: 0.3670804798603058\n",
            "step: 100, loss: 0.5308944582939148\n",
            "step: 110, loss: 0.6614845991134644\n",
            "step: 120, loss: 0.37457215785980225\n",
            "step: 130, loss: 0.1575506031513214\n",
            "step: 140, loss: 0.38978925347328186\n",
            "step: 150, loss: 0.30670365691185\n",
            "step: 160, loss: 0.5829534530639648\n",
            "step: 170, loss: 0.4392128586769104\n",
            "step: 180, loss: 0.4142517149448395\n",
            "step: 190, loss: 0.16294798254966736\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17165847116674118\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2384049892425537\n",
            "step: 10, loss: 0.23356537520885468\n",
            "step: 20, loss: 0.29951077699661255\n",
            "step: 30, loss: 0.23040369153022766\n",
            "step: 40, loss: 0.5254533290863037\n",
            "step: 50, loss: 0.22994045913219452\n",
            "step: 60, loss: 0.3758431077003479\n",
            "step: 70, loss: 0.2874874174594879\n",
            "step: 80, loss: 0.215135857462883\n",
            "step: 90, loss: 0.1656908243894577\n",
            "step: 100, loss: 0.37237653136253357\n",
            "step: 110, loss: 0.43771079182624817\n",
            "step: 120, loss: 0.21496860682964325\n",
            "step: 130, loss: 0.41639605164527893\n",
            "step: 140, loss: 0.366245299577713\n",
            "step: 150, loss: 0.22688212990760803\n",
            "step: 160, loss: 0.30455127358436584\n",
            "step: 170, loss: 0.38771477341651917\n",
            "step: 180, loss: 0.3485363721847534\n",
            "step: 190, loss: 0.14125657081604004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.1732886548587841, f1=0.17763845350052246, best_f1=0.17763845350052246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37308764457702637\n",
            "step: 10, loss: 0.37572842836380005\n",
            "step: 20, loss: 0.14306342601776123\n",
            "step: 30, loss: 0.12983831763267517\n",
            "step: 40, loss: 0.30720996856689453\n",
            "step: 50, loss: 0.5058145523071289\n",
            "step: 60, loss: 0.24166560173034668\n",
            "step: 70, loss: 0.37300655245780945\n",
            "step: 80, loss: 0.3408999741077423\n",
            "step: 90, loss: 0.2786271274089813\n",
            "step: 100, loss: 0.4205271601676941\n",
            "step: 110, loss: 0.3727059066295624\n",
            "step: 120, loss: 0.2563682198524475\n",
            "step: 130, loss: 0.5280582904815674\n",
            "step: 140, loss: 0.3850794732570648\n",
            "step: 150, loss: 0.3132009208202362\n",
            "step: 160, loss: 0.160552516579628\n",
            "step: 170, loss: 0.38917139172554016\n",
            "step: 180, loss: 0.251452773809433\n",
            "step: 190, loss: 0.3092745244503021\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17763845350052246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30981120467185974\n",
            "step: 10, loss: 0.24243669211864471\n",
            "step: 20, loss: 0.3103092312812805\n",
            "step: 30, loss: 0.4803442060947418\n",
            "step: 40, loss: 0.2406260222196579\n",
            "step: 50, loss: 0.3107726573944092\n",
            "step: 60, loss: 0.4279722273349762\n",
            "step: 70, loss: 0.3196614682674408\n",
            "step: 80, loss: 0.31866681575775146\n",
            "step: 90, loss: 0.23533140122890472\n",
            "step: 100, loss: 0.4751221835613251\n",
            "step: 110, loss: 0.23675936460494995\n",
            "step: 120, loss: 0.44699159264564514\n",
            "step: 130, loss: 0.5087694525718689\n",
            "step: 140, loss: 0.194551020860672\n",
            "step: 150, loss: 0.3923151195049286\n",
            "step: 160, loss: 0.38509559631347656\n",
            "step: 170, loss: 0.3916145861148834\n",
            "step: 180, loss: 0.18637993931770325\n",
            "step: 190, loss: 0.3164730668067932\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17763845350052246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31311073899269104\n",
            "step: 10, loss: 0.32056108117103577\n",
            "step: 20, loss: 0.3078354001045227\n",
            "step: 30, loss: 0.19377295672893524\n",
            "step: 40, loss: 0.30551284551620483\n",
            "step: 50, loss: 0.08096975833177567\n",
            "step: 60, loss: 0.14803850650787354\n",
            "step: 70, loss: 0.15400297939777374\n",
            "step: 80, loss: 0.18985776603221893\n",
            "step: 90, loss: 0.2403053194284439\n",
            "step: 100, loss: 0.5375951528549194\n",
            "step: 110, loss: 0.43999040126800537\n",
            "step: 120, loss: 0.4118559658527374\n",
            "step: 130, loss: 0.32220718264579773\n",
            "step: 140, loss: 0.25491467118263245\n",
            "step: 150, loss: 0.29289618134498596\n",
            "step: 160, loss: 0.3744789958000183\n",
            "step: 170, loss: 0.28721216320991516\n",
            "step: 180, loss: 0.2184430956840515\n",
            "step: 190, loss: 0.3049412965774536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.20229633679606343, f1=0.20780648708081362, best_f1=0.20780648708081362\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21767063438892365\n",
            "step: 10, loss: 0.3548811376094818\n",
            "step: 20, loss: 0.20725186169147491\n",
            "step: 30, loss: 0.35881686210632324\n",
            "step: 40, loss: 0.152515709400177\n",
            "step: 50, loss: 0.2894894778728485\n",
            "step: 60, loss: 0.47860458493232727\n",
            "step: 70, loss: 0.23471425473690033\n",
            "step: 80, loss: 0.36408770084381104\n",
            "step: 90, loss: 0.22410070896148682\n",
            "step: 100, loss: 0.2864101529121399\n",
            "step: 110, loss: 0.3776041567325592\n",
            "step: 120, loss: 0.35757797956466675\n",
            "step: 130, loss: 0.28084081411361694\n",
            "step: 140, loss: 0.3624706566333771\n",
            "step: 150, loss: 0.31046953797340393\n",
            "step: 160, loss: 0.16010816395282745\n",
            "step: 170, loss: 0.22944140434265137\n",
            "step: 180, loss: 0.28984153270721436\n",
            "step: 190, loss: 0.25439196825027466\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.2018448182311449, f1=0.20501635768811338, best_f1=0.20780648708081362\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22282510995864868\n",
            "step: 10, loss: 0.09941113740205765\n",
            "step: 20, loss: 0.3054395616054535\n",
            "step: 30, loss: 0.18436011672019958\n",
            "step: 40, loss: 0.30154186487197876\n",
            "step: 50, loss: 0.3533356785774231\n",
            "step: 60, loss: 0.37330129742622375\n",
            "step: 70, loss: 0.15232525765895844\n",
            "step: 80, loss: 0.3229697048664093\n",
            "step: 90, loss: 0.7196939587593079\n",
            "step: 100, loss: 0.36359894275665283\n",
            "step: 110, loss: 0.34882014989852905\n",
            "step: 120, loss: 0.5940971374511719\n",
            "step: 130, loss: 0.285855233669281\n",
            "step: 140, loss: 0.282419890165329\n",
            "step: 150, loss: 0.23816075921058655\n",
            "step: 160, loss: 0.21643340587615967\n",
            "step: 170, loss: 0.5494287014007568\n",
            "step: 180, loss: 0.3471270799636841\n",
            "step: 190, loss: 0.2326165735721588\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.20579874928936895, f1=0.2165206508135169, best_f1=0.2165206508135169\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2509251832962036\n",
            "step: 10, loss: 0.1481095850467682\n",
            "step: 20, loss: 0.29833483695983887\n",
            "step: 30, loss: 0.3590143918991089\n",
            "step: 40, loss: 0.3014131784439087\n",
            "step: 50, loss: 0.10211319476366043\n",
            "step: 60, loss: 0.29828599095344543\n",
            "step: 70, loss: 0.3006417155265808\n",
            "step: 80, loss: 0.4341222941875458\n",
            "step: 90, loss: 0.1473262459039688\n",
            "step: 100, loss: 0.21277201175689697\n",
            "step: 110, loss: 0.21704109013080597\n",
            "step: 120, loss: 0.3655395805835724\n",
            "step: 130, loss: 0.4516887962818146\n",
            "step: 140, loss: 0.34047019481658936\n",
            "step: 150, loss: 0.2813602089881897\n",
            "step: 160, loss: 0.21761293709278107\n",
            "step: 170, loss: 0.3281923532485962\n",
            "step: 180, loss: 0.21683788299560547\n",
            "step: 190, loss: 0.14434944093227386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.2383134738771769, f1=0.24775224775224772, best_f1=0.24775224775224772\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.48076868057250977\n",
            "step: 10, loss: 0.09890998154878616\n",
            "step: 20, loss: 0.20699717104434967\n",
            "step: 30, loss: 0.20552456378936768\n",
            "step: 40, loss: 0.07632259279489517\n",
            "step: 50, loss: 0.29681968688964844\n",
            "step: 60, loss: 0.22563320398330688\n",
            "step: 70, loss: 0.5965323448181152\n",
            "step: 80, loss: 0.3749412000179291\n",
            "step: 90, loss: 0.4185492694377899\n",
            "step: 100, loss: 0.185311958193779\n",
            "step: 110, loss: 0.2407248169183731\n",
            "step: 120, loss: 0.2966213524341583\n",
            "step: 130, loss: 0.6117305755615234\n",
            "step: 140, loss: 0.4092196226119995\n",
            "step: 150, loss: 0.2943286597728729\n",
            "step: 160, loss: 0.2833141088485718\n",
            "step: 170, loss: 0.4453018307685852\n",
            "step: 180, loss: 0.2854449152946472\n",
            "step: 190, loss: 0.12454143166542053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.25507246376811593, f1=0.25686274509803925, best_f1=0.25686274509803925\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4243241548538208\n",
            "step: 10, loss: 0.321537047624588\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.5358858108520508\n",
            "step: 30, loss: 0.3522755205631256\n",
            "step: 40, loss: 0.259328693151474\n",
            "step: 50, loss: 0.4672923982143402\n",
            "step: 60, loss: 0.3631376624107361\n",
            "step: 70, loss: 0.34028881788253784\n",
            "step: 80, loss: 0.3678523302078247\n",
            "step: 90, loss: 0.469625324010849\n",
            "step: 100, loss: 0.2759910523891449\n",
            "step: 110, loss: 0.5062111020088196\n",
            "step: 120, loss: 0.15774224698543549\n",
            "step: 130, loss: 0.21425996720790863\n",
            "step: 140, loss: 0.3288852870464325\n",
            "step: 150, loss: 0.3584023714065552\n",
            "step: 160, loss: 0.2124405950307846\n",
            "step: 170, loss: 0.4114152491092682\n",
            "step: 180, loss: 0.3020594120025635\n",
            "step: 190, loss: 0.14720192551612854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.2684124386252046, f1=0.3140495867768595, best_f1=0.3140495867768595\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.248131662607193\n",
            "step: 10, loss: 0.26343247294425964\n",
            "step: 20, loss: 0.3032388687133789\n",
            "step: 30, loss: 0.3727675974369049\n",
            "step: 40, loss: 0.21474666893482208\n",
            "step: 50, loss: 0.21158884465694427\n",
            "step: 60, loss: 0.26945558190345764\n",
            "step: 70, loss: 0.1733820140361786\n",
            "step: 80, loss: 0.29232776165008545\n",
            "step: 90, loss: 0.28573623299598694\n",
            "step: 100, loss: 0.13262434303760529\n",
            "step: 110, loss: 0.595521867275238\n",
            "step: 120, loss: 0.196120947599411\n",
            "step: 130, loss: 0.23116834461688995\n",
            "step: 140, loss: 0.29628029465675354\n",
            "step: 150, loss: 0.29475143551826477\n",
            "step: 160, loss: 0.42954593896865845\n",
            "step: 170, loss: 0.12142058461904526\n",
            "step: 180, loss: 0.186457097530365\n",
            "step: 190, loss: 0.3432949185371399\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.31043956043956045, f1=0.3505747126436781, best_f1=0.3505747126436781\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23146015405654907\n",
            "step: 10, loss: 0.2083832323551178\n",
            "step: 20, loss: 0.3493533730506897\n",
            "step: 30, loss: 0.09459087252616882\n",
            "step: 40, loss: 0.2164020985364914\n",
            "step: 50, loss: 0.28364861011505127\n",
            "step: 60, loss: 0.21457535028457642\n",
            "step: 70, loss: 0.1976752132177353\n",
            "step: 80, loss: 0.31798112392425537\n",
            "step: 90, loss: 0.1726241260766983\n",
            "step: 100, loss: 0.1865566074848175\n",
            "step: 110, loss: 0.36334601044654846\n",
            "step: 120, loss: 0.25651541352272034\n",
            "step: 130, loss: 0.30272606015205383\n",
            "step: 140, loss: 0.12091168761253357\n",
            "step: 150, loss: 0.34863653779029846\n",
            "step: 160, loss: 0.2905217409133911\n",
            "step: 170, loss: 0.22475716471672058\n",
            "step: 180, loss: 0.2761742174625397\n",
            "step: 190, loss: 0.535416305065155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.3921568627450981, f1=0.4102564102564103, best_f1=0.4102564102564103\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.47018519043922424\n",
            "step: 10, loss: 0.20225386321544647\n",
            "step: 20, loss: 0.23725420236587524\n",
            "step: 30, loss: 0.17884117364883423\n",
            "step: 40, loss: 0.473937451839447\n",
            "step: 50, loss: 0.3133050501346588\n",
            "step: 60, loss: 0.14965729415416718\n",
            "step: 70, loss: 0.20675021409988403\n",
            "step: 80, loss: 0.2268783450126648\n",
            "step: 90, loss: 0.295151948928833\n",
            "step: 100, loss: 0.45167121291160583\n",
            "step: 110, loss: 0.26139596104621887\n",
            "step: 120, loss: 0.20947431027889252\n",
            "step: 130, loss: 0.27456480264663696\n",
            "step: 140, loss: 0.4655981957912445\n",
            "step: 150, loss: 0.16220763325691223\n",
            "step: 160, loss: 0.2859939932823181\n",
            "step: 170, loss: 0.6063927412033081\n",
            "step: 180, loss: 0.15568870306015015\n",
            "step: 190, loss: 0.3010399043560028\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.4219066937119676, f1=0.4426877470355731, best_f1=0.4426877470355731\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 235.83it/s]\n",
            "load_f1 = 0.4227353463587921\n",
            "real_f1 = 0.41295546558704455\n",
            "733it [00:00, 3482.08it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 201.49it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWkqC6MWgNFO"
      },
      "source": [
        "## BASELINE TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPR9KRSgNFO"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62jt5GiEgNFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140d2783-e5ab-4824-e38f-cae25a2ece95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rDownloading:   0% 0.00/481 [00:00<?, ?B/s]\rDownloading: 100% 481/481 [00:00<00:00, 554kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 31.0MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 26.5MB/s]\n",
            "Downloading: 100% 501M/501M [00:08<00:00, 62.3MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4930794835090637\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5465870499610901\n",
            "step: 20, loss: 0.3041968047618866\n",
            "step: 30, loss: 0.4271520972251892\n",
            "step: 40, loss: 0.5424747467041016\n",
            "step: 50, loss: 0.31899556517601013\n",
            "step: 60, loss: 0.6307167410850525\n",
            "step: 70, loss: 0.352102667093277\n",
            "step: 80, loss: 0.2284676432609558\n",
            "step: 90, loss: 0.20688465237617493\n",
            "step: 100, loss: 0.16819076240062714\n",
            "step: 110, loss: 0.3658114969730377\n",
            "step: 120, loss: 0.3153010308742523\n",
            "step: 130, loss: 0.32658815383911133\n",
            "step: 140, loss: 0.3751397728919983\n",
            "step: 150, loss: 0.3064262270927429\n",
            "step: 160, loss: 0.4082469344139099\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 0.3123278319835663\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3319411873817444\n",
            "step: 10, loss: 0.47482603788375854\n",
            "step: 20, loss: 0.31142446398735046\n",
            "step: 30, loss: 0.3156558573246002\n",
            "step: 40, loss: 0.09895572066307068\n",
            "step: 50, loss: 0.5050655603408813\n",
            "step: 60, loss: 0.17862574756145477\n",
            "step: 70, loss: 0.5155009031295776\n",
            "step: 80, loss: 0.24069133400917053\n",
            "step: 90, loss: 0.25847774744033813\n",
            "step: 100, loss: 0.5189167857170105\n",
            "step: 110, loss: 0.2563764452934265\n",
            "step: 120, loss: 0.25162872672080994\n",
            "step: 130, loss: 0.5307506322860718\n",
            "step: 140, loss: 0.5694282650947571\n",
            "step: 150, loss: 0.44899675250053406\n",
            "step: 160, loss: 0.4388158619403839\n",
            "step: 170, loss: 0.39378970861434937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.19433962264150942, f1=0.19461502125649505, best_f1=0.19461502125649505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6319168210029602\n",
            "step: 10, loss: 0.34246060252189636\n",
            "step: 20, loss: 0.2429599165916443\n",
            "step: 30, loss: 0.24198800325393677\n",
            "step: 40, loss: 0.3670227825641632\n",
            "step: 50, loss: 0.5718279480934143\n",
            "step: 60, loss: 0.2973783314228058\n",
            "step: 70, loss: 0.25373414158821106\n",
            "step: 80, loss: 0.38310277462005615\n",
            "step: 90, loss: 0.5167660713195801\n",
            "step: 100, loss: 0.2784825563430786\n",
            "step: 110, loss: 0.18638139963150024\n",
            "step: 120, loss: 0.6058558225631714\n",
            "step: 130, loss: 0.5163580775260925\n",
            "step: 140, loss: 0.4552271366119385\n",
            "step: 150, loss: 0.19479845464229584\n",
            "step: 160, loss: 0.16712789237499237\n",
            "step: 170, loss: 0.30379754304885864\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.19424799622819425, f1=0.19250829777145567, best_f1=0.19461502125649505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37987256050109863\n",
            "step: 10, loss: 0.5211026072502136\n",
            "step: 20, loss: 0.2519034445285797\n",
            "step: 30, loss: 0.4216921627521515\n",
            "step: 40, loss: 0.24325767159461975\n",
            "step: 50, loss: 0.35498490929603577\n",
            "step: 60, loss: 0.700549840927124\n",
            "step: 70, loss: 0.325626015663147\n",
            "step: 80, loss: 0.5036882162094116\n",
            "step: 90, loss: 0.2902050018310547\n",
            "step: 100, loss: 0.3592243492603302\n",
            "step: 110, loss: 0.42905256152153015\n",
            "step: 120, loss: 0.46665289998054504\n",
            "step: 130, loss: 0.3294864892959595\n",
            "step: 140, loss: 0.2564072906970978\n",
            "step: 150, loss: 0.7139933705329895\n",
            "step: 160, loss: 0.13061437010765076\n",
            "step: 170, loss: 0.2395503968000412\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.24426350851221318, f1=0.23906249999999996, best_f1=0.23906249999999996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.39351686835289\n",
            "step: 10, loss: 0.2913723289966583\n",
            "step: 20, loss: 0.3214244246482849\n",
            "step: 30, loss: 0.33500733971595764\n",
            "step: 40, loss: 0.21550512313842773\n",
            "step: 50, loss: 0.24982313811779022\n",
            "step: 60, loss: 0.30974894762039185\n",
            "step: 70, loss: 0.3435785472393036\n",
            "step: 80, loss: 0.08852926641702652\n",
            "step: 90, loss: 0.5658125877380371\n",
            "step: 100, loss: 0.24250949919223785\n",
            "step: 110, loss: 0.25329259037971497\n",
            "step: 120, loss: 0.11884021013975143\n",
            "step: 130, loss: 0.2635938227176666\n",
            "step: 140, loss: 0.18290597200393677\n",
            "step: 150, loss: 0.30469411611557007\n",
            "step: 160, loss: 0.25286728143692017\n",
            "step: 170, loss: 0.35961994528770447\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.2816568047337278, f1=0.23884197828709286, best_f1=0.23884197828709286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2856124937534332\n",
            "step: 10, loss: 0.12590299546718597\n",
            "step: 20, loss: 0.31668737530708313\n",
            "step: 30, loss: 0.22964198887348175\n",
            "step: 40, loss: 0.4283159673213959\n",
            "step: 50, loss: 0.23989450931549072\n",
            "step: 60, loss: 0.3543631434440613\n",
            "step: 70, loss: 0.3954121172428131\n",
            "step: 80, loss: 0.1571207195520401\n",
            "step: 90, loss: 0.32212337851524353\n",
            "step: 100, loss: 0.15949013829231262\n",
            "step: 110, loss: 0.4169400632381439\n",
            "step: 120, loss: 0.41251179575920105\n",
            "step: 130, loss: 0.5410447120666504\n",
            "step: 140, loss: 0.26122069358825684\n",
            "step: 150, loss: 0.18122152984142303\n",
            "step: 160, loss: 0.34028202295303345\n",
            "step: 170, loss: 0.2302328646183014\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.308300395256917, f1=0.26631853785900783, best_f1=0.26631853785900783\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23433807492256165\n",
            "step: 10, loss: 0.41663697361946106\n",
            "step: 20, loss: 0.435627281665802\n",
            "step: 30, loss: 0.41186732053756714\n",
            "step: 40, loss: 0.10222917795181274\n",
            "step: 50, loss: 0.34946802258491516\n",
            "step: 60, loss: 0.4621157944202423\n",
            "step: 70, loss: 0.30669522285461426\n",
            "step: 80, loss: 0.12672217190265656\n",
            "step: 90, loss: 0.2999856472015381\n",
            "step: 100, loss: 0.17915621399879456\n",
            "step: 110, loss: 0.13333512842655182\n",
            "step: 120, loss: 0.21083180606365204\n",
            "step: 130, loss: 0.16518352925777435\n",
            "step: 140, loss: 0.1425037533044815\n",
            "step: 150, loss: 0.1506483256816864\n",
            "step: 160, loss: 0.047401271760463715\n",
            "step: 170, loss: 0.1296653002500534\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.6896551724137931, f1=0.6511627906976744, best_f1=0.6511627906976744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2703539729118347\n",
            "step: 10, loss: 0.24977515637874603\n",
            "step: 20, loss: 0.13434648513793945\n",
            "step: 30, loss: 0.0673731192946434\n",
            "step: 40, loss: 0.10963866114616394\n",
            "step: 50, loss: 0.1433248519897461\n",
            "step: 60, loss: 0.1522447168827057\n",
            "step: 70, loss: 0.16081586480140686\n",
            "step: 80, loss: 0.23827935755252838\n",
            "step: 90, loss: 0.10902538150548935\n",
            "step: 100, loss: 0.052458543330430984\n",
            "step: 110, loss: 0.18775109946727753\n",
            "step: 120, loss: 0.37640058994293213\n",
            "step: 130, loss: 0.08108887821435928\n",
            "step: 140, loss: 0.3503463864326477\n",
            "step: 150, loss: 0.07463674247264862\n",
            "step: 160, loss: 0.08736175298690796\n",
            "step: 170, loss: 0.2031695544719696\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7786666666666666, f1=0.7425474254742549, best_f1=0.7425474254742549\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15128092467784882\n",
            "step: 10, loss: 0.07734446227550507\n",
            "step: 20, loss: 0.045489806681871414\n",
            "step: 30, loss: 0.19894611835479736\n",
            "step: 40, loss: 0.22962303459644318\n",
            "step: 50, loss: 0.03088223561644554\n",
            "step: 60, loss: 0.12010589987039566\n",
            "step: 70, loss: 0.18883028626441956\n",
            "step: 80, loss: 0.13592883944511414\n",
            "step: 90, loss: 0.061280373483896255\n",
            "step: 100, loss: 0.11136827617883682\n",
            "step: 110, loss: 0.2057483047246933\n",
            "step: 120, loss: 0.0359930656850338\n",
            "step: 130, loss: 0.2993497848510742\n",
            "step: 140, loss: 0.043928276747465134\n",
            "step: 150, loss: 0.4727402329444885\n",
            "step: 160, loss: 0.06743235141038895\n",
            "step: 170, loss: 0.3211849331855774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.7727272727272727, f1=0.736842105263158, best_f1=0.7425474254742549\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07360083609819412\n",
            "step: 10, loss: 0.07207586616277695\n",
            "step: 20, loss: 0.1308695375919342\n",
            "step: 30, loss: 0.14975605905056\n",
            "step: 40, loss: 0.08203106373548508\n",
            "step: 50, loss: 0.12131412327289581\n",
            "step: 60, loss: 0.20789124071598053\n",
            "step: 70, loss: 0.12413854897022247\n",
            "step: 80, loss: 0.06138812005519867\n",
            "step: 90, loss: 0.03019580990076065\n",
            "step: 100, loss: 0.026760483160614967\n",
            "step: 110, loss: 0.09876273572444916\n",
            "step: 120, loss: 0.06803948432207108\n",
            "step: 130, loss: 0.046568140387535095\n",
            "step: 140, loss: 0.029278839007019997\n",
            "step: 150, loss: 0.14114823937416077\n",
            "step: 160, loss: 0.31442442536354065\n",
            "step: 170, loss: 0.2255336344242096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7760416666666666, f1=0.760705289672544, best_f1=0.7425474254742549\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3052861988544464\n",
            "step: 10, loss: 0.049647919833660126\n",
            "step: 20, loss: 0.02255602926015854\n",
            "step: 30, loss: 0.03600144013762474\n",
            "step: 40, loss: 0.013207349926233292\n",
            "step: 50, loss: 0.05005945637822151\n",
            "step: 60, loss: 0.09876657277345657\n",
            "step: 70, loss: 0.017809752374887466\n",
            "step: 80, loss: 0.012773816473782063\n",
            "step: 90, loss: 0.20574939250946045\n",
            "step: 100, loss: 0.2661709189414978\n",
            "step: 110, loss: 0.02267805114388466\n",
            "step: 120, loss: 0.031160712242126465\n",
            "step: 130, loss: 0.0786866545677185\n",
            "step: 140, loss: 0.346443235874176\n",
            "step: 150, loss: 0.18953852355480194\n",
            "step: 160, loss: 0.11803745478391647\n",
            "step: 170, loss: 0.18734219670295715\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.7893462469733656, f1=0.7572815533980582, best_f1=0.7572815533980582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012392907403409481\n",
            "step: 10, loss: 0.014905543997883797\n",
            "step: 20, loss: 0.09237778186798096\n",
            "step: 30, loss: 0.1774597316980362\n",
            "step: 40, loss: 0.013741087168455124\n",
            "step: 50, loss: 0.017128774896264076\n",
            "step: 60, loss: 0.053332023322582245\n",
            "step: 70, loss: 0.13718892633914948\n",
            "step: 80, loss: 0.006649924907833338\n",
            "step: 90, loss: 0.13120712339878082\n",
            "step: 100, loss: 0.0150469820946455\n",
            "step: 110, loss: 0.06590566784143448\n",
            "step: 120, loss: 0.054296255111694336\n",
            "step: 130, loss: 0.2257746458053589\n",
            "step: 140, loss: 0.005527657922357321\n",
            "step: 150, loss: 0.010586312972009182\n",
            "step: 160, loss: 0.05860096588730812\n",
            "step: 170, loss: 0.31685930490493774\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7721179624664879, f1=0.7688311688311689, best_f1=0.7572815533980582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009798492304980755\n",
            "step: 10, loss: 0.01641228422522545\n",
            "step: 20, loss: 0.00556891830638051\n",
            "step: 30, loss: 0.2162536382675171\n",
            "step: 40, loss: 0.11700519919395447\n",
            "step: 50, loss: 0.03151160106062889\n",
            "step: 60, loss: 0.009325645864009857\n",
            "step: 70, loss: 0.14101450145244598\n",
            "step: 80, loss: 0.011283793486654758\n",
            "step: 90, loss: 0.02780883200466633\n",
            "step: 100, loss: 0.07106882333755493\n",
            "step: 110, loss: 0.0073404377326369286\n",
            "step: 120, loss: 0.006897909566760063\n",
            "step: 130, loss: 0.013493605889379978\n",
            "step: 140, loss: 0.2808806002140045\n",
            "step: 150, loss: 0.03608294576406479\n",
            "step: 160, loss: 0.03038705699145794\n",
            "step: 170, loss: 0.01168256439268589\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7676767676767677, f1=0.768472906403941, best_f1=0.7572815533980582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03248007595539093\n",
            "step: 10, loss: 0.02727149985730648\n",
            "step: 20, loss: 0.10026582330465317\n",
            "step: 30, loss: 0.028816141188144684\n",
            "step: 40, loss: 0.008013967424631119\n",
            "step: 50, loss: 0.003460143692791462\n",
            "step: 60, loss: 0.03814990818500519\n",
            "step: 70, loss: 0.019264930859208107\n",
            "step: 80, loss: 0.02040315978229046\n",
            "step: 90, loss: 0.0059101940132677555\n",
            "step: 100, loss: 0.014973842538893223\n",
            "step: 110, loss: 0.0621350072324276\n",
            "step: 120, loss: 0.005954614840447903\n",
            "step: 130, loss: 0.12993551790714264\n",
            "step: 140, loss: 0.027099302038550377\n",
            "step: 150, loss: 0.01613781601190567\n",
            "step: 160, loss: 0.035434942692518234\n",
            "step: 170, loss: 0.08921388536691666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7769784172661871, f1=0.7488372093023257, best_f1=0.7572815533980582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004207303281873465\n",
            "step: 10, loss: 0.009268997237086296\n",
            "step: 20, loss: 0.1984678953886032\n",
            "step: 30, loss: 0.04562685266137123\n",
            "step: 40, loss: 0.005938574206084013\n",
            "step: 50, loss: 0.004005913157016039\n",
            "step: 60, loss: 0.01380037609487772\n",
            "step: 70, loss: 0.03098808228969574\n",
            "step: 80, loss: 0.003453896613791585\n",
            "step: 90, loss: 0.005679788067936897\n",
            "step: 100, loss: 0.055714838206768036\n",
            "step: 110, loss: 0.004907675553113222\n",
            "step: 120, loss: 0.0062421453185379505\n",
            "step: 130, loss: 0.13540972769260406\n",
            "step: 140, loss: 0.005134521052241325\n",
            "step: 150, loss: 0.009020286612212658\n",
            "step: 160, loss: 0.18718867003917694\n",
            "step: 170, loss: 0.005759597290307283\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7745358090185676, f1=0.7751937984496124, best_f1=0.7572815533980582\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 297.15it/s]\n",
            "load_f1 = 0.7924528301886793\n",
            "real_f1 = 0.7832167832167831\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 203.12it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djX3yHRNgNFP"
      },
      "source": [
        "## BASELINE DIRTY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b011EMgogNFP"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_ai4a3YgNFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "993a6431-f484-4f45-c129-667294d99314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.592879056930542\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.43627461791038513\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.46455642580986023\n",
            "step: 30, loss: 0.3698261082172394\n",
            "step: 40, loss: 0.3175038695335388\n",
            "step: 50, loss: 0.574275016784668\n",
            "step: 60, loss: 0.45768845081329346\n",
            "step: 70, loss: 0.4116548001766205\n",
            "step: 80, loss: 0.5340365171432495\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 90, loss: 0.43537387251853943\n",
            "step: 100, loss: 0.49701282382011414\n",
            "step: 110, loss: 0.4310148358345032\n",
            "step: 120, loss: 0.1703328639268875\n",
            "step: 130, loss: 0.2850131690502167\n",
            "step: 140, loss: 0.21710236370563507\n",
            "step: 150, loss: 0.1990084946155548\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "step: 160, loss: 0.3041011691093445\n",
            "step: 170, loss: 0.16995786130428314\n",
            "step: 180, loss: 0.30775558948516846\n",
            "step: 190, loss: 0.06788313388824463\n",
            "step: 200, loss: 0.16111406683921814\n",
            "step: 210, loss: 0.12177230417728424\n",
            "step: 220, loss: 0.1260145753622055\n",
            "step: 230, loss: 0.102922722697258\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9125799573560768, f1=0.9274725274725274, best_f1=0.9274725274725274\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1209663674235344\n",
            "step: 10, loss: 0.19697800278663635\n",
            "step: 20, loss: 0.09306845813989639\n",
            "step: 30, loss: 0.10085217654705048\n",
            "step: 40, loss: 0.06485087424516678\n",
            "step: 50, loss: 0.05930695682764053\n",
            "step: 60, loss: 0.056187793612480164\n",
            "step: 70, loss: 0.13115271925926208\n",
            "step: 80, loss: 0.02051679417490959\n",
            "step: 90, loss: 0.03333275765180588\n",
            "step: 100, loss: 0.03773847222328186\n",
            "step: 110, loss: 0.08026367425918579\n",
            "step: 120, loss: 0.10728968679904938\n",
            "step: 130, loss: 0.09455772489309311\n",
            "step: 140, loss: 0.018438806757330894\n",
            "step: 150, loss: 0.18952874839305878\n",
            "step: 160, loss: 0.042221080511808395\n",
            "step: 170, loss: 0.004737692419439554\n",
            "step: 180, loss: 0.054761599749326706\n",
            "step: 190, loss: 0.02310527302324772\n",
            "step: 200, loss: 0.035351336002349854\n",
            "step: 210, loss: 0.08144903182983398\n",
            "step: 220, loss: 0.14206069707870483\n",
            "step: 230, loss: 0.00691111758351326\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.95, f1=0.9407821229050279, best_f1=0.9407821229050279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02266438864171505\n",
            "step: 10, loss: 0.07307031005620956\n",
            "step: 20, loss: 0.05790058895945549\n",
            "step: 30, loss: 0.03446324169635773\n",
            "step: 40, loss: 0.015951650217175484\n",
            "step: 50, loss: 0.05201761797070503\n",
            "step: 60, loss: 0.12981246411800385\n",
            "step: 70, loss: 0.028353720903396606\n",
            "step: 80, loss: 0.10223980247974396\n",
            "step: 90, loss: 0.06746754050254822\n",
            "step: 100, loss: 0.04142234846949577\n",
            "step: 110, loss: 0.02625700831413269\n",
            "step: 120, loss: 0.009157312102615833\n",
            "step: 130, loss: 0.033780645579099655\n",
            "step: 140, loss: 0.019273225218057632\n",
            "step: 150, loss: 0.19033308327198029\n",
            "step: 160, loss: 0.055258966982364655\n",
            "step: 170, loss: 0.008825724944472313\n",
            "step: 180, loss: 0.03050648421049118\n",
            "step: 190, loss: 0.05450309440493584\n",
            "step: 200, loss: 0.04630909487605095\n",
            "step: 210, loss: 0.003191757947206497\n",
            "step: 220, loss: 0.06538532674312592\n",
            "step: 230, loss: 0.11682838946580887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9488888888888889, f1=0.938496583143508, best_f1=0.9407821229050279\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07896661758422852\n",
            "step: 10, loss: 0.05116098001599312\n",
            "step: 20, loss: 0.007612364832311869\n",
            "step: 30, loss: 0.0011766301468014717\n",
            "step: 40, loss: 0.13804858922958374\n",
            "step: 50, loss: 0.06657427549362183\n",
            "step: 60, loss: 0.015766890719532967\n",
            "step: 70, loss: 0.06228207051753998\n",
            "step: 80, loss: 0.049534015357494354\n",
            "step: 90, loss: 0.04458199813961983\n",
            "step: 100, loss: 0.03924540430307388\n",
            "step: 110, loss: 0.005717406515032053\n",
            "step: 120, loss: 0.03487911820411682\n",
            "step: 130, loss: 0.10352380573749542\n",
            "step: 140, loss: 0.0264634657651186\n",
            "step: 150, loss: 0.009272013790905476\n",
            "step: 160, loss: 0.062294457107782364\n",
            "step: 170, loss: 0.013686558231711388\n",
            "step: 180, loss: 0.03356611728668213\n",
            "step: 190, loss: 0.07415856420993805\n",
            "step: 200, loss: 0.13344907760620117\n",
            "step: 210, loss: 0.015729617327451706\n",
            "step: 220, loss: 0.002083013765513897\n",
            "step: 230, loss: 0.011536172591149807\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9598214285714285, f1=0.9510807736063709, best_f1=0.9510807736063709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007107822224497795\n",
            "step: 10, loss: 0.024370471015572548\n",
            "step: 20, loss: 0.1540871113538742\n",
            "step: 30, loss: 0.03129488602280617\n",
            "step: 40, loss: 0.02001381665468216\n",
            "step: 50, loss: 0.06941690295934677\n",
            "step: 60, loss: 0.10741329193115234\n",
            "step: 70, loss: 0.008115621283650398\n",
            "step: 80, loss: 0.06933657824993134\n",
            "step: 90, loss: 0.10342833399772644\n",
            "step: 100, loss: 0.0027654957957565784\n",
            "step: 110, loss: 0.001671009580604732\n",
            "step: 120, loss: 0.015259466134011745\n",
            "step: 130, loss: 0.023311372846364975\n",
            "step: 140, loss: 0.055546533316373825\n",
            "step: 150, loss: 0.05026515573263168\n",
            "step: 160, loss: 0.005679121240973473\n",
            "step: 170, loss: 0.03602629527449608\n",
            "step: 180, loss: 0.0061403848230838776\n",
            "step: 190, loss: 0.12768006324768066\n",
            "step: 200, loss: 0.1758567839860916\n",
            "step: 210, loss: 0.019546281546354294\n",
            "step: 220, loss: 0.009207235649228096\n",
            "step: 230, loss: 0.011159537360072136\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9576837416481068, f1=0.9525959367945823, best_f1=0.9510807736063709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005220579914748669\n",
            "step: 10, loss: 0.008585810661315918\n",
            "step: 20, loss: 0.0031307123135775328\n",
            "step: 30, loss: 0.0031836931593716145\n",
            "step: 40, loss: 0.0008899637614376843\n",
            "step: 50, loss: 0.00478532537817955\n",
            "step: 60, loss: 0.04132074490189552\n",
            "step: 70, loss: 0.15395373106002808\n",
            "step: 80, loss: 0.048360615968704224\n",
            "step: 90, loss: 0.021066386252641678\n",
            "step: 100, loss: 0.024335280060768127\n",
            "step: 110, loss: 0.08961564302444458\n",
            "step: 120, loss: 0.003528594272211194\n",
            "step: 130, loss: 0.0035861143842339516\n",
            "step: 140, loss: 0.002481528790667653\n",
            "step: 150, loss: 0.0004225968150421977\n",
            "step: 160, loss: 0.007493996061384678\n",
            "step: 170, loss: 0.003993799909949303\n",
            "step: 180, loss: 0.007574041374027729\n",
            "step: 190, loss: 0.00621317932382226\n",
            "step: 200, loss: 0.030809618532657623\n",
            "step: 210, loss: 0.0030331166926771402\n",
            "step: 220, loss: 0.007691949140280485\n",
            "step: 230, loss: 0.0032664660830050707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9568106312292359, f1=0.9519553072625698, best_f1=0.9510807736063709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009888955391943455\n",
            "step: 10, loss: 0.0024734437465667725\n",
            "step: 20, loss: 0.0010525773977860808\n",
            "step: 30, loss: 0.002718496834859252\n",
            "step: 40, loss: 0.14021803438663483\n",
            "step: 50, loss: 0.06724955141544342\n",
            "step: 60, loss: 0.0033910961356014013\n",
            "step: 70, loss: 0.022597646340727806\n",
            "step: 80, loss: 0.0018120544264093041\n",
            "step: 90, loss: 0.00359835266135633\n",
            "step: 100, loss: 0.0009404721786268055\n",
            "step: 110, loss: 0.0007173437625169754\n",
            "step: 120, loss: 0.006921835243701935\n",
            "step: 130, loss: 0.003625878831371665\n",
            "step: 140, loss: 0.0007290838402695954\n",
            "step: 150, loss: 0.20092187821865082\n",
            "step: 160, loss: 0.0211599450558424\n",
            "step: 170, loss: 0.043090879917144775\n",
            "step: 180, loss: 0.0028447939548641443\n",
            "step: 190, loss: 0.13382959365844727\n",
            "step: 200, loss: 0.008903201669454575\n",
            "step: 210, loss: 0.023617422208189964\n",
            "step: 220, loss: 0.004329224582761526\n",
            "step: 230, loss: 0.002877853810787201\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9621380846325166, f1=0.9430523917995445, best_f1=0.9430523917995445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007579687051475048\n",
            "step: 10, loss: 0.00327614089474082\n",
            "step: 20, loss: 0.005511037539690733\n",
            "step: 30, loss: 0.0018369307508692145\n",
            "step: 40, loss: 0.0028001198079437017\n",
            "step: 50, loss: 0.004617180209606886\n",
            "step: 60, loss: 0.004439632408320904\n",
            "step: 70, loss: 0.02394748106598854\n",
            "step: 80, loss: 0.10449492931365967\n",
            "step: 90, loss: 0.0019712685607373714\n",
            "step: 100, loss: 0.0009673578315414488\n",
            "step: 110, loss: 0.02023221366107464\n",
            "step: 120, loss: 0.0006797181558795273\n",
            "step: 130, loss: 0.00959775224328041\n",
            "step: 140, loss: 0.00036682962672784925\n",
            "step: 150, loss: 0.23352056741714478\n",
            "step: 160, loss: 0.00053685053717345\n",
            "step: 170, loss: 0.07705270498991013\n",
            "step: 180, loss: 0.0027310410514473915\n",
            "step: 190, loss: 0.00797877088189125\n",
            "step: 200, loss: 0.019062276929616928\n",
            "step: 210, loss: 0.004328195936977863\n",
            "step: 220, loss: 0.0010746862972155213\n",
            "step: 230, loss: 0.008528148755431175\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.96045197740113, f1=0.9437428243398392, best_f1=0.9430523917995445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0036543449386954308\n",
            "step: 10, loss: 0.0007327469647862017\n",
            "step: 20, loss: 0.0009375429362989962\n",
            "step: 30, loss: 0.0009473390528000891\n",
            "step: 40, loss: 0.01049860566854477\n",
            "step: 50, loss: 0.00085030886111781\n",
            "step: 60, loss: 0.000950898218434304\n",
            "step: 70, loss: 0.0010256882524117827\n",
            "step: 80, loss: 0.0006102052866481245\n",
            "step: 90, loss: 0.05429573357105255\n",
            "step: 100, loss: 0.0011027767322957516\n",
            "step: 110, loss: 0.0006251267623156309\n",
            "step: 120, loss: 0.18196441233158112\n",
            "step: 130, loss: 0.026339493691921234\n",
            "step: 140, loss: 0.07349026948213577\n",
            "step: 150, loss: 0.0009740700479596853\n",
            "step: 160, loss: 0.01939528062939644\n",
            "step: 170, loss: 0.012964557856321335\n",
            "step: 180, loss: 0.0010116015328094363\n",
            "step: 190, loss: 0.000656951277051121\n",
            "step: 200, loss: 0.028186185285449028\n",
            "step: 210, loss: 0.016087565571069717\n",
            "step: 220, loss: 0.0007268710760399699\n",
            "step: 230, loss: 0.0005731579149141908\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9610678531701891, f1=0.9457013574660633, best_f1=0.9430523917995445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008265148499049246\n",
            "step: 10, loss: 0.0010686382884159684\n",
            "step: 20, loss: 0.0011447574943304062\n",
            "step: 30, loss: 0.0004689322377089411\n",
            "step: 40, loss: 0.0010437901364639401\n",
            "step: 50, loss: 0.0009616222814656794\n",
            "step: 60, loss: 0.0007416074513457716\n",
            "step: 70, loss: 0.0016604912234470248\n",
            "step: 80, loss: 0.0005892579210922122\n",
            "step: 90, loss: 0.0006361012347042561\n",
            "step: 100, loss: 0.000915797776542604\n",
            "step: 110, loss: 0.0012216351460665464\n",
            "step: 120, loss: 0.0010330486111342907\n",
            "step: 130, loss: 0.0032775234431028366\n",
            "step: 140, loss: 0.0006048688082955778\n",
            "step: 150, loss: 0.001050539780408144\n",
            "step: 160, loss: 0.0004012049757875502\n",
            "step: 170, loss: 0.0006933324038982391\n",
            "step: 180, loss: 0.001537222764454782\n",
            "step: 190, loss: 0.004176237620413303\n",
            "step: 200, loss: 0.003389562014490366\n",
            "step: 210, loss: 0.035964637994766235\n",
            "step: 220, loss: 0.020511051639914513\n",
            "step: 230, loss: 0.0006265530246309936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9655937846836848, f1=0.9492671927846673, best_f1=0.9492671927846673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026884363032877445\n",
            "step: 10, loss: 0.0017358913319185376\n",
            "step: 20, loss: 0.001445063971914351\n",
            "step: 30, loss: 0.008525797165930271\n",
            "step: 40, loss: 0.0004979819641448557\n",
            "step: 50, loss: 0.0013993083266541362\n",
            "step: 60, loss: 0.02220565639436245\n",
            "step: 70, loss: 0.0006760578835383058\n",
            "step: 80, loss: 0.0006706491112709045\n",
            "step: 90, loss: 0.19165180623531342\n",
            "step: 100, loss: 0.0008439146913588047\n",
            "step: 110, loss: 0.0006271966267377138\n",
            "step: 120, loss: 0.0006771594053134322\n",
            "step: 130, loss: 0.0006569123943336308\n",
            "step: 140, loss: 0.003684178227558732\n",
            "step: 150, loss: 0.000960711797233671\n",
            "step: 160, loss: 0.015299015678465366\n",
            "step: 170, loss: 0.0018104196060448885\n",
            "step: 180, loss: 0.001594672561623156\n",
            "step: 190, loss: 0.0006452499073930085\n",
            "step: 200, loss: 0.02059878595173359\n",
            "step: 210, loss: 0.0009735642815940082\n",
            "step: 220, loss: 0.0009002229198813438\n",
            "step: 230, loss: 0.0007258497644215822\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9627959413754228, f1=0.9507445589919816, best_f1=0.9492671927846673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015131074003875256\n",
            "step: 10, loss: 0.0006001396104693413\n",
            "step: 20, loss: 0.001058118650689721\n",
            "step: 30, loss: 0.2841537892818451\n",
            "step: 40, loss: 0.0008489011088386178\n",
            "step: 50, loss: 0.0008632770041003823\n",
            "step: 60, loss: 0.03846285864710808\n",
            "step: 70, loss: 0.0008229077211581171\n",
            "step: 80, loss: 0.00039693573489785194\n",
            "step: 90, loss: 0.0007572612375952303\n",
            "step: 100, loss: 0.0004951974260620773\n",
            "step: 110, loss: 0.00041254505049437284\n",
            "step: 120, loss: 0.009708481840789318\n",
            "step: 130, loss: 0.0006487150676548481\n",
            "step: 140, loss: 0.0007136339554563165\n",
            "step: 150, loss: 0.0007054531597532332\n",
            "step: 160, loss: 0.0005539966514334083\n",
            "step: 170, loss: 0.002191950101405382\n",
            "step: 180, loss: 0.00041237223194912076\n",
            "step: 190, loss: 0.0007265431340783834\n",
            "step: 200, loss: 0.00046207275590859354\n",
            "step: 210, loss: 0.13261090219020844\n",
            "step: 220, loss: 0.003418172476813197\n",
            "step: 230, loss: 0.000801107962615788\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9640449438202248, f1=0.9532497149372862, best_f1=0.9492671927846673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007656426634639502\n",
            "step: 10, loss: 0.009713065810501575\n",
            "step: 20, loss: 0.0011504535796120763\n",
            "step: 30, loss: 0.013928421773016453\n",
            "step: 40, loss: 0.0013388519873842597\n",
            "step: 50, loss: 0.007528219372034073\n",
            "step: 60, loss: 0.0006769471219740808\n",
            "step: 70, loss: 0.0004942219820804894\n",
            "step: 80, loss: 0.002066671149805188\n",
            "step: 90, loss: 0.0005243937484920025\n",
            "step: 100, loss: 0.0009398983092978597\n",
            "step: 110, loss: 0.0009396012756042182\n",
            "step: 120, loss: 0.0007625535945408046\n",
            "step: 130, loss: 0.000908120593521744\n",
            "step: 140, loss: 0.0006220921059139073\n",
            "step: 150, loss: 0.000362495455192402\n",
            "step: 160, loss: 0.003280120901763439\n",
            "step: 170, loss: 0.0009286498534493148\n",
            "step: 180, loss: 0.0698290541768074\n",
            "step: 190, loss: 0.0033097248524427414\n",
            "step: 200, loss: 0.00034365494502708316\n",
            "step: 210, loss: 0.0006833134102635086\n",
            "step: 220, loss: 0.0007661546696908772\n",
            "step: 230, loss: 0.04749785736203194\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.961625282167043, f1=0.944700460829493, best_f1=0.9492671927846673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005059733521193266\n",
            "step: 10, loss: 0.0004890129785053432\n",
            "step: 20, loss: 0.0035524037666618824\n",
            "step: 30, loss: 0.0009642249206081033\n",
            "step: 40, loss: 0.00509807700291276\n",
            "step: 50, loss: 0.00047045049723237753\n",
            "step: 60, loss: 0.0005756297032348812\n",
            "step: 70, loss: 0.00120927847456187\n",
            "step: 80, loss: 0.000669394270516932\n",
            "step: 90, loss: 0.0010335644474253058\n",
            "step: 100, loss: 0.0008057214436121285\n",
            "step: 110, loss: 0.001235164818353951\n",
            "step: 120, loss: 0.0003220764338038862\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "step: 130, loss: 0.030344510450959206\n",
            "step: 140, loss: 0.0017856867052614689\n",
            "step: 150, loss: 0.00038350350223481655\n",
            "step: 160, loss: 0.0008228135411627591\n",
            "step: 170, loss: 0.0007637323578819633\n",
            "step: 180, loss: 0.0009400478447787464\n",
            "step: 190, loss: 0.000540971290320158\n",
            "step: 200, loss: 0.000732544285710901\n",
            "step: 210, loss: 0.001866013859398663\n",
            "step: 220, loss: 0.0006592720164917409\n",
            "step: 230, loss: 0.0003657522320281714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9596412556053813, f1=0.9500000000000001, best_f1=0.9492671927846673\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03902708739042282\n",
            "step: 10, loss: 0.0006829540943726897\n",
            "step: 20, loss: 0.000647092645522207\n",
            "step: 30, loss: 0.0007759995060041547\n",
            "step: 40, loss: 0.0005279112956486642\n",
            "step: 50, loss: 0.0006300462991930544\n",
            "step: 60, loss: 0.00042169223888777196\n",
            "step: 70, loss: 0.002220324007794261\n",
            "step: 80, loss: 0.0006508767837658525\n",
            "step: 90, loss: 0.0005003067199140787\n",
            "step: 100, loss: 0.00043166460818611085\n",
            "step: 110, loss: 0.0007152455509640276\n",
            "step: 120, loss: 0.007300327531993389\n",
            "step: 130, loss: 0.0004408300737850368\n",
            "step: 140, loss: 0.0007146683637984097\n",
            "step: 150, loss: 0.0005443163099698722\n",
            "step: 160, loss: 0.0016515852184966207\n",
            "step: 170, loss: 0.0003940412134397775\n",
            "step: 180, loss: 0.0006647692061960697\n",
            "step: 190, loss: 0.0003189428534824401\n",
            "step: 200, loss: 0.0005429005832411349\n",
            "step: 210, loss: 0.0015174847794696689\n",
            "step: 220, loss: 0.000619755417574197\n",
            "step: 230, loss: 0.0005951581988483667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9596412556053813, f1=0.9501133786848073, best_f1=0.9492671927846673\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 204.87it/s]\n",
            "load_f1 = 0.9675977653631285\n",
            "real_f1 = 0.9634551495016611\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 204.84it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h62Yut_pgNFQ"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGEElkeagNFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2164db34-f6a9-4068-a2cb-85b0ac048fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6273968815803528\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.4277028441429138\n",
            "step: 20, loss: 0.30690792202949524\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.3865823745727539\n",
            "step: 40, loss: 0.38493210077285767\n",
            "step: 50, loss: 0.6546292901039124\n",
            "step: 60, loss: 0.3613128662109375\n",
            "step: 70, loss: 0.4594770669937134\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.5001224279403687\n",
            "step: 90, loss: 0.42506495118141174\n",
            "step: 100, loss: 0.6016262769699097\n",
            "step: 110, loss: 0.38430672883987427\n",
            "step: 120, loss: 0.5205445289611816\n",
            "step: 130, loss: 0.5045889019966125\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 140, loss: 0.5132622718811035\n",
            "step: 150, loss: 0.25808602571487427\n",
            "step: 160, loss: 0.38461485505104065\n",
            "step: 170, loss: 0.30832457542419434\n",
            "step: 180, loss: 0.3854077458381653\n",
            "step: 190, loss: 0.5565117001533508\n",
            "step: 200, loss: 0.21801310777664185\n",
            "step: 210, loss: 0.12024745345115662\n",
            "step: 220, loss: 0.07813156396150589\n",
            "step: 230, loss: 0.33988863229751587\n",
            "step: 240, loss: 0.08414514362812042\n",
            "step: 250, loss: 0.12494271248579025\n",
            "step: 260, loss: 0.8877171874046326\n",
            "step: 270, loss: 0.297230988740921\n",
            "step: 280, loss: 0.150853231549263\n",
            "step: 290, loss: 0.2601287066936493\n",
            "step: 300, loss: 0.17560748755931854\n",
            "step: 310, loss: 0.4456561207771301\n",
            "step: 320, loss: 0.17006517946720123\n",
            "step: 330, loss: 0.25612014532089233\n",
            "step: 340, loss: 0.3742695152759552\n",
            "step: 350, loss: 0.27153530716896057\n",
            "step: 360, loss: 0.03691401332616806\n",
            "step: 370, loss: 0.06872732937335968\n",
            "step: 380, loss: 0.17096033692359924\n",
            "step: 390, loss: 0.09864038974046707\n",
            "step: 400, loss: 0.0847342386841774\n",
            "step: 410, loss: 0.2514558434486389\n",
            "step: 420, loss: 0.05040498450398445\n",
            "step: 430, loss: 0.05722483992576599\n",
            "step: 440, loss: 0.17048269510269165\n",
            "step: 450, loss: 0.23815299570560455\n",
            "step: 460, loss: 0.14863677322864532\n",
            "step: 470, loss: 0.06937512010335922\n",
            "step: 480, loss: 0.2537558078765869\n",
            "step: 490, loss: 0.07214799523353577\n",
            "step: 500, loss: 0.1015365868806839\n",
            "step: 510, loss: 0.274165540933609\n",
            "step: 520, loss: 0.2625181972980499\n",
            "step: 530, loss: 0.0879729688167572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8794664125774178, f1=0.8690307328605201, best_f1=0.8690307328605201\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05890211835503578\n",
            "step: 10, loss: 0.16783717274665833\n",
            "step: 20, loss: 0.17242546379566193\n",
            "step: 30, loss: 0.11628584563732147\n",
            "step: 40, loss: 0.22164392471313477\n",
            "step: 50, loss: 0.15056288242340088\n",
            "step: 60, loss: 0.1482192724943161\n",
            "step: 70, loss: 0.05494583398103714\n",
            "step: 80, loss: 0.0599275566637516\n",
            "step: 90, loss: 0.022138679400086403\n",
            "step: 100, loss: 0.07204565405845642\n",
            "step: 110, loss: 0.14438611268997192\n",
            "step: 120, loss: 0.18032871186733246\n",
            "step: 130, loss: 0.022524511441588402\n",
            "step: 140, loss: 0.12828654050827026\n",
            "step: 150, loss: 0.10393574088811874\n",
            "step: 160, loss: 0.08751347661018372\n",
            "step: 170, loss: 0.11085531860589981\n",
            "step: 180, loss: 0.1551351696252823\n",
            "step: 190, loss: 0.07869629561901093\n",
            "step: 200, loss: 0.27983155846595764\n",
            "step: 210, loss: 0.06612598150968552\n",
            "step: 220, loss: 0.01234554685652256\n",
            "step: 230, loss: 0.10941710323095322\n",
            "step: 240, loss: 0.07040606439113617\n",
            "step: 250, loss: 0.05283575505018234\n",
            "step: 260, loss: 0.053673114627599716\n",
            "step: 270, loss: 0.046507351100444794\n",
            "step: 280, loss: 0.061810072511434555\n",
            "step: 290, loss: 0.07985055446624756\n",
            "step: 300, loss: 0.14194288849830627\n",
            "step: 310, loss: 0.07039877027273178\n",
            "step: 320, loss: 0.11663883179426193\n",
            "step: 330, loss: 0.06628741323947906\n",
            "step: 340, loss: 0.22425949573516846\n",
            "step: 350, loss: 0.006868560798466206\n",
            "step: 360, loss: 0.1184544786810875\n",
            "step: 370, loss: 0.1488162875175476\n",
            "step: 380, loss: 0.19384439289569855\n",
            "step: 390, loss: 0.024978918954730034\n",
            "step: 400, loss: 0.0971783772110939\n",
            "step: 410, loss: 0.09426165372133255\n",
            "step: 420, loss: 0.1604440063238144\n",
            "step: 430, loss: 0.045853909105062485\n",
            "step: 440, loss: 0.006516596302390099\n",
            "step: 450, loss: 0.05986351892352104\n",
            "step: 460, loss: 0.17512844502925873\n",
            "step: 470, loss: 0.1399141401052475\n",
            "step: 480, loss: 0.04063142463564873\n",
            "step: 490, loss: 0.2339000552892685\n",
            "step: 500, loss: 0.012390629388391972\n",
            "step: 510, loss: 0.0845717117190361\n",
            "step: 520, loss: 0.30892908573150635\n",
            "step: 530, loss: 0.13258977234363556\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9124886052871468, f1=0.8990406578346277, best_f1=0.8990406578346277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1417836993932724\n",
            "step: 10, loss: 0.19677746295928955\n",
            "step: 20, loss: 0.09766430407762527\n",
            "step: 30, loss: 0.14741921424865723\n",
            "step: 40, loss: 0.059805724769830704\n",
            "step: 50, loss: 0.10147634148597717\n",
            "step: 60, loss: 0.04385441914200783\n",
            "step: 70, loss: 0.05103740096092224\n",
            "step: 80, loss: 0.10563507676124573\n",
            "step: 90, loss: 0.05697248503565788\n",
            "step: 100, loss: 0.2005024403333664\n",
            "step: 110, loss: 0.07652401179075241\n",
            "step: 120, loss: 0.15590450167655945\n",
            "step: 130, loss: 0.06385517865419388\n",
            "step: 140, loss: 0.04057922214269638\n",
            "step: 150, loss: 0.09728733450174332\n",
            "step: 160, loss: 0.10428363084793091\n",
            "step: 170, loss: 0.014743112027645111\n",
            "step: 180, loss: 0.038581281900405884\n",
            "step: 190, loss: 0.06715650111436844\n",
            "step: 200, loss: 0.07715366035699844\n",
            "step: 210, loss: 0.13392551243305206\n",
            "step: 220, loss: 0.06868532299995422\n",
            "step: 230, loss: 0.05265286937355995\n",
            "step: 240, loss: 0.071482352912426\n",
            "step: 250, loss: 0.3547905683517456\n",
            "step: 260, loss: 0.0803355947136879\n",
            "step: 270, loss: 0.04282333701848984\n",
            "step: 280, loss: 0.11539648473262787\n",
            "step: 290, loss: 0.025491071864962578\n",
            "step: 300, loss: 0.1563318818807602\n",
            "step: 310, loss: 0.2751661241054535\n",
            "step: 320, loss: 0.030590562149882317\n",
            "step: 330, loss: 0.02115909568965435\n",
            "step: 340, loss: 0.029869215562939644\n",
            "step: 350, loss: 0.23751112818717957\n",
            "step: 360, loss: 0.12036118656396866\n",
            "step: 370, loss: 0.07746583968400955\n",
            "step: 380, loss: 0.021137477830052376\n",
            "step: 390, loss: 0.09883896261453629\n",
            "step: 400, loss: 0.13438774645328522\n",
            "step: 410, loss: 0.05071466416120529\n",
            "step: 420, loss: 0.18589460849761963\n",
            "step: 430, loss: 0.090882308781147\n",
            "step: 440, loss: 0.21688221395015717\n",
            "step: 450, loss: 0.18276487290859222\n",
            "step: 460, loss: 0.1966874748468399\n",
            "step: 470, loss: 0.0738791972398758\n",
            "step: 480, loss: 0.10392347723245621\n",
            "step: 490, loss: 0.0433230847120285\n",
            "step: 500, loss: 0.021681295707821846\n",
            "step: 510, loss: 0.1143827885389328\n",
            "step: 520, loss: 0.030864842236042023\n",
            "step: 530, loss: 0.18638849258422852\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9210770659238626, f1=0.9041608228143994, best_f1=0.9041608228143994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0343252494931221\n",
            "step: 10, loss: 0.025840122252702713\n",
            "step: 20, loss: 0.0926009938120842\n",
            "step: 30, loss: 0.12920445203781128\n",
            "step: 40, loss: 0.01645660772919655\n",
            "step: 50, loss: 0.03377489373087883\n",
            "step: 60, loss: 0.03887239471077919\n",
            "step: 70, loss: 0.08616064488887787\n",
            "step: 80, loss: 0.24517561495304108\n",
            "step: 90, loss: 0.027489297091960907\n",
            "step: 100, loss: 0.004995354451239109\n",
            "step: 110, loss: 0.33801698684692383\n",
            "step: 120, loss: 0.0711163729429245\n",
            "step: 130, loss: 0.14571383595466614\n",
            "step: 140, loss: 0.09497997164726257\n",
            "step: 150, loss: 0.015026104636490345\n",
            "step: 160, loss: 0.08121898025274277\n",
            "step: 170, loss: 0.027238773182034492\n",
            "step: 180, loss: 0.07027540355920792\n",
            "step: 190, loss: 0.059289876371622086\n",
            "step: 200, loss: 0.04671529680490494\n",
            "step: 210, loss: 0.00764441629871726\n",
            "step: 220, loss: 0.009744751267135143\n",
            "step: 230, loss: 0.05662692338228226\n",
            "step: 240, loss: 0.011057784780859947\n",
            "step: 250, loss: 0.12087734788656235\n",
            "step: 260, loss: 0.05101603642106056\n",
            "step: 270, loss: 0.21617665886878967\n",
            "step: 280, loss: 0.1129024475812912\n",
            "step: 290, loss: 0.0679287537932396\n",
            "step: 300, loss: 0.009224290028214455\n",
            "step: 310, loss: 0.01254588458687067\n",
            "step: 320, loss: 0.1596175730228424\n",
            "step: 330, loss: 0.012909499928355217\n",
            "step: 340, loss: 0.032188352197408676\n",
            "step: 350, loss: 0.2124362736940384\n",
            "step: 360, loss: 0.04170601814985275\n",
            "step: 370, loss: 0.0077374167740345\n",
            "step: 380, loss: 0.17935003340244293\n",
            "step: 390, loss: 0.003825494786724448\n",
            "step: 400, loss: 0.015005689114332199\n",
            "step: 410, loss: 0.007756764534860849\n",
            "step: 420, loss: 0.01787135750055313\n",
            "step: 430, loss: 0.0072056301869452\n",
            "step: 440, loss: 0.012457245029509068\n",
            "step: 450, loss: 0.09588806331157684\n",
            "step: 460, loss: 0.04726782813668251\n",
            "step: 470, loss: 0.006242056377232075\n",
            "step: 480, loss: 0.0818096324801445\n",
            "step: 490, loss: 0.03624546155333519\n",
            "step: 500, loss: 0.18084852397441864\n",
            "step: 510, loss: 0.1311032772064209\n",
            "step: 520, loss: 0.057056132704019547\n",
            "step: 530, loss: 0.08544886857271194\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9298085688240657, f1=0.9153952843273233, best_f1=0.9153952843273233\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006512920837849379\n",
            "step: 10, loss: 0.07485006749629974\n",
            "step: 20, loss: 0.012619705870747566\n",
            "step: 30, loss: 0.11247479915618896\n",
            "step: 40, loss: 0.003483563894405961\n",
            "step: 50, loss: 0.07042382657527924\n",
            "step: 60, loss: 0.08021953701972961\n",
            "step: 70, loss: 0.01907760091125965\n",
            "step: 80, loss: 0.055560141801834106\n",
            "step: 90, loss: 0.06403230875730515\n",
            "step: 100, loss: 0.09718898683786392\n",
            "step: 110, loss: 0.055009566247463226\n",
            "step: 120, loss: 0.13827846944332123\n",
            "step: 130, loss: 0.06085829436779022\n",
            "step: 140, loss: 0.03460550680756569\n",
            "step: 150, loss: 0.03582683950662613\n",
            "step: 160, loss: 0.021976064890623093\n",
            "step: 170, loss: 0.11760387569665909\n",
            "step: 180, loss: 0.005064898636192083\n",
            "step: 190, loss: 0.015185498632490635\n",
            "step: 200, loss: 0.017122406512498856\n",
            "step: 210, loss: 0.009869802743196487\n",
            "step: 220, loss: 0.06885411590337753\n",
            "step: 230, loss: 0.019375089555978775\n",
            "step: 240, loss: 0.07061906158924103\n",
            "step: 250, loss: 0.07806318998336792\n",
            "step: 260, loss: 0.001682321191765368\n",
            "step: 270, loss: 0.0026501694228500128\n",
            "step: 280, loss: 0.009077226743102074\n",
            "step: 290, loss: 0.008082564920186996\n",
            "step: 300, loss: 0.4514341950416565\n",
            "step: 310, loss: 0.06596847623586655\n",
            "step: 320, loss: 0.11772923171520233\n",
            "step: 330, loss: 0.015730755403637886\n",
            "step: 340, loss: 0.017604323104023933\n",
            "step: 350, loss: 0.003410415956750512\n",
            "step: 360, loss: 0.0018047827761620283\n",
            "step: 370, loss: 0.008966137655079365\n",
            "step: 380, loss: 0.0014107389142736793\n",
            "step: 390, loss: 0.21927565336227417\n",
            "step: 400, loss: 0.005263683386147022\n",
            "step: 410, loss: 0.03113688714802265\n",
            "step: 420, loss: 0.21255135536193848\n",
            "step: 430, loss: 0.16176865994930267\n",
            "step: 440, loss: 0.06806915253400803\n",
            "step: 450, loss: 0.10212826728820801\n",
            "step: 460, loss: 0.04823039099574089\n",
            "step: 470, loss: 0.015246845781803131\n",
            "step: 480, loss: 0.027549970895051956\n",
            "step: 490, loss: 0.05229967460036278\n",
            "step: 500, loss: 0.07505322247743607\n",
            "step: 510, loss: 0.029281537979841232\n",
            "step: 520, loss: 0.0400499664247036\n",
            "step: 530, loss: 0.011475740000605583\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9305108145421077, f1=0.9133271202236719, best_f1=0.9133271202236719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04072171449661255\n",
            "step: 10, loss: 0.002613755175843835\n",
            "step: 20, loss: 0.003651437582448125\n",
            "step: 30, loss: 0.031300704926252365\n",
            "step: 40, loss: 0.03108053095638752\n",
            "step: 50, loss: 0.00285562826320529\n",
            "step: 60, loss: 0.0662572905421257\n",
            "step: 70, loss: 0.005745571572333574\n",
            "step: 80, loss: 0.00980030931532383\n",
            "step: 90, loss: 0.16643521189689636\n",
            "step: 100, loss: 0.006790728308260441\n",
            "step: 110, loss: 0.027407104149460793\n",
            "step: 120, loss: 0.03628314658999443\n",
            "step: 130, loss: 0.012561457231640816\n",
            "step: 140, loss: 0.07167254388332367\n",
            "step: 150, loss: 0.006239232141524553\n",
            "step: 160, loss: 0.09554939717054367\n",
            "step: 170, loss: 0.01945933699607849\n",
            "step: 180, loss: 0.0032601128332316875\n",
            "step: 190, loss: 0.10369851440191269\n",
            "step: 200, loss: 0.014891011640429497\n",
            "step: 210, loss: 0.14321835339069366\n",
            "step: 220, loss: 0.02253834903240204\n",
            "step: 230, loss: 0.09591241925954819\n",
            "step: 240, loss: 0.11818729341030121\n",
            "step: 250, loss: 0.011669296771287918\n",
            "step: 260, loss: 0.03267413750290871\n",
            "step: 270, loss: 0.012273915112018585\n",
            "step: 280, loss: 0.05793905630707741\n",
            "step: 290, loss: 0.029719578102231026\n",
            "step: 300, loss: 0.03405291214585304\n",
            "step: 310, loss: 0.17845018208026886\n",
            "step: 320, loss: 0.03360734134912491\n",
            "step: 330, loss: 0.05453015863895416\n",
            "step: 340, loss: 0.002680526813492179\n",
            "step: 350, loss: 0.0036713522858917713\n",
            "step: 360, loss: 0.05757157504558563\n",
            "step: 370, loss: 0.004087959881871939\n",
            "step: 380, loss: 0.0157973300665617\n",
            "step: 390, loss: 0.025217000395059586\n",
            "step: 400, loss: 0.04335714504122734\n",
            "step: 410, loss: 0.0020084979478269815\n",
            "step: 420, loss: 0.008605523966252804\n",
            "step: 430, loss: 0.0017680602613836527\n",
            "step: 440, loss: 0.014025435782968998\n",
            "step: 450, loss: 0.36441943049430847\n",
            "step: 460, loss: 0.013627860695123672\n",
            "step: 470, loss: 0.0038959323428571224\n",
            "step: 480, loss: 0.035882893949747086\n",
            "step: 490, loss: 0.192092165350914\n",
            "step: 500, loss: 0.04393267631530762\n",
            "step: 510, loss: 0.03881125897169113\n",
            "step: 520, loss: 0.005637420807033777\n",
            "step: 530, loss: 0.008042127825319767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.922863741339492, f1=0.9086672879776329, best_f1=0.9133271202236719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007150425110012293\n",
            "step: 10, loss: 0.002258060732856393\n",
            "step: 20, loss: 0.008458222262561321\n",
            "step: 30, loss: 0.029740657657384872\n",
            "step: 40, loss: 0.013197189196944237\n",
            "step: 50, loss: 0.07017440348863602\n",
            "step: 60, loss: 0.007965533062815666\n",
            "step: 70, loss: 0.004920929204672575\n",
            "step: 80, loss: 0.004362739156931639\n",
            "step: 90, loss: 0.0022340998984873295\n",
            "step: 100, loss: 0.016634397208690643\n",
            "step: 110, loss: 0.0073596579022705555\n",
            "step: 120, loss: 0.019904036074876785\n",
            "step: 130, loss: 0.0007860968471504748\n",
            "step: 140, loss: 0.002009795745834708\n",
            "step: 150, loss: 0.0018645233940333128\n",
            "step: 160, loss: 0.02694987691938877\n",
            "step: 170, loss: 0.006688939873129129\n",
            "step: 180, loss: 0.06260187923908234\n",
            "step: 190, loss: 0.009595640003681183\n",
            "step: 200, loss: 0.017119908705353737\n",
            "step: 210, loss: 0.006892874836921692\n",
            "step: 220, loss: 0.00332638225518167\n",
            "step: 230, loss: 0.016926521435379982\n",
            "step: 240, loss: 0.10549873858690262\n",
            "step: 250, loss: 0.0630280002951622\n",
            "step: 260, loss: 0.01793079823255539\n",
            "step: 270, loss: 0.010497910901904106\n",
            "step: 280, loss: 0.014983047731220722\n",
            "step: 290, loss: 0.007232113741338253\n",
            "step: 300, loss: 0.00431145029142499\n",
            "step: 310, loss: 0.014432324096560478\n",
            "step: 320, loss: 0.0067541575990617275\n",
            "step: 330, loss: 0.031591352075338364\n",
            "step: 340, loss: 0.012230297550559044\n",
            "step: 350, loss: 0.004336215555667877\n",
            "step: 360, loss: 0.002805825090035796\n",
            "step: 370, loss: 0.010532137006521225\n",
            "step: 380, loss: 0.00837814249098301\n",
            "step: 390, loss: 0.027849435806274414\n",
            "step: 400, loss: 0.03743501007556915\n",
            "step: 410, loss: 0.009145201183855534\n",
            "step: 420, loss: 0.08199642598628998\n",
            "step: 430, loss: 0.0007501025684177876\n",
            "step: 440, loss: 0.002103498438373208\n",
            "step: 450, loss: 0.0907483771443367\n",
            "step: 460, loss: 0.005270335357636213\n",
            "step: 470, loss: 0.29900893568992615\n",
            "step: 480, loss: 0.004628514871001244\n",
            "step: 490, loss: 0.13538673520088196\n",
            "step: 500, loss: 0.0035297716967761517\n",
            "step: 510, loss: 0.004224393982440233\n",
            "step: 520, loss: 0.007700163405388594\n",
            "step: 530, loss: 0.0017062602564692497\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9278350515463917, f1=0.9069437883797827, best_f1=0.9133271202236719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018400212284177542\n",
            "step: 10, loss: 0.006032527890056372\n",
            "step: 20, loss: 0.001793667208403349\n",
            "step: 30, loss: 0.038626980036497116\n",
            "step: 40, loss: 0.0009274072363041341\n",
            "step: 50, loss: 0.001059786882251501\n",
            "step: 60, loss: 0.0013440982438623905\n",
            "step: 70, loss: 0.038506947457790375\n",
            "step: 80, loss: 0.000578170467633754\n",
            "step: 90, loss: 0.03905090317130089\n",
            "step: 100, loss: 0.008369258604943752\n",
            "step: 110, loss: 0.0043771169148385525\n",
            "step: 120, loss: 0.007164048030972481\n",
            "step: 130, loss: 0.053053054958581924\n",
            "step: 140, loss: 0.0004902890650555491\n",
            "step: 150, loss: 0.0016010603867471218\n",
            "step: 160, loss: 0.04135638102889061\n",
            "step: 170, loss: 0.13189579546451569\n",
            "step: 180, loss: 0.004271500278264284\n",
            "step: 190, loss: 0.012428775429725647\n",
            "step: 200, loss: 0.019006572663784027\n",
            "step: 210, loss: 0.22801309823989868\n",
            "step: 220, loss: 0.0034396613482385874\n",
            "step: 230, loss: 0.22438064217567444\n",
            "step: 240, loss: 0.007971297018229961\n",
            "step: 250, loss: 0.02428949996829033\n",
            "step: 260, loss: 0.0021077285055071115\n",
            "step: 270, loss: 0.04805154353380203\n",
            "step: 280, loss: 0.008257873356342316\n",
            "step: 290, loss: 0.024312948808073997\n",
            "step: 300, loss: 0.001733680721372366\n",
            "step: 310, loss: 0.0020563025027513504\n",
            "step: 320, loss: 0.009106753394007683\n",
            "step: 330, loss: 0.0008633667603135109\n",
            "step: 340, loss: 0.013598502613604069\n",
            "step: 350, loss: 0.0004357954312581569\n",
            "step: 360, loss: 0.03348921611905098\n",
            "step: 370, loss: 0.061668865382671356\n",
            "step: 380, loss: 0.0010789852822199464\n",
            "step: 390, loss: 0.03457681089639664\n",
            "step: 400, loss: 0.00570266367867589\n",
            "step: 410, loss: 0.006620282772928476\n",
            "step: 420, loss: 0.002685744082555175\n",
            "step: 430, loss: 0.025347448885440826\n",
            "step: 440, loss: 0.007383698131889105\n",
            "step: 450, loss: 0.05626385658979416\n",
            "step: 460, loss: 0.014503154903650284\n",
            "step: 470, loss: 0.08740068972110748\n",
            "step: 480, loss: 0.015958277508616447\n",
            "step: 490, loss: 0.023322992026805878\n",
            "step: 500, loss: 0.13605313003063202\n",
            "step: 510, loss: 0.1462005227804184\n",
            "step: 520, loss: 0.0029063241090625525\n",
            "step: 530, loss: 0.022007741034030914\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9297501178689297, f1=0.9100478468899522, best_f1=0.9133271202236719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007910503190942109\n",
            "step: 10, loss: 0.008323208428919315\n",
            "step: 20, loss: 0.0024536524433642626\n",
            "step: 30, loss: 0.06249842420220375\n",
            "step: 40, loss: 0.00389784830622375\n",
            "step: 50, loss: 0.0035672097001224756\n",
            "step: 60, loss: 0.0028799984138458967\n",
            "step: 70, loss: 0.0723731517791748\n",
            "step: 80, loss: 0.014708256348967552\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.08189558982849121\n",
            "step: 100, loss: 0.050670161843299866\n",
            "step: 110, loss: 0.06944870203733444\n",
            "step: 120, loss: 0.008479436859488487\n",
            "step: 130, loss: 0.0026563939172774553\n",
            "step: 140, loss: 0.0363367535173893\n",
            "step: 150, loss: 0.010161385871469975\n",
            "step: 160, loss: 0.01591724343597889\n",
            "step: 170, loss: 0.02555600181221962\n",
            "step: 180, loss: 0.014264506287872791\n",
            "step: 190, loss: 0.010196978226304054\n",
            "step: 200, loss: 0.0006760972901247442\n",
            "step: 210, loss: 0.03216733783483505\n",
            "step: 220, loss: 0.0009619120974093676\n",
            "step: 230, loss: 0.013963832519948483\n",
            "step: 240, loss: 0.0030114201363176107\n",
            "step: 250, loss: 0.004639694932848215\n",
            "step: 260, loss: 0.021875828504562378\n",
            "step: 270, loss: 0.006717166863381863\n",
            "step: 280, loss: 0.04764091968536377\n",
            "step: 290, loss: 0.002427492057904601\n",
            "step: 300, loss: 0.0038963346742093563\n",
            "step: 310, loss: 0.04926535114645958\n",
            "step: 320, loss: 0.0005975901731289923\n",
            "step: 330, loss: 0.0022135069593787193\n",
            "step: 340, loss: 0.001477596233598888\n",
            "step: 350, loss: 0.036871157586574554\n",
            "step: 360, loss: 0.0008345263777300715\n",
            "step: 370, loss: 0.011165001429617405\n",
            "step: 380, loss: 0.0013423284981399775\n",
            "step: 390, loss: 0.00042644268251024187\n",
            "step: 400, loss: 0.014098198153078556\n",
            "step: 410, loss: 0.003836600808426738\n",
            "step: 420, loss: 0.0006516557186841965\n",
            "step: 430, loss: 0.0020847534760832787\n",
            "step: 440, loss: 0.0050431108102202415\n",
            "step: 450, loss: 0.20622530579566956\n",
            "step: 460, loss: 0.10929320007562637\n",
            "step: 470, loss: 0.0011727282544597983\n",
            "step: 480, loss: 0.0004913975717499852\n",
            "step: 490, loss: 0.0015033322852104902\n",
            "step: 500, loss: 0.003516431199386716\n",
            "step: 510, loss: 0.007507513742893934\n",
            "step: 520, loss: 0.07423471659421921\n",
            "step: 530, loss: 0.034351006150245667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9253456221198156, f1=0.9153810191678354, best_f1=0.9133271202236719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08205285668373108\n",
            "step: 10, loss: 0.0007496149046346545\n",
            "step: 20, loss: 0.0018846483435481787\n",
            "step: 30, loss: 0.005359150469303131\n",
            "step: 40, loss: 0.0051935045048594475\n",
            "step: 50, loss: 0.007860398851335049\n",
            "step: 60, loss: 0.0020899176597595215\n",
            "step: 70, loss: 0.001456160913221538\n",
            "step: 80, loss: 0.00257049105130136\n",
            "step: 90, loss: 0.0009562387713231146\n",
            "step: 100, loss: 0.0012211394496262074\n",
            "step: 110, loss: 0.0060219489969313145\n",
            "step: 120, loss: 0.0005772248841822147\n",
            "step: 130, loss: 0.000851387856528163\n",
            "step: 140, loss: 0.0014366492396220565\n",
            "step: 150, loss: 0.008472184650599957\n",
            "step: 160, loss: 0.07897371053695679\n",
            "step: 170, loss: 0.0012771107722073793\n",
            "step: 180, loss: 0.006176554132252932\n",
            "step: 190, loss: 0.0022264837753027678\n",
            "step: 200, loss: 0.01999930664896965\n",
            "step: 210, loss: 0.01807377301156521\n",
            "step: 220, loss: 0.0013547297567129135\n",
            "step: 230, loss: 0.0009112640982493758\n",
            "step: 240, loss: 0.02804764173924923\n",
            "step: 250, loss: 0.0023353183642029762\n",
            "step: 260, loss: 0.1981593370437622\n",
            "step: 270, loss: 0.006700091063976288\n",
            "step: 280, loss: 0.005551077891141176\n",
            "step: 290, loss: 0.0008607865311205387\n",
            "step: 300, loss: 0.0012826549354940653\n",
            "step: 310, loss: 0.007766381837427616\n",
            "step: 320, loss: 0.005559214390814304\n",
            "step: 330, loss: 0.0023080697283148766\n",
            "step: 340, loss: 0.0008909482276067138\n",
            "step: 350, loss: 0.0651603639125824\n",
            "step: 360, loss: 0.0009260232327505946\n",
            "step: 370, loss: 0.003235175274312496\n",
            "step: 380, loss: 0.004473415203392506\n",
            "step: 390, loss: 0.000538280641194433\n",
            "step: 400, loss: 0.05616549029946327\n",
            "step: 410, loss: 0.002129905391484499\n",
            "step: 420, loss: 0.000691277498845011\n",
            "step: 430, loss: 0.0022835314739495516\n",
            "step: 440, loss: 0.0024196822196245193\n",
            "step: 450, loss: 0.000713749963324517\n",
            "step: 460, loss: 0.0009362497949041426\n",
            "step: 470, loss: 0.08093162626028061\n",
            "step: 480, loss: 0.005087852478027344\n",
            "step: 490, loss: 0.0008162165759131312\n",
            "step: 500, loss: 0.006211560219526291\n",
            "step: 510, loss: 0.0024712313897907734\n",
            "step: 520, loss: 0.0006742349360138178\n",
            "step: 530, loss: 0.002423943020403385\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9274004683840749, f1=0.9166274140367404, best_f1=0.9133271202236719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001366847543977201\n",
            "step: 10, loss: 0.006323530804365873\n",
            "step: 20, loss: 0.010915164835751057\n",
            "step: 30, loss: 0.0007213834905996919\n",
            "step: 40, loss: 0.0009466066840104759\n",
            "step: 50, loss: 0.0006962153711356223\n",
            "step: 60, loss: 0.0023831266444176435\n",
            "step: 70, loss: 0.0011865360429510474\n",
            "step: 80, loss: 0.004149154294282198\n",
            "step: 90, loss: 0.004355488810688257\n",
            "step: 100, loss: 0.002579648746177554\n",
            "step: 110, loss: 0.0011457508662715554\n",
            "step: 120, loss: 0.045681484043598175\n",
            "step: 130, loss: 0.0009620178025215864\n",
            "step: 140, loss: 0.006168849300593138\n",
            "step: 150, loss: 0.0013989447616040707\n",
            "step: 160, loss: 0.0009620531345717609\n",
            "step: 170, loss: 0.0022137085907161236\n",
            "step: 180, loss: 0.0012428261106833816\n",
            "step: 190, loss: 0.005277146585285664\n",
            "step: 200, loss: 0.0005677009467035532\n",
            "step: 210, loss: 0.0005370258004404604\n",
            "step: 220, loss: 0.0009009453933686018\n",
            "step: 230, loss: 0.00038034829776734114\n",
            "step: 240, loss: 0.009633179754018784\n",
            "step: 250, loss: 0.0005888204323127866\n",
            "step: 260, loss: 0.00040819469722919166\n",
            "step: 270, loss: 0.0007753782556392252\n",
            "step: 280, loss: 0.0003286390856374055\n",
            "step: 290, loss: 0.00044992819312028587\n",
            "step: 300, loss: 0.00035224496969021857\n",
            "step: 310, loss: 0.001194618409499526\n",
            "step: 320, loss: 0.00040702527621760964\n",
            "step: 330, loss: 0.00029094977071508765\n",
            "step: 340, loss: 0.0013210487086325884\n",
            "step: 350, loss: 0.006255178712308407\n",
            "step: 360, loss: 0.0003696104104164988\n",
            "step: 370, loss: 0.0006422695587389171\n",
            "step: 380, loss: 0.0013714596861973405\n",
            "step: 390, loss: 0.00106335140299052\n",
            "step: 400, loss: 0.0026668482460081577\n",
            "step: 410, loss: 0.0015941762831062078\n",
            "step: 420, loss: 0.0025477581657469273\n",
            "step: 430, loss: 0.006508261896669865\n",
            "step: 440, loss: 0.0011201683664694428\n",
            "step: 450, loss: 0.0011104975128546357\n",
            "step: 460, loss: 0.10057494044303894\n",
            "step: 470, loss: 0.0012274312321096659\n",
            "step: 480, loss: 0.0008248291560448706\n",
            "step: 490, loss: 0.0015240737702697515\n",
            "step: 500, loss: 0.0006384022999554873\n",
            "step: 510, loss: 0.0006038458086550236\n",
            "step: 520, loss: 0.0006338090752251446\n",
            "step: 530, loss: 0.012425493448972702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9262180974477957, f1=0.9181395348837208, best_f1=0.9133271202236719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007224069559015334\n",
            "step: 10, loss: 0.01090015284717083\n",
            "step: 20, loss: 0.014490167610347271\n",
            "step: 30, loss: 0.00017372674483340234\n",
            "step: 40, loss: 0.1413085013628006\n",
            "step: 50, loss: 0.0007508599082939327\n",
            "step: 60, loss: 0.000562961446121335\n",
            "step: 70, loss: 0.0006626421236433089\n",
            "step: 80, loss: 0.0005659498274326324\n",
            "step: 90, loss: 0.0009538863669149578\n",
            "step: 100, loss: 0.20820747315883636\n",
            "step: 110, loss: 0.004952315241098404\n",
            "step: 120, loss: 0.006543746683746576\n",
            "step: 130, loss: 0.0014367976691573858\n",
            "step: 140, loss: 0.0005375643377192318\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 150, loss: 0.0003414075472392142\n",
            "step: 160, loss: 0.0016804822953417897\n",
            "step: 170, loss: 0.00033698847983032465\n",
            "step: 180, loss: 0.000721242802683264\n",
            "step: 190, loss: 0.001100299647077918\n",
            "step: 200, loss: 0.0004296144179534167\n",
            "step: 210, loss: 0.0007861628546379507\n",
            "step: 220, loss: 0.001300671137869358\n",
            "step: 230, loss: 0.0006327331066131592\n",
            "step: 240, loss: 0.0007039280026219785\n",
            "step: 250, loss: 0.0005763796507380903\n",
            "step: 260, loss: 0.0014594290405511856\n",
            "step: 270, loss: 0.0015023835003376007\n",
            "step: 280, loss: 0.0034452860709279776\n",
            "step: 290, loss: 0.002592191332951188\n",
            "step: 300, loss: 0.000997656607069075\n",
            "step: 310, loss: 0.0010708183981478214\n",
            "step: 320, loss: 0.00904066115617752\n",
            "step: 330, loss: 0.0037881885655224323\n",
            "step: 340, loss: 0.0008652397082187235\n",
            "step: 350, loss: 0.0003342572890687734\n",
            "step: 360, loss: 0.0008293531718663871\n",
            "step: 370, loss: 0.0015646808315068483\n",
            "step: 380, loss: 0.0010802499018609524\n",
            "step: 390, loss: 0.0009582058992236853\n",
            "step: 400, loss: 0.000721910095307976\n",
            "step: 410, loss: 0.01830342598259449\n",
            "step: 420, loss: 0.0020964930299669504\n",
            "step: 430, loss: 0.0003664351534098387\n",
            "step: 440, loss: 0.12830710411071777\n",
            "step: 450, loss: 0.0032455851323902607\n",
            "step: 460, loss: 0.0009437738917768002\n",
            "step: 470, loss: 0.002670567948371172\n",
            "step: 480, loss: 0.013421609066426754\n",
            "step: 490, loss: 0.0018154517747461796\n",
            "step: 500, loss: 0.0003407628391869366\n",
            "step: 510, loss: 0.004466691054403782\n",
            "step: 520, loss: 0.008841187693178654\n",
            "step: 530, loss: 0.0004812315455637872\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9296693060083837, f1=0.9140989729225022, best_f1=0.9133271202236719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002481637638993561\n",
            "step: 10, loss: 0.0004572120087686926\n",
            "step: 20, loss: 0.0009201596258208156\n",
            "step: 30, loss: 0.00020814039453398436\n",
            "step: 40, loss: 0.05300331115722656\n",
            "step: 50, loss: 0.0005102620925754309\n",
            "step: 60, loss: 0.00020506835426203907\n",
            "step: 70, loss: 0.030152752995491028\n",
            "step: 80, loss: 0.0012379074469208717\n",
            "step: 90, loss: 0.0003209621936548501\n",
            "step: 100, loss: 0.00025843470939435065\n",
            "step: 110, loss: 0.00021526761702261865\n",
            "step: 120, loss: 0.0002377385535510257\n",
            "step: 130, loss: 0.0003538618329912424\n",
            "step: 140, loss: 0.002209064085036516\n",
            "step: 150, loss: 0.0006673352909274399\n",
            "step: 160, loss: 0.0013095615431666374\n",
            "step: 170, loss: 0.0001429217663826421\n",
            "step: 180, loss: 0.000288552837446332\n",
            "step: 190, loss: 0.0003207878617104143\n",
            "step: 200, loss: 0.00037301413249224424\n",
            "step: 210, loss: 0.0006447884952649474\n",
            "step: 220, loss: 0.000422877463279292\n",
            "step: 230, loss: 0.0008195508853532374\n",
            "step: 240, loss: 0.00048420947859995067\n",
            "step: 250, loss: 0.015629548579454422\n",
            "step: 260, loss: 0.2328745722770691\n",
            "step: 270, loss: 0.0005358998314477503\n",
            "step: 280, loss: 0.05685415863990784\n",
            "step: 290, loss: 0.0003743474662769586\n",
            "step: 300, loss: 0.0005362536758184433\n",
            "step: 310, loss: 0.0009394206572324038\n",
            "step: 320, loss: 0.0003934031701646745\n",
            "step: 330, loss: 0.0006195915630087256\n",
            "step: 340, loss: 0.0015737836947664618\n",
            "step: 350, loss: 0.0005035026115365326\n",
            "step: 360, loss: 0.18910248577594757\n",
            "step: 370, loss: 0.0012766099534928799\n",
            "step: 380, loss: 0.0008228283259086311\n",
            "step: 390, loss: 0.0009912890382111073\n",
            "step: 400, loss: 0.000844735826831311\n",
            "step: 410, loss: 0.002197731053456664\n",
            "step: 420, loss: 0.0007403908530250192\n",
            "step: 430, loss: 0.0007283115992322564\n",
            "step: 440, loss: 0.00027453788788989186\n",
            "step: 450, loss: 0.0020819876808673143\n",
            "step: 460, loss: 0.004039658233523369\n",
            "step: 470, loss: 0.0008969917544163764\n",
            "step: 480, loss: 0.0008910810574889183\n",
            "step: 490, loss: 0.00035224881139583886\n",
            "step: 500, loss: 0.0003796280943788588\n",
            "step: 510, loss: 0.025644607841968536\n",
            "step: 520, loss: 0.0003434997342992574\n",
            "step: 530, loss: 0.0004189138126093894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9263351749539595, f1=0.911955514365153, best_f1=0.9133271202236719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003234915202483535\n",
            "step: 10, loss: 0.0014188281493261456\n",
            "step: 20, loss: 0.0005624886252917349\n",
            "step: 30, loss: 0.0006906106136739254\n",
            "step: 40, loss: 0.0004502391966525465\n",
            "step: 50, loss: 0.19030477106571198\n",
            "step: 60, loss: 0.002352580428123474\n",
            "step: 70, loss: 0.001303448691032827\n",
            "step: 80, loss: 0.00111842539627105\n",
            "step: 90, loss: 0.00036680669290944934\n",
            "step: 100, loss: 0.0014112937496975064\n",
            "step: 110, loss: 0.0006169176776893437\n",
            "step: 120, loss: 0.0007355424459092319\n",
            "step: 130, loss: 0.00043868570355698466\n",
            "step: 140, loss: 0.0008254295098595321\n",
            "step: 150, loss: 0.001227573724463582\n",
            "step: 160, loss: 0.0004047782567795366\n",
            "step: 170, loss: 0.001410845317877829\n",
            "step: 180, loss: 0.00023575829982291907\n",
            "step: 190, loss: 0.001405783579684794\n",
            "step: 200, loss: 0.0006866236217319965\n",
            "step: 210, loss: 0.0013634293572977185\n",
            "step: 220, loss: 0.00048180672456510365\n",
            "step: 230, loss: 0.00048452967894263566\n",
            "step: 240, loss: 0.001040223753079772\n",
            "step: 250, loss: 0.0018154600402340293\n",
            "step: 260, loss: 0.00045328980195336044\n",
            "step: 270, loss: 0.0006171968416310847\n",
            "step: 280, loss: 0.0003933035477530211\n",
            "step: 290, loss: 0.00017912450130097568\n",
            "step: 300, loss: 0.0012911342782899737\n",
            "step: 310, loss: 0.00022004585480317473\n",
            "step: 320, loss: 0.0004568055737763643\n",
            "step: 330, loss: 0.0006474683759734035\n",
            "step: 340, loss: 0.00034438882721588016\n",
            "step: 350, loss: 0.0003183943044859916\n",
            "step: 360, loss: 0.0006817459361627698\n",
            "step: 370, loss: 0.0017455037450417876\n",
            "step: 380, loss: 0.04636595770716667\n",
            "step: 390, loss: 0.00019484921358525753\n",
            "step: 400, loss: 0.0012075472623109818\n",
            "step: 410, loss: 0.00016909727128222585\n",
            "step: 420, loss: 0.0005693468847312033\n",
            "step: 430, loss: 0.0009186483221128583\n",
            "step: 440, loss: 0.0009824368171393871\n",
            "step: 450, loss: 0.00048574325046502054\n",
            "step: 460, loss: 0.004200538620352745\n",
            "step: 470, loss: 0.00041323748882859945\n",
            "step: 480, loss: 0.0004830828111153096\n",
            "step: 490, loss: 0.0005167472991161048\n",
            "step: 500, loss: 0.000609254464507103\n",
            "step: 510, loss: 0.0009029911016114056\n",
            "step: 520, loss: 0.0002321530773770064\n",
            "step: 530, loss: 0.00039333465974777937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9287377736376339, f1=0.9178789300797747, best_f1=0.9133271202236719\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003389054036233574\n",
            "step: 10, loss: 0.010688497684895992\n",
            "step: 20, loss: 0.000496537599246949\n",
            "step: 30, loss: 0.0018274645553901792\n",
            "step: 40, loss: 0.0003695031045936048\n",
            "step: 50, loss: 0.0008394865435548127\n",
            "step: 60, loss: 0.0010318681597709656\n",
            "step: 70, loss: 0.0005070038023404777\n",
            "step: 80, loss: 0.00040199767681770027\n",
            "step: 90, loss: 0.0005562249571084976\n",
            "step: 100, loss: 0.00012262686504982412\n",
            "step: 110, loss: 0.002515587490051985\n",
            "step: 120, loss: 0.0002685635699890554\n",
            "step: 130, loss: 0.0013392339460551739\n",
            "step: 140, loss: 0.000940683763474226\n",
            "step: 150, loss: 0.0004253521328791976\n",
            "step: 160, loss: 0.010339401662349701\n",
            "step: 170, loss: 0.002126851584762335\n",
            "step: 180, loss: 0.0011412949534133077\n",
            "step: 190, loss: 0.000584052933845669\n",
            "step: 200, loss: 0.0001985540147870779\n",
            "step: 210, loss: 0.0005245613865554333\n",
            "step: 220, loss: 0.0006193115841597319\n",
            "step: 230, loss: 0.0008285414660349488\n",
            "step: 240, loss: 0.00024941086303442717\n",
            "step: 250, loss: 0.0010076462058350444\n",
            "step: 260, loss: 0.026229223236441612\n",
            "step: 270, loss: 0.0005168567295186222\n",
            "step: 280, loss: 0.00022731476929038763\n",
            "step: 290, loss: 0.0004695513052865863\n",
            "step: 300, loss: 0.0005406475975178182\n",
            "step: 310, loss: 0.03237896412611008\n",
            "step: 320, loss: 0.00035348202800378203\n",
            "step: 330, loss: 0.000636004377156496\n",
            "step: 340, loss: 0.0004586479626595974\n",
            "step: 350, loss: 0.0014043146511539817\n",
            "step: 360, loss: 0.009444328024983406\n",
            "step: 370, loss: 0.0008139002602547407\n",
            "step: 380, loss: 0.001232022768817842\n",
            "step: 390, loss: 0.0005293911672197282\n",
            "step: 400, loss: 0.01427729893475771\n",
            "step: 410, loss: 0.0011535121593624353\n",
            "step: 420, loss: 0.0005790971335954964\n",
            "step: 430, loss: 0.00019264676666352898\n",
            "step: 440, loss: 0.00115727458614856\n",
            "step: 450, loss: 0.0035630532074719667\n",
            "step: 460, loss: 0.002999075222760439\n",
            "step: 470, loss: 0.0002702352940104902\n",
            "step: 480, loss: 0.006885637994855642\n",
            "step: 490, loss: 0.0012845188612118363\n",
            "step: 500, loss: 0.00023966087610460818\n",
            "step: 510, loss: 0.0002794433676172048\n",
            "step: 520, loss: 0.00033958189305849373\n",
            "step: 530, loss: 0.0002452280605211854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9301453352086263, f1=0.9139072847682119, best_f1=0.9133271202236719\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:22, 251.13it/s]\n",
            "load_f1 = 0.9327231121281464\n",
            "real_f1 = 0.9287696577243294\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 215.29it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10svv34hgw7-"
      },
      "source": [
        "# DK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA_JUmr4slhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c208385-2821-4cba-ca56-c8a9d81bc783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 496 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=63d9ea4133620f36d532cdbcdf8e76c3bbbf55a15da784732fa9a5b879f163db\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rj79mlb1/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdNk8ikFgw7-"
      },
      "source": [
        "## DK STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCvdP9vMgw7_",
        "outputId": "a0856129-0dd1-4e13-d390-3b93fab8b477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.442573606967926\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4333333333333333, f1=0.4186046511627907, best_f1=0.4186046511627907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4974696934223175\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.37209302325581395, f1=0.3225806451612903, best_f1=0.4186046511627907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4251985549926758\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.3333333333333333, f1=0.36363636363636365, best_f1=0.4186046511627907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2997533977031708\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.46153846153846156, f1=0.4285714285714286, best_f1=0.4285714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3065313696861267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.41791044776119407, f1=0.3846153846153846, best_f1=0.4285714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.246802419424057\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.43999999999999995, f1=0.43902439024390244, best_f1=0.4285714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5252156257629395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.4117647058823529, f1=0.4363636363636364, best_f1=0.4285714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.456521600484848\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.5531914893617021, f1=0.5853658536585367, best_f1=0.5853658536585367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38176339864730835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.5531914893617021, f1=0.5909090909090909, best_f1=0.5853658536585367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.49750906229019165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.5263157894736842, f1=0.4736842105263159, best_f1=0.5853658536585367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.44134050607681274\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.6046511627906977, f1=0.5641025641025641, best_f1=0.5641025641025641\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28847476840019226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.6341463414634146, f1=0.631578947368421, best_f1=0.631578947368421\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3156852126121521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.6666666666666666, f1=0.6857142857142857, best_f1=0.6857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18991759419441223\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6666666666666666, f1=0.6857142857142857, best_f1=0.6857142857142857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3327644169330597\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6666666666666666, f1=0.6857142857142857, best_f1=0.6857142857142857\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 122687.77it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.6086956521739131\n",
            "real_f1 = 0.5283018867924528\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 213.02it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VIiiAcAgw8B",
        "outputId": "15742e12-24fd-48b0-e650-5d936fe9e157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5868017673492432\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.5303964018821716\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.6087181568145752\n",
            "step: 30, loss: 0.30104970932006836\n",
            "step: 40, loss: 0.3203975558280945\n",
            "step: 50, loss: 0.43472784757614136\n",
            "step: 60, loss: 0.16723884642124176\n",
            "step: 70, loss: 0.06282869726419449\n",
            "step: 80, loss: 0.04541340097784996\n",
            "step: 90, loss: 0.08021446317434311\n",
            "step: 100, loss: 0.14491596817970276\n",
            "step: 110, loss: 0.16777876019477844\n",
            "step: 120, loss: 0.05940144509077072\n",
            "step: 130, loss: 0.047851987183094025\n",
            "step: 140, loss: 0.01721017062664032\n",
            "step: 150, loss: 0.08284691721200943\n",
            "step: 160, loss: 0.2171693742275238\n",
            "step: 170, loss: 0.04499254375696182\n",
            "step: 180, loss: 0.00943361222743988\n",
            "step: 190, loss: 0.02620837092399597\n",
            "step: 200, loss: 0.020276522263884544\n",
            "step: 210, loss: 0.01957329921424389\n",
            "step: 220, loss: 0.014484627172350883\n",
            "step: 230, loss: 0.03532220423221588\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9596510359869138, f1=0.9604395604395605, best_f1=0.9604395604395605\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013290779665112495\n",
            "step: 10, loss: 0.2455165535211563\n",
            "step: 20, loss: 0.03412420675158501\n",
            "step: 30, loss: 0.04547726362943649\n",
            "step: 40, loss: 0.05412640422582626\n",
            "step: 50, loss: 0.016161508858203888\n",
            "step: 60, loss: 0.012289069592952728\n",
            "step: 70, loss: 0.050241127610206604\n",
            "step: 80, loss: 0.011585721746087074\n",
            "step: 90, loss: 0.027680393308401108\n",
            "step: 100, loss: 0.006637485697865486\n",
            "step: 110, loss: 0.005179265979677439\n",
            "step: 120, loss: 0.0224575437605381\n",
            "step: 130, loss: 0.005043352022767067\n",
            "step: 140, loss: 0.04216935113072395\n",
            "step: 150, loss: 0.055152587592601776\n",
            "step: 160, loss: 0.0018716754857450724\n",
            "step: 170, loss: 0.0023696941789239645\n",
            "step: 180, loss: 0.00797708984464407\n",
            "step: 190, loss: 0.2595987617969513\n",
            "step: 200, loss: 0.0065934802405536175\n",
            "step: 210, loss: 0.037545058876276016\n",
            "step: 220, loss: 0.0020313311833888292\n",
            "step: 230, loss: 0.0027768106665462255\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9776785714285714, f1=0.9730337078651685, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01397774275392294\n",
            "step: 10, loss: 0.01180912833660841\n",
            "step: 20, loss: 0.0008765783859416842\n",
            "step: 30, loss: 0.0006051703239791095\n",
            "step: 40, loss: 0.0052988058887422085\n",
            "step: 50, loss: 0.03160085529088974\n",
            "step: 60, loss: 0.00334958010353148\n",
            "step: 70, loss: 0.0025895400904119015\n",
            "step: 80, loss: 0.03609447926282883\n",
            "step: 90, loss: 0.08423855155706406\n",
            "step: 100, loss: 0.014950834214687347\n",
            "step: 110, loss: 0.0013560457155108452\n",
            "step: 120, loss: 0.0006955372518859804\n",
            "step: 130, loss: 0.004031600896269083\n",
            "step: 140, loss: 0.0034982645884156227\n",
            "step: 150, loss: 0.11142630130052567\n",
            "step: 160, loss: 0.006264129187911749\n",
            "step: 170, loss: 0.001692923135124147\n",
            "step: 180, loss: 0.012444669380784035\n",
            "step: 190, loss: 0.01121970359236002\n",
            "step: 200, loss: 0.021874431520700455\n",
            "step: 210, loss: 0.07960058003664017\n",
            "step: 220, loss: 0.061528462916612625\n",
            "step: 230, loss: 0.010821610689163208\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9574944071588367, f1=0.9491150442477876, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.061930324882268906\n",
            "step: 10, loss: 0.006982884369790554\n",
            "step: 20, loss: 0.007096052169799805\n",
            "step: 30, loss: 0.0009750334429554641\n",
            "step: 40, loss: 0.1521649807691574\n",
            "step: 50, loss: 0.005416335538029671\n",
            "step: 60, loss: 0.00175286119338125\n",
            "step: 70, loss: 0.011995486915111542\n",
            "step: 80, loss: 0.000675040006171912\n",
            "step: 90, loss: 0.0026299715973436832\n",
            "step: 100, loss: 0.0036076055839657784\n",
            "step: 110, loss: 0.0032138878013938665\n",
            "step: 120, loss: 0.003285262268036604\n",
            "step: 130, loss: 0.008400422520935535\n",
            "step: 140, loss: 0.028325233608484268\n",
            "step: 150, loss: 0.040680211037397385\n",
            "step: 160, loss: 0.0005384706892073154\n",
            "step: 170, loss: 0.0058127534575760365\n",
            "step: 180, loss: 0.044764623045921326\n",
            "step: 190, loss: 0.04684678837656975\n",
            "step: 200, loss: 0.1949651539325714\n",
            "step: 210, loss: 0.0018178779864683747\n",
            "step: 220, loss: 0.01043738704174757\n",
            "step: 230, loss: 0.040904466062784195\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9697648376259798, f1=0.9830890642615557, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007497934275306761\n",
            "step: 10, loss: 0.0008794284658506513\n",
            "step: 20, loss: 0.0011738031171262264\n",
            "step: 30, loss: 0.00272737885825336\n",
            "step: 40, loss: 0.0007387110381387174\n",
            "step: 50, loss: 0.014791597612202168\n",
            "step: 60, loss: 0.0019666836597025394\n",
            "step: 70, loss: 0.002929435111582279\n",
            "step: 80, loss: 0.01844063214957714\n",
            "step: 90, loss: 0.04095025733113289\n",
            "step: 100, loss: 0.0006879913853481412\n",
            "step: 110, loss: 0.008250445127487183\n",
            "step: 120, loss: 0.10183583199977875\n",
            "step: 130, loss: 0.0034721402917057276\n",
            "step: 140, loss: 0.05197524651885033\n",
            "step: 150, loss: 0.00802411139011383\n",
            "step: 160, loss: 0.0018514409894123673\n",
            "step: 170, loss: 0.002645130967721343\n",
            "step: 180, loss: 0.00828650314360857\n",
            "step: 190, loss: 0.005843111779540777\n",
            "step: 200, loss: 0.0051979669369757175\n",
            "step: 210, loss: 0.0012892084196209908\n",
            "step: 220, loss: 0.0012615922605618834\n",
            "step: 230, loss: 0.003048193408176303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9764837625979844, f1=0.9808773903262092, best_f1=0.9730337078651685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007348362705670297\n",
            "step: 10, loss: 0.002790065249428153\n",
            "step: 20, loss: 0.00364795234054327\n",
            "step: 30, loss: 0.0024013358633965254\n",
            "step: 40, loss: 0.001189662260003388\n",
            "step: 50, loss: 0.001305465935729444\n",
            "step: 60, loss: 0.0036575093399733305\n",
            "step: 70, loss: 0.006998412776738405\n",
            "step: 80, loss: 0.12646058201789856\n",
            "step: 90, loss: 0.01766761764883995\n",
            "step: 100, loss: 0.0011661634780466557\n",
            "step: 110, loss: 0.02380548045039177\n",
            "step: 120, loss: 0.0004422999045345932\n",
            "step: 130, loss: 0.000555702019482851\n",
            "step: 140, loss: 0.0015774747589603066\n",
            "step: 150, loss: 0.0003180886269547045\n",
            "step: 160, loss: 0.0009619990014471114\n",
            "step: 170, loss: 0.0017880311934277415\n",
            "step: 180, loss: 0.0006329126772470772\n",
            "step: 190, loss: 0.002822005655616522\n",
            "step: 200, loss: 0.023332417011260986\n",
            "step: 210, loss: 0.0026175417006015778\n",
            "step: 220, loss: 0.004075496457517147\n",
            "step: 230, loss: 0.0024504815228283405\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.978865406006674, f1=0.9798206278026906, best_f1=0.9798206278026906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002296363702043891\n",
            "step: 10, loss: 0.0018591789994388819\n",
            "step: 20, loss: 0.0007798050064593554\n",
            "step: 30, loss: 0.0005642481846734881\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 40, loss: 0.00099333340767771\n",
            "step: 50, loss: 0.0008932147175073624\n",
            "step: 60, loss: 0.00045663898345083\n",
            "step: 70, loss: 0.0012990310788154602\n",
            "step: 80, loss: 0.00958824809640646\n",
            "step: 90, loss: 0.00205918587744236\n",
            "step: 100, loss: 0.0005520335980691016\n",
            "step: 110, loss: 0.0008898924570530653\n",
            "step: 120, loss: 0.0004636074008885771\n",
            "step: 130, loss: 0.0006996559677645564\n",
            "step: 140, loss: 0.0005792079027742147\n",
            "step: 150, loss: 0.0016459410544484854\n",
            "step: 160, loss: 0.0004831101978197694\n",
            "step: 170, loss: 0.0022721595596522093\n",
            "step: 180, loss: 0.0016419498715549707\n",
            "step: 190, loss: 0.00035390237462706864\n",
            "step: 200, loss: 0.03647252544760704\n",
            "step: 210, loss: 0.013854135759174824\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "step: 220, loss: 0.16118748486042023\n",
            "step: 230, loss: 0.0013469394762068987\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9670329670329669, f1=0.977728285077951, best_f1=0.9798206278026906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009993293788284063\n",
            "step: 10, loss: 0.024071257561445236\n",
            "step: 20, loss: 0.0020055631175637245\n",
            "step: 30, loss: 0.0013372896937653422\n",
            "step: 40, loss: 0.0015180015470832586\n",
            "step: 50, loss: 0.0005407326389104128\n",
            "step: 60, loss: 0.0006784183206036687\n",
            "step: 70, loss: 0.00027044740272685885\n",
            "step: 80, loss: 0.006557963322848082\n",
            "step: 90, loss: 0.00029504502890631557\n",
            "step: 100, loss: 0.0010029622353613377\n",
            "step: 110, loss: 0.004209799692034721\n",
            "step: 120, loss: 0.015535845421254635\n",
            "step: 130, loss: 0.0025836117565631866\n",
            "step: 140, loss: 0.0007292920490726829\n",
            "step: 150, loss: 0.03120441734790802\n",
            "step: 160, loss: 0.004663487896323204\n",
            "step: 170, loss: 0.014887184835970402\n",
            "step: 180, loss: 0.004877225961536169\n",
            "step: 190, loss: 0.004139437805861235\n",
            "step: 200, loss: 0.0014018821530044079\n",
            "step: 210, loss: 0.00389050948433578\n",
            "step: 220, loss: 0.0019125493708997965\n",
            "step: 230, loss: 0.0012927408097311854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9810901001112348, f1=0.9798657718120806, best_f1=0.9798657718120806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011342902667820454\n",
            "step: 10, loss: 0.001331416191533208\n",
            "step: 20, loss: 0.001285469625145197\n",
            "step: 30, loss: 0.0012728988658636808\n",
            "step: 40, loss: 0.0008129930938594043\n",
            "step: 50, loss: 0.003211717354133725\n",
            "step: 60, loss: 0.006032873410731554\n",
            "step: 70, loss: 0.0595279298722744\n",
            "step: 80, loss: 0.115447998046875\n",
            "step: 90, loss: 0.005095178727060556\n",
            "step: 100, loss: 0.0005259407334960997\n",
            "step: 110, loss: 0.0020164574962109327\n",
            "step: 120, loss: 0.03541059046983719\n",
            "step: 130, loss: 0.018584605306386948\n",
            "step: 140, loss: 0.0006872297963127494\n",
            "step: 150, loss: 0.0008931467309594154\n",
            "step: 160, loss: 0.0015811027260497212\n",
            "step: 170, loss: 0.0007837883895263076\n",
            "step: 180, loss: 0.0007285673054866493\n",
            "step: 190, loss: 0.0004908624105155468\n",
            "step: 200, loss: 0.06622958183288574\n",
            "step: 210, loss: 0.04275340959429741\n",
            "step: 220, loss: 0.002901152940467\n",
            "step: 230, loss: 0.0012109605595469475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9774774774774775, f1=0.9831271091113611, best_f1=0.9798657718120806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013079928467050195\n",
            "step: 10, loss: 0.022257603704929352\n",
            "step: 20, loss: 0.0019216835498809814\n",
            "step: 30, loss: 0.00016439439787063748\n",
            "step: 40, loss: 0.0021090111695230007\n",
            "step: 50, loss: 0.000338671263307333\n",
            "step: 60, loss: 0.0009934710105881095\n",
            "step: 70, loss: 0.0870940238237381\n",
            "step: 80, loss: 0.001984131056815386\n",
            "step: 90, loss: 0.0007131509482860565\n",
            "step: 100, loss: 0.0014120929408818483\n",
            "step: 110, loss: 0.004426844883710146\n",
            "step: 120, loss: 0.00022131310834083706\n",
            "step: 130, loss: 0.00024363935517612845\n",
            "step: 140, loss: 0.0003129101824015379\n",
            "step: 150, loss: 0.0010688824113458395\n",
            "step: 160, loss: 7.030742563074455e-05\n",
            "step: 170, loss: 0.0001746952038956806\n",
            "step: 180, loss: 0.0016722202999517322\n",
            "step: 190, loss: 0.00027799050440080464\n",
            "step: 200, loss: 0.00044227400212548673\n",
            "step: 210, loss: 0.004355356562882662\n",
            "step: 220, loss: 0.13140715658664703\n",
            "step: 230, loss: 0.0003463605826254934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9832402234636871, f1=0.9787709497206705, best_f1=0.9787709497206705\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019440145115368068\n",
            "step: 10, loss: 0.0006165459053590894\n",
            "step: 20, loss: 0.00033479565172456205\n",
            "step: 30, loss: 0.00046682695392519236\n",
            "step: 40, loss: 0.0001307904749410227\n",
            "step: 50, loss: 0.00021073524840176105\n",
            "step: 60, loss: 0.0034116506576538086\n",
            "step: 70, loss: 0.006117125973105431\n",
            "step: 80, loss: 0.00014765012019779533\n",
            "step: 90, loss: 0.13744212687015533\n",
            "step: 100, loss: 0.003067820332944393\n",
            "step: 110, loss: 0.00040334544610232115\n",
            "step: 120, loss: 0.0009524776833131909\n",
            "step: 130, loss: 0.0007793134427629411\n",
            "step: 140, loss: 0.006586987525224686\n",
            "step: 150, loss: 0.0017480365931987762\n",
            "step: 160, loss: 0.004907221533358097\n",
            "step: 170, loss: 0.007366582751274109\n",
            "step: 180, loss: 0.0008948651957325637\n",
            "step: 190, loss: 0.00042942946311086416\n",
            "step: 200, loss: 0.006564405281096697\n",
            "step: 210, loss: 0.0005058203241787851\n",
            "step: 220, loss: 0.0024796498473733664\n",
            "step: 230, loss: 0.00022283545695245266\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9841628959276018, f1=0.9841628959276018, best_f1=0.9841628959276018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000315613899147138\n",
            "step: 10, loss: 0.00011087710299761966\n",
            "step: 20, loss: 0.009711544029414654\n",
            "step: 30, loss: 0.0743827074766159\n",
            "step: 40, loss: 0.0005130994832143188\n",
            "step: 50, loss: 0.002215523738414049\n",
            "step: 60, loss: 0.00021712869056500494\n",
            "step: 70, loss: 0.00014451405149884522\n",
            "step: 80, loss: 7.258655386976898e-05\n",
            "step: 90, loss: 0.001083848183043301\n",
            "step: 100, loss: 0.00012310975580476224\n",
            "step: 110, loss: 0.0001745995250530541\n",
            "step: 120, loss: 0.0003206938854418695\n",
            "step: 130, loss: 0.0006946310750208795\n",
            "step: 140, loss: 0.010913748294115067\n",
            "step: 150, loss: 0.00029247987549751997\n",
            "step: 160, loss: 0.00016837204748298973\n",
            "step: 170, loss: 0.00048221476026810706\n",
            "step: 180, loss: 0.0009256932535208762\n",
            "step: 190, loss: 0.004802991636097431\n",
            "step: 200, loss: 0.00030947092454880476\n",
            "step: 210, loss: 0.0003006780752912164\n",
            "step: 220, loss: 0.000535497150849551\n",
            "step: 230, loss: 0.00017282857152167708\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9854096520763187, f1=0.9853768278965129, best_f1=0.9853768278965129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00029317886219359934\n",
            "step: 10, loss: 8.628959767520428e-05\n",
            "step: 20, loss: 0.00029415020253509283\n",
            "step: 30, loss: 0.002576008439064026\n",
            "step: 40, loss: 0.00021478786948136985\n",
            "step: 50, loss: 0.00017038837540894747\n",
            "step: 60, loss: 0.0001583555422257632\n",
            "step: 70, loss: 0.00017058629600796849\n",
            "step: 80, loss: 6.435388786485419e-05\n",
            "step: 90, loss: 0.000292244425509125\n",
            "step: 100, loss: 0.0005811178707517684\n",
            "step: 110, loss: 0.00012808249448426068\n",
            "step: 120, loss: 0.0003363295109011233\n",
            "step: 130, loss: 0.00017524519353173673\n",
            "step: 140, loss: 8.094128861557692e-05\n",
            "step: 150, loss: 0.0006036101258359849\n",
            "step: 160, loss: 0.007032713852822781\n",
            "step: 170, loss: 0.0016666986048221588\n",
            "step: 180, loss: 0.0026415165048092604\n",
            "step: 190, loss: 0.00033414672361686826\n",
            "step: 200, loss: 3.616835965658538e-05\n",
            "step: 210, loss: 0.002853778889402747\n",
            "step: 220, loss: 0.00011326429375912994\n",
            "step: 230, loss: 0.00014629379438702017\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9854096520763187, f1=0.984304932735426, best_f1=0.9853768278965129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012389026232995093\n",
            "step: 10, loss: 9.055442205863073e-05\n",
            "step: 20, loss: 0.00011456471111159772\n",
            "step: 30, loss: 0.00018645681848283857\n",
            "step: 40, loss: 0.00012778873497154564\n",
            "step: 50, loss: 0.00014306310913525522\n",
            "step: 60, loss: 0.0001449325936846435\n",
            "step: 70, loss: 8.630291267763823e-05\n",
            "step: 80, loss: 8.72675227583386e-05\n",
            "step: 90, loss: 0.0002272811980219558\n",
            "step: 100, loss: 0.00023876252816990018\n",
            "step: 110, loss: 0.00020595832029357553\n",
            "step: 120, loss: 2.7643076464300975e-05\n",
            "step: 130, loss: 0.00042614858830347657\n",
            "step: 140, loss: 6.530038808705285e-05\n",
            "step: 150, loss: 0.001122455345466733\n",
            "step: 160, loss: 0.0002476402441971004\n",
            "step: 170, loss: 0.00014146386820357293\n",
            "step: 180, loss: 8.652765245642513e-05\n",
            "step: 190, loss: 0.0012747830478474498\n",
            "step: 200, loss: 0.0001506501721451059\n",
            "step: 210, loss: 0.00022758122941013426\n",
            "step: 220, loss: 0.00030278830672614276\n",
            "step: 230, loss: 0.00028587071574293077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9854423292273236, f1=0.9854096520763187, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00029267376521602273\n",
            "step: 10, loss: 0.00015054189134389162\n",
            "step: 20, loss: 0.0018352713668718934\n",
            "step: 30, loss: 0.0009678143542259932\n",
            "step: 40, loss: 4.235597953083925e-05\n",
            "step: 50, loss: 0.00011033355258405209\n",
            "step: 60, loss: 0.0432741604745388\n",
            "step: 70, loss: 0.00010603765258565545\n",
            "step: 80, loss: 0.00017636535631027073\n",
            "step: 90, loss: 9.054451220436022e-05\n",
            "step: 100, loss: 3.599856063374318e-05\n",
            "step: 110, loss: 0.00024247141845989972\n",
            "step: 120, loss: 0.00036779852234758437\n",
            "step: 130, loss: 0.0001753930700942874\n",
            "step: 140, loss: 0.006652530282735825\n",
            "step: 150, loss: 0.00011618502321653068\n",
            "step: 160, loss: 0.00015650827845092863\n",
            "step: 170, loss: 8.913016790756956e-05\n",
            "step: 180, loss: 0.00015852152137085795\n",
            "step: 190, loss: 7.287576590897515e-05\n",
            "step: 200, loss: 0.00014900797395966947\n",
            "step: 210, loss: 0.0003649965801741928\n",
            "step: 220, loss: 0.00033592397812753916\n",
            "step: 230, loss: 0.00015512193203903735\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9854423292273236, f1=0.9854096520763187, best_f1=0.9854096520763187\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 214.28it/s]\n",
            "load_f1 = 0.9832402234636871\n",
            "real_f1 = 0.9821029082774049\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 214.32it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4v1tmXbgw8B"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUUIV1IBgw8B",
        "outputId": "166f6e1a-088d-42e6-a115-a51cf88ed2e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6652434468269348\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4723648726940155\n",
            "step: 20, loss: 0.3251165449619293\n",
            "step: 30, loss: 0.44075268507003784\n",
            "step: 40, loss: 0.4648902416229248\n",
            "step: 50, loss: 0.6543816328048706\n",
            "step: 60, loss: 0.3684346079826355\n",
            "step: 70, loss: 0.4345562160015106\n",
            "step: 80, loss: 0.4679276943206787\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.3820081651210785\n",
            "step: 100, loss: 0.3790214955806732\n",
            "step: 110, loss: 0.41244378685951233\n",
            "step: 120, loss: 0.28037771582603455\n",
            "step: 130, loss: 0.2622748911380768\n",
            "step: 140, loss: 0.21284779906272888\n",
            "step: 150, loss: 0.12382815778255463\n",
            "step: 160, loss: 0.3034604489803314\n",
            "step: 170, loss: 0.14161278307437897\n",
            "step: 180, loss: 0.19843876361846924\n",
            "step: 190, loss: 0.2014438658952713\n",
            "step: 200, loss: 0.16640833020210266\n",
            "step: 210, loss: 0.033055536448955536\n",
            "step: 220, loss: 0.18455569446086884\n",
            "step: 230, loss: 0.23359642922878265\n",
            "step: 240, loss: 0.0844600647687912\n",
            "step: 250, loss: 0.11345154047012329\n",
            "step: 260, loss: 0.3098145127296448\n",
            "step: 270, loss: 0.2867448627948761\n",
            "step: 280, loss: 0.06341332942247391\n",
            "step: 290, loss: 0.1375025510787964\n",
            "step: 300, loss: 0.2712412178516388\n",
            "step: 310, loss: 0.29679039120674133\n",
            "step: 320, loss: 0.14931756258010864\n",
            "step: 330, loss: 0.14169837534427643\n",
            "step: 340, loss: 0.31841593980789185\n",
            "step: 350, loss: 0.23580101132392883\n",
            "step: 360, loss: 0.04506562277674675\n",
            "step: 370, loss: 0.021174848079681396\n",
            "step: 380, loss: 0.2879686653614044\n",
            "step: 390, loss: 0.05099291726946831\n",
            "step: 400, loss: 0.08034560084342957\n",
            "step: 410, loss: 0.2512272000312805\n",
            "step: 420, loss: 0.08409107476472855\n",
            "step: 430, loss: 0.03145221620798111\n",
            "step: 440, loss: 0.056381627917289734\n",
            "step: 450, loss: 0.11576838791370392\n",
            "step: 460, loss: 0.06704898178577423\n",
            "step: 470, loss: 0.059442803263664246\n",
            "step: 480, loss: 0.2177848368883133\n",
            "step: 490, loss: 0.27024468779563904\n",
            "step: 500, loss: 0.0638984739780426\n",
            "step: 510, loss: 0.03490550071001053\n",
            "step: 520, loss: 0.16892385482788086\n",
            "step: 530, loss: 0.028710976243019104\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.906163753449862, f1=0.9023941068139963, best_f1=0.9023941068139963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13568924367427826\n",
            "step: 10, loss: 0.11631272733211517\n",
            "step: 20, loss: 0.07974106818437576\n",
            "step: 30, loss: 0.20445384085178375\n",
            "step: 40, loss: 0.05096627399325371\n",
            "step: 50, loss: 0.08128396421670914\n",
            "step: 60, loss: 0.14563001692295074\n",
            "step: 70, loss: 0.086742103099823\n",
            "step: 80, loss: 0.1225394532084465\n",
            "step: 90, loss: 0.05971783772110939\n",
            "step: 100, loss: 0.22303406894207\n",
            "step: 110, loss: 0.04270852729678154\n",
            "step: 120, loss: 0.13825125992298126\n",
            "step: 130, loss: 0.012318521738052368\n",
            "step: 140, loss: 0.09326015412807465\n",
            "step: 150, loss: 0.047542303800582886\n",
            "step: 160, loss: 0.04348450154066086\n",
            "step: 170, loss: 0.1250791847705841\n",
            "step: 180, loss: 0.02680068276822567\n",
            "step: 190, loss: 0.04614294692873955\n",
            "step: 200, loss: 0.27687788009643555\n",
            "step: 210, loss: 0.0353095717728138\n",
            "step: 220, loss: 0.002431064611300826\n",
            "step: 230, loss: 0.08076395094394684\n",
            "step: 240, loss: 0.061559613794088364\n",
            "step: 250, loss: 0.06734943389892578\n",
            "step: 260, loss: 0.17093220353126526\n",
            "step: 270, loss: 0.02478877268731594\n",
            "step: 280, loss: 0.07643566280603409\n",
            "step: 290, loss: 0.056656207889318466\n",
            "step: 300, loss: 0.13572561740875244\n",
            "step: 310, loss: 0.1088688001036644\n",
            "step: 320, loss: 0.17212608456611633\n",
            "step: 330, loss: 0.10441456735134125\n",
            "step: 340, loss: 0.20957203209400177\n",
            "step: 350, loss: 0.041226014494895935\n",
            "step: 360, loss: 0.061398621648550034\n",
            "step: 370, loss: 0.005252487026154995\n",
            "step: 380, loss: 0.19427160918712616\n",
            "step: 390, loss: 0.04802774265408516\n",
            "step: 400, loss: 0.04003090411424637\n",
            "step: 410, loss: 0.061395008116960526\n",
            "step: 420, loss: 0.038138967007398605\n",
            "step: 430, loss: 0.16709594428539276\n",
            "step: 440, loss: 0.01975214295089245\n",
            "step: 450, loss: 0.09016364067792892\n",
            "step: 460, loss: 0.13600437343120575\n",
            "step: 470, loss: 0.1453266441822052\n",
            "step: 480, loss: 0.03592713922262192\n",
            "step: 490, loss: 0.06781899929046631\n",
            "step: 500, loss: 0.005575215443968773\n",
            "step: 510, loss: 0.04542222619056702\n",
            "step: 520, loss: 0.22093677520751953\n",
            "step: 530, loss: 0.049904439598321915\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9219924812030075, f1=0.9171736078614879, best_f1=0.9171736078614879\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11910157650709152\n",
            "step: 10, loss: 0.0339091420173645\n",
            "step: 20, loss: 0.01439501903951168\n",
            "step: 30, loss: 0.07318463921546936\n",
            "step: 40, loss: 0.09564206004142761\n",
            "step: 50, loss: 0.03787769749760628\n",
            "step: 60, loss: 0.13253434002399445\n",
            "step: 70, loss: 0.020344141870737076\n",
            "step: 80, loss: 0.07863129675388336\n",
            "step: 90, loss: 0.01702469028532505\n",
            "step: 100, loss: 0.14695514738559723\n",
            "step: 110, loss: 0.03868431970477104\n",
            "step: 120, loss: 0.06557601690292358\n",
            "step: 130, loss: 0.07108256965875626\n",
            "step: 140, loss: 0.003160880645737052\n",
            "step: 150, loss: 0.04167594015598297\n",
            "step: 160, loss: 0.0722230076789856\n",
            "step: 170, loss: 0.03721383586525917\n",
            "step: 180, loss: 0.033764760941267014\n",
            "step: 190, loss: 0.04642583429813385\n",
            "step: 200, loss: 0.060700833797454834\n",
            "step: 210, loss: 0.057683784514665604\n",
            "step: 220, loss: 0.04997160658240318\n",
            "step: 230, loss: 0.05163269862532616\n",
            "step: 240, loss: 0.055201176553964615\n",
            "step: 250, loss: 0.18741309642791748\n",
            "step: 260, loss: 0.036150284111499786\n",
            "step: 270, loss: 0.009572066366672516\n",
            "step: 280, loss: 0.00585085479542613\n",
            "step: 290, loss: 0.021002132445573807\n",
            "step: 300, loss: 0.23198045790195465\n",
            "step: 310, loss: 0.08449673652648926\n",
            "step: 320, loss: 0.026759102940559387\n",
            "step: 330, loss: 0.022869639098644257\n",
            "step: 340, loss: 0.03619345650076866\n",
            "step: 350, loss: 0.1056692972779274\n",
            "step: 360, loss: 0.0160038024187088\n",
            "step: 370, loss: 0.16831931471824646\n",
            "step: 380, loss: 0.10534704476594925\n",
            "step: 390, loss: 0.032480206340551376\n",
            "step: 400, loss: 0.07414699345827103\n",
            "step: 410, loss: 0.023486468940973282\n",
            "step: 420, loss: 0.024154437705874443\n",
            "step: 430, loss: 0.09122572094202042\n",
            "step: 440, loss: 0.16772383451461792\n",
            "step: 450, loss: 0.05807707831263542\n",
            "step: 460, loss: 0.08154414594173431\n",
            "step: 470, loss: 0.011216538026928902\n",
            "step: 480, loss: 0.11362702399492264\n",
            "step: 490, loss: 0.029095645993947983\n",
            "step: 500, loss: 0.020356081426143646\n",
            "step: 510, loss: 0.06506579369306564\n",
            "step: 520, loss: 0.05716366693377495\n",
            "step: 530, loss: 0.02398582361638546\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9221501390176089, f1=0.9044117647058824, best_f1=0.9044117647058824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06410125643014908\n",
            "step: 10, loss: 0.006770572159439325\n",
            "step: 20, loss: 0.06706070154905319\n",
            "step: 30, loss: 0.007599192205816507\n",
            "step: 40, loss: 0.0045409733429551125\n",
            "step: 50, loss: 0.010142285376787186\n",
            "step: 60, loss: 0.00797477550804615\n",
            "step: 70, loss: 0.08550787717103958\n",
            "step: 80, loss: 0.030547192320227623\n",
            "step: 90, loss: 0.09708571434020996\n",
            "step: 100, loss: 0.001549137057736516\n",
            "step: 110, loss: 0.04689177870750427\n",
            "step: 120, loss: 0.004674754571169615\n",
            "step: 130, loss: 0.031707219779491425\n",
            "step: 140, loss: 0.08566943556070328\n",
            "step: 150, loss: 0.11685473471879959\n",
            "step: 160, loss: 0.029976096004247665\n",
            "step: 170, loss: 0.01684706285595894\n",
            "step: 180, loss: 0.05042900890111923\n",
            "step: 190, loss: 0.0907582938671112\n",
            "step: 200, loss: 0.044201262295246124\n",
            "step: 210, loss: 0.014488345943391323\n",
            "step: 220, loss: 0.05340532585978508\n",
            "step: 230, loss: 0.00973384641110897\n",
            "step: 240, loss: 0.007633610628545284\n",
            "step: 250, loss: 0.12381824851036072\n",
            "step: 260, loss: 0.02460431307554245\n",
            "step: 270, loss: 0.046443697065114975\n",
            "step: 280, loss: 0.02788027748465538\n",
            "step: 290, loss: 0.010303042829036713\n",
            "step: 300, loss: 0.008315439336001873\n",
            "step: 310, loss: 0.002794332103803754\n",
            "step: 320, loss: 0.20799019932746887\n",
            "step: 330, loss: 0.014779730699956417\n",
            "step: 340, loss: 0.010596890933811665\n",
            "step: 350, loss: 0.07892747223377228\n",
            "step: 360, loss: 0.0655096173286438\n",
            "step: 370, loss: 0.013322332873940468\n",
            "step: 380, loss: 0.006943321321159601\n",
            "step: 390, loss: 0.007733731996268034\n",
            "step: 400, loss: 0.03835929557681084\n",
            "step: 410, loss: 0.13153104484081268\n",
            "step: 420, loss: 0.0045766946859657764\n",
            "step: 430, loss: 0.02405104972422123\n",
            "step: 440, loss: 0.015138121321797371\n",
            "step: 450, loss: 0.07332035154104233\n",
            "step: 460, loss: 0.12692587077617645\n",
            "step: 470, loss: 0.003769254544749856\n",
            "step: 480, loss: 0.0035911768209189177\n",
            "step: 490, loss: 0.10004901885986328\n",
            "step: 500, loss: 0.11823298037052155\n",
            "step: 510, loss: 0.02427057735621929\n",
            "step: 520, loss: 0.004001827910542488\n",
            "step: 530, loss: 0.03770942986011505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9293577981651376, f1=0.9250457038391224, best_f1=0.9250457038391224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003038073657080531\n",
            "step: 10, loss: 0.07143709063529968\n",
            "step: 20, loss: 0.025701604783535004\n",
            "step: 30, loss: 0.0928143560886383\n",
            "step: 40, loss: 0.009304207749664783\n",
            "step: 50, loss: 0.04476756602525711\n",
            "step: 60, loss: 0.10114248842000961\n",
            "step: 70, loss: 0.017512034624814987\n",
            "step: 80, loss: 0.0019089921843260527\n",
            "step: 90, loss: 0.0024996509309858084\n",
            "step: 100, loss: 0.08384035527706146\n",
            "step: 110, loss: 0.0026897662319242954\n",
            "step: 120, loss: 0.17986588180065155\n",
            "step: 130, loss: 0.024313444271683693\n",
            "step: 140, loss: 0.05985192954540253\n",
            "step: 150, loss: 0.018312308937311172\n",
            "step: 160, loss: 0.07584203034639359\n",
            "step: 170, loss: 0.04349120706319809\n",
            "step: 180, loss: 0.009843642823398113\n",
            "step: 190, loss: 0.009072426706552505\n",
            "step: 200, loss: 0.003558869007974863\n",
            "step: 210, loss: 0.025682728737592697\n",
            "step: 220, loss: 0.036200232803821564\n",
            "step: 230, loss: 0.04872851446270943\n",
            "step: 240, loss: 0.00720701040700078\n",
            "step: 250, loss: 0.13156980276107788\n",
            "step: 260, loss: 0.002026609843596816\n",
            "step: 270, loss: 0.009641983546316624\n",
            "step: 280, loss: 0.0158229973167181\n",
            "step: 290, loss: 0.0449296273291111\n",
            "step: 300, loss: 0.15182174742221832\n",
            "step: 310, loss: 0.01315772533416748\n",
            "step: 320, loss: 0.026125309988856316\n",
            "step: 330, loss: 0.0013779832515865564\n",
            "step: 340, loss: 0.0022629695013165474\n",
            "step: 350, loss: 0.1163230836391449\n",
            "step: 360, loss: 0.010776743292808533\n",
            "step: 370, loss: 0.0207385066896677\n",
            "step: 380, loss: 0.00191060162615031\n",
            "step: 390, loss: 0.10117201507091522\n",
            "step: 400, loss: 0.02471182309091091\n",
            "step: 410, loss: 0.2065262794494629\n",
            "step: 420, loss: 0.04735656827688217\n",
            "step: 430, loss: 0.06014274060726166\n",
            "step: 440, loss: 0.0027223071083426476\n",
            "step: 450, loss: 0.011693427339196205\n",
            "step: 460, loss: 0.0264419075101614\n",
            "step: 470, loss: 0.00409238925203681\n",
            "step: 480, loss: 0.011354085057973862\n",
            "step: 490, loss: 0.042954180389642715\n",
            "step: 500, loss: 0.012186449021100998\n",
            "step: 510, loss: 0.0024094947148114443\n",
            "step: 520, loss: 0.21046127378940582\n",
            "step: 530, loss: 0.005172154866158962\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9291412482402628, f1=0.9255813953488372, best_f1=0.9250457038391224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01074245199561119\n",
            "step: 10, loss: 0.0025607554707676172\n",
            "step: 20, loss: 0.006844340357929468\n",
            "step: 30, loss: 0.001063216826878488\n",
            "step: 40, loss: 0.004278147593140602\n",
            "step: 50, loss: 0.001229635439813137\n",
            "step: 60, loss: 0.01995936594903469\n",
            "step: 70, loss: 0.0008386190747842193\n",
            "step: 80, loss: 0.010272343643009663\n",
            "step: 90, loss: 0.001753800199367106\n",
            "step: 100, loss: 0.21992714703083038\n",
            "step: 110, loss: 0.006425416562706232\n",
            "step: 120, loss: 0.01178034022450447\n",
            "step: 130, loss: 0.005073246080428362\n",
            "step: 140, loss: 0.0026943490374833345\n",
            "step: 150, loss: 0.0009849839843809605\n",
            "step: 160, loss: 0.024984147399663925\n",
            "step: 170, loss: 0.015144598670303822\n",
            "step: 180, loss: 0.0016348648350685835\n",
            "step: 190, loss: 0.005041792057454586\n",
            "step: 200, loss: 0.04822163283824921\n",
            "step: 210, loss: 0.00806631799787283\n",
            "step: 220, loss: 0.002214849693700671\n",
            "step: 230, loss: 0.01737535372376442\n",
            "step: 240, loss: 0.005165571346879005\n",
            "step: 250, loss: 0.12520739436149597\n",
            "step: 260, loss: 0.005955324042588472\n",
            "step: 270, loss: 0.0010787987848743796\n",
            "step: 280, loss: 0.002759713213890791\n",
            "step: 290, loss: 0.02851695753633976\n",
            "step: 300, loss: 0.003748063463717699\n",
            "step: 310, loss: 0.05909484624862671\n",
            "step: 320, loss: 0.0004737502313219011\n",
            "step: 330, loss: 0.020778784528374672\n",
            "step: 340, loss: 0.0028225164860486984\n",
            "step: 350, loss: 0.0029675087425857782\n",
            "step: 360, loss: 0.004942090250551701\n",
            "step: 370, loss: 0.003190811024978757\n",
            "step: 380, loss: 0.0028619710355997086\n",
            "step: 390, loss: 0.015389896929264069\n",
            "step: 400, loss: 0.02219116874039173\n",
            "step: 410, loss: 0.0037799563724547625\n",
            "step: 420, loss: 0.033099498599767685\n",
            "step: 430, loss: 0.056777093559503555\n",
            "step: 440, loss: 0.00031104523804970086\n",
            "step: 450, loss: 0.12157014012336731\n",
            "step: 460, loss: 0.0023472988978028297\n",
            "step: 470, loss: 0.009580710902810097\n",
            "step: 480, loss: 0.005318242125213146\n",
            "step: 490, loss: 0.1491769254207611\n",
            "step: 500, loss: 0.010693352669477463\n",
            "step: 510, loss: 0.24714191257953644\n",
            "step: 520, loss: 0.0012973365373909473\n",
            "step: 530, loss: 0.02101433277130127\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.936249418334109, f1=0.926012098650535, best_f1=0.926012098650535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013532016426324844\n",
            "step: 10, loss: 0.008267749100923538\n",
            "step: 20, loss: 0.00472627766430378\n",
            "step: 30, loss: 0.009186208248138428\n",
            "step: 40, loss: 0.03653456270694733\n",
            "step: 50, loss: 0.0037722066044807434\n",
            "step: 60, loss: 0.0011939076939597726\n",
            "step: 70, loss: 0.03899817541241646\n",
            "step: 80, loss: 0.0007500685169361532\n",
            "step: 90, loss: 0.000448198348749429\n",
            "step: 100, loss: 0.013616614043712616\n",
            "step: 110, loss: 0.0005590365617536008\n",
            "step: 120, loss: 0.0016974236350506544\n",
            "step: 130, loss: 0.0005387827986851335\n",
            "step: 140, loss: 0.011455214582383633\n",
            "step: 150, loss: 0.00021555197599809617\n",
            "step: 160, loss: 0.0005550140049308538\n",
            "step: 170, loss: 0.0012459468562155962\n",
            "step: 180, loss: 0.001834275433793664\n",
            "step: 190, loss: 0.006786706857383251\n",
            "step: 200, loss: 0.0008856943459250033\n",
            "step: 210, loss: 0.05106797814369202\n",
            "step: 220, loss: 0.022790241986513138\n",
            "step: 230, loss: 0.01267692819237709\n",
            "step: 240, loss: 0.033734843134880066\n",
            "step: 250, loss: 0.018189413473010063\n",
            "step: 260, loss: 0.0008867944125086069\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 270, loss: 0.0012373790377750993\n",
            "step: 280, loss: 0.16415612399578094\n",
            "step: 290, loss: 0.05092954263091087\n",
            "step: 300, loss: 0.0015770135214552283\n",
            "step: 310, loss: 0.004822412505745888\n",
            "step: 320, loss: 0.004654985386878252\n",
            "step: 330, loss: 0.13499537110328674\n",
            "step: 340, loss: 0.019856184720993042\n",
            "step: 350, loss: 0.012394482269883156\n",
            "step: 360, loss: 0.0007632781635038555\n",
            "step: 370, loss: 0.004821042064577341\n",
            "step: 380, loss: 0.0021114007104188204\n",
            "step: 390, loss: 0.007216375321149826\n",
            "step: 400, loss: 0.09364328533411026\n",
            "step: 410, loss: 0.0003110429388470948\n",
            "step: 420, loss: 0.031645942479372025\n",
            "step: 430, loss: 0.0049869897775352\n",
            "step: 440, loss: 0.007007793057709932\n",
            "step: 450, loss: 0.03342703357338905\n",
            "step: 460, loss: 0.008003365248441696\n",
            "step: 470, loss: 0.13267140090465546\n",
            "step: 480, loss: 0.0025950309354811907\n",
            "step: 490, loss: 0.0012425676686689258\n",
            "step: 500, loss: 0.008805044926702976\n",
            "step: 510, loss: 0.0029744955245405436\n",
            "step: 520, loss: 0.004792392253875732\n",
            "step: 530, loss: 0.002663536462932825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9341317365269461, f1=0.9238578680203047, best_f1=0.926012098650535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10251633822917938\n",
            "step: 10, loss: 0.0018987864023074508\n",
            "step: 20, loss: 0.0009049175423569977\n",
            "step: 30, loss: 0.0015215068124234676\n",
            "step: 40, loss: 0.00041211259667761624\n",
            "step: 50, loss: 0.010256236419081688\n",
            "step: 60, loss: 0.0007983961259014904\n",
            "step: 70, loss: 0.0007842948543839157\n",
            "step: 80, loss: 0.08442802727222443\n",
            "step: 90, loss: 0.00243221502751112\n",
            "step: 100, loss: 0.020285731181502342\n",
            "step: 110, loss: 0.0003554256691131741\n",
            "step: 120, loss: 0.003471873700618744\n",
            "step: 130, loss: 0.0003454037068877369\n",
            "step: 140, loss: 0.0037050163373351097\n",
            "step: 150, loss: 0.0005379949579946697\n",
            "step: 160, loss: 0.012787909246981144\n",
            "step: 170, loss: 0.08206281065940857\n",
            "step: 180, loss: 0.008695210330188274\n",
            "step: 190, loss: 0.08162462711334229\n",
            "step: 200, loss: 0.00307994126342237\n",
            "step: 210, loss: 0.04374394565820694\n",
            "step: 220, loss: 0.0044361078180372715\n",
            "step: 230, loss: 0.0009545075590722263\n",
            "step: 240, loss: 0.024090319871902466\n",
            "step: 250, loss: 0.00027645655791275203\n",
            "step: 260, loss: 0.00048602320021018386\n",
            "step: 270, loss: 0.11893031001091003\n",
            "step: 280, loss: 0.005001573823392391\n",
            "step: 290, loss: 0.05667665973305702\n",
            "step: 300, loss: 0.0006684773834422231\n",
            "step: 310, loss: 0.001207932480610907\n",
            "step: 320, loss: 0.001677391934208572\n",
            "step: 330, loss: 0.0010638624662533402\n",
            "step: 340, loss: 0.007252648007124662\n",
            "step: 350, loss: 0.0009446752374060452\n",
            "step: 360, loss: 0.0021107823122292757\n",
            "step: 370, loss: 0.3379226326942444\n",
            "step: 380, loss: 0.0019549785647541285\n",
            "step: 390, loss: 0.0038065172266215086\n",
            "step: 400, loss: 0.001132859499193728\n",
            "step: 410, loss: 0.004183936398476362\n",
            "step: 420, loss: 0.00784701481461525\n",
            "step: 430, loss: 0.0017089064931496978\n",
            "step: 440, loss: 0.005389206111431122\n",
            "step: 450, loss: 0.0027500446885824203\n",
            "step: 460, loss: 0.00708260340616107\n",
            "step: 470, loss: 0.02156652882695198\n",
            "step: 480, loss: 0.00907844491302967\n",
            "step: 490, loss: 0.0017340043559670448\n",
            "step: 500, loss: 0.005656599532812834\n",
            "step: 510, loss: 0.05272187665104866\n",
            "step: 520, loss: 0.0017019484657794237\n",
            "step: 530, loss: 0.0079165855422616\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9311141932501156, f1=0.9301895515487749, best_f1=0.926012098650535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.360196418128908e-05\n",
            "step: 10, loss: 0.001448913593776524\n",
            "step: 20, loss: 0.01866241730749607\n",
            "step: 30, loss: 0.05329504236578941\n",
            "step: 40, loss: 0.0005037139053456485\n",
            "step: 50, loss: 0.004001085180789232\n",
            "step: 60, loss: 0.00048413011245429516\n",
            "step: 70, loss: 0.00021912208467256278\n",
            "step: 80, loss: 0.03998805955052376\n",
            "step: 90, loss: 0.0006928802467882633\n",
            "step: 100, loss: 0.0070956978015601635\n",
            "step: 110, loss: 0.04452894255518913\n",
            "step: 120, loss: 0.0003042797325178981\n",
            "step: 130, loss: 0.000860055792145431\n",
            "step: 140, loss: 0.0006911581149324775\n",
            "step: 150, loss: 0.05041806399822235\n",
            "step: 160, loss: 0.03286856412887573\n",
            "step: 170, loss: 0.005993454717099667\n",
            "step: 180, loss: 0.033293452113866806\n",
            "step: 190, loss: 0.00012009111378574744\n",
            "step: 200, loss: 0.0003186861576978117\n",
            "step: 210, loss: 0.21579857170581818\n",
            "step: 220, loss: 0.004441088531166315\n",
            "step: 230, loss: 0.000939554360229522\n",
            "step: 240, loss: 0.0005486804293468595\n",
            "step: 250, loss: 0.012025812640786171\n",
            "step: 260, loss: 0.00033107836497947574\n",
            "step: 270, loss: 0.0007033527945168316\n",
            "step: 280, loss: 0.0019039945909753442\n",
            "step: 290, loss: 0.00032945440034382045\n",
            "step: 300, loss: 0.0008935061050578952\n",
            "step: 310, loss: 0.036860208958387375\n",
            "step: 320, loss: 0.0009992364794015884\n",
            "step: 330, loss: 0.0036589426454156637\n",
            "step: 340, loss: 0.0029846022371202707\n",
            "step: 350, loss: 0.01778321899473667\n",
            "step: 360, loss: 0.015044387429952621\n",
            "step: 370, loss: 0.004939831793308258\n",
            "step: 380, loss: 0.0005113205406814814\n",
            "step: 390, loss: 0.000595874444115907\n",
            "step: 400, loss: 0.0021195935551077127\n",
            "step: 410, loss: 0.00040412115049548447\n",
            "step: 420, loss: 0.0006268964498303831\n",
            "step: 430, loss: 0.0011675709392875433\n",
            "step: 440, loss: 0.00017123576253652573\n",
            "step: 450, loss: 0.13574114441871643\n",
            "step: 460, loss: 0.04315014183521271\n",
            "step: 470, loss: 0.012213274836540222\n",
            "step: 480, loss: 0.000214169587707147\n",
            "step: 490, loss: 0.012577357701957226\n",
            "step: 500, loss: 0.0010820586467161775\n",
            "step: 510, loss: 0.013289852067828178\n",
            "step: 520, loss: 0.002276989398524165\n",
            "step: 530, loss: 0.13666768372058868\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9332706766917293, f1=0.9296435272045028, best_f1=0.926012098650535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002233044244349003\n",
            "step: 10, loss: 0.0006585845840163529\n",
            "step: 20, loss: 0.0014817247865721583\n",
            "step: 30, loss: 0.003936532884836197\n",
            "step: 40, loss: 0.0002843996917363256\n",
            "step: 50, loss: 0.0004174987552687526\n",
            "step: 60, loss: 0.00608689384534955\n",
            "step: 70, loss: 0.00032161775743588805\n",
            "step: 80, loss: 0.0009380432311445475\n",
            "step: 90, loss: 0.002517521847039461\n",
            "step: 100, loss: 0.0008449433371424675\n",
            "step: 110, loss: 0.0009525961359031498\n",
            "step: 120, loss: 8.161891310010105e-05\n",
            "step: 130, loss: 0.044820960611104965\n",
            "step: 140, loss: 0.0007165326969698071\n",
            "step: 150, loss: 0.03168732672929764\n",
            "step: 160, loss: 0.006255232263356447\n",
            "step: 170, loss: 0.0025728829205036163\n",
            "step: 180, loss: 0.0013315402902662754\n",
            "step: 190, loss: 9.25147978705354e-05\n",
            "step: 200, loss: 0.0005692057893611491\n",
            "step: 210, loss: 0.00011691252439050004\n",
            "step: 220, loss: 0.00027528367354534566\n",
            "step: 230, loss: 0.0001556008355692029\n",
            "step: 240, loss: 0.0004044604138471186\n",
            "step: 250, loss: 0.006842257920652628\n",
            "step: 260, loss: 0.003219913924112916\n",
            "step: 270, loss: 0.0009478587890043855\n",
            "step: 280, loss: 0.01679692044854164\n",
            "step: 290, loss: 0.00993403885513544\n",
            "step: 300, loss: 0.07509253174066544\n",
            "step: 310, loss: 0.16540805995464325\n",
            "step: 320, loss: 0.0012295753695070744\n",
            "step: 330, loss: 0.07475519180297852\n",
            "step: 340, loss: 0.0025294432416558266\n",
            "step: 350, loss: 0.006960299331694841\n",
            "step: 360, loss: 0.0004451801360119134\n",
            "step: 370, loss: 0.008568075485527515\n",
            "step: 380, loss: 0.000937555218115449\n",
            "step: 390, loss: 0.00024157523876056075\n",
            "step: 400, loss: 0.0007827997324056923\n",
            "step: 410, loss: 0.01085786521434784\n",
            "step: 420, loss: 0.0002636596036609262\n",
            "step: 430, loss: 0.0031182426027953625\n",
            "step: 440, loss: 0.00023377645993605256\n",
            "step: 450, loss: 0.059627484530210495\n",
            "step: 460, loss: 0.00015435484237968922\n",
            "step: 470, loss: 0.0002512590435799211\n",
            "step: 480, loss: 0.0017431197920814157\n",
            "step: 490, loss: 0.007740362547338009\n",
            "step: 500, loss: 0.05410348251461983\n",
            "step: 510, loss: 0.00012732816685456783\n",
            "step: 520, loss: 0.048080217093229294\n",
            "step: 530, loss: 0.003105292795225978\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9374143444495202, f1=0.9265845873233014, best_f1=0.9265845873233014\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014573815860785544\n",
            "step: 10, loss: 0.00018986838404089212\n",
            "step: 20, loss: 0.0005102744325995445\n",
            "step: 30, loss: 0.0011212604586035013\n",
            "step: 40, loss: 9.526668873149902e-05\n",
            "step: 50, loss: 0.0001429775875294581\n",
            "step: 60, loss: 0.0002579215506557375\n",
            "step: 70, loss: 0.0007777144201099873\n",
            "step: 80, loss: 0.0006349141476675868\n",
            "step: 90, loss: 0.002176721580326557\n",
            "step: 100, loss: 0.00012796904775314033\n",
            "step: 110, loss: 0.0001221046841237694\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.0018132012337446213\n",
            "step: 130, loss: 0.00020621228031814098\n",
            "step: 140, loss: 0.00014624203322455287\n",
            "step: 150, loss: 0.1036423072218895\n",
            "step: 160, loss: 0.03203440085053444\n",
            "step: 170, loss: 0.005892599932849407\n",
            "step: 180, loss: 7.146002462832257e-05\n",
            "step: 190, loss: 0.0035539132077246904\n",
            "step: 200, loss: 0.00010932824807241559\n",
            "step: 210, loss: 0.0016706384485587478\n",
            "step: 220, loss: 0.00045010101166553795\n",
            "step: 230, loss: 0.0018901810981333256\n",
            "step: 240, loss: 0.007278529927134514\n",
            "step: 250, loss: 0.0022736890241503716\n",
            "step: 260, loss: 0.0003669720608741045\n",
            "step: 270, loss: 0.0010146201821044087\n",
            "step: 280, loss: 0.0007348342915065587\n",
            "step: 290, loss: 6.830389611423016e-05\n",
            "step: 300, loss: 6.655407923972234e-05\n",
            "step: 310, loss: 0.00045117721310816705\n",
            "step: 320, loss: 0.0001350288075627759\n",
            "step: 330, loss: 0.0003500218444969505\n",
            "step: 340, loss: 0.006718043703585863\n",
            "step: 350, loss: 9.724259143695235e-05\n",
            "step: 360, loss: 0.0002671251422725618\n",
            "step: 370, loss: 0.00024880186538212\n",
            "step: 380, loss: 0.023838786408305168\n",
            "step: 390, loss: 0.00027955856057815254\n",
            "step: 400, loss: 0.000160664101713337\n",
            "step: 410, loss: 0.00011697543232003227\n",
            "step: 420, loss: 0.0038683153688907623\n",
            "step: 430, loss: 0.0002030300092883408\n",
            "step: 440, loss: 0.00033180328318849206\n",
            "step: 450, loss: 0.0012222277000546455\n",
            "step: 460, loss: 0.0004109964065719396\n",
            "step: 470, loss: 0.0002985477331094444\n",
            "step: 480, loss: 0.0007124762050807476\n",
            "step: 490, loss: 0.00020493954070843756\n",
            "step: 500, loss: 0.0004403766943141818\n",
            "step: 510, loss: 0.0013120429357513785\n",
            "step: 520, loss: 0.0013671047054231167\n",
            "step: 530, loss: 0.005624587647616863\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9317865429234339, f1=0.9275092936802973, best_f1=0.9265845873233014\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00023770674306433648\n",
            "step: 10, loss: 0.002088862005621195\n",
            "step: 20, loss: 0.0011597013799473643\n",
            "step: 30, loss: 0.0013171113096177578\n",
            "step: 40, loss: 0.0030573552940040827\n",
            "step: 50, loss: 0.00027022062567994\n",
            "step: 60, loss: 0.00028604999533854425\n",
            "step: 70, loss: 0.004868556745350361\n",
            "step: 80, loss: 0.21745851635932922\n",
            "step: 90, loss: 0.009377523325383663\n",
            "step: 100, loss: 0.003962954506278038\n",
            "step: 110, loss: 0.0006088009686209261\n",
            "step: 120, loss: 0.0007992405444383621\n",
            "step: 130, loss: 0.0008001121459528804\n",
            "step: 140, loss: 0.0003970330872107297\n",
            "step: 150, loss: 0.02950785495340824\n",
            "step: 160, loss: 0.004720507655292749\n",
            "step: 170, loss: 0.000967629486694932\n",
            "step: 180, loss: 0.0007756820996291935\n",
            "step: 190, loss: 0.0008700520265847445\n",
            "step: 200, loss: 0.000210317550227046\n",
            "step: 210, loss: 0.2134658396244049\n",
            "step: 220, loss: 0.0007023180951364338\n",
            "step: 230, loss: 0.009485973045229912\n",
            "step: 240, loss: 0.0008545981254428625\n",
            "step: 250, loss: 0.0003531741094775498\n",
            "step: 260, loss: 0.0002708166721276939\n",
            "step: 270, loss: 0.0005475524230860174\n",
            "step: 280, loss: 0.0043219225481152534\n",
            "step: 290, loss: 0.00021415407536551356\n",
            "step: 300, loss: 0.0005435977363958955\n",
            "step: 310, loss: 0.0002264254871988669\n",
            "step: 320, loss: 0.0018783927662298083\n",
            "step: 330, loss: 0.012347886338829994\n",
            "step: 340, loss: 0.00011803188681369647\n",
            "step: 350, loss: 9.811102063395083e-05\n",
            "step: 360, loss: 0.00013735379616264254\n",
            "step: 370, loss: 9.560152102494612e-05\n",
            "step: 380, loss: 7.930156425572932e-05\n",
            "step: 390, loss: 0.0033766361884772778\n",
            "step: 400, loss: 0.00011506272858241573\n",
            "step: 410, loss: 0.0011841236846521497\n",
            "step: 420, loss: 0.0003844845632556826\n",
            "step: 430, loss: 9.327563020633534e-05\n",
            "step: 440, loss: 0.00023947504814714193\n",
            "step: 450, loss: 0.0018828802276402712\n",
            "step: 460, loss: 0.0003061036695726216\n",
            "step: 470, loss: 0.19544294476509094\n",
            "step: 480, loss: 0.0002683417114894837\n",
            "step: 490, loss: 0.0018138671293854713\n",
            "step: 500, loss: 0.000269322597887367\n",
            "step: 510, loss: 0.01566426083445549\n",
            "step: 520, loss: 0.0002592570090200752\n",
            "step: 530, loss: 0.00044730110676027834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9388322520852641, f1=0.9249999999999999, best_f1=0.9249999999999999\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006235329783521593\n",
            "step: 10, loss: 0.0001290192740270868\n",
            "step: 20, loss: 0.00020326831145212054\n",
            "step: 30, loss: 0.0016332182567566633\n",
            "step: 40, loss: 0.0008513341890648007\n",
            "step: 50, loss: 0.0002372766175540164\n",
            "step: 60, loss: 0.0005186967318877578\n",
            "step: 70, loss: 0.00020238077559042722\n",
            "step: 80, loss: 0.0004173356282990426\n",
            "step: 90, loss: 0.0005922869895584881\n",
            "step: 100, loss: 0.00039874581852927804\n",
            "step: 110, loss: 0.0001144652851507999\n",
            "step: 120, loss: 0.00019314035307615995\n",
            "step: 130, loss: 0.0001900486822705716\n",
            "step: 140, loss: 0.00023407266417052597\n",
            "step: 150, loss: 0.0002932953939307481\n",
            "step: 160, loss: 0.0007534656906500459\n",
            "step: 170, loss: 0.0009350960026495159\n",
            "step: 180, loss: 0.000229288125410676\n",
            "step: 190, loss: 0.000176691755768843\n",
            "step: 200, loss: 0.00017433628090657294\n",
            "step: 210, loss: 0.000383699283702299\n",
            "step: 220, loss: 0.00022798575810156763\n",
            "step: 230, loss: 0.00036324639222584665\n",
            "step: 240, loss: 0.0019160595256835222\n",
            "step: 250, loss: 0.0002341000799788162\n",
            "step: 260, loss: 0.005296738818287849\n",
            "step: 270, loss: 0.00014560521231032908\n",
            "step: 280, loss: 0.00012376521772239357\n",
            "step: 290, loss: 0.0002380291116423905\n",
            "step: 300, loss: 0.011856806464493275\n",
            "step: 310, loss: 0.00021785467106383294\n",
            "step: 320, loss: 0.0008445764542557299\n",
            "step: 330, loss: 0.005427011288702488\n",
            "step: 340, loss: 0.0008166056359186769\n",
            "step: 350, loss: 9.491860691923648e-05\n",
            "step: 360, loss: 0.18055446445941925\n",
            "step: 370, loss: 0.00016052761930041015\n",
            "step: 380, loss: 0.0003646781842689961\n",
            "step: 390, loss: 0.0007854369468986988\n",
            "step: 400, loss: 0.0011666918871924281\n",
            "step: 410, loss: 0.0002533879305701703\n",
            "step: 420, loss: 0.0020215415861457586\n",
            "step: 430, loss: 0.0006112311384640634\n",
            "step: 440, loss: 0.00010240777191938832\n",
            "step: 450, loss: 0.00023338156461250037\n",
            "step: 460, loss: 0.0007084052776917815\n",
            "step: 470, loss: 0.0006368159083649516\n",
            "step: 480, loss: 6.53291936032474e-05\n",
            "step: 490, loss: 0.0003329381288494915\n",
            "step: 500, loss: 7.679233385715634e-05\n",
            "step: 510, loss: 0.00015288320719264448\n",
            "step: 520, loss: 0.00022909812105353922\n",
            "step: 530, loss: 0.0001130021337303333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9362101313320826, f1=0.9260299625468165, best_f1=0.9249999999999999\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014158028352539986\n",
            "step: 10, loss: 0.0001469468988943845\n",
            "step: 20, loss: 0.0003451584780123085\n",
            "step: 30, loss: 0.000182838921318762\n",
            "step: 40, loss: 0.00022610688756685704\n",
            "step: 50, loss: 0.00021711159206461161\n",
            "step: 60, loss: 0.00020830084395129234\n",
            "step: 70, loss: 0.016544196754693985\n",
            "step: 80, loss: 0.0002395837800577283\n",
            "step: 90, loss: 0.00010191415640292689\n",
            "step: 100, loss: 0.00033634461578913033\n",
            "step: 110, loss: 0.0003426242037676275\n",
            "step: 120, loss: 0.00016032136045396328\n",
            "step: 130, loss: 0.00013894654694013298\n",
            "step: 140, loss: 0.0006722360267303884\n",
            "step: 150, loss: 0.0002305235539097339\n",
            "step: 160, loss: 0.0002525142044760287\n",
            "step: 170, loss: 0.0016920791240409017\n",
            "step: 180, loss: 0.00018870811618398875\n",
            "step: 190, loss: 0.0014142110012471676\n",
            "step: 200, loss: 0.0001960785302799195\n",
            "step: 210, loss: 0.0014376454055309296\n",
            "step: 220, loss: 9.686304110800847e-05\n",
            "step: 230, loss: 0.0001399132888764143\n",
            "step: 240, loss: 0.00027770467568188906\n",
            "step: 250, loss: 0.0017045291606336832\n",
            "step: 260, loss: 0.00014830603322479874\n",
            "step: 270, loss: 0.00018181526684202254\n",
            "step: 280, loss: 0.00010143908730242401\n",
            "step: 290, loss: 6.0927348386030644e-05\n",
            "step: 300, loss: 0.00036627482040785253\n",
            "step: 310, loss: 5.810282891616225e-05\n",
            "step: 320, loss: 0.006063106469810009\n",
            "step: 330, loss: 0.0002881695982068777\n",
            "step: 340, loss: 0.0002984104794450104\n",
            "step: 350, loss: 9.992378909373656e-05\n",
            "step: 360, loss: 0.0002356457116547972\n",
            "step: 370, loss: 0.054162316024303436\n",
            "step: 380, loss: 0.00039020032272674143\n",
            "step: 390, loss: 0.0006341909756883979\n",
            "step: 400, loss: 0.0011924817226827145\n",
            "step: 410, loss: 7.343998004216701e-05\n",
            "step: 420, loss: 0.00013187369040679187\n",
            "step: 430, loss: 0.0005363732343539596\n",
            "step: 440, loss: 0.0006225177785381675\n",
            "step: 450, loss: 0.00032518056104891\n",
            "step: 460, loss: 0.001063942676410079\n",
            "step: 470, loss: 0.00044506846461445093\n",
            "step: 480, loss: 0.00014326354721561074\n",
            "step: 490, loss: 0.0002707403909880668\n",
            "step: 500, loss: 0.21864920854568481\n",
            "step: 510, loss: 0.00018940464360639453\n",
            "step: 520, loss: 0.00014847413694951683\n",
            "step: 530, loss: 0.000530216668266803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9382830626450116, f1=0.9302973977695168, best_f1=0.9249999999999999\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00015375838847830892\n",
            "step: 10, loss: 0.00028071634005755186\n",
            "step: 20, loss: 0.0010049989214166999\n",
            "step: 30, loss: 0.0022159989457577467\n",
            "step: 40, loss: 0.00025003316113725305\n",
            "step: 50, loss: 0.0002248992386739701\n",
            "step: 60, loss: 0.00032764492789283395\n",
            "step: 70, loss: 9.918045543599874e-05\n",
            "step: 80, loss: 0.00014947287854738533\n",
            "step: 90, loss: 0.000284438778180629\n",
            "step: 100, loss: 0.0001560462696943432\n",
            "step: 110, loss: 0.00023156352108344436\n",
            "step: 120, loss: 0.00020860157383140177\n",
            "step: 130, loss: 0.00011205138434888795\n",
            "step: 140, loss: 0.00010673079668777063\n",
            "step: 150, loss: 0.0001540264638606459\n",
            "step: 160, loss: 0.00014107627794146538\n",
            "step: 170, loss: 6.027829658705741e-05\n",
            "step: 180, loss: 0.002787384670227766\n",
            "step: 190, loss: 0.0010577145731076598\n",
            "step: 200, loss: 0.00014256716531235725\n",
            "step: 210, loss: 6.840316927991807e-05\n",
            "step: 220, loss: 0.0001467294932808727\n",
            "step: 230, loss: 0.22284908592700958\n",
            "step: 240, loss: 0.0001376079162582755\n",
            "step: 250, loss: 9.967236110242084e-05\n",
            "step: 260, loss: 0.00011854703916469589\n",
            "step: 270, loss: 0.000500204972922802\n",
            "step: 280, loss: 0.00011672227992676198\n",
            "step: 290, loss: 0.00025499192997813225\n",
            "step: 300, loss: 0.00010245169687550515\n",
            "step: 310, loss: 0.0001654848747421056\n",
            "step: 320, loss: 0.0001717273407848552\n",
            "step: 330, loss: 0.0012912803795188665\n",
            "step: 340, loss: 0.0001723509922157973\n",
            "step: 350, loss: 0.0004248374898452312\n",
            "step: 360, loss: 0.009961182251572609\n",
            "step: 370, loss: 0.00021143113553989679\n",
            "step: 380, loss: 0.00016231724293902516\n",
            "step: 390, loss: 0.0001577324146637693\n",
            "step: 400, loss: 0.0011097162496298552\n",
            "step: 410, loss: 0.0002619009464979172\n",
            "step: 420, loss: 0.00013699823466595262\n",
            "step: 430, loss: 8.010712917894125e-05\n",
            "step: 440, loss: 0.022492432966828346\n",
            "step: 450, loss: 0.0002730783016886562\n",
            "step: 460, loss: 0.00013810826931148767\n",
            "step: 470, loss: 0.0003547675732988864\n",
            "step: 480, loss: 0.0006968821980990469\n",
            "step: 490, loss: 0.0002753751177806407\n",
            "step: 500, loss: 0.010031540878117085\n",
            "step: 510, loss: 9.960844909073785e-05\n",
            "step: 520, loss: 0.00014234479749575257\n",
            "step: 530, loss: 0.00011953710054513067\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9381682938168294, f1=0.9297347603536529, best_f1=0.9249999999999999\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:22, 255.14it/s]\n",
            "load_f1 = 0.9362907031618688\n",
            "real_f1 = 0.9327058823529412\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 213.83it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqkZ1fXggw8C",
        "outputId": "9703736b-107b-4149-8aee-22f2a07dbead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 626kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 41.1MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 27.1MB/s]\n",
            "Downloading: 100% 501M/501M [00:08<00:00, 61.8MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4723604917526245\n",
            "step: 10, loss: 0.4304710030555725\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.43054988980293274\n",
            "step: 30, loss: 0.37666183710098267\n",
            "step: 40, loss: 0.3559582233428955\n",
            "step: 50, loss: 0.4335707724094391\n",
            "step: 60, loss: 0.5193862318992615\n",
            "step: 70, loss: 0.325621098279953\n",
            "step: 80, loss: 0.3900032937526703\n",
            "step: 90, loss: 0.3044467568397522\n",
            "step: 100, loss: 0.26332470774650574\n",
            "step: 110, loss: 0.2755528390407562\n",
            "step: 120, loss: 0.4257674217224121\n",
            "step: 130, loss: 0.22427153587341309\n",
            "step: 140, loss: 0.45159465074539185\n",
            "step: 150, loss: 0.394521564245224\n",
            "step: 160, loss: 0.4781840443611145\n",
            "step: 170, loss: 0.2379453331232071\n",
            "step: 180, loss: 0.3999466896057129\n",
            "step: 190, loss: 0.6294710636138916\n",
            "step: 200, loss: 0.37338340282440186\n",
            "step: 210, loss: 0.5120723247528076\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3682137131690979\n",
            "step: 10, loss: 0.19759532809257507\n",
            "step: 20, loss: 0.5242725610733032\n",
            "step: 30, loss: 0.4936849772930145\n",
            "step: 40, loss: 0.4614982306957245\n",
            "step: 50, loss: 0.23269827663898468\n",
            "step: 60, loss: 0.31792837381362915\n",
            "step: 70, loss: 0.43318405747413635\n",
            "step: 80, loss: 0.3166216015815735\n",
            "step: 90, loss: 0.376044899225235\n",
            "step: 100, loss: 0.525455892086029\n",
            "step: 110, loss: 0.4003477692604065\n",
            "step: 120, loss: 0.23708359897136688\n",
            "step: 130, loss: 0.16913147270679474\n",
            "step: 140, loss: 0.22191300988197327\n",
            "step: 150, loss: 0.42380237579345703\n",
            "step: 160, loss: 0.14513243734836578\n",
            "step: 170, loss: 0.5383549332618713\n",
            "step: 180, loss: 0.35108682513237\n",
            "step: 190, loss: 0.31859248876571655\n",
            "step: 200, loss: 0.13752807676792145\n",
            "step: 210, loss: 0.3411328196525574\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2850416302680969\n",
            "step: 10, loss: 0.2430819422006607\n",
            "step: 20, loss: 0.47222158312797546\n",
            "step: 30, loss: 0.2720026671886444\n",
            "step: 40, loss: 0.4515448808670044\n",
            "step: 50, loss: 0.4626781940460205\n",
            "step: 60, loss: 0.4895934462547302\n",
            "step: 70, loss: 0.1745324283838272\n",
            "step: 80, loss: 0.4909784495830536\n",
            "step: 90, loss: 0.25392356514930725\n",
            "step: 100, loss: 0.36883828043937683\n",
            "step: 110, loss: 0.2313239872455597\n",
            "step: 120, loss: 0.23339487612247467\n",
            "step: 130, loss: 0.1578003615140915\n",
            "step: 140, loss: 0.3788661062717438\n",
            "step: 150, loss: 0.31403082609176636\n",
            "step: 160, loss: 0.20573914051055908\n",
            "step: 170, loss: 0.39430293440818787\n",
            "step: 180, loss: 0.24889756739139557\n",
            "step: 190, loss: 0.16210216283798218\n",
            "step: 200, loss: 0.23030376434326172\n",
            "step: 210, loss: 0.2614881694316864\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3308877646923065\n",
            "step: 10, loss: 0.3052481412887573\n",
            "step: 20, loss: 0.3109497129917145\n",
            "step: 30, loss: 0.23608100414276123\n",
            "step: 40, loss: 0.2411547303199768\n",
            "step: 50, loss: 0.24495677649974823\n",
            "step: 60, loss: 0.4811515510082245\n",
            "step: 70, loss: 0.25202009081840515\n",
            "step: 80, loss: 0.24603474140167236\n",
            "step: 90, loss: 0.4434071183204651\n",
            "step: 100, loss: 0.37829700112342834\n",
            "step: 110, loss: 0.6003188490867615\n",
            "step: 120, loss: 0.38540202379226685\n",
            "step: 130, loss: 0.6806856393814087\n",
            "step: 140, loss: 0.49365273118019104\n",
            "step: 150, loss: 0.38724541664123535\n",
            "step: 160, loss: 0.3150523900985718\n",
            "step: 170, loss: 0.17197838425636292\n",
            "step: 180, loss: 0.08194691687822342\n",
            "step: 190, loss: 0.17282108962535858\n",
            "step: 200, loss: 0.31506192684173584\n",
            "step: 210, loss: 0.3871901333332062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3833116292953491\n",
            "step: 10, loss: 0.3218291401863098\n",
            "step: 20, loss: 0.31340131163597107\n",
            "step: 30, loss: 0.24155253171920776\n",
            "step: 40, loss: 0.4372464716434479\n",
            "step: 50, loss: 0.3796613812446594\n",
            "step: 60, loss: 0.38145601749420166\n",
            "step: 70, loss: 0.2320481687784195\n",
            "step: 80, loss: 0.4502662122249603\n",
            "step: 90, loss: 0.44950759410858154\n",
            "step: 100, loss: 0.24897059798240662\n",
            "step: 110, loss: 0.16006232798099518\n",
            "step: 120, loss: 0.23604242503643036\n",
            "step: 130, loss: 0.3118494749069214\n",
            "step: 140, loss: 0.5288630723953247\n",
            "step: 150, loss: 0.31718695163726807\n",
            "step: 160, loss: 0.24615810811519623\n",
            "step: 170, loss: 0.3854867219924927\n",
            "step: 180, loss: 0.23780867457389832\n",
            "step: 190, loss: 0.5242255330085754\n",
            "step: 200, loss: 0.3750189244747162\n",
            "step: 210, loss: 0.24113129079341888\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18030251562595367\n",
            "step: 10, loss: 0.37001630663871765\n",
            "step: 20, loss: 0.31286612153053284\n",
            "step: 30, loss: 0.24797922372817993\n",
            "step: 40, loss: 0.30711403489112854\n",
            "step: 50, loss: 0.6098356246948242\n",
            "step: 60, loss: 0.3074911832809448\n",
            "step: 70, loss: 0.43939635157585144\n",
            "step: 80, loss: 0.2970495820045471\n",
            "step: 90, loss: 0.3095347285270691\n",
            "step: 100, loss: 0.3164246082305908\n",
            "step: 110, loss: 0.39015117287635803\n",
            "step: 120, loss: 0.25383666157722473\n",
            "step: 130, loss: 0.24807430803775787\n",
            "step: 140, loss: 0.39796701073646545\n",
            "step: 150, loss: 0.19126775860786438\n",
            "step: 160, loss: 0.16450154781341553\n",
            "step: 170, loss: 0.45448943972587585\n",
            "step: 180, loss: 0.37428995966911316\n",
            "step: 190, loss: 0.44216763973236084\n",
            "step: 200, loss: 0.31360405683517456\n",
            "step: 210, loss: 0.3754833936691284\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.45421814918518066\n",
            "step: 10, loss: 0.2968152165412903\n",
            "step: 20, loss: 0.43327197432518005\n",
            "step: 30, loss: 0.25251248478889465\n",
            "step: 40, loss: 0.24736814200878143\n",
            "step: 50, loss: 0.3780701160430908\n",
            "step: 60, loss: 0.3153366148471832\n",
            "step: 70, loss: 0.2506773769855499\n",
            "step: 80, loss: 0.5109924077987671\n",
            "step: 90, loss: 0.3832811415195465\n",
            "step: 100, loss: 0.4397260546684265\n",
            "step: 110, loss: 0.3088163137435913\n",
            "step: 120, loss: 0.31548917293548584\n",
            "step: 130, loss: 0.471363365650177\n",
            "step: 140, loss: 0.3108009099960327\n",
            "step: 150, loss: 0.30874431133270264\n",
            "step: 160, loss: 0.5365762710571289\n",
            "step: 170, loss: 0.6346709728240967\n",
            "step: 180, loss: 0.24692243337631226\n",
            "step: 190, loss: 0.2377167046070099\n",
            "step: 200, loss: 0.30960407853126526\n",
            "step: 210, loss: 0.31973686814308167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6169945001602173\n",
            "step: 10, loss: 0.3186132609844208\n",
            "step: 20, loss: 0.2652955949306488\n",
            "step: 30, loss: 0.24508114159107208\n",
            "step: 40, loss: 0.24415740370750427\n",
            "step: 50, loss: 0.17292919754981995\n",
            "step: 60, loss: 0.17279286682605743\n",
            "step: 70, loss: 0.4543134272098541\n",
            "step: 80, loss: 0.31644493341445923\n",
            "step: 90, loss: 0.36939379572868347\n",
            "step: 100, loss: 0.6067623496055603\n",
            "step: 110, loss: 0.3896312117576599\n",
            "step: 120, loss: 0.30769357085227966\n",
            "step: 130, loss: 0.16874469816684723\n",
            "step: 140, loss: 0.31164032220840454\n",
            "step: 150, loss: 0.4635770618915558\n",
            "step: 160, loss: 0.45656558871269226\n",
            "step: 170, loss: 0.524874210357666\n",
            "step: 180, loss: 0.3099824786186218\n",
            "step: 190, loss: 0.23211321234703064\n",
            "step: 200, loss: 0.4592163562774658\n",
            "step: 210, loss: 0.3807610273361206\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5764499306678772\n",
            "step: 10, loss: 0.2736869752407074\n",
            "step: 20, loss: 0.37176644802093506\n",
            "step: 30, loss: 0.16956345736980438\n",
            "step: 40, loss: 0.16159170866012573\n",
            "step: 50, loss: 0.4586215317249298\n",
            "step: 60, loss: 0.17594748735427856\n",
            "step: 70, loss: 0.3830184042453766\n",
            "step: 80, loss: 0.32256221771240234\n",
            "step: 90, loss: 0.31204473972320557\n",
            "step: 100, loss: 0.4634765386581421\n",
            "step: 110, loss: 0.5209469199180603\n",
            "step: 120, loss: 0.3772510290145874\n",
            "step: 130, loss: 0.23866526782512665\n",
            "step: 140, loss: 0.5877878069877625\n",
            "step: 150, loss: 0.121940016746521\n",
            "step: 160, loss: 0.384701132774353\n",
            "step: 170, loss: 0.3133842349052429\n",
            "step: 180, loss: 0.4540957510471344\n",
            "step: 190, loss: 0.31064844131469727\n",
            "step: 200, loss: 0.3147696852684021\n",
            "step: 210, loss: 0.31526485085487366\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3114335536956787\n",
            "step: 10, loss: 0.3785233199596405\n",
            "step: 20, loss: 0.2569655478000641\n",
            "step: 30, loss: 0.16964951157569885\n",
            "step: 40, loss: 0.3146226108074188\n",
            "step: 50, loss: 0.5039660334587097\n",
            "step: 60, loss: 0.2463492751121521\n",
            "step: 70, loss: 0.38180238008499146\n",
            "step: 80, loss: 0.16303236782550812\n",
            "step: 90, loss: 0.38854414224624634\n",
            "step: 100, loss: 0.4547804892063141\n",
            "step: 110, loss: 0.17400118708610535\n",
            "step: 120, loss: 0.45383691787719727\n",
            "step: 130, loss: 0.2441311478614807\n",
            "step: 140, loss: 0.38071611523628235\n",
            "step: 150, loss: 0.2519019842147827\n",
            "step: 160, loss: 0.3773283064365387\n",
            "step: 170, loss: 0.24600493907928467\n",
            "step: 180, loss: 0.3147714138031006\n",
            "step: 190, loss: 0.2380709946155548\n",
            "step: 200, loss: 0.6562443971633911\n",
            "step: 210, loss: 0.25391069054603577\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.309752494096756\n",
            "step: 10, loss: 0.31729063391685486\n",
            "step: 20, loss: 0.379136860370636\n",
            "step: 30, loss: 0.17055431008338928\n",
            "step: 40, loss: 0.4560791254043579\n",
            "step: 50, loss: 0.4378022253513336\n",
            "step: 60, loss: 0.44887444376945496\n",
            "step: 70, loss: 0.17252065241336823\n",
            "step: 80, loss: 0.5276632905006409\n",
            "step: 90, loss: 0.3743063807487488\n",
            "step: 100, loss: 0.5124719738960266\n",
            "step: 110, loss: 0.44321081042289734\n",
            "step: 120, loss: 0.43257004022598267\n",
            "step: 130, loss: 0.24898450076580048\n",
            "step: 140, loss: 0.3219008147716522\n",
            "step: 150, loss: 0.3110612630844116\n",
            "step: 160, loss: 0.1681905835866928\n",
            "step: 170, loss: 0.311848908662796\n",
            "step: 180, loss: 0.2385735809803009\n",
            "step: 190, loss: 0.44766825437545776\n",
            "step: 200, loss: 0.18052417039871216\n",
            "step: 210, loss: 0.3869333863258362\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24250636994838715\n",
            "step: 10, loss: 0.32618263363838196\n",
            "step: 20, loss: 0.4463657736778259\n",
            "step: 30, loss: 0.32167285680770874\n",
            "step: 40, loss: 0.177677184343338\n",
            "step: 50, loss: 0.4642024636268616\n",
            "step: 60, loss: 0.10839825868606567\n",
            "step: 70, loss: 0.4526368975639343\n",
            "step: 80, loss: 0.3164482116699219\n",
            "step: 90, loss: 0.384159117937088\n",
            "step: 100, loss: 0.1201367974281311\n",
            "step: 110, loss: 0.25119978189468384\n",
            "step: 120, loss: 0.2518548369407654\n",
            "step: 130, loss: 0.43726998567581177\n",
            "step: 140, loss: 0.3905302882194519\n",
            "step: 150, loss: 0.10461939871311188\n",
            "step: 160, loss: 0.24594029784202576\n",
            "step: 170, loss: 0.2404123693704605\n",
            "step: 180, loss: 0.310908704996109\n",
            "step: 190, loss: 0.37438997626304626\n",
            "step: 200, loss: 0.17679236829280853\n",
            "step: 210, loss: 0.31195423007011414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3105607330799103\n",
            "step: 10, loss: 0.24109108746051788\n",
            "step: 20, loss: 0.31492120027542114\n",
            "step: 30, loss: 0.24560169875621796\n",
            "step: 40, loss: 0.2486899346113205\n",
            "step: 50, loss: 0.24327769875526428\n",
            "step: 60, loss: 0.5772477388381958\n",
            "step: 70, loss: 0.4424489736557007\n",
            "step: 80, loss: 0.3772781491279602\n",
            "step: 90, loss: 0.2406550496816635\n",
            "step: 100, loss: 0.387044757604599\n",
            "step: 110, loss: 0.24521638453006744\n",
            "step: 120, loss: 0.30779051780700684\n",
            "step: 130, loss: 0.3190784156322479\n",
            "step: 140, loss: 0.1623355597257614\n",
            "step: 150, loss: 0.3078306019306183\n",
            "step: 160, loss: 0.5493983626365662\n",
            "step: 170, loss: 0.23894605040550232\n",
            "step: 180, loss: 0.24261705577373505\n",
            "step: 190, loss: 0.2447420060634613\n",
            "step: 200, loss: 0.3721892237663269\n",
            "step: 210, loss: 0.3799295425415039\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24763526022434235\n",
            "step: 10, loss: 0.38501712679862976\n",
            "step: 20, loss: 0.5030198097229004\n",
            "step: 30, loss: 0.24767768383026123\n",
            "step: 40, loss: 0.3103160858154297\n",
            "step: 50, loss: 0.2494354546070099\n",
            "step: 60, loss: 0.38298550248146057\n",
            "step: 70, loss: 0.2488422393798828\n",
            "step: 80, loss: 0.3126703202724457\n",
            "step: 90, loss: 0.1656987965106964\n",
            "step: 100, loss: 0.2467321902513504\n",
            "step: 110, loss: 0.4519009590148926\n",
            "step: 120, loss: 0.2453806847333908\n",
            "step: 130, loss: 0.31417179107666016\n",
            "step: 140, loss: 0.31918513774871826\n",
            "step: 150, loss: 0.30793631076812744\n",
            "step: 160, loss: 0.17462775111198425\n",
            "step: 170, loss: 0.45499178767204285\n",
            "step: 180, loss: 0.23918461799621582\n",
            "step: 190, loss: 0.3711497485637665\n",
            "step: 200, loss: 0.17144501209259033\n",
            "step: 210, loss: 0.4470568299293518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24084824323654175\n",
            "step: 10, loss: 0.24403275549411774\n",
            "step: 20, loss: 0.5208642482757568\n",
            "step: 30, loss: 0.24484167993068695\n",
            "step: 40, loss: 0.3764853775501251\n",
            "step: 50, loss: 0.17043940722942352\n",
            "step: 60, loss: 0.3141552209854126\n",
            "step: 70, loss: 0.447885662317276\n",
            "step: 80, loss: 0.2441485971212387\n",
            "step: 90, loss: 0.4568396806716919\n",
            "step: 100, loss: 0.31156033277511597\n",
            "step: 110, loss: 0.30907177925109863\n",
            "step: 120, loss: 0.44351252913475037\n",
            "step: 130, loss: 0.17415347695350647\n",
            "step: 140, loss: 0.3133874535560608\n",
            "step: 150, loss: 0.519874095916748\n",
            "step: 160, loss: 0.3800473213195801\n",
            "step: 170, loss: 0.3804018795490265\n",
            "step: 180, loss: 0.3153533339500427\n",
            "step: 190, loss: 0.3086782991886139\n",
            "step: 200, loss: 0.31611499190330505\n",
            "step: 210, loss: 0.4524657428264618\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 477.78it/s]\n",
            "load_f1 = 0.18519984170953702\n",
            "real_f1 = 0.18519984170953702\n",
            "267it [00:00, 1284.96it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 201.35it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXvTChDGgw8D"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11f1e84-1b1c-4b04-ba33-9c23c3f07dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 434kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 745kB/s] \n",
            "Downloading: 100% 456k/456k [00:00<00:00, 511kB/s]\n",
            "Downloading: 100% 501M/501M [00:07<00:00, 69.9MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4736657738685608\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4085082411766052\n",
            "step: 20, loss: 0.35495859384536743\n",
            "step: 30, loss: 0.36131083965301514\n",
            "step: 40, loss: 0.23235072195529938\n",
            "step: 50, loss: 0.3212697207927704\n",
            "step: 60, loss: 0.44941744208335876\n",
            "step: 70, loss: 0.4740677773952484\n",
            "step: 80, loss: 0.174323171377182\n",
            "step: 90, loss: 0.2898094356060028\n",
            "step: 100, loss: 0.4342058300971985\n",
            "step: 110, loss: 0.23106907308101654\n",
            "step: 120, loss: 0.30908843874931335\n",
            "step: 130, loss: 0.31253665685653687\n",
            "step: 140, loss: 0.16675825417041779\n",
            "step: 150, loss: 0.31390178203582764\n",
            "step: 160, loss: 0.23104630410671234\n",
            "step: 170, loss: 0.3856554627418518\n",
            "step: 180, loss: 0.141786590218544\n",
            "step: 190, loss: 0.16678209602832794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.2692750287686997, f1=0.2883355176933159, best_f1=0.2883355176933159\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.33381029963493347\n",
            "step: 10, loss: 0.3284919559955597\n",
            "step: 20, loss: 0.6651467680931091\n",
            "step: 30, loss: 0.19048231840133667\n",
            "step: 40, loss: 0.282368928194046\n",
            "step: 50, loss: 0.35418376326560974\n",
            "step: 60, loss: 0.355333536863327\n",
            "step: 70, loss: 0.19042618572711945\n",
            "step: 80, loss: 0.09391983598470688\n",
            "step: 90, loss: 0.17782706022262573\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.18316057324409485\n",
            "step: 110, loss: 0.14256636798381805\n",
            "step: 120, loss: 0.19337785243988037\n",
            "step: 130, loss: 0.1519353687763214\n",
            "step: 140, loss: 0.21216394007205963\n",
            "step: 150, loss: 0.2837497293949127\n",
            "step: 160, loss: 0.24857476353645325\n",
            "step: 170, loss: 0.03612251579761505\n",
            "step: 180, loss: 0.09905631095170975\n",
            "step: 190, loss: 0.16980868577957153\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7486033519553073, f1=0.7878787878787877, best_f1=0.7878787878787877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20341043174266815\n",
            "step: 10, loss: 0.11474420130252838\n",
            "step: 20, loss: 0.07725691050291061\n",
            "step: 30, loss: 0.13504382967948914\n",
            "step: 40, loss: 0.031163403764367104\n",
            "step: 50, loss: 0.2507587969303131\n",
            "step: 60, loss: 0.059484995901584625\n",
            "step: 70, loss: 0.19424034655094147\n",
            "step: 80, loss: 0.10717833042144775\n",
            "step: 90, loss: 0.07280588150024414\n",
            "step: 100, loss: 0.01793050579726696\n",
            "step: 110, loss: 0.09935293346643448\n",
            "step: 120, loss: 0.2743418514728546\n",
            "step: 130, loss: 0.06168784201145172\n",
            "step: 140, loss: 0.0624723918735981\n",
            "step: 150, loss: 0.3269207179546356\n",
            "step: 160, loss: 0.09982187300920486\n",
            "step: 170, loss: 0.33456340432167053\n",
            "step: 180, loss: 0.20292919874191284\n",
            "step: 190, loss: 0.09219623357057571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7675070028011204, f1=0.8108108108108106, best_f1=0.8108108108108106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025134002789855003\n",
            "step: 10, loss: 0.14403624832630157\n",
            "step: 20, loss: 0.20042963325977325\n",
            "step: 30, loss: 0.05865662172436714\n",
            "step: 40, loss: 0.037064291536808014\n",
            "step: 50, loss: 0.1227351576089859\n",
            "step: 60, loss: 0.23245316743850708\n",
            "step: 70, loss: 0.07458916306495667\n",
            "step: 80, loss: 0.014026151970028877\n",
            "step: 90, loss: 0.01619255729019642\n",
            "step: 100, loss: 0.2082766890525818\n",
            "step: 110, loss: 0.22759920358657837\n",
            "step: 120, loss: 0.03896089270710945\n",
            "step: 130, loss: 0.058867428451776505\n",
            "step: 140, loss: 0.19797223806381226\n",
            "step: 150, loss: 0.12348150461912155\n",
            "step: 160, loss: 0.0392812043428421\n",
            "step: 170, loss: 0.027662910521030426\n",
            "step: 180, loss: 0.04321730509400368\n",
            "step: 190, loss: 0.01466427929699421\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.7988505747126435, f1=0.8076923076923078, best_f1=0.8076923076923078\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11464536190032959\n",
            "step: 10, loss: 0.1208314597606659\n",
            "step: 20, loss: 0.06306721270084381\n",
            "step: 30, loss: 0.07997939735651016\n",
            "step: 40, loss: 0.06633511930704117\n",
            "step: 50, loss: 0.049795884639024734\n",
            "step: 60, loss: 0.014018640853464603\n",
            "step: 70, loss: 0.2184801548719406\n",
            "step: 80, loss: 0.013779952190816402\n",
            "step: 90, loss: 0.015766389667987823\n",
            "step: 100, loss: 0.21053196489810944\n",
            "step: 110, loss: 0.30132782459259033\n",
            "step: 120, loss: 0.013947729952633381\n",
            "step: 130, loss: 0.20224568247795105\n",
            "step: 140, loss: 0.12701238691806793\n",
            "step: 150, loss: 0.11470449715852737\n",
            "step: 160, loss: 0.01145044807344675\n",
            "step: 170, loss: 0.05347111448645592\n",
            "step: 180, loss: 0.05087735876441002\n",
            "step: 190, loss: 0.018334561958909035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8328767123287673, f1=0.8074866310160429, best_f1=0.8074866310160429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024010559543967247\n",
            "step: 10, loss: 0.016228104010224342\n",
            "step: 20, loss: 0.12622548639774323\n",
            "step: 30, loss: 0.08438555896282196\n",
            "step: 40, loss: 0.06283234059810638\n",
            "step: 50, loss: 0.11462833732366562\n",
            "step: 60, loss: 0.053619153797626495\n",
            "step: 70, loss: 0.11331823468208313\n",
            "step: 80, loss: 0.13735131919384003\n",
            "step: 90, loss: 0.0053710429929196835\n",
            "step: 100, loss: 0.15499262511730194\n",
            "step: 110, loss: 0.012166688218712807\n",
            "step: 120, loss: 0.030143234878778458\n",
            "step: 130, loss: 0.158427894115448\n",
            "step: 140, loss: 0.021713344380259514\n",
            "step: 150, loss: 0.008595481514930725\n",
            "step: 160, loss: 0.2286728173494339\n",
            "step: 170, loss: 0.17419247329235077\n",
            "step: 180, loss: 0.032903365790843964\n",
            "step: 190, loss: 0.10352245718240738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8068181818181819, f1=0.8166666666666668, best_f1=0.8074866310160429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008768647909164429\n",
            "step: 10, loss: 0.18515458703041077\n",
            "step: 20, loss: 0.006245137192308903\n",
            "step: 30, loss: 0.010410208255052567\n",
            "step: 40, loss: 0.00821542926132679\n",
            "step: 50, loss: 0.002178728813305497\n",
            "step: 60, loss: 0.009461804293096066\n",
            "step: 70, loss: 0.003111285623162985\n",
            "step: 80, loss: 0.0030664110090583563\n",
            "step: 90, loss: 0.0045376247726380825\n",
            "step: 100, loss: 0.18664434552192688\n",
            "step: 110, loss: 0.16723008453845978\n",
            "step: 120, loss: 0.006188520230352879\n",
            "step: 130, loss: 0.04319660738110542\n",
            "step: 140, loss: 0.028423329815268517\n",
            "step: 150, loss: 0.1314331442117691\n",
            "step: 160, loss: 0.15138383209705353\n",
            "step: 170, loss: 0.021548660472035408\n",
            "step: 180, loss: 0.0031084895599633455\n",
            "step: 190, loss: 0.02814135141670704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8177083333333333, f1=0.8134715025906736, best_f1=0.8074866310160429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024736231192946434\n",
            "step: 10, loss: 0.0895603820681572\n",
            "step: 20, loss: 0.017697656527161598\n",
            "step: 30, loss: 0.18115650117397308\n",
            "step: 40, loss: 0.03222188353538513\n",
            "step: 50, loss: 0.018970727920532227\n",
            "step: 60, loss: 0.01424458809196949\n",
            "step: 70, loss: 0.009414112195372581\n",
            "step: 80, loss: 0.18102623522281647\n",
            "step: 90, loss: 0.02005523070693016\n",
            "step: 100, loss: 0.0034087689127773046\n",
            "step: 110, loss: 0.005517825484275818\n",
            "step: 120, loss: 0.019650451838970184\n",
            "step: 130, loss: 0.0017661405727267265\n",
            "step: 140, loss: 0.027942148968577385\n",
            "step: 150, loss: 0.021203337237238884\n",
            "step: 160, loss: 0.10993639379739761\n",
            "step: 170, loss: 0.001746136462315917\n",
            "step: 180, loss: 0.004134473390877247\n",
            "step: 190, loss: 0.040374912321567535\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8131868131868132, f1=0.8337874659400545, best_f1=0.8074866310160429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012300764210522175\n",
            "step: 10, loss: 0.0008387180278077722\n",
            "step: 20, loss: 0.00477201584726572\n",
            "step: 30, loss: 0.01484409999102354\n",
            "step: 40, loss: 0.1323455572128296\n",
            "step: 50, loss: 0.01828695647418499\n",
            "step: 60, loss: 0.009224959649145603\n",
            "step: 70, loss: 0.002782511757686734\n",
            "step: 80, loss: 0.10829399526119232\n",
            "step: 90, loss: 0.02100498043000698\n",
            "step: 100, loss: 0.06484396010637283\n",
            "step: 110, loss: 0.060074079781770706\n",
            "step: 120, loss: 0.0064178030006587505\n",
            "step: 130, loss: 0.022979464381933212\n",
            "step: 140, loss: 0.23332694172859192\n",
            "step: 150, loss: 0.003446411108598113\n",
            "step: 160, loss: 0.002172360895201564\n",
            "step: 170, loss: 0.003462093183770776\n",
            "step: 180, loss: 0.008497827686369419\n",
            "step: 190, loss: 0.0012969061499461532\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8133704735376045, f1=0.8164383561643836, best_f1=0.8074866310160429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018325246637687087\n",
            "step: 10, loss: 0.0034202630631625652\n",
            "step: 20, loss: 0.02107042819261551\n",
            "step: 30, loss: 0.14762945473194122\n",
            "step: 40, loss: 0.0028178489301353693\n",
            "step: 50, loss: 0.03950141742825508\n",
            "step: 60, loss: 0.0020388762932270765\n",
            "step: 70, loss: 0.0009895694674924016\n",
            "step: 80, loss: 0.017106425017118454\n",
            "step: 90, loss: 0.2326725423336029\n",
            "step: 100, loss: 0.15563692152500153\n",
            "step: 110, loss: 0.08932171016931534\n",
            "step: 120, loss: 0.01014491356909275\n",
            "step: 130, loss: 0.0043565756641328335\n",
            "step: 140, loss: 0.006224576383829117\n",
            "step: 150, loss: 0.0061805471777915955\n",
            "step: 160, loss: 0.0022909652907401323\n",
            "step: 170, loss: 0.003336644731462002\n",
            "step: 180, loss: 0.00859172735363245\n",
            "step: 190, loss: 0.0008602842572145164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7979274611398963, f1=0.7948051948051948, best_f1=0.8074866310160429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012975919060409069\n",
            "step: 10, loss: 0.0007983638206496835\n",
            "step: 20, loss: 0.0010858859168365598\n",
            "step: 30, loss: 0.02250901237130165\n",
            "step: 40, loss: 0.001056235283613205\n",
            "step: 50, loss: 0.05928169935941696\n",
            "step: 60, loss: 0.0036648146342486143\n",
            "step: 70, loss: 0.0009906168561428785\n",
            "step: 80, loss: 0.014123180881142616\n",
            "step: 90, loss: 0.0011876261560246348\n",
            "step: 100, loss: 0.0017928596353158355\n",
            "step: 110, loss: 0.15407666563987732\n",
            "step: 120, loss: 0.0020771503914147615\n",
            "step: 130, loss: 0.015037139877676964\n",
            "step: 140, loss: 0.001445940462872386\n",
            "step: 150, loss: 0.007451294921338558\n",
            "step: 160, loss: 0.0032779790926724672\n",
            "step: 170, loss: 0.005708937533199787\n",
            "step: 180, loss: 0.0054599991999566555\n",
            "step: 190, loss: 0.0008602036978118122\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8146067415730337, f1=0.8212290502793296, best_f1=0.8074866310160429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024795904755592346\n",
            "step: 10, loss: 0.0005833492032252252\n",
            "step: 20, loss: 0.0011196195846423507\n",
            "step: 30, loss: 0.0010567338904365897\n",
            "step: 40, loss: 0.0009131000260822475\n",
            "step: 50, loss: 0.0008715499425306916\n",
            "step: 60, loss: 0.003699617926031351\n",
            "step: 70, loss: 0.0006802097777836025\n",
            "step: 80, loss: 0.028670301660895348\n",
            "step: 90, loss: 0.00064591143745929\n",
            "step: 100, loss: 0.002248275326564908\n",
            "step: 110, loss: 0.005148060619831085\n",
            "step: 120, loss: 0.003508793655782938\n",
            "step: 130, loss: 0.002304184017702937\n",
            "step: 140, loss: 0.0008571039070375264\n",
            "step: 150, loss: 0.0770438089966774\n",
            "step: 160, loss: 0.014126328751444817\n",
            "step: 170, loss: 0.005217605736106634\n",
            "step: 180, loss: 0.0006835493841208518\n",
            "step: 190, loss: 0.0007546594133600593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7977528089887641, f1=0.7955182072829131, best_f1=0.8074866310160429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008760781493037939\n",
            "step: 10, loss: 0.0009288575383834541\n",
            "step: 20, loss: 0.004408939275890589\n",
            "step: 30, loss: 0.01668134331703186\n",
            "step: 40, loss: 0.0016516755567863584\n",
            "step: 50, loss: 0.0005622666794806719\n",
            "step: 60, loss: 0.007465103175491095\n",
            "step: 70, loss: 0.004080415703356266\n",
            "step: 80, loss: 0.0004899904597550631\n",
            "step: 90, loss: 0.000350973685272038\n",
            "step: 100, loss: 0.00046523427590727806\n",
            "step: 110, loss: 0.0028886510990560055\n",
            "step: 120, loss: 0.011761216446757317\n",
            "step: 130, loss: 0.0007159888045862317\n",
            "step: 140, loss: 0.01663973741233349\n",
            "step: 150, loss: 0.0004418087482918054\n",
            "step: 160, loss: 0.00044074218021705747\n",
            "step: 170, loss: 0.0006433703820221126\n",
            "step: 180, loss: 0.0006991180125623941\n",
            "step: 190, loss: 0.07778561115264893\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8112676056338027, f1=0.8032786885245903, best_f1=0.8074866310160429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00046489739906974137\n",
            "step: 10, loss: 0.0007556556956842542\n",
            "step: 20, loss: 0.000621790299192071\n",
            "step: 30, loss: 0.0004516477056313306\n",
            "step: 40, loss: 0.002439639065414667\n",
            "step: 50, loss: 0.0009170313132926822\n",
            "step: 60, loss: 0.04845133796334267\n",
            "step: 70, loss: 0.0027953756507486105\n",
            "step: 80, loss: 0.017444007098674774\n",
            "step: 90, loss: 0.002158456714823842\n",
            "step: 100, loss: 0.0027431505732238293\n",
            "step: 110, loss: 0.0008435146883130074\n",
            "step: 120, loss: 0.0010373006807640195\n",
            "step: 130, loss: 0.0005055837100371718\n",
            "step: 140, loss: 0.0006722526741214097\n",
            "step: 150, loss: 0.0006186846876516938\n",
            "step: 160, loss: 0.000573475263081491\n",
            "step: 170, loss: 0.0009411073406226933\n",
            "step: 180, loss: 0.000713622197508812\n",
            "step: 190, loss: 0.00046299066161736846\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8228571428571428, f1=0.8033240997229917, best_f1=0.8074866310160429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0028515136800706387\n",
            "step: 10, loss: 0.00045080724521540105\n",
            "step: 20, loss: 0.0069000557996332645\n",
            "step: 30, loss: 0.004221953917294741\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 40, loss: 0.0008920878753997386\n",
            "step: 50, loss: 0.0006394676747731864\n",
            "step: 60, loss: 0.0009905170882120728\n",
            "step: 70, loss: 0.02379794232547283\n",
            "step: 80, loss: 0.0005881928955204785\n",
            "step: 90, loss: 0.01291744876652956\n",
            "step: 100, loss: 0.003521207720041275\n",
            "step: 110, loss: 0.0005540518322959542\n",
            "step: 120, loss: 0.0004679172416217625\n",
            "step: 130, loss: 0.0016671048942953348\n",
            "step: 140, loss: 0.0005427206051535904\n",
            "step: 150, loss: 0.0005881825345568359\n",
            "step: 160, loss: 0.0004776332061737776\n",
            "step: 170, loss: 0.2097119837999344\n",
            "step: 180, loss: 0.0004478819901123643\n",
            "step: 190, loss: 0.0014939281390979886\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8228571428571428, f1=0.8033240997229917, best_f1=0.8074866310160429\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:14, 145.82it/s]\n",
            "load_f1 = 0.8355795148247979\n",
            "real_f1 = 0.8201058201058201\n",
            "733it [00:00, 3662.50it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:33, 133.23it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSCCmtSggw8E"
      },
      "source": [
        "## DK TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbIZQYfgw8E"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736a1f09-3c00-48b5-d3d1-462a1947d4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.468228280544281\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.436666876077652\n",
            "step: 20, loss: 0.3280518054962158\n",
            "step: 30, loss: 0.5055815577507019\n",
            "step: 40, loss: 0.5471956729888916\n",
            "step: 50, loss: 0.2987542450428009\n",
            "step: 60, loss: 0.5374029278755188\n",
            "step: 70, loss: 0.29869112372398376\n",
            "step: 80, loss: 0.2860306203365326\n",
            "step: 90, loss: 0.21064060926437378\n",
            "step: 100, loss: 0.14774104952812195\n",
            "step: 110, loss: 0.4137565493583679\n",
            "step: 120, loss: 0.30103763937950134\n",
            "step: 130, loss: 0.3141029477119446\n",
            "step: 140, loss: 0.38507455587387085\n",
            "step: 150, loss: 0.30632394552230835\n",
            "step: 160, loss: 0.3819105923175812\n",
            "step: 170, loss: 0.30066928267478943\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.23413762287756926, f1=0.22265625, best_f1=0.22265625\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3181728720664978\n",
            "step: 10, loss: 0.449340283870697\n",
            "step: 20, loss: 0.34861499071121216\n",
            "step: 30, loss: 0.30304381251335144\n",
            "step: 40, loss: 0.07790058851242065\n",
            "step: 50, loss: 0.4233095049858093\n",
            "step: 60, loss: 0.17766880989074707\n",
            "step: 70, loss: 0.48833590745925903\n",
            "step: 80, loss: 0.2310100942850113\n",
            "step: 90, loss: 0.2483326941728592\n",
            "step: 100, loss: 0.5337349772453308\n",
            "step: 110, loss: 0.29286929965019226\n",
            "step: 120, loss: 0.24221400916576385\n",
            "step: 130, loss: 0.5264973640441895\n",
            "step: 140, loss: 0.5018743872642517\n",
            "step: 150, loss: 0.43808168172836304\n",
            "step: 160, loss: 0.3976886570453644\n",
            "step: 170, loss: 0.36525145173072815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.24796380090497736, f1=0.2380038387715931, best_f1=0.2380038387715931\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5825449228286743\n",
            "step: 10, loss: 0.30311691761016846\n",
            "step: 20, loss: 0.23282502591609955\n",
            "step: 30, loss: 0.23110316693782806\n",
            "step: 40, loss: 0.3562746047973633\n",
            "step: 50, loss: 0.5810937285423279\n",
            "step: 60, loss: 0.2751709222793579\n",
            "step: 70, loss: 0.23460876941680908\n",
            "step: 80, loss: 0.3921346068382263\n",
            "step: 90, loss: 0.5278863906860352\n",
            "step: 100, loss: 0.23720799386501312\n",
            "step: 110, loss: 0.17311899363994598\n",
            "step: 120, loss: 0.4793703854084015\n",
            "step: 130, loss: 0.5074244737625122\n",
            "step: 140, loss: 0.41755783557891846\n",
            "step: 150, loss: 0.20421993732452393\n",
            "step: 160, loss: 0.16590774059295654\n",
            "step: 170, loss: 0.31160441040992737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.3016393442622951, f1=0.2777777777777778, best_f1=0.2777777777777778\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4115790128707886\n",
            "step: 10, loss: 0.5140698552131653\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.18008467555046082\n",
            "step: 30, loss: 0.33277642726898193\n",
            "step: 40, loss: 0.20545713603496552\n",
            "step: 50, loss: 0.3300466537475586\n",
            "step: 60, loss: 0.469350129365921\n",
            "step: 70, loss: 0.2967468500137329\n",
            "step: 80, loss: 0.4584999084472656\n",
            "step: 90, loss: 0.3226306736469269\n",
            "step: 100, loss: 0.26472437381744385\n",
            "step: 110, loss: 0.4588603973388672\n",
            "step: 120, loss: 0.47185686230659485\n",
            "step: 130, loss: 0.15858419239521027\n",
            "step: 140, loss: 0.5123539566993713\n",
            "step: 150, loss: 0.9495685696601868\n",
            "step: 160, loss: 0.17930968105793\n",
            "step: 170, loss: 0.2668409049510956\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6099585062240663, f1=0.625531914893617, best_f1=0.625531914893617\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3023938536643982\n",
            "step: 10, loss: 0.21888715028762817\n",
            "step: 20, loss: 0.18639828264713287\n",
            "step: 30, loss: 0.07399938255548477\n",
            "step: 40, loss: 0.18563951551914215\n",
            "step: 50, loss: 0.11207620054483414\n",
            "step: 60, loss: 0.30426672101020813\n",
            "step: 70, loss: 0.09399683773517609\n",
            "step: 80, loss: 0.03900960087776184\n",
            "step: 90, loss: 0.2806570529937744\n",
            "step: 100, loss: 0.07282810658216476\n",
            "step: 110, loss: 0.2031085342168808\n",
            "step: 120, loss: 0.09170851111412048\n",
            "step: 130, loss: 0.03892055153846741\n",
            "step: 140, loss: 0.09377388656139374\n",
            "step: 150, loss: 0.12392992526292801\n",
            "step: 160, loss: 0.2219681441783905\n",
            "step: 170, loss: 0.14261257648468018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7707317073170732, f1=0.7554479418886199, best_f1=0.7554479418886199\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1469840705394745\n",
            "step: 10, loss: 0.03689025342464447\n",
            "step: 20, loss: 0.2572741210460663\n",
            "step: 30, loss: 0.01957692578434944\n",
            "step: 40, loss: 0.07056240737438202\n",
            "step: 50, loss: 0.18189674615859985\n",
            "step: 60, loss: 0.11095859110355377\n",
            "step: 70, loss: 0.08419103175401688\n",
            "step: 80, loss: 0.08133908361196518\n",
            "step: 90, loss: 0.18044336140155792\n",
            "step: 100, loss: 0.05561079829931259\n",
            "step: 110, loss: 0.20997938513755798\n",
            "step: 120, loss: 0.2212788611650467\n",
            "step: 130, loss: 0.1797635853290558\n",
            "step: 140, loss: 0.18735532462596893\n",
            "step: 150, loss: 0.25160637497901917\n",
            "step: 160, loss: 0.2777596414089203\n",
            "step: 170, loss: 0.060199007391929626\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7855421686746988, f1=0.7625899280575539, best_f1=0.7625899280575539\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13464604318141937\n",
            "step: 10, loss: 0.13233570754528046\n",
            "step: 20, loss: 0.2006557434797287\n",
            "step: 30, loss: 0.29579171538352966\n",
            "step: 40, loss: 0.022347569465637207\n",
            "step: 50, loss: 0.0390123575925827\n",
            "step: 60, loss: 0.10842785239219666\n",
            "step: 70, loss: 0.03926295414566994\n",
            "step: 80, loss: 0.10133066773414612\n",
            "step: 90, loss: 0.05372961610555649\n",
            "step: 100, loss: 0.01250184141099453\n",
            "step: 110, loss: 0.045640040189027786\n",
            "step: 120, loss: 0.039422258734703064\n",
            "step: 130, loss: 0.10588866472244263\n",
            "step: 140, loss: 0.04610278457403183\n",
            "step: 150, loss: 0.13899827003479004\n",
            "step: 160, loss: 0.03463472053408623\n",
            "step: 170, loss: 0.05185304954648018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.804177545691906, f1=0.7589743589743588, best_f1=0.7589743589743588\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16688235104084015\n",
            "step: 10, loss: 0.01678118109703064\n",
            "step: 20, loss: 0.05986028164625168\n",
            "step: 30, loss: 0.005538852885365486\n",
            "step: 40, loss: 0.16515471041202545\n",
            "step: 50, loss: 0.03052217699587345\n",
            "step: 60, loss: 0.039443835616111755\n",
            "step: 70, loss: 0.08631112426519394\n",
            "step: 80, loss: 0.16179661452770233\n",
            "step: 90, loss: 0.08749232441186905\n",
            "step: 100, loss: 0.0066065313294529915\n",
            "step: 110, loss: 0.10386821627616882\n",
            "step: 120, loss: 0.36398571729660034\n",
            "step: 130, loss: 0.027225086465477943\n",
            "step: 140, loss: 0.0951484739780426\n",
            "step: 150, loss: 0.022283442318439484\n",
            "step: 160, loss: 0.008334673941135406\n",
            "step: 170, loss: 0.15650033950805664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8030690537084398, f1=0.7660668380462724, best_f1=0.7589743589743588\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025973239913582802\n",
            "step: 10, loss: 0.031425535678863525\n",
            "step: 20, loss: 0.019521918147802353\n",
            "step: 30, loss: 0.1347009390592575\n",
            "step: 40, loss: 0.04238610342144966\n",
            "step: 50, loss: 0.005855815950781107\n",
            "step: 60, loss: 0.08086606115102768\n",
            "step: 70, loss: 0.12749414145946503\n",
            "step: 80, loss: 0.0034386315383017063\n",
            "step: 90, loss: 0.08065260946750641\n",
            "step: 100, loss: 0.09389431774616241\n",
            "step: 110, loss: 0.037636347115039825\n",
            "step: 120, loss: 0.018912039697170258\n",
            "step: 130, loss: 0.23109892010688782\n",
            "step: 140, loss: 0.03599195182323456\n",
            "step: 150, loss: 0.2960360646247864\n",
            "step: 160, loss: 0.03344593197107315\n",
            "step: 170, loss: 0.03535836562514305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8073878627968338, f1=0.763157894736842, best_f1=0.763157894736842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13619881868362427\n",
            "step: 10, loss: 0.051475949585437775\n",
            "step: 20, loss: 0.1363130360841751\n",
            "step: 30, loss: 0.08209428191184998\n",
            "step: 40, loss: 0.026426002383232117\n",
            "step: 50, loss: 0.060883667320013046\n",
            "step: 60, loss: 0.12114103883504868\n",
            "step: 70, loss: 0.011295230127871037\n",
            "step: 80, loss: 0.028022238984704018\n",
            "step: 90, loss: 0.004410641733556986\n",
            "step: 100, loss: 0.022693810984492302\n",
            "step: 110, loss: 0.012098686769604683\n",
            "step: 120, loss: 0.018980061635375023\n",
            "step: 130, loss: 0.022488849237561226\n",
            "step: 140, loss: 0.006517288740724325\n",
            "step: 150, loss: 0.023475343361496925\n",
            "step: 160, loss: 0.11436014622449875\n",
            "step: 170, loss: 0.060858502984046936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7748691099476439, f1=0.7611548556430446, best_f1=0.763157894736842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.045883432030677795\n",
            "step: 10, loss: 0.001960438210517168\n",
            "step: 20, loss: 0.05860396474599838\n",
            "step: 30, loss: 0.05608599632978439\n",
            "step: 40, loss: 0.044749025255441666\n",
            "step: 50, loss: 0.09258533269166946\n",
            "step: 60, loss: 0.043990932404994965\n",
            "step: 70, loss: 0.0014077863888815045\n",
            "step: 80, loss: 0.007769694086164236\n",
            "step: 90, loss: 0.08597040921449661\n",
            "step: 100, loss: 0.25261324644088745\n",
            "step: 110, loss: 0.0025732964277267456\n",
            "step: 120, loss: 0.007524797692894936\n",
            "step: 130, loss: 0.05444395914673805\n",
            "step: 140, loss: 0.12824822962284088\n",
            "step: 150, loss: 0.045370399951934814\n",
            "step: 160, loss: 0.060900021344423294\n",
            "step: 170, loss: 0.22085873782634735\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7936507936507937, f1=0.7586206896551725, best_f1=0.763157894736842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005154763348400593\n",
            "step: 10, loss: 0.03394082561135292\n",
            "step: 20, loss: 0.01265487726777792\n",
            "step: 30, loss: 0.06203009560704231\n",
            "step: 40, loss: 0.040411315858364105\n",
            "step: 50, loss: 0.0024050516076385975\n",
            "step: 60, loss: 0.022480633109807968\n",
            "step: 70, loss: 0.06194687262177467\n",
            "step: 80, loss: 0.004075606353580952\n",
            "step: 90, loss: 0.03447999060153961\n",
            "step: 100, loss: 0.004478045739233494\n",
            "step: 110, loss: 0.04883892461657524\n",
            "step: 120, loss: 0.04550553485751152\n",
            "step: 130, loss: 0.15509547293186188\n",
            "step: 140, loss: 0.0031949239782989025\n",
            "step: 150, loss: 0.0032273794058710337\n",
            "step: 160, loss: 0.06558464467525482\n",
            "step: 170, loss: 0.025851868093013763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7892156862745098, f1=0.7642679900744417, best_f1=0.763157894736842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005965011194348335\n",
            "step: 10, loss: 0.024469301104545593\n",
            "step: 20, loss: 0.006073683500289917\n",
            "step: 30, loss: 0.030672328546643257\n",
            "step: 40, loss: 0.02526368945837021\n",
            "step: 50, loss: 0.035385895520448685\n",
            "step: 60, loss: 0.004433070309460163\n",
            "step: 70, loss: 0.03074328601360321\n",
            "step: 80, loss: 0.0014039411908015609\n",
            "step: 90, loss: 0.004171835258603096\n",
            "step: 100, loss: 0.04052966833114624\n",
            "step: 110, loss: 0.0018707021372392774\n",
            "step: 120, loss: 0.005307129118591547\n",
            "step: 130, loss: 0.0043645212426781654\n",
            "step: 140, loss: 0.014512826688587666\n",
            "step: 150, loss: 0.0014787460677325726\n",
            "step: 160, loss: 0.028873061761260033\n",
            "step: 170, loss: 0.0022331979125738144\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7949367088607596, f1=0.7518796992481204, best_f1=0.763157894736842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016001543262973428\n",
            "step: 10, loss: 0.004544997587800026\n",
            "step: 20, loss: 0.0339859314262867\n",
            "step: 30, loss: 0.03482525050640106\n",
            "step: 40, loss: 0.0026799372863024473\n",
            "step: 50, loss: 0.0019592507742345333\n",
            "step: 60, loss: 0.009247456677258015\n",
            "step: 70, loss: 0.00890295673161745\n",
            "step: 80, loss: 0.006823115982115269\n",
            "step: 90, loss: 0.005871148779988289\n",
            "step: 100, loss: 0.0011255725985392928\n",
            "step: 110, loss: 0.01429822389036417\n",
            "step: 120, loss: 0.02722003310918808\n",
            "step: 130, loss: 0.12842732667922974\n",
            "step: 140, loss: 0.035603102296590805\n",
            "step: 150, loss: 0.016346139833331108\n",
            "step: 160, loss: 0.030978567898273468\n",
            "step: 170, loss: 0.049193937331438065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.7926509186351706, f1=0.7676240208877285, best_f1=0.763157894736842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004584414418786764\n",
            "step: 10, loss: 0.01388260256499052\n",
            "step: 20, loss: 0.22611476480960846\n",
            "step: 30, loss: 0.0038309672381728888\n",
            "step: 40, loss: 0.000684835365973413\n",
            "step: 50, loss: 0.0007920420030131936\n",
            "step: 60, loss: 0.0014965386362746358\n",
            "step: 70, loss: 0.012725657783448696\n",
            "step: 80, loss: 0.0016770090442150831\n",
            "step: 90, loss: 0.0014160617720335722\n",
            "step: 100, loss: 0.01596944034099579\n",
            "step: 110, loss: 0.00986375194042921\n",
            "step: 120, loss: 0.01196654699742794\n",
            "step: 130, loss: 0.00647604838013649\n",
            "step: 140, loss: 0.005365564487874508\n",
            "step: 150, loss: 0.0045166234485805035\n",
            "step: 160, loss: 0.030161703005433083\n",
            "step: 170, loss: 0.0009560775943100452\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7894736842105263, f1=0.763157894736842, best_f1=0.763157894736842\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 219.95it/s]\n",
            "load_f1 = 0.735576923076923\n",
            "real_f1 = 0.7342995169082126\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 134.80it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HZE1zMQgw8F"
      },
      "source": [
        "## DK DIRTY - oK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jg7qrOQgw8F"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853cabbc-1d45-439f-c6d8-ef7b555562e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6279656887054443\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.42634278535842896\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.4729517996311188\n",
            "step: 30, loss: 0.35656365752220154\n",
            "step: 40, loss: 0.3370693325996399\n",
            "step: 50, loss: 0.7324949502944946\n",
            "step: 60, loss: 0.46691861748695374\n",
            "step: 70, loss: 0.41002601385116577\n",
            "step: 80, loss: 0.5868573188781738\n",
            "step: 90, loss: 0.2465859055519104\n",
            "step: 100, loss: 0.5803751349449158\n",
            "step: 110, loss: 0.5226289629936218\n",
            "step: 120, loss: 0.4152146875858307\n",
            "step: 130, loss: 0.2998940944671631\n",
            "step: 140, loss: 0.3430609703063965\n",
            "step: 150, loss: 0.5298281311988831\n",
            "step: 160, loss: 0.20419323444366455\n",
            "step: 170, loss: 0.3549209237098694\n",
            "step: 180, loss: 0.26754534244537354\n",
            "step: 190, loss: 0.08197126537561417\n",
            "step: 200, loss: 0.20036497712135315\n",
            "step: 210, loss: 0.3581770062446594\n",
            "step: 220, loss: 0.16341249644756317\n",
            "step: 230, loss: 0.07199973613023758\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8507936507936508, f1=0.8443496801705757, best_f1=0.8443496801705757\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06546354293823242\n",
            "step: 10, loss: 0.31064102053642273\n",
            "step: 20, loss: 0.1109025776386261\n",
            "step: 30, loss: 0.2948839068412781\n",
            "step: 40, loss: 0.13031907379627228\n",
            "step: 50, loss: 0.050674185156822205\n",
            "step: 60, loss: 0.02626366913318634\n",
            "step: 70, loss: 0.10988323390483856\n",
            "step: 80, loss: 0.040045663714408875\n",
            "step: 90, loss: 0.08037838339805603\n",
            "step: 100, loss: 0.050494227558374405\n",
            "step: 110, loss: 0.16911578178405762\n",
            "step: 120, loss: 0.04862217977643013\n",
            "step: 130, loss: 0.07481884211301804\n",
            "step: 140, loss: 0.04869983717799187\n",
            "step: 150, loss: 0.17100916802883148\n",
            "step: 160, loss: 0.13222377002239227\n",
            "step: 170, loss: 0.004277760628610849\n",
            "step: 180, loss: 0.033810943365097046\n",
            "step: 190, loss: 0.019585955888032913\n",
            "step: 200, loss: 0.15524275600910187\n",
            "step: 210, loss: 0.0826081857085228\n",
            "step: 220, loss: 0.1798919290304184\n",
            "step: 230, loss: 0.004366443492472172\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9463307776560789, f1=0.9477196885428254, best_f1=0.9477196885428254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0854584127664566\n",
            "step: 10, loss: 0.05610809102654457\n",
            "step: 20, loss: 0.06441565603017807\n",
            "step: 30, loss: 0.06069318950176239\n",
            "step: 40, loss: 0.0822448581457138\n",
            "step: 50, loss: 0.04026154801249504\n",
            "step: 60, loss: 0.06267418712377548\n",
            "step: 70, loss: 0.10610753297805786\n",
            "step: 80, loss: 0.026298336684703827\n",
            "step: 90, loss: 0.05787596106529236\n",
            "step: 100, loss: 0.06695034354925156\n",
            "step: 110, loss: 0.018618563190102577\n",
            "step: 120, loss: 0.013139354065060616\n",
            "step: 130, loss: 0.033633917570114136\n",
            "step: 140, loss: 0.006797193083912134\n",
            "step: 150, loss: 0.09905465692281723\n",
            "step: 160, loss: 0.010056020691990852\n",
            "step: 170, loss: 0.002867491915822029\n",
            "step: 180, loss: 0.11032086610794067\n",
            "step: 190, loss: 0.07696081697940826\n",
            "step: 200, loss: 0.020102597773075104\n",
            "step: 210, loss: 0.005994857754558325\n",
            "step: 220, loss: 0.1224244013428688\n",
            "step: 230, loss: 0.06527833640575409\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9539473684210527, f1=0.9565217391304347, best_f1=0.9565217391304347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09570752829313278\n",
            "step: 10, loss: 0.01898021064698696\n",
            "step: 20, loss: 0.0048052589409053326\n",
            "step: 30, loss: 0.0019844730850309134\n",
            "step: 40, loss: 0.13979817926883698\n",
            "step: 50, loss: 0.026156999170780182\n",
            "step: 60, loss: 0.05660145729780197\n",
            "step: 70, loss: 0.03750486299395561\n",
            "step: 80, loss: 0.015160778537392616\n",
            "step: 90, loss: 0.09736104309558868\n",
            "step: 100, loss: 0.0603974312543869\n",
            "step: 110, loss: 0.007684402167797089\n",
            "step: 120, loss: 0.02594120241701603\n",
            "step: 130, loss: 0.006372493226081133\n",
            "step: 140, loss: 0.005922677926719189\n",
            "step: 150, loss: 0.010061549954116344\n",
            "step: 160, loss: 0.012395060621201992\n",
            "step: 170, loss: 0.0047273882664740086\n",
            "step: 180, loss: 0.07563607394695282\n",
            "step: 190, loss: 0.010304532945156097\n",
            "step: 200, loss: 0.12211423367261887\n",
            "step: 210, loss: 0.0025683243293315172\n",
            "step: 220, loss: 0.0026776615995913744\n",
            "step: 230, loss: 0.01056975033134222\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9665178571428571, f1=0.9580022701475595, best_f1=0.9580022701475595\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002391661750152707\n",
            "step: 10, loss: 0.036853838711977005\n",
            "step: 20, loss: 0.07822806388139725\n",
            "step: 30, loss: 0.013161142356693745\n",
            "step: 40, loss: 0.026813147589564323\n",
            "step: 50, loss: 0.03259899839758873\n",
            "step: 60, loss: 0.09601069986820221\n",
            "step: 70, loss: 0.01437241118401289\n",
            "step: 80, loss: 0.05437208339571953\n",
            "step: 90, loss: 0.05578872188925743\n",
            "step: 100, loss: 0.0005210903473198414\n",
            "step: 110, loss: 0.007141198497265577\n",
            "step: 120, loss: 0.004636053461581469\n",
            "step: 130, loss: 0.0022024260833859444\n",
            "step: 140, loss: 0.1005476713180542\n",
            "step: 150, loss: 0.021129127591848373\n",
            "step: 160, loss: 0.0009738240041770041\n",
            "step: 170, loss: 0.03885950148105621\n",
            "step: 180, loss: 0.026109740138053894\n",
            "step: 190, loss: 0.10844758152961731\n",
            "step: 200, loss: 0.02646993286907673\n",
            "step: 210, loss: 0.0064199939370155334\n",
            "step: 220, loss: 0.02947630174458027\n",
            "step: 230, loss: 0.015139181166887283\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9723145071982282, f1=0.9708520179372198, best_f1=0.9708520179372198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009292985196225345\n",
            "step: 10, loss: 0.003285585204139352\n",
            "step: 20, loss: 0.0032260995358228683\n",
            "step: 30, loss: 0.0015355634968727827\n",
            "step: 40, loss: 0.0005443824920803308\n",
            "step: 50, loss: 0.014059459790587425\n",
            "step: 60, loss: 0.12599456310272217\n",
            "step: 70, loss: 0.11687352508306503\n",
            "step: 80, loss: 0.034783292561769485\n",
            "step: 90, loss: 0.0020995105151087046\n",
            "step: 100, loss: 0.0015036994591355324\n",
            "step: 110, loss: 0.009686626493930817\n",
            "step: 120, loss: 0.0010818653972819448\n",
            "step: 130, loss: 0.01531602069735527\n",
            "step: 140, loss: 0.002923050429672003\n",
            "step: 150, loss: 0.004311999771744013\n",
            "step: 160, loss: 0.002631785813719034\n",
            "step: 170, loss: 0.0014617444248870015\n",
            "step: 180, loss: 0.003346384735777974\n",
            "step: 190, loss: 0.005952255334705114\n",
            "step: 200, loss: 0.002659567166119814\n",
            "step: 210, loss: 0.002549707191064954\n",
            "step: 220, loss: 0.020190274342894554\n",
            "step: 230, loss: 0.0012219061609357595\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9696287964004499, f1=0.963718820861678, best_f1=0.9708520179372198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012066283961758018\n",
            "step: 10, loss: 0.0005945658776909113\n",
            "step: 20, loss: 0.003343098098412156\n",
            "step: 30, loss: 0.0010912674479186535\n",
            "step: 40, loss: 0.0014656024286523461\n",
            "step: 50, loss: 0.004781703464686871\n",
            "step: 60, loss: 0.003123482456430793\n",
            "step: 70, loss: 0.0028012169059365988\n",
            "step: 80, loss: 0.0012282554525882006\n",
            "step: 90, loss: 0.0027250314597040415\n",
            "step: 100, loss: 0.0005713102291338146\n",
            "step: 110, loss: 0.0010042282519862056\n",
            "step: 120, loss: 0.047766879200935364\n",
            "step: 130, loss: 0.006839178502559662\n",
            "step: 140, loss: 0.0010341006563976407\n",
            "step: 150, loss: 0.2214994579553604\n",
            "step: 160, loss: 0.0006542472983710468\n",
            "step: 170, loss: 0.0015479158610105515\n",
            "step: 180, loss: 0.0009091483661904931\n",
            "step: 190, loss: 0.1117030680179596\n",
            "step: 200, loss: 0.0021688782144337893\n",
            "step: 210, loss: 0.006035075057297945\n",
            "step: 220, loss: 0.002301272237673402\n",
            "step: 230, loss: 0.0181965883821249\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9367681498829039, f1=0.9223416965352449, best_f1=0.9708520179372198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0422699861228466\n",
            "step: 10, loss: 0.0038297686260193586\n",
            "step: 20, loss: 0.006682452745735645\n",
            "step: 30, loss: 0.001916907262057066\n",
            "step: 40, loss: 0.0016315871616825461\n",
            "step: 50, loss: 0.007233213633298874\n",
            "step: 60, loss: 0.002180559327825904\n",
            "step: 70, loss: 0.00045421053073368967\n",
            "step: 80, loss: 0.0973396748304367\n",
            "step: 90, loss: 0.005230179987847805\n",
            "step: 100, loss: 0.011991791427135468\n",
            "step: 110, loss: 0.009498367086052895\n",
            "step: 120, loss: 0.01600363291800022\n",
            "step: 130, loss: 0.005213124677538872\n",
            "step: 140, loss: 0.00038863776717334986\n",
            "step: 150, loss: 0.18016935884952545\n",
            "step: 160, loss: 0.0008110290509648621\n",
            "step: 170, loss: 0.009452472440898418\n",
            "step: 180, loss: 0.0018649811390787363\n",
            "step: 190, loss: 0.0056971898302435875\n",
            "step: 200, loss: 0.024647993966937065\n",
            "step: 210, loss: 0.003925316967070103\n",
            "step: 220, loss: 0.0012995079159736633\n",
            "step: 230, loss: 0.0008594793034717441\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9698324022346367, f1=0.9578107183580388, best_f1=0.9708520179372198\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013866517692804337\n",
            "step: 10, loss: 0.0014505821745842695\n",
            "step: 20, loss: 0.001164392801001668\n",
            "step: 30, loss: 0.0038113927002996206\n",
            "step: 40, loss: 0.16323935985565186\n",
            "step: 50, loss: 0.001973165897652507\n",
            "step: 60, loss: 0.0017517171800136566\n",
            "step: 70, loss: 0.015086548402905464\n",
            "step: 80, loss: 0.0033073099330067635\n",
            "step: 90, loss: 0.14733503758907318\n",
            "step: 100, loss: 0.0010427674278616905\n",
            "step: 110, loss: 0.00035801157355308533\n",
            "step: 120, loss: 0.025670507922768593\n",
            "step: 130, loss: 0.00460497010499239\n",
            "step: 140, loss: 0.012700100429356098\n",
            "step: 150, loss: 0.0006334125646390021\n",
            "step: 160, loss: 0.0717429667711258\n",
            "step: 170, loss: 0.0004662818100769073\n",
            "step: 180, loss: 0.0005455560167320073\n",
            "step: 190, loss: 0.0006503271288238466\n",
            "step: 200, loss: 0.0008850594749674201\n",
            "step: 210, loss: 0.0007823788328096271\n",
            "step: 220, loss: 0.0005180962034501135\n",
            "step: 230, loss: 0.0005762330838479102\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9785794813979707, f1=0.9657534246575342, best_f1=0.9657534246575342\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008422807441093028\n",
            "step: 10, loss: 0.0007891812128946185\n",
            "step: 20, loss: 0.0004651197523344308\n",
            "step: 30, loss: 0.00017096954979933798\n",
            "step: 40, loss: 0.0011755269952118397\n",
            "step: 50, loss: 0.00015926976629998535\n",
            "step: 60, loss: 0.0002971486246678978\n",
            "step: 70, loss: 0.00435241125524044\n",
            "step: 80, loss: 0.00022342696320265532\n",
            "step: 90, loss: 0.0004863193607889116\n",
            "step: 100, loss: 0.00019206557772122324\n",
            "step: 110, loss: 0.008193562738597393\n",
            "step: 120, loss: 0.00010351950186304748\n",
            "step: 130, loss: 0.01946563832461834\n",
            "step: 140, loss: 0.0001299217256018892\n",
            "step: 150, loss: 0.0019453297136351466\n",
            "step: 160, loss: 8.542246359866112e-05\n",
            "step: 170, loss: 0.0011699238093569875\n",
            "step: 180, loss: 0.30117976665496826\n",
            "step: 190, loss: 0.001144132693298161\n",
            "step: 200, loss: 0.0005861440440639853\n",
            "step: 210, loss: 0.0024800929240882397\n",
            "step: 220, loss: 0.0005896310904063284\n",
            "step: 230, loss: 0.00045891935587860644\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.976271186440678, f1=0.9584295612009238, best_f1=0.9657534246575342\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00027807449805550277\n",
            "step: 10, loss: 0.0005928651662543416\n",
            "step: 20, loss: 0.0002808299905154854\n",
            "step: 30, loss: 0.0067019169218838215\n",
            "step: 40, loss: 0.00023386995599139482\n",
            "step: 50, loss: 0.0012310580350458622\n",
            "step: 60, loss: 0.006241902709007263\n",
            "step: 70, loss: 0.00044514486216939986\n",
            "step: 80, loss: 0.0005327895632945001\n",
            "step: 90, loss: 0.051564771682024\n",
            "step: 100, loss: 0.0002673457784112543\n",
            "step: 110, loss: 0.0011095245135948062\n",
            "step: 120, loss: 0.00030277561745606363\n",
            "step: 130, loss: 0.00012232545122969896\n",
            "step: 140, loss: 0.002148005645722151\n",
            "step: 150, loss: 0.0004951465525664389\n",
            "step: 160, loss: 0.01055504847317934\n",
            "step: 170, loss: 0.15333403646945953\n",
            "step: 180, loss: 0.011918717063963413\n",
            "step: 190, loss: 0.0006030109361745417\n",
            "step: 200, loss: 0.008155248127877712\n",
            "step: 210, loss: 0.0003229143039789051\n",
            "step: 220, loss: 0.0009350280161015689\n",
            "step: 230, loss: 0.0016741572180762887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9788182831661093, f1=0.9662921348314607, best_f1=0.9662921348314607\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019302857108414173\n",
            "step: 10, loss: 0.00012204437371110544\n",
            "step: 20, loss: 0.0008045663707889616\n",
            "step: 30, loss: 0.03476254269480705\n",
            "step: 40, loss: 0.0002564018068369478\n",
            "step: 50, loss: 0.0002776375040411949\n",
            "step: 60, loss: 0.001690295641310513\n",
            "step: 70, loss: 0.0001100774752558209\n",
            "step: 80, loss: 8.514522050973028e-05\n",
            "step: 90, loss: 0.006072851829230785\n",
            "step: 100, loss: 0.00011668707884382457\n",
            "step: 110, loss: 0.00014254178677219898\n",
            "step: 120, loss: 0.00014982452557887882\n",
            "step: 130, loss: 0.0002724990772549063\n",
            "step: 140, loss: 0.00015461008297279477\n",
            "step: 150, loss: 0.00020171959477011114\n",
            "step: 160, loss: 0.0032661932054907084\n",
            "step: 170, loss: 0.00029937236104160547\n",
            "step: 180, loss: 0.00024001255223993212\n",
            "step: 190, loss: 0.001276442315429449\n",
            "step: 200, loss: 0.0005142696900293231\n",
            "step: 210, loss: 0.0416165292263031\n",
            "step: 220, loss: 9.975220746127889e-05\n",
            "step: 230, loss: 0.0003066598728764802\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9799107142857142, f1=0.9628796400449944, best_f1=0.9628796400449944\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002902248816099018\n",
            "step: 10, loss: 0.005607893690466881\n",
            "step: 20, loss: 0.00016865527140907943\n",
            "step: 30, loss: 6.440935248974711e-05\n",
            "step: 40, loss: 0.0007117597269825637\n",
            "step: 50, loss: 0.003916042856872082\n",
            "step: 60, loss: 0.00034436071291565895\n",
            "step: 70, loss: 0.0001663608563831076\n",
            "step: 80, loss: 0.0037494597490876913\n",
            "step: 90, loss: 0.000706656719557941\n",
            "step: 100, loss: 0.0002615690464153886\n",
            "step: 110, loss: 0.00046947653754614294\n",
            "step: 120, loss: 0.00016317496192641556\n",
            "step: 130, loss: 0.0006149573600850999\n",
            "step: 140, loss: 0.00011790992721216753\n",
            "step: 150, loss: 0.0002552381483837962\n",
            "step: 160, loss: 0.01790563575923443\n",
            "step: 170, loss: 0.00011068764433730394\n",
            "step: 180, loss: 0.005467269569635391\n",
            "step: 190, loss: 0.00010453631693962961\n",
            "step: 200, loss: 3.469549847068265e-05\n",
            "step: 210, loss: 0.00013047784159425646\n",
            "step: 220, loss: 0.00028423607000149786\n",
            "step: 230, loss: 0.0022362901363521814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9740112994350283, f1=0.9589041095890412, best_f1=0.9628796400449944\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000271655066171661\n",
            "step: 10, loss: 0.00018895151151809841\n",
            "step: 20, loss: 0.0005309656262397766\n",
            "step: 30, loss: 0.0006464957259595394\n",
            "step: 40, loss: 0.00021810246107634157\n",
            "step: 50, loss: 8.758968760957941e-05\n",
            "step: 60, loss: 0.00014510143955703825\n",
            "step: 70, loss: 0.0007497541955672204\n",
            "step: 80, loss: 0.0006265437114052474\n",
            "step: 90, loss: 0.00028098930488340557\n",
            "step: 100, loss: 0.00014191213995218277\n",
            "step: 110, loss: 0.00021100160665810108\n",
            "step: 120, loss: 5.384681571740657e-05\n",
            "step: 130, loss: 0.0004032707947771996\n",
            "step: 140, loss: 0.0003265418636146933\n",
            "step: 150, loss: 0.0024371568579226732\n",
            "step: 160, loss: 0.00024238074547611177\n",
            "step: 170, loss: 0.00013297572149895132\n",
            "step: 180, loss: 0.0009321335237473249\n",
            "step: 190, loss: 8.737562166061252e-05\n",
            "step: 200, loss: 0.0004519208159763366\n",
            "step: 210, loss: 8.525617886334658e-05\n",
            "step: 220, loss: 0.0002576721308287233\n",
            "step: 230, loss: 6.50084184599109e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.978675645342312, f1=0.9648924122310306, best_f1=0.9628796400449944\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001308258855715394\n",
            "step: 10, loss: 0.0002496731176506728\n",
            "step: 20, loss: 0.0006910550873726606\n",
            "step: 30, loss: 0.0001065947362803854\n",
            "step: 40, loss: 6.775976362405345e-05\n",
            "step: 50, loss: 0.00010572135943220928\n",
            "step: 60, loss: 0.002353296149522066\n",
            "step: 70, loss: 0.007429852616041899\n",
            "step: 80, loss: 0.00016112883167807013\n",
            "step: 90, loss: 8.649779192637652e-05\n",
            "step: 100, loss: 0.0009044783073477447\n",
            "step: 110, loss: 0.00011028223525499925\n",
            "step: 120, loss: 0.03683643415570259\n",
            "step: 130, loss: 0.0009000226855278015\n",
            "step: 140, loss: 0.0017834256868809462\n",
            "step: 150, loss: 0.00016608666919637471\n",
            "step: 160, loss: 0.0006971480324864388\n",
            "step: 170, loss: 8.785104000708088e-05\n",
            "step: 180, loss: 0.00016429588140454143\n",
            "step: 190, loss: 0.00035230661160312593\n",
            "step: 200, loss: 0.0021040558349341154\n",
            "step: 210, loss: 0.0005746794049628079\n",
            "step: 220, loss: 7.825395732652396e-05\n",
            "step: 230, loss: 9.827130270423368e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9764837625979844, f1=0.9638009049773756, best_f1=0.9628796400449944\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:17, 142.20it/s]\n",
            "load_f1 = 0.9821029082774049\n",
            "real_f1 = 0.9788182831661093\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 136.26it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYEeB2m-gw8G"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1485290-5ef6-49f5-ff69-1a85dcac829e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6389544606208801\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5037962198257446\n",
            "step: 20, loss: 0.32592684030532837\n",
            "step: 30, loss: 0.42839083075523376\n",
            "step: 40, loss: 0.32375508546829224\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.3798993229866028\n",
            "step: 60, loss: 0.2227325737476349\n",
            "step: 70, loss: 0.20521916449069977\n",
            "step: 80, loss: 0.105820432305336\n",
            "step: 90, loss: 0.19560301303863525\n",
            "step: 100, loss: 0.1545707732439041\n",
            "step: 110, loss: 0.19842161238193512\n",
            "step: 120, loss: 0.39214619994163513\n",
            "step: 130, loss: 0.22692032158374786\n",
            "step: 140, loss: 0.14939731359481812\n",
            "step: 150, loss: 0.20017389953136444\n",
            "step: 160, loss: 0.19623328745365143\n",
            "step: 170, loss: 0.15158611536026\n",
            "step: 180, loss: 0.11652106791734695\n",
            "step: 190, loss: 0.20102299749851227\n",
            "step: 200, loss: 0.11259198933839798\n",
            "step: 210, loss: 0.018986262381076813\n",
            "step: 220, loss: 0.08419016003608704\n",
            "step: 230, loss: 0.21794740855693817\n",
            "step: 240, loss: 0.041142962872982025\n",
            "step: 250, loss: 0.05590023100376129\n",
            "step: 260, loss: 0.3458285331726074\n",
            "step: 270, loss: 0.33452603220939636\n",
            "step: 280, loss: 0.04522467404603958\n",
            "step: 290, loss: 0.06635979562997818\n",
            "step: 300, loss: 0.19878576695919037\n",
            "step: 310, loss: 0.22761908173561096\n",
            "step: 320, loss: 0.3146844804286957\n",
            "step: 330, loss: 0.10536574572324753\n",
            "step: 340, loss: 0.3581877648830414\n",
            "step: 350, loss: 0.19319921731948853\n",
            "step: 360, loss: 0.030142955482006073\n",
            "step: 370, loss: 0.03950817883014679\n",
            "step: 380, loss: 0.2273423969745636\n",
            "step: 390, loss: 0.10199730843305588\n",
            "step: 400, loss: 0.04267338663339615\n",
            "step: 410, loss: 0.2683485448360443\n",
            "step: 420, loss: 0.02021108567714691\n",
            "step: 430, loss: 0.030020756646990776\n",
            "step: 440, loss: 0.045066121965646744\n",
            "step: 450, loss: 0.23802755773067474\n",
            "step: 460, loss: 0.0408393070101738\n",
            "step: 470, loss: 0.055891409516334534\n",
            "step: 480, loss: 0.09586033970117569\n",
            "step: 490, loss: 0.15885592997074127\n",
            "step: 500, loss: 0.09679906815290451\n",
            "step: 510, loss: 0.19624102115631104\n",
            "step: 520, loss: 0.09753461182117462\n",
            "step: 530, loss: 0.0509609617292881\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9181818181818181, f1=0.9100045682960255, best_f1=0.9100045682960255\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08216872066259384\n",
            "step: 10, loss: 0.10407128930091858\n",
            "step: 20, loss: 0.0444609634578228\n",
            "step: 30, loss: 0.04560723155736923\n",
            "step: 40, loss: 0.15325501561164856\n",
            "step: 50, loss: 0.0884077250957489\n",
            "step: 60, loss: 0.05525137111544609\n",
            "step: 70, loss: 0.03652910888195038\n",
            "step: 80, loss: 0.03391186520457268\n",
            "step: 90, loss: 0.03220114856958389\n",
            "step: 100, loss: 0.1456587016582489\n",
            "step: 110, loss: 0.0202960092574358\n",
            "step: 120, loss: 0.11420410871505737\n",
            "step: 130, loss: 0.04793522506952286\n",
            "step: 140, loss: 0.10709229856729507\n",
            "step: 150, loss: 0.12325486540794373\n",
            "step: 160, loss: 0.03841632232069969\n",
            "step: 170, loss: 0.06005676090717316\n",
            "step: 180, loss: 0.17753253877162933\n",
            "step: 190, loss: 0.009008681401610374\n",
            "step: 200, loss: 0.2482953518629074\n",
            "step: 210, loss: 0.04763041064143181\n",
            "step: 220, loss: 0.005925398785620928\n",
            "step: 230, loss: 0.17700067162513733\n",
            "step: 240, loss: 0.014624780975282192\n",
            "step: 250, loss: 0.08231694996356964\n",
            "step: 260, loss: 0.07207413017749786\n",
            "step: 270, loss: 0.032007958739995956\n",
            "step: 280, loss: 0.09062787145376205\n",
            "step: 290, loss: 0.039600152522325516\n",
            "step: 300, loss: 0.07312898337841034\n",
            "step: 310, loss: 0.019414415583014488\n",
            "step: 320, loss: 0.05935566872358322\n",
            "step: 330, loss: 0.07440978288650513\n",
            "step: 340, loss: 0.0953124389052391\n",
            "step: 350, loss: 0.004342540167272091\n",
            "step: 360, loss: 0.05129224434494972\n",
            "step: 370, loss: 0.017414642497897148\n",
            "step: 380, loss: 0.17271126806735992\n",
            "step: 390, loss: 0.011242740787565708\n",
            "step: 400, loss: 0.20970435440540314\n",
            "step: 410, loss: 0.19538481533527374\n",
            "step: 420, loss: 0.018675556406378746\n",
            "step: 430, loss: 0.041912540793418884\n",
            "step: 440, loss: 0.016427230089902878\n",
            "step: 450, loss: 0.051229462027549744\n",
            "step: 460, loss: 0.07360034435987473\n",
            "step: 470, loss: 0.10207147896289825\n",
            "step: 480, loss: 0.015447210520505905\n",
            "step: 490, loss: 0.18423186242580414\n",
            "step: 500, loss: 0.019812053069472313\n",
            "step: 510, loss: 0.17371107637882233\n",
            "step: 520, loss: 0.4413681626319885\n",
            "step: 530, loss: 0.15628862380981445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9158708503865394, f1=0.9124886052871468, best_f1=0.9100045682960255\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1281951665878296\n",
            "step: 10, loss: 0.20190073549747467\n",
            "step: 20, loss: 0.02211545966565609\n",
            "step: 30, loss: 0.06070111691951752\n",
            "step: 40, loss: 0.0954495444893837\n",
            "step: 50, loss: 0.03385042026638985\n",
            "step: 60, loss: 0.012383269146084785\n",
            "step: 70, loss: 0.06137944012880325\n",
            "step: 80, loss: 0.07517939060926437\n",
            "step: 90, loss: 0.026372859254479408\n",
            "step: 100, loss: 0.1015608087182045\n",
            "step: 110, loss: 0.05124542489647865\n",
            "step: 120, loss: 0.11945945769548416\n",
            "step: 130, loss: 0.04059898853302002\n",
            "step: 140, loss: 0.024837683886289597\n",
            "step: 150, loss: 0.035854946821928024\n",
            "step: 160, loss: 0.22911812365055084\n",
            "step: 170, loss: 0.03175603970885277\n",
            "step: 180, loss: 0.03350360319018364\n",
            "step: 190, loss: 0.008206138387322426\n",
            "step: 200, loss: 0.03466416895389557\n",
            "step: 210, loss: 0.04382472485303879\n",
            "step: 220, loss: 0.12132017314434052\n",
            "step: 230, loss: 0.04190938174724579\n",
            "step: 240, loss: 0.11375036090612411\n",
            "step: 250, loss: 0.2167806476354599\n",
            "step: 260, loss: 0.1291973739862442\n",
            "step: 270, loss: 0.014301649294793606\n",
            "step: 280, loss: 0.026718558743596077\n",
            "step: 290, loss: 0.10671976953744888\n",
            "step: 300, loss: 0.23884230852127075\n",
            "step: 310, loss: 0.06937729567289352\n",
            "step: 320, loss: 0.022478953003883362\n",
            "step: 330, loss: 0.03274247422814369\n",
            "step: 340, loss: 0.017731381580233574\n",
            "step: 350, loss: 0.08451211452484131\n",
            "step: 360, loss: 0.00448169931769371\n",
            "step: 370, loss: 0.028700657188892365\n",
            "step: 380, loss: 0.019112661480903625\n",
            "step: 390, loss: 0.006007126532495022\n",
            "step: 400, loss: 0.15219531953334808\n",
            "step: 410, loss: 0.02075961045920849\n",
            "step: 420, loss: 0.007852469570934772\n",
            "step: 430, loss: 0.0920446589589119\n",
            "step: 440, loss: 0.20476889610290527\n",
            "step: 450, loss: 0.023537492379546165\n",
            "step: 460, loss: 0.1186978742480278\n",
            "step: 470, loss: 0.025063149631023407\n",
            "step: 480, loss: 0.10341240465641022\n",
            "step: 490, loss: 0.013615871779620647\n",
            "step: 500, loss: 0.014540459029376507\n",
            "step: 510, loss: 0.025611624121665955\n",
            "step: 520, loss: 0.03545744717121124\n",
            "step: 530, loss: 0.060473810881376266\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9363295880149813, f1=0.9102990033222591, best_f1=0.9102990033222591\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017740746960043907\n",
            "step: 10, loss: 0.024152478203177452\n",
            "step: 20, loss: 0.09353101253509521\n",
            "step: 30, loss: 0.13517743349075317\n",
            "step: 40, loss: 0.013374621979892254\n",
            "step: 50, loss: 0.003397943452000618\n",
            "step: 60, loss: 0.03342008218169212\n",
            "step: 70, loss: 0.03819878771901131\n",
            "step: 80, loss: 0.11817427724599838\n",
            "step: 90, loss: 0.10363473743200302\n",
            "step: 100, loss: 0.0028715450316667557\n",
            "step: 110, loss: 0.3034752607345581\n",
            "step: 120, loss: 0.040742769837379456\n",
            "step: 130, loss: 0.10061364620923996\n",
            "step: 140, loss: 0.025917144492268562\n",
            "step: 150, loss: 0.009253119118511677\n",
            "step: 160, loss: 0.02056056633591652\n",
            "step: 170, loss: 0.016012420877814293\n",
            "step: 180, loss: 0.015098053961992264\n",
            "step: 190, loss: 0.016438771039247513\n",
            "step: 200, loss: 0.004406225401908159\n",
            "step: 210, loss: 0.0011433707550168037\n",
            "step: 220, loss: 0.012162022292613983\n",
            "step: 230, loss: 0.013290273025631905\n",
            "step: 240, loss: 0.0070624700747430325\n",
            "step: 250, loss: 0.15686240792274475\n",
            "step: 260, loss: 0.01599464751780033\n",
            "step: 270, loss: 0.10272775590419769\n",
            "step: 280, loss: 0.0026139444671571255\n",
            "step: 290, loss: 0.030392002314329147\n",
            "step: 300, loss: 0.0078567024320364\n",
            "step: 310, loss: 0.040546588599681854\n",
            "step: 320, loss: 0.0326363667845726\n",
            "step: 330, loss: 0.009614333510398865\n",
            "step: 340, loss: 0.005727630574256182\n",
            "step: 350, loss: 0.005338788032531738\n",
            "step: 360, loss: 0.034018177539110184\n",
            "step: 370, loss: 0.006155801471322775\n",
            "step: 380, loss: 0.006077714264392853\n",
            "step: 390, loss: 0.0002451346954330802\n",
            "step: 400, loss: 0.030658552423119545\n",
            "step: 410, loss: 0.04773630201816559\n",
            "step: 420, loss: 0.02766355127096176\n",
            "step: 430, loss: 0.007065534126013517\n",
            "step: 440, loss: 0.0040089706890285015\n",
            "step: 450, loss: 0.11489245295524597\n",
            "step: 460, loss: 0.024882089346647263\n",
            "step: 470, loss: 0.002683642553165555\n",
            "step: 480, loss: 0.06647050380706787\n",
            "step: 490, loss: 0.006339612882584333\n",
            "step: 500, loss: 0.05354165658354759\n",
            "step: 510, loss: 0.12012383341789246\n",
            "step: 520, loss: 0.013777555897831917\n",
            "step: 530, loss: 0.1304786652326584\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9418334108887855, f1=0.9270346117867166, best_f1=0.9270346117867166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003112774807959795\n",
            "step: 10, loss: 0.04678045213222504\n",
            "step: 20, loss: 0.010444516316056252\n",
            "step: 30, loss: 0.03653524070978165\n",
            "step: 40, loss: 0.012537037022411823\n",
            "step: 50, loss: 0.11854472756385803\n",
            "step: 60, loss: 0.02803504467010498\n",
            "step: 70, loss: 0.012632050551474094\n",
            "step: 80, loss: 0.0004783446784131229\n",
            "step: 90, loss: 0.03087322600185871\n",
            "step: 100, loss: 0.01669938676059246\n",
            "step: 110, loss: 0.022502539679408073\n",
            "step: 120, loss: 0.012632324360311031\n",
            "step: 130, loss: 0.0040377541445195675\n",
            "step: 140, loss: 0.0030437635723501444\n",
            "step: 150, loss: 0.019053051248192787\n",
            "step: 160, loss: 0.061746995896101\n",
            "step: 170, loss: 0.020911769941449165\n",
            "step: 180, loss: 0.022249441593885422\n",
            "step: 190, loss: 0.0018891753861680627\n",
            "step: 200, loss: 0.0033137453719973564\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 210, loss: 0.05278677120804787\n",
            "step: 220, loss: 0.01649647206068039\n",
            "step: 230, loss: 0.01480130199342966\n",
            "step: 240, loss: 0.029989516362547874\n",
            "step: 250, loss: 0.1408003568649292\n",
            "step: 260, loss: 0.0013616118812933564\n",
            "step: 270, loss: 0.004064307082444429\n",
            "step: 280, loss: 0.007299345452338457\n",
            "step: 290, loss: 0.0005937184323556721\n",
            "step: 300, loss: 0.19465281069278717\n",
            "step: 310, loss: 0.03904904052615166\n",
            "step: 320, loss: 0.15952955186367035\n",
            "step: 330, loss: 0.0245680995285511\n",
            "step: 340, loss: 0.0047199479304254055\n",
            "step: 350, loss: 0.004466185346245766\n",
            "step: 360, loss: 0.0007034278241917491\n",
            "step: 370, loss: 0.022570542991161346\n",
            "step: 380, loss: 0.0008143170853145421\n",
            "step: 390, loss: 0.010663309134542942\n",
            "step: 400, loss: 0.022054653614759445\n",
            "step: 410, loss: 0.011475787498056889\n",
            "step: 420, loss: 0.2553100287914276\n",
            "step: 430, loss: 0.11252263188362122\n",
            "step: 440, loss: 0.0018837377429008484\n",
            "step: 450, loss: 0.020967863500118256\n",
            "step: 460, loss: 0.09108888357877731\n",
            "step: 470, loss: 0.008361663669347763\n",
            "step: 480, loss: 0.020784573629498482\n",
            "step: 490, loss: 0.005315558984875679\n",
            "step: 500, loss: 0.0497484989464283\n",
            "step: 510, loss: 0.002075841184705496\n",
            "step: 520, loss: 0.021061161532998085\n",
            "step: 530, loss: 0.06867124140262604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9371271225332722, f1=0.9160092807424594, best_f1=0.9270346117867166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032231705263257027\n",
            "step: 10, loss: 0.0052661900408566\n",
            "step: 20, loss: 0.001944153686054051\n",
            "step: 30, loss: 0.009063954465091228\n",
            "step: 40, loss: 0.002740789670497179\n",
            "step: 50, loss: 0.002777877962216735\n",
            "step: 60, loss: 0.01143517903983593\n",
            "step: 70, loss: 0.0019823124166578054\n",
            "step: 80, loss: 0.005612978246062994\n",
            "step: 90, loss: 0.07263123989105225\n",
            "step: 100, loss: 0.1506366729736328\n",
            "step: 110, loss: 0.007773080375045538\n",
            "step: 120, loss: 0.0759362131357193\n",
            "step: 130, loss: 0.014120414853096008\n",
            "step: 140, loss: 0.003606649348512292\n",
            "step: 150, loss: 0.00430792523548007\n",
            "step: 160, loss: 0.01358600053936243\n",
            "step: 170, loss: 0.005140029825270176\n",
            "step: 180, loss: 0.0012397004757076502\n",
            "step: 190, loss: 0.09376617521047592\n",
            "step: 200, loss: 0.010960179381072521\n",
            "step: 210, loss: 0.0016084349481388927\n",
            "step: 220, loss: 0.010106639005243778\n",
            "step: 230, loss: 0.003992223180830479\n",
            "step: 240, loss: 0.0033821638207882643\n",
            "step: 250, loss: 0.09563019126653671\n",
            "step: 260, loss: 0.030062740668654442\n",
            "step: 270, loss: 0.043088093400001526\n",
            "step: 280, loss: 0.003488582791760564\n",
            "step: 290, loss: 0.0046455408446490765\n",
            "step: 300, loss: 0.014161588624119759\n",
            "step: 310, loss: 0.011419082060456276\n",
            "step: 320, loss: 0.0008239186136052012\n",
            "step: 330, loss: 0.04355624318122864\n",
            "step: 340, loss: 0.0004006222588941455\n",
            "step: 350, loss: 0.0009523734916001558\n",
            "step: 360, loss: 0.0035273958928883076\n",
            "step: 370, loss: 0.003804554929956794\n",
            "step: 380, loss: 0.0001152541080955416\n",
            "step: 390, loss: 0.004052366595715284\n",
            "step: 400, loss: 0.029163029044866562\n",
            "step: 410, loss: 0.015263229608535767\n",
            "step: 420, loss: 0.004885219503194094\n",
            "step: 430, loss: 0.05097469314932823\n",
            "step: 440, loss: 0.0036637485027313232\n",
            "step: 450, loss: 0.145896315574646\n",
            "step: 460, loss: 0.006567580159753561\n",
            "step: 470, loss: 0.0065543800592422485\n",
            "step: 480, loss: 0.003086814656853676\n",
            "step: 490, loss: 0.004923861473798752\n",
            "step: 500, loss: 0.0035533271729946136\n",
            "step: 510, loss: 0.0007419213652610779\n",
            "step: 520, loss: 0.0015977936564013362\n",
            "step: 530, loss: 0.0015265787951648235\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9336426914153132, f1=0.9177570093457944, best_f1=0.9270346117867166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019972561858594418\n",
            "step: 10, loss: 0.0010174672352150083\n",
            "step: 20, loss: 0.012327433563768864\n",
            "step: 30, loss: 0.05608770623803139\n",
            "step: 40, loss: 0.0008528506732545793\n",
            "step: 50, loss: 0.0070818024687469006\n",
            "step: 60, loss: 0.0011145583121106029\n",
            "step: 70, loss: 0.0003988793177995831\n",
            "step: 80, loss: 0.013931389898061752\n",
            "step: 90, loss: 0.0006839275592938066\n",
            "step: 100, loss: 0.03264468535780907\n",
            "step: 110, loss: 0.0013856159057468176\n",
            "step: 120, loss: 0.01447706762701273\n",
            "step: 130, loss: 0.0025125322863459587\n",
            "step: 140, loss: 0.0017170746577903628\n",
            "step: 150, loss: 0.0016028754180297256\n",
            "step: 160, loss: 0.0009846563916653395\n",
            "step: 170, loss: 0.0027253341395407915\n",
            "step: 180, loss: 0.014016491360962391\n",
            "step: 190, loss: 0.03538395091891289\n",
            "step: 200, loss: 0.0004349146329332143\n",
            "step: 210, loss: 0.0016762695740908384\n",
            "step: 220, loss: 0.0003111833066213876\n",
            "step: 230, loss: 0.005899286363273859\n",
            "step: 240, loss: 0.007451179437339306\n",
            "step: 250, loss: 0.0014726620865985751\n",
            "step: 260, loss: 0.0006727228756062686\n",
            "step: 270, loss: 0.00028299426776356995\n",
            "step: 280, loss: 0.00050592195475474\n",
            "step: 290, loss: 0.0014143413864076138\n",
            "step: 300, loss: 0.0002561870205681771\n",
            "step: 310, loss: 0.0019483452197164297\n",
            "step: 320, loss: 0.04228222742676735\n",
            "step: 330, loss: 0.07709917426109314\n",
            "step: 340, loss: 0.002070090500637889\n",
            "step: 350, loss: 0.004841519985347986\n",
            "step: 360, loss: 0.05371318385004997\n",
            "step: 370, loss: 0.003983228001743555\n",
            "step: 380, loss: 0.0021381909027695656\n",
            "step: 390, loss: 0.01677568629384041\n",
            "step: 400, loss: 0.011851407587528229\n",
            "step: 410, loss: 0.00023409645655192435\n",
            "step: 420, loss: 0.02535650134086609\n",
            "step: 430, loss: 0.007650649640709162\n",
            "step: 440, loss: 0.0005736211896874011\n",
            "step: 450, loss: 0.0061559248715639114\n",
            "step: 460, loss: 0.0027885681483894587\n",
            "step: 470, loss: 0.13499052822589874\n",
            "step: 480, loss: 0.008310130797326565\n",
            "step: 490, loss: 0.0012001382419839501\n",
            "step: 500, loss: 0.0007449891418218613\n",
            "step: 510, loss: 0.0029760291799902916\n",
            "step: 520, loss: 0.0007221864070743322\n",
            "step: 530, loss: 0.0015609099064022303\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9412861136999067, f1=0.91828058573453, best_f1=0.9270346117867166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008670588140375912\n",
            "step: 10, loss: 0.0029360430780798197\n",
            "step: 20, loss: 0.003495139069855213\n",
            "step: 30, loss: 0.04880039766430855\n",
            "step: 40, loss: 0.00043486899812705815\n",
            "step: 50, loss: 0.0015068453503772616\n",
            "step: 60, loss: 0.0032000758219510317\n",
            "step: 70, loss: 0.1198335662484169\n",
            "step: 80, loss: 0.027225850149989128\n",
            "step: 90, loss: 0.002965047024190426\n",
            "step: 100, loss: 0.007277938537299633\n",
            "step: 110, loss: 0.0004200826515443623\n",
            "step: 120, loss: 0.00023601812426932156\n",
            "step: 130, loss: 0.0012922120513394475\n",
            "step: 140, loss: 0.0014485253486782312\n",
            "step: 150, loss: 0.00015217013424262404\n",
            "step: 160, loss: 0.0068990690633654594\n",
            "step: 170, loss: 0.10897748917341232\n",
            "step: 180, loss: 0.0033542602322995663\n",
            "step: 190, loss: 0.006716785952448845\n",
            "step: 200, loss: 0.025929193943738937\n",
            "step: 210, loss: 0.022712426260113716\n",
            "step: 220, loss: 0.05580299720168114\n",
            "step: 230, loss: 0.020299164578318596\n",
            "step: 240, loss: 0.004297539591789246\n",
            "step: 250, loss: 0.0004684797895606607\n",
            "step: 260, loss: 0.0001974530896404758\n",
            "step: 270, loss: 0.01809442602097988\n",
            "step: 280, loss: 0.00034963051439262927\n",
            "step: 290, loss: 0.0017137560062110424\n",
            "step: 300, loss: 0.00017939215467777103\n",
            "step: 310, loss: 0.0005525902379304171\n",
            "step: 320, loss: 0.00043977960012853146\n",
            "step: 330, loss: 0.00020773608412127942\n",
            "step: 340, loss: 0.0054682050831615925\n",
            "step: 350, loss: 7.808498048689216e-05\n",
            "step: 360, loss: 0.009248107671737671\n",
            "step: 370, loss: 0.0006262484821490943\n",
            "step: 380, loss: 0.001749142655171454\n",
            "step: 390, loss: 0.00045138399582356215\n",
            "step: 400, loss: 0.0009482354507781565\n",
            "step: 410, loss: 0.0016647193115204573\n",
            "step: 420, loss: 0.000579641608055681\n",
            "step: 430, loss: 0.0019841522444039583\n",
            "step: 440, loss: 0.021275555714964867\n",
            "step: 450, loss: 0.0023179673589766026\n",
            "step: 460, loss: 0.0029674572870135307\n",
            "step: 470, loss: 0.02874019555747509\n",
            "step: 480, loss: 0.01507902517914772\n",
            "step: 490, loss: 0.000966049381531775\n",
            "step: 500, loss: 0.06006579101085663\n",
            "step: 510, loss: 0.016779735684394836\n",
            "step: 520, loss: 0.000248784723225981\n",
            "step: 530, loss: 0.0010594648774713278\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9400921658986175, f1=0.9306839186691312, best_f1=0.9270346117867166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005400690715759993\n",
            "step: 10, loss: 0.00455357413738966\n",
            "step: 20, loss: 0.14729678630828857\n",
            "step: 30, loss: 0.06636299937963486\n",
            "step: 40, loss: 0.02002984844148159\n",
            "step: 50, loss: 0.0013357005082070827\n",
            "step: 60, loss: 0.001295054447837174\n",
            "step: 70, loss: 0.0015974657144397497\n",
            "step: 80, loss: 0.002402914920821786\n",
            "step: 90, loss: 0.06168947368860245\n",
            "step: 100, loss: 0.0018179869512096047\n",
            "step: 110, loss: 0.016451887786388397\n",
            "step: 120, loss: 0.0028222985565662384\n",
            "step: 130, loss: 0.0002632595715112984\n",
            "step: 140, loss: 0.0008366734837181866\n",
            "step: 150, loss: 0.017088737338781357\n",
            "step: 160, loss: 0.0009003740851767361\n",
            "step: 170, loss: 0.016035055741667747\n",
            "step: 180, loss: 0.0005946720484644175\n",
            "step: 190, loss: 0.00016004905046429485\n",
            "step: 200, loss: 9.730477177072316e-05\n",
            "step: 210, loss: 0.07331323623657227\n",
            "step: 220, loss: 0.00019526334654074162\n",
            "step: 230, loss: 0.000994460890069604\n",
            "step: 240, loss: 0.0013114181347191334\n",
            "step: 250, loss: 0.006756719667464495\n",
            "step: 260, loss: 0.03254237771034241\n",
            "step: 270, loss: 0.01551765762269497\n",
            "step: 280, loss: 0.0007394292042590678\n",
            "step: 290, loss: 0.0006594533915631473\n",
            "step: 300, loss: 0.0013591154711320996\n",
            "step: 310, loss: 0.00013994322216603905\n",
            "step: 320, loss: 0.00021131498215254396\n",
            "step: 330, loss: 0.00028128564008511603\n",
            "step: 340, loss: 0.001033903332427144\n",
            "step: 350, loss: 0.005330143496394157\n",
            "step: 360, loss: 0.0014489793684333563\n",
            "step: 370, loss: 0.0006859877030365169\n",
            "step: 380, loss: 0.000948809552937746\n",
            "step: 390, loss: 0.000632519309874624\n",
            "step: 400, loss: 0.0016774858813732862\n",
            "step: 410, loss: 0.0011411579325795174\n",
            "step: 420, loss: 0.0003996138402726501\n",
            "step: 430, loss: 0.16378408670425415\n",
            "step: 440, loss: 0.0004505810502450913\n",
            "step: 450, loss: 0.0038017896004021168\n",
            "step: 460, loss: 0.018794726580381393\n",
            "step: 470, loss: 0.00022169531439431012\n",
            "step: 480, loss: 0.00011736510350601748\n",
            "step: 490, loss: 0.0017313417047262192\n",
            "step: 500, loss: 0.0047581931576132774\n",
            "step: 510, loss: 0.002745227422565222\n",
            "step: 520, loss: 0.03240828588604927\n",
            "step: 530, loss: 0.01672876439988613\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9407372841810545, f1=0.9258049463369109, best_f1=0.9270346117867166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0043505108915269375\n",
            "step: 10, loss: 0.0004302221059333533\n",
            "step: 20, loss: 0.00020156337996013463\n",
            "step: 30, loss: 0.003431012388318777\n",
            "step: 40, loss: 0.0006283809198066592\n",
            "step: 50, loss: 0.0006900609587319195\n",
            "step: 60, loss: 0.0006405394524335861\n",
            "step: 70, loss: 0.000403459562221542\n",
            "step: 80, loss: 0.0002346794935874641\n",
            "step: 90, loss: 0.0002884833374992013\n",
            "step: 100, loss: 0.055600959807634354\n",
            "step: 110, loss: 0.05352189391851425\n",
            "step: 120, loss: 0.0016055564628913999\n",
            "step: 130, loss: 0.0007180828251875937\n",
            "step: 140, loss: 0.0042383503168821335\n",
            "step: 150, loss: 0.001070067286491394\n",
            "step: 160, loss: 0.04516424238681793\n",
            "step: 170, loss: 0.0006642877706326544\n",
            "step: 180, loss: 0.012463822960853577\n",
            "step: 190, loss: 0.0033127893693745136\n",
            "step: 200, loss: 0.00015709105355199426\n",
            "step: 210, loss: 0.004282026551663876\n",
            "step: 220, loss: 0.0005239093443378806\n",
            "step: 230, loss: 0.00027042353758588433\n",
            "step: 240, loss: 0.0002982448786497116\n",
            "step: 250, loss: 0.00020043857512064278\n",
            "step: 260, loss: 0.005187671165913343\n",
            "step: 270, loss: 0.0008195448899641633\n",
            "step: 280, loss: 0.00620149215683341\n",
            "step: 290, loss: 0.0007553101750090718\n",
            "step: 300, loss: 0.0007445216178894043\n",
            "step: 310, loss: 0.09090220928192139\n",
            "step: 320, loss: 0.001942290342412889\n",
            "step: 330, loss: 0.021671701222658157\n",
            "step: 340, loss: 0.00013287724868860096\n",
            "step: 350, loss: 0.0003977588494308293\n",
            "step: 360, loss: 0.00015531932876911014\n",
            "step: 370, loss: 0.0001744187029544264\n",
            "step: 380, loss: 0.002327019115909934\n",
            "step: 390, loss: 0.00012330089521128684\n",
            "step: 400, loss: 0.000220409594476223\n",
            "step: 410, loss: 0.00014432996977120638\n",
            "step: 420, loss: 0.0032450095750391483\n",
            "step: 430, loss: 0.0016158074140548706\n",
            "step: 440, loss: 0.0006097028963267803\n",
            "step: 450, loss: 0.03535338118672371\n",
            "step: 460, loss: 0.001220375532284379\n",
            "step: 470, loss: 0.010670822113752365\n",
            "step: 480, loss: 0.00022483928478322923\n",
            "step: 490, loss: 6.896322884131223e-05\n",
            "step: 500, loss: 0.0004373928240966052\n",
            "step: 510, loss: 3.0318606150103733e-05\n",
            "step: 520, loss: 5.9548292483668774e-05\n",
            "step: 530, loss: 0.0008881750982254744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9394650398873767, f1=0.9273323956868261, best_f1=0.9270346117867166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.133209975203499e-05\n",
            "step: 10, loss: 7.148718577809632e-05\n",
            "step: 20, loss: 0.01534039806574583\n",
            "step: 30, loss: 3.91099565604236e-05\n",
            "step: 40, loss: 0.0001856351736932993\n",
            "step: 50, loss: 3.4937278542201966e-05\n",
            "step: 60, loss: 6.857769767520949e-05\n",
            "step: 70, loss: 0.0014081698609516025\n",
            "step: 80, loss: 0.0048409332521259785\n",
            "step: 90, loss: 0.0009406769531778991\n",
            "step: 100, loss: 0.0011187989730387926\n",
            "step: 110, loss: 0.00011050880857510492\n",
            "step: 120, loss: 0.0004868104588240385\n",
            "step: 130, loss: 5.011531902709976e-05\n",
            "step: 140, loss: 6.625335663557053e-05\n",
            "step: 150, loss: 0.0011347298277541995\n",
            "step: 160, loss: 6.849091005278751e-05\n",
            "step: 170, loss: 9.782981942407787e-05\n",
            "step: 180, loss: 4.431097113410942e-05\n",
            "step: 190, loss: 0.00015937944408506155\n",
            "step: 200, loss: 1.5813600839464925e-05\n",
            "step: 210, loss: 0.0001200932965730317\n",
            "step: 220, loss: 0.0012752312468364835\n",
            "step: 230, loss: 2.4783206754364073e-05\n",
            "step: 240, loss: 0.002875553211197257\n",
            "step: 250, loss: 8.480856922687963e-05\n",
            "step: 260, loss: 0.000505251984577626\n",
            "step: 270, loss: 0.004018457140773535\n",
            "step: 280, loss: 5.067258098279126e-05\n",
            "step: 290, loss: 4.1906347178155556e-05\n",
            "step: 300, loss: 0.0008242235635407269\n",
            "step: 310, loss: 0.000680859200656414\n",
            "step: 320, loss: 0.004668467212468386\n",
            "step: 330, loss: 3.838686097878963e-05\n",
            "step: 340, loss: 0.0013241316191852093\n",
            "step: 350, loss: 4.3599593482213095e-05\n",
            "step: 360, loss: 5.1582555897766724e-05\n",
            "step: 370, loss: 0.0005813354509882629\n",
            "step: 380, loss: 0.004166029393672943\n",
            "step: 390, loss: 0.00668302970007062\n",
            "step: 400, loss: 2.797221895889379e-05\n",
            "step: 410, loss: 0.00029709129012189806\n",
            "step: 420, loss: 0.05573005974292755\n",
            "step: 430, loss: 0.00019820783927571028\n",
            "step: 440, loss: 2.4221211788244545e-05\n",
            "step: 450, loss: 0.0002484689757693559\n",
            "step: 460, loss: 0.0006522481562569737\n",
            "step: 470, loss: 0.0002157177368644625\n",
            "step: 480, loss: 1.8153046767110936e-05\n",
            "step: 490, loss: 3.0185921787051484e-05\n",
            "step: 500, loss: 4.236742097418755e-05\n",
            "step: 510, loss: 1.9464188881102018e-05\n",
            "step: 520, loss: 7.462058420060202e-05\n",
            "step: 530, loss: 3.694889892358333e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9407407407407408, f1=0.9290681502086231, best_f1=0.9270346117867166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.811357212020084e-05\n",
            "step: 10, loss: 7.56334702600725e-05\n",
            "step: 20, loss: 6.285551353357732e-05\n",
            "step: 30, loss: 1.1838787941087503e-05\n",
            "step: 40, loss: 3.549527173163369e-05\n",
            "step: 50, loss: 0.00026174724916927516\n",
            "step: 60, loss: 0.00011702445044647902\n",
            "step: 70, loss: 0.08110152184963226\n",
            "step: 80, loss: 4.428827378433198e-05\n",
            "step: 90, loss: 0.00039002051926217973\n",
            "step: 100, loss: 0.05332622677087784\n",
            "step: 110, loss: 0.007422544993460178\n",
            "step: 120, loss: 0.00011817103222711012\n",
            "step: 130, loss: 0.00030647983658127487\n",
            "step: 140, loss: 0.00021949890651740134\n",
            "step: 150, loss: 0.0006787551683373749\n",
            "step: 160, loss: 0.002636122517287731\n",
            "step: 170, loss: 7.788729999447241e-05\n",
            "step: 180, loss: 0.000408309482736513\n",
            "step: 190, loss: 6.945132918190211e-05\n",
            "step: 200, loss: 8.320189226651564e-05\n",
            "step: 210, loss: 0.0009252433083020151\n",
            "step: 220, loss: 4.295519465813413e-05\n",
            "step: 230, loss: 4.591992183122784e-05\n",
            "step: 240, loss: 0.0022241142578423023\n",
            "step: 250, loss: 0.00015492341481149197\n",
            "step: 260, loss: 3.556919182301499e-05\n",
            "step: 270, loss: 2.839993976522237e-05\n",
            "step: 280, loss: 0.014067750424146652\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 290, loss: 5.7147652114508674e-05\n",
            "step: 300, loss: 0.00020694313570857048\n",
            "step: 310, loss: 0.001603981712833047\n",
            "step: 320, loss: 0.03436854109168053\n",
            "step: 330, loss: 0.0002148322091670707\n",
            "step: 340, loss: 0.02788003720343113\n",
            "step: 350, loss: 9.074129047803581e-05\n",
            "step: 360, loss: 0.0006195244495756924\n",
            "step: 370, loss: 1.2155404874647502e-05\n",
            "step: 380, loss: 0.0004864012007601559\n",
            "step: 390, loss: 0.007850281894207\n",
            "step: 400, loss: 2.5182134777423926e-05\n",
            "step: 410, loss: 0.0027814649511128664\n",
            "step: 420, loss: 0.002016404876485467\n",
            "step: 430, loss: 0.0008502421551384032\n",
            "step: 440, loss: 0.0025340032298117876\n",
            "step: 450, loss: 0.01020789984613657\n",
            "step: 460, loss: 0.0006560960318893194\n",
            "step: 470, loss: 0.0023703293409198523\n",
            "step: 480, loss: 0.0015814048238098621\n",
            "step: 490, loss: 0.0014464394189417362\n",
            "step: 500, loss: 4.351206007413566e-05\n",
            "step: 510, loss: 0.029697036370635033\n",
            "step: 520, loss: 0.010098441503942013\n",
            "step: 530, loss: 0.0002945071319118142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.94547134935305, f1=0.9306839186691312, best_f1=0.9306839186691312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.374403164954856e-05\n",
            "step: 10, loss: 0.039856646209955215\n",
            "step: 20, loss: 9.57817246671766e-05\n",
            "step: 30, loss: 2.925258013419807e-05\n",
            "step: 40, loss: 0.022888634353876114\n",
            "step: 50, loss: 0.00011953315697610378\n",
            "step: 60, loss: 0.0002076198870781809\n",
            "step: 70, loss: 0.0004918453050777316\n",
            "step: 80, loss: 0.012960166670382023\n",
            "step: 90, loss: 3.55513438989874e-05\n",
            "step: 100, loss: 0.00022909634571988136\n",
            "step: 110, loss: 0.00013480168126989156\n",
            "step: 120, loss: 7.114997424650937e-05\n",
            "step: 130, loss: 0.00034575717290863395\n",
            "step: 140, loss: 0.00017930175818037242\n",
            "step: 150, loss: 0.00013808876974508166\n",
            "step: 160, loss: 0.0009963734773918986\n",
            "step: 170, loss: 0.00015303936379496008\n",
            "step: 180, loss: 4.3350377382012084e-05\n",
            "step: 190, loss: 5.121979120303877e-05\n",
            "step: 200, loss: 3.531498441589065e-05\n",
            "step: 210, loss: 0.0009525195928290486\n",
            "step: 220, loss: 1.9929577319999225e-05\n",
            "step: 230, loss: 0.00023816524480935186\n",
            "step: 240, loss: 0.0007758997962810099\n",
            "step: 250, loss: 4.1781971958698705e-05\n",
            "step: 260, loss: 0.0001537479110993445\n",
            "step: 270, loss: 6.729971210006624e-05\n",
            "step: 280, loss: 0.05733225494623184\n",
            "step: 290, loss: 1.3045742889516987e-05\n",
            "step: 300, loss: 0.00010849712998606265\n",
            "step: 310, loss: 8.869838347891346e-05\n",
            "step: 320, loss: 1.963920658454299e-05\n",
            "step: 330, loss: 0.040905941277742386\n",
            "step: 340, loss: 0.00013885532098356634\n",
            "step: 350, loss: 6.569417746504769e-05\n",
            "step: 360, loss: 0.05823296681046486\n",
            "step: 370, loss: 0.011759252287447453\n",
            "step: 380, loss: 0.0011915276991203427\n",
            "step: 390, loss: 0.0003266696003265679\n",
            "step: 400, loss: 0.0002865652786567807\n",
            "step: 410, loss: 2.5446541258133948e-05\n",
            "step: 420, loss: 2.353172021685168e-05\n",
            "step: 430, loss: 3.5636847314890474e-05\n",
            "step: 440, loss: 2.032441807386931e-05\n",
            "step: 450, loss: 0.00014391900913324207\n",
            "step: 460, loss: 9.712758037494496e-05\n",
            "step: 470, loss: 0.0034952780697494745\n",
            "step: 480, loss: 2.0047389625688083e-05\n",
            "step: 490, loss: 1.3991895684739575e-05\n",
            "step: 500, loss: 3.634481618064456e-05\n",
            "step: 510, loss: 5.5500462622148916e-05\n",
            "step: 520, loss: 3.1880779715720564e-05\n",
            "step: 530, loss: 2.814567051245831e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9387379087977891, f1=0.9217954650624711, best_f1=0.9306839186691312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.6925812310073525e-05\n",
            "step: 10, loss: 6.149403634481132e-05\n",
            "step: 20, loss: 1.4166998880682513e-05\n",
            "step: 30, loss: 0.00023499118105974048\n",
            "step: 40, loss: 2.7109921575174667e-05\n",
            "step: 50, loss: 0.021635612472891808\n",
            "step: 60, loss: 0.0005497133824974298\n",
            "step: 70, loss: 3.352300700498745e-05\n",
            "step: 80, loss: 4.6916746214265004e-05\n",
            "step: 90, loss: 1.3060603123449255e-05\n",
            "step: 100, loss: 2.7177922675036825e-05\n",
            "step: 110, loss: 3.9940794522408396e-05\n",
            "step: 120, loss: 2.398540345893707e-05\n",
            "step: 130, loss: 1.2859525668318383e-05\n",
            "step: 140, loss: 0.0001410551049048081\n",
            "step: 150, loss: 3.310311876703054e-05\n",
            "step: 160, loss: 3.117047162959352e-05\n",
            "step: 170, loss: 2.160220901714638e-05\n",
            "step: 180, loss: 2.7560665330383927e-05\n",
            "step: 190, loss: 3.282114994362928e-05\n",
            "step: 200, loss: 3.548320091795176e-05\n",
            "step: 210, loss: 9.563953790348023e-05\n",
            "step: 220, loss: 0.0001966948912013322\n",
            "step: 230, loss: 1.2840913768741302e-05\n",
            "step: 240, loss: 1.528052416688297e-05\n",
            "step: 250, loss: 2.3862663510954008e-05\n",
            "step: 260, loss: 2.404807491984684e-05\n",
            "step: 270, loss: 3.200861465302296e-05\n",
            "step: 280, loss: 1.2855703062086832e-05\n",
            "step: 290, loss: 1.5068262655404396e-05\n",
            "step: 300, loss: 0.00011651866952888668\n",
            "step: 310, loss: 4.0981794882100075e-05\n",
            "step: 320, loss: 1.252054516953649e-05\n",
            "step: 330, loss: 0.0002471235056873411\n",
            "step: 340, loss: 4.193964559817687e-05\n",
            "step: 350, loss: 1.960178451554384e-05\n",
            "step: 360, loss: 1.9299872292322107e-05\n",
            "step: 370, loss: 2.5072022253880277e-05\n",
            "step: 380, loss: 0.0013312924420461059\n",
            "step: 390, loss: 9.037472409545444e-06\n",
            "step: 400, loss: 2.8232290787855163e-05\n",
            "step: 410, loss: 0.00019932363647967577\n",
            "step: 420, loss: 0.0008958350517787039\n",
            "step: 430, loss: 0.0001278077543247491\n",
            "step: 440, loss: 0.0004166796861682087\n",
            "step: 450, loss: 1.546321072964929e-05\n",
            "step: 460, loss: 7.85543816164136e-05\n",
            "step: 470, loss: 2.0011686501675285e-05\n",
            "step: 480, loss: 1.558625081088394e-05\n",
            "step: 490, loss: 1.3787116586172488e-05\n",
            "step: 500, loss: 2.406796374998521e-05\n",
            "step: 510, loss: 0.00019188941223546863\n",
            "step: 520, loss: 1.6568941646255553e-05\n",
            "step: 530, loss: 5.0008056859951466e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9406858202038925, f1=0.9256505576208177, best_f1=0.9306839186691312\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.327985541545786e-05\n",
            "step: 10, loss: 6.0081525589339435e-05\n",
            "step: 20, loss: 3.4567925467854366e-05\n",
            "step: 30, loss: 9.652680455474183e-05\n",
            "step: 40, loss: 0.0007844914798624814\n",
            "step: 50, loss: 0.0005304288351908326\n",
            "step: 60, loss: 2.0420986402314156e-05\n",
            "step: 70, loss: 1.6778165445430204e-05\n",
            "step: 80, loss: 1.3481582755048294e-05\n",
            "step: 90, loss: 0.00012454346870072186\n",
            "step: 100, loss: 7.200939307949739e-06\n",
            "step: 110, loss: 1.899455310194753e-05\n",
            "step: 120, loss: 1.5347852240665816e-05\n",
            "step: 130, loss: 0.00023172864166554064\n",
            "step: 140, loss: 1.215172596857883e-05\n",
            "step: 150, loss: 1.2002759831375442e-05\n",
            "step: 160, loss: 2.3484681150875986e-05\n",
            "step: 170, loss: 1.5049585272208788e-05\n",
            "step: 180, loss: 1.356733264401555e-05\n",
            "step: 190, loss: 9.93942521745339e-05\n",
            "step: 200, loss: 1.1421572708059102e-05\n",
            "step: 210, loss: 8.88739014044404e-05\n",
            "step: 220, loss: 3.423505768296309e-05\n",
            "step: 230, loss: 2.047362067969516e-05\n",
            "step: 240, loss: 1.2256004993105307e-05\n",
            "step: 250, loss: 0.00022391066886484623\n",
            "step: 260, loss: 1.2457204320526216e-05\n",
            "step: 270, loss: 2.0607314581866376e-05\n",
            "step: 280, loss: 2.679432145669125e-05\n",
            "step: 290, loss: 0.0005152954836376011\n",
            "step: 300, loss: 2.3001879526418634e-05\n",
            "step: 310, loss: 5.8000463468488306e-05\n",
            "step: 320, loss: 1.473320480727125e-05\n",
            "step: 330, loss: 0.00014378408377524465\n",
            "step: 340, loss: 2.2443909983849153e-05\n",
            "step: 350, loss: 8.68431743583642e-05\n",
            "step: 360, loss: 2.8464915885706432e-05\n",
            "step: 370, loss: 3.2877182093216106e-05\n",
            "step: 380, loss: 1.711739423626568e-05\n",
            "step: 390, loss: 0.00011685894423862919\n",
            "step: 400, loss: 0.00843277107924223\n",
            "step: 410, loss: 1.902017174870707e-05\n",
            "step: 420, loss: 1.1868602996401023e-05\n",
            "step: 430, loss: 8.832581443130039e-06\n",
            "step: 440, loss: 0.00012236012844368815\n",
            "step: 450, loss: 0.0006368214380927384\n",
            "step: 460, loss: 0.000134775647893548\n",
            "step: 470, loss: 1.1239076229685452e-05\n",
            "step: 480, loss: 0.0037566591054201126\n",
            "step: 490, loss: 1.3015968761465047e-05\n",
            "step: 500, loss: 4.487911064643413e-05\n",
            "step: 510, loss: 2.351096372876782e-05\n",
            "step: 520, loss: 1.9322375010233372e-05\n",
            "step: 530, loss: 1.0330101758881938e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9411219286045434, f1=0.9253592953175707, best_f1=0.9306839186691312\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:35, 160.91it/s]\n",
            "load_f1 = 0.9414498141263941\n",
            "real_f1 = 0.9383057090239412\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 143.94it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnXzXaaYhstq"
      },
      "source": [
        "# DITTO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r23AxFPnhstr"
      },
      "source": [
        "## DITTO STRUCTURED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba58fdc-7492-41c9-92ba-7d46558073d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.422829806804657\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.345679012345679, f1=0.30000000000000004, best_f1=0.30000000000000004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4226915240287781\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.3181818181818182, f1=0.3111111111111111, best_f1=0.30000000000000004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.41993170976638794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.34210526315789475, f1=0.35294117647058826, best_f1=0.30000000000000004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25956565141677856\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.423076923076923, f1=0.358974358974359, best_f1=0.358974358974359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28212061524391174\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.41791044776119407, f1=0.3773584905660377, best_f1=0.358974358974359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2509179711341858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.3636363636363636, f1=0.34782608695652173, best_f1=0.358974358974359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5572521686553955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.358974358974359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.49403059482574463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.28, f1=0.2745098039215686, best_f1=0.358974358974359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.36697012186050415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.3870967741935484, f1=0.29629629629629634, best_f1=0.358974358974359\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.43818768858909607\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.4897959183673469, f1=0.5128205128205129, best_f1=0.5128205128205129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5211028456687927\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.43750000000000006, f1=0.42857142857142855, best_f1=0.5128205128205129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3046155869960785\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.5263157894736842, f1=0.5161290322580646, best_f1=0.5161290322580646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30589133501052856\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.4727272727272727, f1=0.5853658536585367, best_f1=0.5161290322580646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2553427815437317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.4905660377358491, f1=0.5853658536585367, best_f1=0.5161290322580646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.34741783142089844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.4905660377358491, f1=0.5853658536585367, best_f1=0.5161290322580646\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 123202.60it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.46153846153846156\n",
            "real_f1 = 0.43750000000000006\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 141.92it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bang43Lhsts"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c63b9b-c4ad-41c5-9a45-98de75be9d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5636671781539917\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.44711920619010925\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.5398500561714172\n",
            "step: 30, loss: 0.2914138436317444\n",
            "step: 40, loss: 0.3590284287929535\n",
            "step: 50, loss: 0.5874623656272888\n",
            "step: 60, loss: 0.480815589427948\n",
            "step: 70, loss: 0.41552168130874634\n",
            "step: 80, loss: 0.6294143199920654\n",
            "step: 90, loss: 0.44097548723220825\n",
            "step: 100, loss: 0.5174350738525391\n",
            "step: 110, loss: 0.5943555235862732\n",
            "step: 120, loss: 0.4370608925819397\n",
            "step: 130, loss: 0.35224977135658264\n",
            "step: 140, loss: 0.3821382522583008\n",
            "step: 150, loss: 0.5593423247337341\n",
            "step: 160, loss: 0.5098389983177185\n",
            "step: 170, loss: 0.5057608485221863\n",
            "step: 180, loss: 0.3403456509113312\n",
            "step: 190, loss: 0.10818175226449966\n",
            "step: 200, loss: 0.1743936985731125\n",
            "step: 210, loss: 0.2014603614807129\n",
            "step: 220, loss: 0.2701810896396637\n",
            "step: 230, loss: 0.009849537163972855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8631813125695216, f1=0.8820286659316429, best_f1=0.8820286659316429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05433689430356026\n",
            "step: 10, loss: 0.210844486951828\n",
            "step: 20, loss: 0.09981407225131989\n",
            "step: 30, loss: 0.08081388473510742\n",
            "step: 40, loss: 0.06457076221704483\n",
            "step: 50, loss: 0.011023938655853271\n",
            "step: 60, loss: 0.016166800633072853\n",
            "step: 70, loss: 0.16654027998447418\n",
            "step: 80, loss: 0.011722440831363201\n",
            "step: 90, loss: 0.03946695104241371\n",
            "step: 100, loss: 0.06059879809617996\n",
            "step: 110, loss: 0.08606406301259995\n",
            "step: 120, loss: 0.004168601240962744\n",
            "step: 130, loss: 0.17201294004917145\n",
            "step: 140, loss: 0.009984279051423073\n",
            "step: 150, loss: 0.044787075370550156\n",
            "step: 160, loss: 0.01437772624194622\n",
            "step: 170, loss: 0.026120884343981743\n",
            "step: 180, loss: 0.008202768862247467\n",
            "step: 190, loss: 0.0596257708966732\n",
            "step: 200, loss: 0.009162255562841892\n",
            "step: 210, loss: 0.22084787487983704\n",
            "step: 220, loss: 0.0530737042427063\n",
            "step: 230, loss: 0.03470532223582268\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9611542730299667, f1=0.9709821428571428, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02519683726131916\n",
            "step: 10, loss: 0.013266921043395996\n",
            "step: 20, loss: 0.012450497597455978\n",
            "step: 30, loss: 0.001527939923107624\n",
            "step: 40, loss: 0.046315573155879974\n",
            "step: 50, loss: 0.005182642489671707\n",
            "step: 60, loss: 0.021446356549859047\n",
            "step: 70, loss: 0.006889257580041885\n",
            "step: 80, loss: 0.0030678242910653353\n",
            "step: 90, loss: 0.024150514975190163\n",
            "step: 100, loss: 0.018364770337939262\n",
            "step: 110, loss: 0.009485527873039246\n",
            "step: 120, loss: 0.032188281416893005\n",
            "step: 130, loss: 0.0037102934438735247\n",
            "step: 140, loss: 0.046992044895887375\n",
            "step: 150, loss: 0.09248426556587219\n",
            "step: 160, loss: 0.004998256918042898\n",
            "step: 170, loss: 0.0015767848817631602\n",
            "step: 180, loss: 0.040052078664302826\n",
            "step: 190, loss: 0.07959777861833572\n",
            "step: 200, loss: 0.07181651890277863\n",
            "step: 210, loss: 0.005230489186942577\n",
            "step: 220, loss: 0.025675782933831215\n",
            "step: 230, loss: 0.028622429817914963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9798657718120806, f1=0.9842696629213483, best_f1=0.9842696629213483\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010147296823561192\n",
            "step: 10, loss: 0.0035419638734310865\n",
            "step: 20, loss: 0.06534707546234131\n",
            "step: 30, loss: 0.0017445062985643744\n",
            "step: 40, loss: 0.16957612335681915\n",
            "step: 50, loss: 0.0077243996784091\n",
            "step: 60, loss: 0.017688974738121033\n",
            "step: 70, loss: 0.1847972720861435\n",
            "step: 80, loss: 0.006782884243875742\n",
            "step: 90, loss: 0.028220057487487793\n",
            "step: 100, loss: 0.0161898173391819\n",
            "step: 110, loss: 0.0015684411628171802\n",
            "step: 120, loss: 0.0014443229883909225\n",
            "step: 130, loss: 0.011808088980615139\n",
            "step: 140, loss: 0.0003217694174963981\n",
            "step: 150, loss: 0.0002962341532111168\n",
            "step: 160, loss: 0.006052056327462196\n",
            "step: 170, loss: 0.0801510140299797\n",
            "step: 180, loss: 0.12716278433799744\n",
            "step: 190, loss: 0.0009822896681725979\n",
            "step: 200, loss: 0.0240604467689991\n",
            "step: 210, loss: 0.004464657977223396\n",
            "step: 220, loss: 0.004504515323787928\n",
            "step: 230, loss: 0.002690643072128296\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9786276715410572, f1=0.9831271091113611, best_f1=0.9842696629213483\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01652432046830654\n",
            "step: 10, loss: 0.003782259998843074\n",
            "step: 20, loss: 0.08296452462673187\n",
            "step: 30, loss: 0.0012121874606236815\n",
            "step: 40, loss: 0.0049170684069395065\n",
            "step: 50, loss: 0.012001731432974339\n",
            "step: 60, loss: 0.034413158893585205\n",
            "step: 70, loss: 0.05344155803322792\n",
            "step: 80, loss: 0.03490067273378372\n",
            "step: 90, loss: 0.029498320072889328\n",
            "step: 100, loss: 0.0002416380593786016\n",
            "step: 110, loss: 0.009365974925458431\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 120, loss: 0.05940457060933113\n",
            "step: 130, loss: 0.0250677652657032\n",
            "step: 140, loss: 0.08047596365213394\n",
            "step: 150, loss: 0.15672282874584198\n",
            "step: 160, loss: 0.03270735964179039\n",
            "step: 170, loss: 0.08840573579072952\n",
            "step: 180, loss: 0.011220552958548069\n",
            "step: 190, loss: 0.011949364095926285\n",
            "step: 200, loss: 0.015168348327279091\n",
            "step: 210, loss: 0.002994536655023694\n",
            "step: 220, loss: 0.01017130445688963\n",
            "step: 230, loss: 0.011136564426124096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.983050847457627, f1=0.9875424688561721, best_f1=0.9875424688561721\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001296313712373376\n",
            "step: 10, loss: 0.023783622309565544\n",
            "step: 20, loss: 0.0013111650478094816\n",
            "step: 30, loss: 0.021091589704155922\n",
            "step: 40, loss: 0.0004969686851836741\n",
            "step: 50, loss: 0.008713659830391407\n",
            "step: 60, loss: 0.031143661588430405\n",
            "step: 70, loss: 0.00043924301280640066\n",
            "step: 80, loss: 0.013029232621192932\n",
            "step: 90, loss: 0.016282090917229652\n",
            "step: 100, loss: 0.0008650583913549781\n",
            "step: 110, loss: 0.11362996697425842\n",
            "step: 120, loss: 0.0002959483826998621\n",
            "step: 130, loss: 0.05266730859875679\n",
            "step: 140, loss: 0.006631330121308565\n",
            "step: 150, loss: 0.002339593367651105\n",
            "step: 160, loss: 0.006471929140388966\n",
            "step: 170, loss: 0.00045578714343719184\n",
            "step: 180, loss: 0.00033229784457944334\n",
            "step: 190, loss: 0.0027514277026057243\n",
            "step: 200, loss: 0.009115728549659252\n",
            "step: 210, loss: 0.001321954419836402\n",
            "step: 220, loss: 0.10745427012443542\n",
            "step: 230, loss: 0.0004528223071247339\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.984304932735426, f1=0.9784824462061155, best_f1=0.9784824462061155\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016018958762288094\n",
            "step: 10, loss: 0.000506450654938817\n",
            "step: 20, loss: 0.0001506220578448847\n",
            "step: 30, loss: 8.598714339314029e-05\n",
            "step: 40, loss: 0.0016296057729050517\n",
            "step: 50, loss: 0.0002877672668546438\n",
            "step: 60, loss: 0.00018288390128873289\n",
            "step: 70, loss: 0.0002951862115878612\n",
            "step: 80, loss: 0.0038829382974654436\n",
            "step: 90, loss: 0.00144118198659271\n",
            "step: 100, loss: 0.001063240459188819\n",
            "step: 110, loss: 0.0001849825493991375\n",
            "step: 120, loss: 0.0031573795713484287\n",
            "step: 130, loss: 0.0001700633147265762\n",
            "step: 140, loss: 0.00024041591677814722\n",
            "step: 150, loss: 0.015927033498883247\n",
            "step: 160, loss: 0.00023017071362119168\n",
            "step: 170, loss: 0.0023829860147088766\n",
            "step: 180, loss: 0.004143036901950836\n",
            "step: 190, loss: 0.0001193142743431963\n",
            "step: 200, loss: 0.003686124226078391\n",
            "step: 210, loss: 0.00027161481557413936\n",
            "step: 220, loss: 0.00014468046720139682\n",
            "step: 230, loss: 0.003039587987586856\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9844097995545658, f1=0.9842342342342343, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00022214764612726867\n",
            "step: 10, loss: 0.002521441550925374\n",
            "step: 20, loss: 0.001158541301265359\n",
            "step: 30, loss: 0.0003226103144697845\n",
            "step: 40, loss: 0.025545889511704445\n",
            "step: 50, loss: 0.000447397178504616\n",
            "step: 60, loss: 0.00041365320794284344\n",
            "step: 70, loss: 0.00015426345635205507\n",
            "step: 80, loss: 0.00044039799831807613\n",
            "step: 90, loss: 0.00017898519581649452\n",
            "step: 100, loss: 0.0016063280636444688\n",
            "step: 110, loss: 0.008756055496633053\n",
            "step: 120, loss: 0.00046042934991419315\n",
            "step: 130, loss: 0.0005618403665721416\n",
            "step: 140, loss: 0.0003728965821210295\n",
            "step: 150, loss: 0.02048114873468876\n",
            "step: 160, loss: 0.0017063500126823783\n",
            "step: 170, loss: 0.042576227337121964\n",
            "step: 180, loss: 0.0033891659695655107\n",
            "step: 190, loss: 0.002116963267326355\n",
            "step: 200, loss: 9.613288420950994e-05\n",
            "step: 210, loss: 0.0023527902085334063\n",
            "step: 220, loss: 0.006639664061367512\n",
            "step: 230, loss: 0.007528066635131836\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9820627802690582, f1=0.979591836734694, best_f1=0.9842342342342343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021785960416309536\n",
            "step: 10, loss: 0.00040250312304124236\n",
            "step: 20, loss: 0.12403469532728195\n",
            "step: 30, loss: 0.00041793289710767567\n",
            "step: 40, loss: 0.00039703049696981907\n",
            "step: 50, loss: 0.0017814399907365441\n",
            "step: 60, loss: 0.002653210423886776\n",
            "step: 70, loss: 0.06621233373880386\n",
            "step: 80, loss: 0.0016771507216617465\n",
            "step: 90, loss: 0.06418151408433914\n",
            "step: 100, loss: 0.00011606374027905986\n",
            "step: 110, loss: 0.00015473426901735365\n",
            "step: 120, loss: 0.007146098650991917\n",
            "step: 130, loss: 0.0003944797208532691\n",
            "step: 140, loss: 0.0013642878038808703\n",
            "step: 150, loss: 0.00023450936714652926\n",
            "step: 160, loss: 0.00047648866893723607\n",
            "step: 170, loss: 0.00020325025252532214\n",
            "step: 180, loss: 0.008449436165392399\n",
            "step: 190, loss: 0.0053817881271243095\n",
            "step: 200, loss: 0.0006297044456005096\n",
            "step: 210, loss: 0.001967457588762045\n",
            "step: 220, loss: 0.0001784659834811464\n",
            "step: 230, loss: 0.00024879787815734744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9864559819413092, f1=0.984090909090909, best_f1=0.984090909090909\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016129047435242683\n",
            "step: 10, loss: 0.00015630151028744876\n",
            "step: 20, loss: 0.0037590127903968096\n",
            "step: 30, loss: 8.345271635334939e-05\n",
            "step: 40, loss: 0.0016175901982933283\n",
            "step: 50, loss: 0.01635199412703514\n",
            "step: 60, loss: 0.00016275691450573504\n",
            "step: 70, loss: 0.0002519915287848562\n",
            "step: 80, loss: 0.00011302579514449462\n",
            "step: 90, loss: 7.993293547770008e-05\n",
            "step: 100, loss: 0.00023692422837484628\n",
            "step: 110, loss: 0.0023727002553641796\n",
            "step: 120, loss: 0.00010575114720268175\n",
            "step: 130, loss: 0.0001546186103951186\n",
            "step: 140, loss: 0.00013102867524139583\n",
            "step: 150, loss: 0.014341719448566437\n",
            "step: 160, loss: 0.0001071694350684993\n",
            "step: 170, loss: 0.0001387877855449915\n",
            "step: 180, loss: 0.0036098239943385124\n",
            "step: 190, loss: 0.0010047554969787598\n",
            "step: 200, loss: 0.0009127795347012579\n",
            "step: 210, loss: 0.15636050701141357\n",
            "step: 220, loss: 0.0002305733214598149\n",
            "step: 230, loss: 0.010273666121065617\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9840546697038726, f1=0.9817767653758542, best_f1=0.984090909090909\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007425214862450957\n",
            "step: 10, loss: 0.0024188512470573187\n",
            "step: 20, loss: 0.0005368830752559006\n",
            "step: 30, loss: 0.0003530595568008721\n",
            "step: 40, loss: 5.087699901196174e-05\n",
            "step: 50, loss: 0.00020003062672913074\n",
            "step: 60, loss: 0.0015509941149502993\n",
            "step: 70, loss: 0.0018888169433921576\n",
            "step: 80, loss: 0.0003071664832532406\n",
            "step: 90, loss: 0.005168256349861622\n",
            "step: 100, loss: 0.00038361220504157245\n",
            "step: 110, loss: 0.006304699927568436\n",
            "step: 120, loss: 0.00039827165892347693\n",
            "step: 130, loss: 0.00011000464292010292\n",
            "step: 140, loss: 0.000166057056048885\n",
            "step: 150, loss: 0.0002025776047958061\n",
            "step: 160, loss: 0.048595547676086426\n",
            "step: 170, loss: 0.001967905554920435\n",
            "step: 180, loss: 0.00015581384650431573\n",
            "step: 190, loss: 0.00017461962124798447\n",
            "step: 200, loss: 0.0051185558550059795\n",
            "step: 210, loss: 9.550434333505109e-05\n",
            "step: 220, loss: 0.0024597805459052324\n",
            "step: 230, loss: 0.0024708088021725416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9831271091113611, f1=0.9852440408626559, best_f1=0.984090909090909\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003154588339384645\n",
            "step: 10, loss: 0.0006393385701812804\n",
            "step: 20, loss: 0.003098512766882777\n",
            "step: 30, loss: 0.0334937646985054\n",
            "step: 40, loss: 0.00015628428081981838\n",
            "step: 50, loss: 0.00031886951182968915\n",
            "step: 60, loss: 0.001052479026839137\n",
            "step: 70, loss: 0.001589268445968628\n",
            "step: 80, loss: 0.00018820175318978727\n",
            "step: 90, loss: 0.0025462552439421415\n",
            "step: 100, loss: 0.00010471537098055705\n",
            "step: 110, loss: 6.949093949515373e-05\n",
            "step: 120, loss: 0.00011953273497056216\n",
            "step: 130, loss: 0.0037367776967585087\n",
            "step: 140, loss: 0.00012555537978187203\n",
            "step: 150, loss: 0.00019579687796067446\n",
            "step: 160, loss: 0.0009187937248498201\n",
            "step: 170, loss: 0.012379252351820469\n",
            "step: 180, loss: 0.0002081599086523056\n",
            "step: 190, loss: 0.0021726733539253473\n",
            "step: 200, loss: 0.00396308908239007\n",
            "step: 210, loss: 0.003049410181120038\n",
            "step: 220, loss: 0.0005120654823258519\n",
            "step: 230, loss: 0.0036688433028757572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9820627802690582, f1=0.9796380090497738, best_f1=0.984090909090909\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027725850231945515\n",
            "step: 10, loss: 5.430261080618948e-05\n",
            "step: 20, loss: 0.0006476076669059694\n",
            "step: 30, loss: 4.797057408723049e-05\n",
            "step: 40, loss: 7.12958790245466e-05\n",
            "step: 50, loss: 0.0004190905310679227\n",
            "step: 60, loss: 0.008875411935150623\n",
            "step: 70, loss: 0.0003464212059043348\n",
            "step: 80, loss: 0.00018523013568483293\n",
            "step: 90, loss: 4.9835321988211945e-05\n",
            "step: 100, loss: 0.0004642051353584975\n",
            "step: 110, loss: 0.005275449715554714\n",
            "step: 120, loss: 7.66482698963955e-05\n",
            "step: 130, loss: 8.054973295656964e-05\n",
            "step: 140, loss: 5.8280227676732466e-05\n",
            "step: 150, loss: 7.892078428994864e-05\n",
            "step: 160, loss: 0.0003503094194456935\n",
            "step: 170, loss: 0.00040834309766069055\n",
            "step: 180, loss: 0.01752229779958725\n",
            "step: 190, loss: 0.0004982220707461238\n",
            "step: 200, loss: 3.193755037500523e-05\n",
            "step: 210, loss: 0.0006613027071580291\n",
            "step: 220, loss: 0.00014356324390973896\n",
            "step: 230, loss: 0.0006235099281184375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9852774631936579, f1=0.9829351535836178, best_f1=0.984090909090909\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012661839718930423\n",
            "step: 10, loss: 7.193889177870005e-05\n",
            "step: 20, loss: 0.001347532612271607\n",
            "step: 30, loss: 0.00036243474460206926\n",
            "step: 40, loss: 0.000748537655454129\n",
            "step: 50, loss: 6.290247983997688e-05\n",
            "step: 60, loss: 8.110873750410974e-05\n",
            "step: 70, loss: 0.15365830063819885\n",
            "step: 80, loss: 0.00015803858696017414\n",
            "step: 90, loss: 0.002506461227312684\n",
            "step: 100, loss: 0.0003668049757834524\n",
            "step: 110, loss: 0.00020050164312124252\n",
            "step: 120, loss: 3.120899054920301e-05\n",
            "step: 130, loss: 0.00028524361550807953\n",
            "step: 140, loss: 7.155702769523486e-05\n",
            "step: 150, loss: 9.854482777882367e-05\n",
            "step: 160, loss: 0.002379852579906583\n",
            "step: 170, loss: 0.00011521793931024149\n",
            "step: 180, loss: 6.580066110473126e-05\n",
            "step: 190, loss: 0.002628766931593418\n",
            "step: 200, loss: 0.0002535637468099594\n",
            "step: 210, loss: 0.00010808013757923618\n",
            "step: 220, loss: 0.00012773652269970626\n",
            "step: 230, loss: 5.8979905588785186e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9875706214689265, f1=0.9829738933030647, best_f1=0.9829738933030647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004937507212162018\n",
            "step: 10, loss: 0.0004324932233430445\n",
            "step: 20, loss: 0.0037092517595738173\n",
            "step: 30, loss: 8.902431727619842e-05\n",
            "step: 40, loss: 4.2610117816366255e-05\n",
            "step: 50, loss: 8.821905066724867e-05\n",
            "step: 60, loss: 0.015965469181537628\n",
            "step: 70, loss: 0.0002185619086958468\n",
            "step: 80, loss: 5.986998075968586e-05\n",
            "step: 90, loss: 9.399048576597124e-05\n",
            "step: 100, loss: 3.59203404514119e-05\n",
            "step: 110, loss: 0.004233133047819138\n",
            "step: 120, loss: 0.0048450748436152935\n",
            "step: 130, loss: 0.00345942797139287\n",
            "step: 140, loss: 0.01232092548161745\n",
            "step: 150, loss: 0.00018497955170460045\n",
            "step: 160, loss: 0.0024385375436395407\n",
            "step: 170, loss: 7.100641960278153e-05\n",
            "step: 180, loss: 0.0003189168346580118\n",
            "step: 190, loss: 0.00014379926142282784\n",
            "step: 200, loss: 0.0016721318243071437\n",
            "step: 210, loss: 0.01896514743566513\n",
            "step: 220, loss: 0.00018268398707732558\n",
            "step: 230, loss: 8.789252751739696e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9853107344632768, f1=0.9829738933030647, best_f1=0.9829738933030647\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:17, 143.85it/s]\n",
            "load_f1 = 0.9864559819413092\n",
            "real_f1 = 0.9864559819413092\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 134.11it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrVM9KP9hstt"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93faeb8e-458a-4042-e4e1-868bb5424b49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.617284893989563\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.4782482385635376\n",
            "step: 20, loss: 0.26416030526161194\n",
            "step: 30, loss: 0.3762555718421936\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 40, loss: 0.2739384174346924\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 50, loss: 1.0934109687805176\n",
            "step: 60, loss: 0.24329063296318054\n",
            "step: 70, loss: 0.25844255089759827\n",
            "step: 80, loss: 0.08348731696605682\n",
            "step: 90, loss: 0.14782845973968506\n",
            "step: 100, loss: 0.2858273983001709\n",
            "step: 110, loss: 0.11756345629692078\n",
            "step: 120, loss: 0.14633987843990326\n",
            "step: 130, loss: 0.1415497362613678\n",
            "step: 140, loss: 0.33209797739982605\n",
            "step: 150, loss: 0.10859663039445877\n",
            "step: 160, loss: 0.17688730359077454\n",
            "step: 170, loss: 0.11697231233119965\n",
            "step: 180, loss: 0.2103944569826126\n",
            "step: 190, loss: 0.2411806732416153\n",
            "step: 200, loss: 0.08068548887968063\n",
            "step: 210, loss: 0.036091435700654984\n",
            "step: 220, loss: 0.07114587724208832\n",
            "step: 230, loss: 0.20119167864322662\n",
            "step: 240, loss: 0.04284684360027313\n",
            "step: 250, loss: 0.11961301416158676\n",
            "step: 260, loss: 0.17455345392227173\n",
            "step: 270, loss: 0.374035120010376\n",
            "step: 280, loss: 0.13779973983764648\n",
            "step: 290, loss: 0.1344231367111206\n",
            "step: 300, loss: 0.161468505859375\n",
            "step: 310, loss: 0.1975269466638565\n",
            "step: 320, loss: 0.061528582125902176\n",
            "step: 330, loss: 0.05920959636569023\n",
            "step: 340, loss: 0.4707822799682617\n",
            "step: 350, loss: 0.2059343159198761\n",
            "step: 360, loss: 0.07145454734563828\n",
            "step: 370, loss: 0.03629399091005325\n",
            "step: 380, loss: 0.13232187926769257\n",
            "step: 390, loss: 0.04798588901758194\n",
            "step: 400, loss: 0.05557509511709213\n",
            "step: 410, loss: 0.2910923957824707\n",
            "step: 420, loss: 0.056954339146614075\n",
            "step: 430, loss: 0.10943325608968735\n",
            "step: 440, loss: 0.08835215121507645\n",
            "step: 450, loss: 0.1162901371717453\n",
            "step: 460, loss: 0.0774737223982811\n",
            "step: 470, loss: 0.1139431893825531\n",
            "step: 480, loss: 0.22768770158290863\n",
            "step: 490, loss: 0.2651587724685669\n",
            "step: 500, loss: 0.047581352293491364\n",
            "step: 510, loss: 0.04038761928677559\n",
            "step: 520, loss: 0.12636622786521912\n",
            "step: 530, loss: 0.04483238607645035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.902405810258738, f1=0.9009090909090909, best_f1=0.9009090909090909\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17268840968608856\n",
            "step: 10, loss: 0.08897114545106888\n",
            "step: 20, loss: 0.05880781635642052\n",
            "step: 30, loss: 0.1566813439130783\n",
            "step: 40, loss: 0.0342981182038784\n",
            "step: 50, loss: 0.1337858885526657\n",
            "step: 60, loss: 0.1265101432800293\n",
            "step: 70, loss: 0.1586204469203949\n",
            "step: 80, loss: 0.11411997675895691\n",
            "step: 90, loss: 0.04341376945376396\n",
            "step: 100, loss: 0.18650870025157928\n",
            "step: 110, loss: 0.01991661824285984\n",
            "step: 120, loss: 0.12764319777488708\n",
            "step: 130, loss: 0.07333475351333618\n",
            "step: 140, loss: 0.06699039041996002\n",
            "step: 150, loss: 0.11399953067302704\n",
            "step: 160, loss: 0.04118715599179268\n",
            "step: 170, loss: 0.051527105271816254\n",
            "step: 180, loss: 0.021843433380126953\n",
            "step: 190, loss: 0.03973539173603058\n",
            "step: 200, loss: 0.25730112195014954\n",
            "step: 210, loss: 0.1102830246090889\n",
            "step: 220, loss: 0.0038650077767670155\n",
            "step: 230, loss: 0.10369934141635895\n",
            "step: 240, loss: 0.04512151703238487\n",
            "step: 250, loss: 0.03201952949166298\n",
            "step: 260, loss: 0.1500484198331833\n",
            "step: 270, loss: 0.03312751650810242\n",
            "step: 280, loss: 0.060651738196611404\n",
            "step: 290, loss: 0.1462966650724411\n",
            "step: 300, loss: 0.059456534683704376\n",
            "step: 310, loss: 0.11549657583236694\n",
            "step: 320, loss: 0.1504022628068924\n",
            "step: 330, loss: 0.07149215042591095\n",
            "step: 340, loss: 0.30333590507507324\n",
            "step: 350, loss: 0.08377712219953537\n",
            "step: 360, loss: 0.12130790203809738\n",
            "step: 370, loss: 0.007836582139134407\n",
            "step: 380, loss: 0.12473524361848831\n",
            "step: 390, loss: 0.021747518330812454\n",
            "step: 400, loss: 0.08214353770017624\n",
            "step: 410, loss: 0.10551035404205322\n",
            "step: 420, loss: 0.12238747626543045\n",
            "step: 430, loss: 0.21629106998443604\n",
            "step: 440, loss: 0.02836407534778118\n",
            "step: 450, loss: 0.06698336452245712\n",
            "step: 460, loss: 0.12730246782302856\n",
            "step: 470, loss: 0.138342022895813\n",
            "step: 480, loss: 0.031239259988069534\n",
            "step: 490, loss: 0.2398652881383896\n",
            "step: 500, loss: 0.005841784179210663\n",
            "step: 510, loss: 0.06795677542686462\n",
            "step: 520, loss: 0.4165225028991699\n",
            "step: 530, loss: 0.0658244714140892\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9192660550458716, f1=0.9187471629596006, best_f1=0.9187471629596006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.150130033493042\n",
            "step: 10, loss: 0.0314149335026741\n",
            "step: 20, loss: 0.024425925686955452\n",
            "step: 30, loss: 0.09067873656749725\n",
            "step: 40, loss: 0.03768138214945793\n",
            "step: 50, loss: 0.09324091672897339\n",
            "step: 60, loss: 0.061903856694698334\n",
            "step: 70, loss: 0.013112800195813179\n",
            "step: 80, loss: 0.2578161656856537\n",
            "step: 90, loss: 0.03255307674407959\n",
            "step: 100, loss: 0.15077190101146698\n",
            "step: 110, loss: 0.0723617747426033\n",
            "step: 120, loss: 0.1358250081539154\n",
            "step: 130, loss: 0.03988385200500488\n",
            "step: 140, loss: 0.007366796489804983\n",
            "step: 150, loss: 0.023759102448821068\n",
            "step: 160, loss: 0.027168385684490204\n",
            "step: 170, loss: 0.01638917811214924\n",
            "step: 180, loss: 0.056700751185417175\n",
            "step: 190, loss: 0.008512300439178944\n",
            "step: 200, loss: 0.09656774252653122\n",
            "step: 210, loss: 0.08446631580591202\n",
            "step: 220, loss: 0.017910132184624672\n",
            "step: 230, loss: 0.07040873169898987\n",
            "step: 240, loss: 0.08308633416891098\n",
            "step: 250, loss: 0.12170187383890152\n",
            "step: 260, loss: 0.059775400906801224\n",
            "step: 270, loss: 0.007328015752136707\n",
            "step: 280, loss: 0.015342137776315212\n",
            "step: 290, loss: 0.020687632262706757\n",
            "step: 300, loss: 0.2688521146774292\n",
            "step: 310, loss: 0.17679083347320557\n",
            "step: 320, loss: 0.00703124376013875\n",
            "step: 330, loss: 0.0026813684962689877\n",
            "step: 340, loss: 0.019692936912178993\n",
            "step: 350, loss: 0.1843792349100113\n",
            "step: 360, loss: 0.028090955689549446\n",
            "step: 370, loss: 0.07734725624322891\n",
            "step: 380, loss: 0.11022278666496277\n",
            "step: 390, loss: 0.05685867369174957\n",
            "step: 400, loss: 0.18076737225055695\n",
            "step: 410, loss: 0.03887759894132614\n",
            "step: 420, loss: 0.10739246010780334\n",
            "step: 430, loss: 0.04649639129638672\n",
            "step: 440, loss: 0.23329459130764008\n",
            "step: 450, loss: 0.17680050432682037\n",
            "step: 460, loss: 0.059122879058122635\n",
            "step: 470, loss: 0.017363250255584717\n",
            "step: 480, loss: 0.200330451130867\n",
            "step: 490, loss: 0.03886551037430763\n",
            "step: 500, loss: 0.05955536291003227\n",
            "step: 510, loss: 0.0581543892621994\n",
            "step: 520, loss: 0.01138918288052082\n",
            "step: 530, loss: 0.03109051287174225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9208566108007449, f1=0.912739150723285, best_f1=0.912739150723285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05033033341169357\n",
            "step: 10, loss: 0.009373889304697514\n",
            "step: 20, loss: 0.12361646443605423\n",
            "step: 30, loss: 0.02361442893743515\n",
            "step: 40, loss: 0.022961929440498352\n",
            "step: 50, loss: 0.03570215776562691\n",
            "step: 60, loss: 0.0045125423930585384\n",
            "step: 70, loss: 0.032733410596847534\n",
            "step: 80, loss: 0.15621614456176758\n",
            "step: 90, loss: 0.09164389222860336\n",
            "step: 100, loss: 0.005166068207472563\n",
            "step: 110, loss: 0.05998534709215164\n",
            "step: 120, loss: 0.04375624656677246\n",
            "step: 130, loss: 0.07668532431125641\n",
            "step: 140, loss: 0.1305665224790573\n",
            "step: 150, loss: 0.04442906752228737\n",
            "step: 160, loss: 0.021223751828074455\n",
            "step: 170, loss: 0.011218964122235775\n",
            "step: 180, loss: 0.05121619626879692\n",
            "step: 190, loss: 0.09152851998806\n",
            "step: 200, loss: 0.05709437280893326\n",
            "step: 210, loss: 0.011368729174137115\n",
            "step: 220, loss: 0.025127733126282692\n",
            "step: 230, loss: 0.02273300103843212\n",
            "step: 240, loss: 0.007052634377032518\n",
            "step: 250, loss: 0.05506129562854767\n",
            "step: 260, loss: 0.0023515745997428894\n",
            "step: 270, loss: 0.17676928639411926\n",
            "step: 280, loss: 0.018789982423186302\n",
            "step: 290, loss: 0.03983933478593826\n",
            "step: 300, loss: 0.03731038048863411\n",
            "step: 310, loss: 0.03782513365149498\n",
            "step: 320, loss: 0.17739567160606384\n",
            "step: 330, loss: 0.009925315156579018\n",
            "step: 340, loss: 0.003854963928461075\n",
            "step: 350, loss: 0.1962089091539383\n",
            "step: 360, loss: 0.06561867892742157\n",
            "step: 370, loss: 0.01496550627052784\n",
            "step: 380, loss: 0.005120525136590004\n",
            "step: 390, loss: 0.0036921128630638123\n",
            "step: 400, loss: 0.005258080083876848\n",
            "step: 410, loss: 0.024025658145546913\n",
            "step: 420, loss: 0.015275309793651104\n",
            "step: 430, loss: 0.011382227763533592\n",
            "step: 440, loss: 0.011120524257421494\n",
            "step: 450, loss: 0.12624305486679077\n",
            "step: 460, loss: 0.0813184380531311\n",
            "step: 470, loss: 0.006204581819474697\n",
            "step: 480, loss: 0.004215720575302839\n",
            "step: 490, loss: 0.01452107634395361\n",
            "step: 500, loss: 0.11615893244743347\n",
            "step: 510, loss: 0.07451581209897995\n",
            "step: 520, loss: 0.013263002969324589\n",
            "step: 530, loss: 0.045820172876119614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9302112029384756, f1=0.9231473010064044, best_f1=0.9231473010064044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006927515845745802\n",
            "step: 10, loss: 0.01601782627403736\n",
            "step: 20, loss: 0.009186084382236004\n",
            "step: 30, loss: 0.1691954880952835\n",
            "step: 40, loss: 0.011237859725952148\n",
            "step: 50, loss: 0.07003667205572128\n",
            "step: 60, loss: 0.021835096180438995\n",
            "step: 70, loss: 0.004522776696830988\n",
            "step: 80, loss: 0.00812008511275053\n",
            "step: 90, loss: 0.025008831173181534\n",
            "step: 100, loss: 0.049243975430727005\n",
            "step: 110, loss: 0.006102921906858683\n",
            "step: 120, loss: 0.1743173450231552\n",
            "step: 130, loss: 0.0038408413529396057\n",
            "step: 140, loss: 0.011179951019585133\n",
            "step: 150, loss: 0.02607707493007183\n",
            "step: 160, loss: 0.12838232517242432\n",
            "step: 170, loss: 0.03452931344509125\n",
            "step: 180, loss: 0.007559336721897125\n",
            "step: 190, loss: 0.005091474391520023\n",
            "step: 200, loss: 0.0034642298705875874\n",
            "step: 210, loss: 0.10932209342718124\n",
            "step: 220, loss: 0.019235901534557343\n",
            "step: 230, loss: 0.020300429314374924\n",
            "step: 240, loss: 0.008342843502759933\n",
            "step: 250, loss: 0.07067295163869858\n",
            "step: 260, loss: 0.014347872696816921\n",
            "step: 270, loss: 0.0030306524131447077\n",
            "step: 280, loss: 0.11619210243225098\n",
            "step: 290, loss: 0.040917254984378815\n",
            "step: 300, loss: 0.20227758586406708\n",
            "step: 310, loss: 0.0837261751294136\n",
            "step: 320, loss: 0.09018533676862717\n",
            "step: 330, loss: 0.023619357496500015\n",
            "step: 340, loss: 0.011138034984469414\n",
            "step: 350, loss: 0.025123801082372665\n",
            "step: 360, loss: 0.0008549454505555332\n",
            "step: 370, loss: 0.0011741707567125559\n",
            "step: 380, loss: 0.002104658167809248\n",
            "step: 390, loss: 0.06895823776721954\n",
            "step: 400, loss: 0.11991286277770996\n",
            "step: 410, loss: 0.06075707823038101\n",
            "step: 420, loss: 0.19273002445697784\n",
            "step: 430, loss: 0.04136325418949127\n",
            "step: 440, loss: 0.0040584770031273365\n",
            "step: 450, loss: 0.0016299603739753366\n",
            "step: 460, loss: 0.030864309519529343\n",
            "step: 470, loss: 0.0513264462351799\n",
            "step: 480, loss: 0.03631729632616043\n",
            "step: 490, loss: 0.028187397867441177\n",
            "step: 500, loss: 0.035610396414995193\n",
            "step: 510, loss: 0.02128816582262516\n",
            "step: 520, loss: 0.27299779653549194\n",
            "step: 530, loss: 0.017031561583280563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.932963476652797, f1=0.9314179796107508, best_f1=0.9314179796107508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0251386146992445\n",
            "step: 10, loss: 0.0057729026302695274\n",
            "step: 20, loss: 0.00846484862267971\n",
            "step: 30, loss: 0.0013159915106371045\n",
            "step: 40, loss: 0.05818937346339226\n",
            "step: 50, loss: 0.026824288070201874\n",
            "step: 60, loss: 0.03099253587424755\n",
            "step: 70, loss: 0.004268394783139229\n",
            "step: 80, loss: 0.03770576789975166\n",
            "step: 90, loss: 0.12578411400318146\n",
            "step: 100, loss: 0.05389925092458725\n",
            "step: 110, loss: 0.021039795130491257\n",
            "step: 120, loss: 0.04966992512345314\n",
            "step: 130, loss: 0.0032676421105861664\n",
            "step: 140, loss: 0.0336383692920208\n",
            "step: 150, loss: 0.01015594881027937\n",
            "step: 160, loss: 0.050899941474199295\n",
            "step: 170, loss: 0.0024933863896876574\n",
            "step: 180, loss: 0.008803987875580788\n",
            "step: 190, loss: 0.16579844057559967\n",
            "step: 200, loss: 0.1505102962255478\n",
            "step: 210, loss: 0.004796930588781834\n",
            "step: 220, loss: 0.027705969288945198\n",
            "step: 230, loss: 0.07387477159500122\n",
            "step: 240, loss: 0.10668005049228668\n",
            "step: 250, loss: 0.05267135053873062\n",
            "step: 260, loss: 0.021744804456830025\n",
            "step: 270, loss: 0.010953249409794807\n",
            "step: 280, loss: 0.1050516813993454\n",
            "step: 290, loss: 0.07822467386722565\n",
            "step: 300, loss: 0.004239434376358986\n",
            "step: 310, loss: 0.059788789600133896\n",
            "step: 320, loss: 0.002813818631693721\n",
            "step: 330, loss: 0.01549531053751707\n",
            "step: 340, loss: 0.001067449222318828\n",
            "step: 350, loss: 0.0055549475364387035\n",
            "step: 360, loss: 0.026470327749848366\n",
            "step: 370, loss: 0.013340862467885017\n",
            "step: 380, loss: 0.001798618002794683\n",
            "step: 390, loss: 0.003585513448342681\n",
            "step: 400, loss: 0.05383109673857689\n",
            "step: 410, loss: 0.002912794006988406\n",
            "step: 420, loss: 0.004142374731600285\n",
            "step: 430, loss: 0.00045239069731906056\n",
            "step: 440, loss: 0.0016848387895151973\n",
            "step: 450, loss: 0.175912544131279\n",
            "step: 460, loss: 0.011207680217921734\n",
            "step: 470, loss: 0.0017661906313151121\n",
            "step: 480, loss: 0.012238946743309498\n",
            "step: 490, loss: 0.007146426010876894\n",
            "step: 500, loss: 0.001662468770518899\n",
            "step: 510, loss: 0.26738229393959045\n",
            "step: 520, loss: 0.0003026924387086183\n",
            "step: 530, loss: 0.0021439732518047094\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9286043298019346, f1=0.9174825174825175, best_f1=0.9314179796107508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003126990981400013\n",
            "step: 10, loss: 0.025847947224974632\n",
            "step: 20, loss: 0.008445674553513527\n",
            "step: 30, loss: 0.02131718397140503\n",
            "step: 40, loss: 0.007984454743564129\n",
            "step: 50, loss: 0.015561887994408607\n",
            "step: 60, loss: 0.002972874790430069\n",
            "step: 70, loss: 0.0005530305206775665\n",
            "step: 80, loss: 0.07013770192861557\n",
            "step: 90, loss: 0.0008748235995881259\n",
            "step: 100, loss: 0.004403166007250547\n",
            "step: 110, loss: 0.0032700528390705585\n",
            "step: 120, loss: 0.007362982723861933\n",
            "step: 130, loss: 0.014032007195055485\n",
            "step: 140, loss: 0.011053254827857018\n",
            "step: 150, loss: 0.011300662532448769\n",
            "step: 160, loss: 0.0028897763695567846\n",
            "step: 170, loss: 0.010036535561084747\n",
            "step: 180, loss: 0.10986049473285675\n",
            "step: 190, loss: 0.0479092039167881\n",
            "step: 200, loss: 0.002112818881869316\n",
            "step: 210, loss: 0.001654896535910666\n",
            "step: 220, loss: 0.046246081590652466\n",
            "step: 230, loss: 0.0009951061801984906\n",
            "step: 240, loss: 0.0015333350747823715\n",
            "step: 250, loss: 0.020488496869802475\n",
            "step: 260, loss: 0.0022479002363979816\n",
            "step: 270, loss: 0.01636660285294056\n",
            "step: 280, loss: 0.0055391788482666016\n",
            "step: 290, loss: 0.01459481380879879\n",
            "step: 300, loss: 0.003379348898306489\n",
            "step: 310, loss: 0.07356192171573639\n",
            "step: 320, loss: 0.007871480658650398\n",
            "step: 330, loss: 0.0010709615889936686\n",
            "step: 340, loss: 0.0016459182370454073\n",
            "step: 350, loss: 0.00863460823893547\n",
            "step: 360, loss: 0.00558296637609601\n",
            "step: 370, loss: 0.004046321380883455\n",
            "step: 380, loss: 0.0011179291177541018\n",
            "step: 390, loss: 0.02620280534029007\n",
            "step: 400, loss: 0.015949856489896774\n",
            "step: 410, loss: 0.008249735459685326\n",
            "step: 420, loss: 0.006872677709907293\n",
            "step: 430, loss: 0.04505356401205063\n",
            "step: 440, loss: 0.010788715444505215\n",
            "step: 450, loss: 0.26719415187835693\n",
            "step: 460, loss: 0.005323857069015503\n",
            "step: 470, loss: 0.1582753211259842\n",
            "step: 480, loss: 0.01939881592988968\n",
            "step: 490, loss: 0.00498679606243968\n",
            "step: 500, loss: 0.010003272444009781\n",
            "step: 510, loss: 0.0005342514486983418\n",
            "step: 520, loss: 0.005458083003759384\n",
            "step: 530, loss: 0.002290351316332817\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9215955983493811, f1=0.9092592592592593, best_f1=0.9314179796107508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000960883277002722\n",
            "step: 10, loss: 0.008751815184950829\n",
            "step: 20, loss: 0.0008988957852125168\n",
            "step: 30, loss: 0.006863206159323454\n",
            "step: 40, loss: 0.006608037743717432\n",
            "step: 50, loss: 0.05232347548007965\n",
            "step: 60, loss: 0.001079275505617261\n",
            "step: 70, loss: 0.007398138288408518\n",
            "step: 80, loss: 0.005146307405084372\n",
            "step: 90, loss: 0.027069615200161934\n",
            "step: 100, loss: 0.011090937070548534\n",
            "step: 110, loss: 0.0022715460509061813\n",
            "step: 120, loss: 0.0039028034079819918\n",
            "step: 130, loss: 0.0073945242911577225\n",
            "step: 140, loss: 0.002104600891470909\n",
            "step: 150, loss: 0.004275529645383358\n",
            "step: 160, loss: 0.014258806593716145\n",
            "step: 170, loss: 0.03688448667526245\n",
            "step: 180, loss: 0.005894681438803673\n",
            "step: 190, loss: 0.1110907793045044\n",
            "step: 200, loss: 0.012748047709465027\n",
            "step: 210, loss: 0.05194050073623657\n",
            "step: 220, loss: 0.015756191685795784\n",
            "step: 230, loss: 0.13396646082401276\n",
            "step: 240, loss: 0.03495645895600319\n",
            "step: 250, loss: 0.0013907711254432797\n",
            "step: 260, loss: 0.005435395520180464\n",
            "step: 270, loss: 0.05292563512921333\n",
            "step: 280, loss: 0.0009389918413944542\n",
            "step: 290, loss: 0.03542700409889221\n",
            "step: 300, loss: 0.0024252873845398426\n",
            "step: 310, loss: 0.0014204145409166813\n",
            "step: 320, loss: 0.0038502365350723267\n",
            "step: 330, loss: 0.013034873642027378\n",
            "step: 340, loss: 0.010962097905576229\n",
            "step: 350, loss: 0.0028318173717707396\n",
            "step: 360, loss: 0.030214639380574226\n",
            "step: 370, loss: 0.16304516792297363\n",
            "step: 380, loss: 0.0004678385448642075\n",
            "step: 390, loss: 0.0025676912628114223\n",
            "step: 400, loss: 0.03234437480568886\n",
            "step: 410, loss: 0.006061968393623829\n",
            "step: 420, loss: 0.0005152482772246003\n",
            "step: 430, loss: 0.00317495409399271\n",
            "step: 440, loss: 0.08541222661733627\n",
            "step: 450, loss: 0.0030380531679838896\n",
            "step: 460, loss: 0.036524392664432526\n",
            "step: 470, loss: 0.031705062836408615\n",
            "step: 480, loss: 0.03659329563379288\n",
            "step: 490, loss: 0.006825421005487442\n",
            "step: 500, loss: 0.0033034367952495813\n",
            "step: 510, loss: 0.07784533500671387\n",
            "step: 520, loss: 0.0007455720915459096\n",
            "step: 530, loss: 0.016583019867539406\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9269662921348314, f1=0.9199063231850116, best_f1=0.9314179796107508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005185039481148124\n",
            "step: 10, loss: 0.00573626859113574\n",
            "step: 20, loss: 0.0012510053347796202\n",
            "step: 30, loss: 0.19032733142375946\n",
            "step: 40, loss: 0.0023616435937583447\n",
            "step: 50, loss: 0.001021567964926362\n",
            "step: 60, loss: 0.00032683450262993574\n",
            "step: 70, loss: 0.0022714051883667707\n",
            "step: 80, loss: 0.0038024389650672674\n",
            "step: 90, loss: 0.0374935120344162\n",
            "step: 100, loss: 0.0013368877116590738\n",
            "step: 110, loss: 0.018323004245758057\n",
            "step: 120, loss: 0.0029490827582776546\n",
            "step: 130, loss: 0.0006655059405602515\n",
            "step: 140, loss: 0.0006133682327345014\n",
            "step: 150, loss: 0.0031699086539447308\n",
            "step: 160, loss: 0.001524075516499579\n",
            "step: 170, loss: 0.009833212941884995\n",
            "step: 180, loss: 0.0016673324862495065\n",
            "step: 190, loss: 0.0005782920052297413\n",
            "step: 200, loss: 0.05501784756779671\n",
            "step: 210, loss: 0.14902816712856293\n",
            "step: 220, loss: 0.004255413077771664\n",
            "step: 230, loss: 0.0004896405152976513\n",
            "step: 240, loss: 0.0017327357782050967\n",
            "step: 250, loss: 0.042819101363420486\n",
            "step: 260, loss: 0.03536440059542656\n",
            "step: 270, loss: 0.00944449845701456\n",
            "step: 280, loss: 0.01343455445021391\n",
            "step: 290, loss: 0.0014820712385699153\n",
            "step: 300, loss: 0.002620021114125848\n",
            "step: 310, loss: 0.0078084515407681465\n",
            "step: 320, loss: 0.005134719889611006\n",
            "step: 330, loss: 0.012331499718129635\n",
            "step: 340, loss: 0.03410129249095917\n",
            "step: 350, loss: 0.06584448367357254\n",
            "step: 360, loss: 0.0011448776349425316\n",
            "step: 370, loss: 0.004562468733638525\n",
            "step: 380, loss: 0.0007956580957397819\n",
            "step: 390, loss: 6.141741323517635e-05\n",
            "step: 400, loss: 0.0583292692899704\n",
            "step: 410, loss: 0.001276268158107996\n",
            "step: 420, loss: 0.002065780572593212\n",
            "step: 430, loss: 0.004231240600347519\n",
            "step: 440, loss: 0.00019630538008641452\n",
            "step: 450, loss: 0.005229976959526539\n",
            "step: 460, loss: 0.00043396951514296234\n",
            "step: 470, loss: 0.0006998951430432498\n",
            "step: 480, loss: 0.0007751507801003754\n",
            "step: 490, loss: 0.008083074353635311\n",
            "step: 500, loss: 0.002214076928794384\n",
            "step: 510, loss: 0.008102747611701488\n",
            "step: 520, loss: 0.005369021091610193\n",
            "step: 530, loss: 0.11553199589252472\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9338919925512105, f1=0.9255121042830541, best_f1=0.9255121042830541\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005992721766233444\n",
            "step: 10, loss: 0.002959729405120015\n",
            "step: 20, loss: 0.0008037597872316837\n",
            "step: 30, loss: 0.021741045638918877\n",
            "step: 40, loss: 0.00018528154760133475\n",
            "step: 50, loss: 0.0002935800876002759\n",
            "step: 60, loss: 0.004720487166196108\n",
            "step: 70, loss: 0.007121972739696503\n",
            "step: 80, loss: 0.0021480892319232225\n",
            "step: 90, loss: 0.008022544905543327\n",
            "step: 100, loss: 0.0025052570272237062\n",
            "step: 110, loss: 0.006111218128353357\n",
            "step: 120, loss: 0.00027984281769022346\n",
            "step: 130, loss: 0.08065585047006607\n",
            "step: 140, loss: 0.0030572949908673763\n",
            "step: 150, loss: 0.071358822286129\n",
            "step: 160, loss: 0.01598622463643551\n",
            "step: 170, loss: 0.003937490750104189\n",
            "step: 180, loss: 0.019649315625429153\n",
            "step: 190, loss: 0.002303289482370019\n",
            "step: 200, loss: 0.003943735733628273\n",
            "step: 210, loss: 0.015050258487462997\n",
            "step: 220, loss: 0.0011115869274362922\n",
            "step: 230, loss: 0.0009410008206032217\n",
            "step: 240, loss: 0.0016423598863184452\n",
            "step: 250, loss: 0.01815556176006794\n",
            "step: 260, loss: 0.004198779352009296\n",
            "step: 270, loss: 0.000373553455574438\n",
            "step: 280, loss: 0.008798115886747837\n",
            "step: 290, loss: 0.009173594415187836\n",
            "step: 300, loss: 0.05685081705451012\n",
            "step: 310, loss: 0.014910036697983742\n",
            "step: 320, loss: 0.05782052502036095\n",
            "step: 330, loss: 0.0036791954189538956\n",
            "step: 340, loss: 0.004030929878354073\n",
            "step: 350, loss: 0.00041930811130441725\n",
            "step: 360, loss: 0.00036597560392692685\n",
            "step: 370, loss: 0.0001606543082743883\n",
            "step: 380, loss: 0.001847034553065896\n",
            "step: 390, loss: 0.0005187420174479485\n",
            "step: 400, loss: 0.001469831564463675\n",
            "step: 410, loss: 0.0003560897312127054\n",
            "step: 420, loss: 0.0007333728135563433\n",
            "step: 430, loss: 0.0008061093394644558\n",
            "step: 440, loss: 0.00025405114865861833\n",
            "step: 450, loss: 0.2055104523897171\n",
            "step: 460, loss: 0.0005068997270427644\n",
            "step: 470, loss: 0.001563475583679974\n",
            "step: 480, loss: 0.0022912088315933943\n",
            "step: 490, loss: 0.0011648096842691302\n",
            "step: 500, loss: 0.0010877561289817095\n",
            "step: 510, loss: 0.011052446439862251\n",
            "step: 520, loss: 0.0003902667958755046\n",
            "step: 530, loss: 0.01265582162886858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9305164319248826, f1=0.928772258669166, best_f1=0.9255121042830541\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013146920537110418\n",
            "step: 10, loss: 0.0005849735462106764\n",
            "step: 20, loss: 0.0013305684551596642\n",
            "step: 30, loss: 0.0003890228399541229\n",
            "step: 40, loss: 0.0383707620203495\n",
            "step: 50, loss: 0.00044115117634646595\n",
            "step: 60, loss: 0.008306773379445076\n",
            "step: 70, loss: 0.00036437282687984407\n",
            "step: 80, loss: 0.0006653191521763802\n",
            "step: 90, loss: 0.017144758254289627\n",
            "step: 100, loss: 0.00026783309294842184\n",
            "step: 110, loss: 0.00025730181368999183\n",
            "step: 120, loss: 0.013373693451285362\n",
            "step: 130, loss: 0.0004664890584535897\n",
            "step: 140, loss: 0.001115297549404204\n",
            "step: 150, loss: 0.0012438817648217082\n",
            "step: 160, loss: 0.0014078021049499512\n",
            "step: 170, loss: 0.09832889586687088\n",
            "step: 180, loss: 0.0007477639592252672\n",
            "step: 190, loss: 0.00022522482322528958\n",
            "step: 200, loss: 0.0007285320316441357\n",
            "step: 210, loss: 0.000796284235548228\n",
            "step: 220, loss: 0.015829939395189285\n",
            "step: 230, loss: 0.0002301548229297623\n",
            "step: 240, loss: 0.003089808626100421\n",
            "step: 250, loss: 0.00017669916269369423\n",
            "step: 260, loss: 0.002516944659873843\n",
            "step: 270, loss: 0.003756581572815776\n",
            "step: 280, loss: 0.00026206308393739164\n",
            "step: 290, loss: 0.00017730161198414862\n",
            "step: 300, loss: 0.00015649183478672057\n",
            "step: 310, loss: 0.00037948210956528783\n",
            "step: 320, loss: 0.0011479745153337717\n",
            "step: 330, loss: 2.200473682023585e-05\n",
            "step: 340, loss: 0.002350603463128209\n",
            "step: 350, loss: 0.00013304472668096423\n",
            "step: 360, loss: 0.003943199757486582\n",
            "step: 370, loss: 0.002156467642635107\n",
            "step: 380, loss: 0.01808573305606842\n",
            "step: 390, loss: 0.0011705500073730946\n",
            "step: 400, loss: 0.0004869074618909508\n",
            "step: 410, loss: 0.0008626448106952012\n",
            "step: 420, loss: 0.0009019009885378182\n",
            "step: 430, loss: 0.0020761052146553993\n",
            "step: 440, loss: 9.931746171787381e-05\n",
            "step: 450, loss: 0.0009267627028748393\n",
            "step: 460, loss: 0.05432705581188202\n",
            "step: 470, loss: 0.0020600208081305027\n",
            "step: 480, loss: 0.0006096953293308616\n",
            "step: 490, loss: 0.00017903988191392273\n",
            "step: 500, loss: 0.0002570881333667785\n",
            "step: 510, loss: 0.0015519977314397693\n",
            "step: 520, loss: 0.0005849187145940959\n",
            "step: 530, loss: 0.004614701960235834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9244402985074627, f1=0.9214417744916821, best_f1=0.9255121042830541\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006263298913836479\n",
            "step: 10, loss: 0.000714750902261585\n",
            "step: 20, loss: 0.0017466646386310458\n",
            "step: 30, loss: 0.03130101040005684\n",
            "step: 40, loss: 0.005644228775054216\n",
            "step: 50, loss: 0.00016243643767666072\n",
            "step: 60, loss: 0.00011756987078115344\n",
            "step: 70, loss: 0.003985977731645107\n",
            "step: 80, loss: 0.10755594074726105\n",
            "step: 90, loss: 0.003949123900383711\n",
            "step: 100, loss: 0.007019927725195885\n",
            "step: 110, loss: 0.000842335051856935\n",
            "step: 120, loss: 0.0018181328196078539\n",
            "step: 130, loss: 0.0009278818033635616\n",
            "step: 140, loss: 0.0003269758017268032\n",
            "step: 150, loss: 0.007650346960872412\n",
            "step: 160, loss: 0.011082588694989681\n",
            "step: 170, loss: 0.0005804415559396148\n",
            "step: 180, loss: 0.0003351195773575455\n",
            "step: 190, loss: 0.001573605346493423\n",
            "step: 200, loss: 0.00266434857621789\n",
            "step: 210, loss: 0.0004230165504850447\n",
            "step: 220, loss: 0.0001759290898917243\n",
            "step: 230, loss: 0.0009528547525405884\n",
            "step: 240, loss: 0.0002785473479889333\n",
            "step: 250, loss: 0.0006033890531398356\n",
            "step: 260, loss: 0.005990173667669296\n",
            "step: 270, loss: 0.027417218312621117\n",
            "step: 280, loss: 0.002966279862448573\n",
            "step: 290, loss: 0.0006324401474557817\n",
            "step: 300, loss: 9.542056068312377e-05\n",
            "step: 310, loss: 0.0001662579452386126\n",
            "step: 320, loss: 0.20808792114257812\n",
            "step: 330, loss: 0.017903784289956093\n",
            "step: 340, loss: 7.044738595141098e-05\n",
            "step: 350, loss: 0.0003370067570358515\n",
            "step: 360, loss: 0.0006736614159308374\n",
            "step: 370, loss: 0.0009513969998806715\n",
            "step: 380, loss: 5.6059590860968456e-05\n",
            "step: 390, loss: 0.0069389729760587215\n",
            "step: 400, loss: 0.0004068332491442561\n",
            "step: 410, loss: 0.0007890319684520364\n",
            "step: 420, loss: 0.0004046939720865339\n",
            "step: 430, loss: 0.0005968661862425506\n",
            "step: 440, loss: 0.004741601645946503\n",
            "step: 450, loss: 0.002436507260426879\n",
            "step: 460, loss: 9.632237197365612e-05\n",
            "step: 470, loss: 0.005259502213448286\n",
            "step: 480, loss: 0.0012895534746348858\n",
            "step: 490, loss: 0.000559850363060832\n",
            "step: 500, loss: 0.0001546875573694706\n",
            "step: 510, loss: 0.004658810328692198\n",
            "step: 520, loss: 0.0015437341062352061\n",
            "step: 530, loss: 0.0011652217945083976\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9330855018587362, f1=0.9302540415704387, best_f1=0.9255121042830541\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013044763181824237\n",
            "step: 10, loss: 0.000263387308223173\n",
            "step: 20, loss: 0.0007098221103660762\n",
            "step: 30, loss: 7.786018977640197e-05\n",
            "step: 40, loss: 0.02875479683279991\n",
            "step: 50, loss: 0.0035058134235441685\n",
            "step: 60, loss: 0.0009258160716854036\n",
            "step: 70, loss: 0.004982514772564173\n",
            "step: 80, loss: 0.009490151889622211\n",
            "step: 90, loss: 0.0007718025590293109\n",
            "step: 100, loss: 0.00011377550254110247\n",
            "step: 110, loss: 0.0007242752471938729\n",
            "step: 120, loss: 0.0001996014325413853\n",
            "step: 130, loss: 0.0004580815730150789\n",
            "step: 140, loss: 0.0004967297427356243\n",
            "step: 150, loss: 0.003962190821766853\n",
            "step: 160, loss: 0.007874323055148125\n",
            "step: 170, loss: 0.009197228588163853\n",
            "step: 180, loss: 0.00018789147725328803\n",
            "step: 190, loss: 0.0010363635374233127\n",
            "step: 200, loss: 0.0003092560509685427\n",
            "step: 210, loss: 2.794132160488516e-05\n",
            "step: 220, loss: 0.00012240390060469508\n",
            "step: 230, loss: 0.008360073901712894\n",
            "step: 240, loss: 0.0004099091747775674\n",
            "step: 250, loss: 0.0010469924891367555\n",
            "step: 260, loss: 0.0029625282622873783\n",
            "step: 270, loss: 0.00023811090795788914\n",
            "step: 280, loss: 0.0001053000014508143\n",
            "step: 290, loss: 0.002978615928441286\n",
            "step: 300, loss: 0.0036610509268939495\n",
            "step: 310, loss: 5.087440513307229e-05\n",
            "step: 320, loss: 2.073066934826784e-05\n",
            "step: 330, loss: 0.0035896864719688892\n",
            "step: 340, loss: 0.0016350928926840425\n",
            "step: 350, loss: 0.00012605894880834967\n",
            "step: 360, loss: 0.12892784178256989\n",
            "step: 370, loss: 0.0010891204001381993\n",
            "step: 380, loss: 0.00011460579844424501\n",
            "step: 390, loss: 0.0011690425453707576\n",
            "step: 400, loss: 0.001495064003393054\n",
            "step: 410, loss: 7.505205576308072e-05\n",
            "step: 420, loss: 0.0012957652797922492\n",
            "step: 430, loss: 0.00026067078579217196\n",
            "step: 440, loss: 0.0019298931583762169\n",
            "step: 450, loss: 0.005753530189394951\n",
            "step: 460, loss: 0.0013578905491158366\n",
            "step: 470, loss: 0.10402494668960571\n",
            "step: 480, loss: 9.573882380209398e-06\n",
            "step: 490, loss: 9.879510616883636e-05\n",
            "step: 500, loss: 0.008854140527546406\n",
            "step: 510, loss: 0.00011923150304937735\n",
            "step: 520, loss: 0.00018780262325890362\n",
            "step: 530, loss: 4.9900369049282745e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9338235294117647, f1=0.9274303970789594, best_f1=0.9255121042830541\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004216416738927364\n",
            "step: 10, loss: 0.001035897177644074\n",
            "step: 20, loss: 2.7520265575731173e-05\n",
            "step: 30, loss: 0.0015041290316730738\n",
            "step: 40, loss: 0.007422266528010368\n",
            "step: 50, loss: 0.0018444570014253259\n",
            "step: 60, loss: 0.0016771501395851374\n",
            "step: 70, loss: 9.148837852990255e-05\n",
            "step: 80, loss: 1.5422190699609928e-05\n",
            "step: 90, loss: 3.461895539658144e-05\n",
            "step: 100, loss: 2.3669243091717362e-05\n",
            "step: 110, loss: 0.025403831154108047\n",
            "step: 120, loss: 7.297797310457099e-06\n",
            "step: 130, loss: 6.752800982212648e-05\n",
            "step: 140, loss: 0.019921354949474335\n",
            "step: 150, loss: 0.033069055527448654\n",
            "step: 160, loss: 0.07184824347496033\n",
            "step: 170, loss: 0.00366063485853374\n",
            "step: 180, loss: 0.0006105338106863201\n",
            "step: 190, loss: 0.0009912744862958789\n",
            "step: 200, loss: 0.017923081293702126\n",
            "step: 210, loss: 6.428138294722885e-05\n",
            "step: 220, loss: 3.551621557562612e-05\n",
            "step: 230, loss: 0.00720404414460063\n",
            "step: 240, loss: 0.0008242945768870413\n",
            "step: 250, loss: 0.003921110183000565\n",
            "step: 260, loss: 4.9226495320908725e-05\n",
            "step: 270, loss: 0.0009635213646106422\n",
            "step: 280, loss: 0.014685237780213356\n",
            "step: 290, loss: 0.001512891030870378\n",
            "step: 300, loss: 0.0001232173526659608\n",
            "step: 310, loss: 6.082585969124921e-05\n",
            "step: 320, loss: 0.0001508271525381133\n",
            "step: 330, loss: 0.004759150557219982\n",
            "step: 340, loss: 0.0002828200231306255\n",
            "step: 350, loss: 0.00017654159455560148\n",
            "step: 360, loss: 0.0008864759001880884\n",
            "step: 370, loss: 0.0019514778396114707\n",
            "step: 380, loss: 0.007125849835574627\n",
            "step: 390, loss: 9.921505261445418e-05\n",
            "step: 400, loss: 0.009186021983623505\n",
            "step: 410, loss: 0.0014865214470773935\n",
            "step: 420, loss: 0.00013522320659831166\n",
            "step: 430, loss: 0.008419918827712536\n",
            "step: 440, loss: 0.0013576647033914924\n",
            "step: 450, loss: 4.867386815021746e-05\n",
            "step: 460, loss: 0.026691965758800507\n",
            "step: 470, loss: 2.2040911062504165e-05\n",
            "step: 480, loss: 0.001271312590688467\n",
            "step: 490, loss: 0.004131829831749201\n",
            "step: 500, loss: 0.0011976688401773572\n",
            "step: 510, loss: 0.06857439875602722\n",
            "step: 520, loss: 3.864612881443463e-05\n",
            "step: 530, loss: 0.004223509691655636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9351503759398496, f1=0.9210649229332087, best_f1=0.9210649229332087\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.835950544977095e-05\n",
            "step: 10, loss: 0.007853243499994278\n",
            "step: 20, loss: 0.0016883236821740866\n",
            "step: 30, loss: 0.012468945235013962\n",
            "step: 40, loss: 0.00017634665709920228\n",
            "step: 50, loss: 0.0010033881990239024\n",
            "step: 60, loss: 0.009309557266533375\n",
            "step: 70, loss: 9.256812336388975e-05\n",
            "step: 80, loss: 0.00028507376555353403\n",
            "step: 90, loss: 6.703443796141073e-05\n",
            "step: 100, loss: 9.548242087475955e-05\n",
            "step: 110, loss: 0.022612441331148148\n",
            "step: 120, loss: 0.00011822906526504084\n",
            "step: 130, loss: 0.0068045309744775295\n",
            "step: 140, loss: 0.00013292233052197844\n",
            "step: 150, loss: 8.409210568061098e-05\n",
            "step: 160, loss: 0.0002237637381767854\n",
            "step: 170, loss: 3.770170587813482e-05\n",
            "step: 180, loss: 0.09714673459529877\n",
            "step: 190, loss: 0.11786780506372452\n",
            "step: 200, loss: 0.0012930898228660226\n",
            "step: 210, loss: 0.003529164008796215\n",
            "step: 220, loss: 0.045894477516412735\n",
            "step: 230, loss: 0.002726073609665036\n",
            "step: 240, loss: 0.028229400515556335\n",
            "step: 250, loss: 0.0004715666873380542\n",
            "step: 260, loss: 8.819693175610155e-05\n",
            "step: 270, loss: 0.0011590052163228393\n",
            "step: 280, loss: 0.0002864012203644961\n",
            "step: 290, loss: 0.0006661395309492946\n",
            "step: 300, loss: 0.0003182908985763788\n",
            "step: 310, loss: 5.234772834228352e-05\n",
            "step: 320, loss: 7.097314664861187e-05\n",
            "step: 330, loss: 2.6075807909364812e-05\n",
            "step: 340, loss: 6.431219662772492e-05\n",
            "step: 350, loss: 0.002493486041203141\n",
            "step: 360, loss: 0.0014670920791104436\n",
            "step: 370, loss: 0.0004663038707803935\n",
            "step: 380, loss: 0.0017647844506427646\n",
            "step: 390, loss: 0.009272579103708267\n",
            "step: 400, loss: 0.004301877226680517\n",
            "step: 410, loss: 0.0011085941223427653\n",
            "step: 420, loss: 0.0007119603687897325\n",
            "step: 430, loss: 5.652654363075271e-05\n",
            "step: 440, loss: 0.0027200602926313877\n",
            "step: 450, loss: 0.0005932695930823684\n",
            "step: 460, loss: 0.0010003396309912205\n",
            "step: 470, loss: 1.1775492566812318e-05\n",
            "step: 480, loss: 0.0017830788856372237\n",
            "step: 490, loss: 0.00029169427580200136\n",
            "step: 500, loss: 0.014552651904523373\n",
            "step: 510, loss: 0.00010491106513654813\n",
            "step: 520, loss: 9.433423838345334e-05\n",
            "step: 530, loss: 5.541700011235662e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.9353049907578558, f1=0.9225092250922509, best_f1=0.9225092250922509\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:23, 247.90it/s]\n",
            "load_f1 = 0.9364107395195479\n",
            "real_f1 = 0.9347111319868483\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 211.82it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyGyaWAphstt"
      },
      "source": [
        "### Amazon-Google - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371d40a7-685a-40d8-e720-c497b86f058a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 461kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 6.16MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 3.66MB/s]\n",
            "Downloading: 100% 501M/501M [00:08<00:00, 59.2MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4941563010215759\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.451974481344223\n",
            "step: 20, loss: 0.4838372468948364\n",
            "step: 30, loss: 0.307957261800766\n",
            "step: 40, loss: 0.3020015358924866\n",
            "step: 50, loss: 0.43260034918785095\n",
            "step: 60, loss: 0.46466225385665894\n",
            "step: 70, loss: 0.2989182770252228\n",
            "step: 80, loss: 0.397663950920105\n",
            "step: 90, loss: 0.2640441656112671\n",
            "step: 100, loss: 0.24067182838916779\n",
            "step: 110, loss: 0.2380286157131195\n",
            "step: 120, loss: 0.34966355562210083\n",
            "step: 130, loss: 0.24447320401668549\n",
            "step: 140, loss: 0.5001601576805115\n",
            "step: 150, loss: 0.40885958075523376\n",
            "step: 160, loss: 0.5027796626091003\n",
            "step: 170, loss: 0.2361486256122589\n",
            "step: 180, loss: 0.3756333589553833\n",
            "step: 190, loss: 0.6635289788246155\n",
            "step: 200, loss: 0.3706989288330078\n",
            "step: 210, loss: 0.4994335472583771\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3776681423187256\n",
            "step: 10, loss: 0.18616126477718353\n",
            "step: 20, loss: 0.49745863676071167\n",
            "step: 30, loss: 0.4880135655403137\n",
            "step: 40, loss: 0.4503121078014374\n",
            "step: 50, loss: 0.24758809804916382\n",
            "step: 60, loss: 0.30989038944244385\n",
            "step: 70, loss: 0.4484732449054718\n",
            "step: 80, loss: 0.31104475259780884\n",
            "step: 90, loss: 0.3773651421070099\n",
            "step: 100, loss: 0.5054131746292114\n",
            "step: 110, loss: 0.3824058473110199\n",
            "step: 120, loss: 0.24814960360527039\n",
            "step: 130, loss: 0.16695843636989594\n",
            "step: 140, loss: 0.23963862657546997\n",
            "step: 150, loss: 0.4351697266101837\n",
            "step: 160, loss: 0.16824764013290405\n",
            "step: 170, loss: 0.61766517162323\n",
            "step: 180, loss: 0.3156891465187073\n",
            "step: 190, loss: 0.316119909286499\n",
            "step: 200, loss: 0.14849363267421722\n",
            "step: 210, loss: 0.31330224871635437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25440433621406555\n",
            "step: 10, loss: 0.23188944160938263\n",
            "step: 20, loss: 0.46203377842903137\n",
            "step: 30, loss: 0.2597634792327881\n",
            "step: 40, loss: 0.44708895683288574\n",
            "step: 50, loss: 0.46897977590560913\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.5521711111068726\n",
            "step: 70, loss: 0.25863397121429443\n",
            "step: 80, loss: 0.45650312304496765\n",
            "step: 90, loss: 0.2399752140045166\n",
            "step: 100, loss: 0.39342033863067627\n",
            "step: 110, loss: 0.23340897262096405\n",
            "step: 120, loss: 0.22276754677295685\n",
            "step: 130, loss: 0.20112738013267517\n",
            "step: 140, loss: 0.3533792197704315\n",
            "step: 150, loss: 0.279423326253891\n",
            "step: 160, loss: 0.1985575407743454\n",
            "step: 170, loss: 0.3491940498352051\n",
            "step: 180, loss: 0.23963838815689087\n",
            "step: 190, loss: 0.1479991376399994\n",
            "step: 200, loss: 0.25110796093940735\n",
            "step: 210, loss: 0.30833131074905396\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.2176015473887814, f1=0.2161119150988905, best_f1=0.2161119150988905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.34919992089271545\n",
            "step: 10, loss: 0.28826820850372314\n",
            "step: 20, loss: 0.28257036209106445\n",
            "step: 30, loss: 0.24041350185871124\n",
            "step: 40, loss: 0.2232859581708908\n",
            "step: 50, loss: 0.24460461735725403\n",
            "step: 60, loss: 0.4591006338596344\n",
            "step: 70, loss: 0.23471687734127045\n",
            "step: 80, loss: 0.2263489067554474\n",
            "step: 90, loss: 0.396145761013031\n",
            "step: 100, loss: 0.4232441186904907\n",
            "step: 110, loss: 0.6307476758956909\n",
            "step: 120, loss: 0.3760738968849182\n",
            "step: 130, loss: 0.6752288937568665\n",
            "step: 140, loss: 0.5062586069107056\n",
            "step: 150, loss: 0.38475292921066284\n",
            "step: 160, loss: 0.31255224347114563\n",
            "step: 170, loss: 0.14781507849693298\n",
            "step: 180, loss: 0.11953137814998627\n",
            "step: 190, loss: 0.16442395746707916\n",
            "step: 200, loss: 0.30127450823783875\n",
            "step: 210, loss: 0.38388678431510925\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.2161119150988905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3895803391933441\n",
            "step: 10, loss: 0.3197759985923767\n",
            "step: 20, loss: 0.30750900506973267\n",
            "step: 30, loss: 0.24021480977535248\n",
            "step: 40, loss: 0.44655466079711914\n",
            "step: 50, loss: 0.3823348581790924\n",
            "step: 60, loss: 0.3796880543231964\n",
            "step: 70, loss: 0.24427439272403717\n",
            "step: 80, loss: 0.4358472228050232\n",
            "step: 90, loss: 0.4440169334411621\n",
            "step: 100, loss: 0.23788577318191528\n",
            "step: 110, loss: 0.16965503990650177\n",
            "step: 120, loss: 0.23927856981754303\n",
            "step: 130, loss: 0.3030185401439667\n",
            "step: 140, loss: 0.5291065573692322\n",
            "step: 150, loss: 0.3009544014930725\n",
            "step: 160, loss: 0.23861615359783173\n",
            "step: 170, loss: 0.3930393159389496\n",
            "step: 180, loss: 0.23905400931835175\n",
            "step: 190, loss: 0.5294354557991028\n",
            "step: 200, loss: 0.3781454861164093\n",
            "step: 210, loss: 0.23899757862091064\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.2161119150988905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15963837504386902\n",
            "step: 10, loss: 0.3867270052433014\n",
            "step: 20, loss: 0.3069915175437927\n",
            "step: 30, loss: 0.233957901597023\n",
            "step: 40, loss: 0.3134041428565979\n",
            "step: 50, loss: 0.5989214181900024\n",
            "step: 60, loss: 0.3196878433227539\n",
            "step: 70, loss: 0.44483107328414917\n",
            "step: 80, loss: 0.31694501638412476\n",
            "step: 90, loss: 0.3140357732772827\n",
            "step: 100, loss: 0.31792306900024414\n",
            "step: 110, loss: 0.38168978691101074\n",
            "step: 120, loss: 0.2395024299621582\n",
            "step: 130, loss: 0.23471729457378387\n",
            "step: 140, loss: 0.37480807304382324\n",
            "step: 150, loss: 0.19326868653297424\n",
            "step: 160, loss: 0.16108565032482147\n",
            "step: 170, loss: 0.45016011595726013\n",
            "step: 180, loss: 0.38357529044151306\n",
            "step: 190, loss: 0.4432339668273926\n",
            "step: 200, loss: 0.3249041736125946\n",
            "step: 210, loss: 0.38162508606910706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.2161119150988905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4525177776813507\n",
            "step: 10, loss: 0.3120381236076355\n",
            "step: 20, loss: 0.4303344488143921\n",
            "step: 30, loss: 0.2433795928955078\n",
            "step: 40, loss: 0.25238773226737976\n",
            "step: 50, loss: 0.37471553683280945\n",
            "step: 60, loss: 0.3208082318305969\n",
            "step: 70, loss: 0.24910517036914825\n",
            "step: 80, loss: 0.5127520561218262\n",
            "step: 90, loss: 0.37351176142692566\n",
            "step: 100, loss: 0.4592723250389099\n",
            "step: 110, loss: 0.3177390694618225\n",
            "step: 120, loss: 0.3083241581916809\n",
            "step: 130, loss: 0.4622890055179596\n",
            "step: 140, loss: 0.30236610770225525\n",
            "step: 150, loss: 0.30543145537376404\n",
            "step: 160, loss: 0.5773932337760925\n",
            "step: 170, loss: 0.6402401924133301\n",
            "step: 180, loss: 0.2451862245798111\n",
            "step: 190, loss: 0.24013663828372955\n",
            "step: 200, loss: 0.3109624683856964\n",
            "step: 210, loss: 0.30819758772850037\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.2161119150988905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6240262389183044\n",
            "step: 10, loss: 0.30886131525039673\n",
            "step: 20, loss: 0.2591104805469513\n",
            "step: 30, loss: 0.2526475787162781\n",
            "step: 40, loss: 0.23463204503059387\n",
            "step: 50, loss: 0.16725057363510132\n",
            "step: 60, loss: 0.1721634417772293\n",
            "step: 70, loss: 0.4635390043258667\n",
            "step: 80, loss: 0.31321093440055847\n",
            "step: 90, loss: 0.377731591463089\n",
            "step: 100, loss: 0.6266162991523743\n",
            "step: 110, loss: 0.37847453355789185\n",
            "step: 120, loss: 0.3227516710758209\n",
            "step: 130, loss: 0.17266057431697845\n",
            "step: 140, loss: 0.3190914988517761\n",
            "step: 150, loss: 0.4575999975204468\n",
            "step: 160, loss: 0.45663508772850037\n",
            "step: 170, loss: 0.540723443031311\n",
            "step: 180, loss: 0.2997058033943176\n",
            "step: 190, loss: 0.24100443720817566\n",
            "step: 200, loss: 0.4484339654445648\n",
            "step: 210, loss: 0.3797999322414398\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.2161119150988905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5815706849098206\n",
            "step: 10, loss: 0.2836182415485382\n",
            "step: 20, loss: 0.37373456358909607\n",
            "step: 30, loss: 0.16378848254680634\n",
            "step: 40, loss: 0.1616528481245041\n",
            "step: 50, loss: 0.4564427137374878\n",
            "step: 60, loss: 0.17338347434997559\n",
            "step: 70, loss: 0.3850531280040741\n",
            "step: 80, loss: 0.3119693100452423\n",
            "step: 90, loss: 0.3106548488140106\n",
            "step: 100, loss: 0.47043758630752563\n",
            "step: 110, loss: 0.5249307155609131\n",
            "step: 120, loss: 0.38118839263916016\n",
            "step: 130, loss: 0.24233251810073853\n",
            "step: 140, loss: 0.6070509552955627\n",
            "step: 150, loss: 0.11295387148857117\n",
            "step: 160, loss: 0.38830313086509705\n",
            "step: 170, loss: 0.31512340903282166\n",
            "step: 180, loss: 0.4510428011417389\n",
            "step: 190, loss: 0.30774733424186707\n",
            "step: 200, loss: 0.30570754408836365\n",
            "step: 210, loss: 0.30700892210006714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.2161119150988905\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3094070553779602\n",
            "step: 10, loss: 0.3729322850704193\n",
            "step: 20, loss: 0.25884535908699036\n",
            "step: 30, loss: 0.16396932303905487\n",
            "step: 40, loss: 0.3144213557243347\n",
            "step: 50, loss: 0.4760274887084961\n",
            "step: 60, loss: 0.20847897231578827\n",
            "step: 70, loss: 0.3835851550102234\n",
            "step: 80, loss: 0.12841413915157318\n",
            "step: 90, loss: 0.36454877257347107\n",
            "step: 100, loss: 0.36441633105278015\n",
            "step: 110, loss: 0.12418817728757858\n",
            "step: 120, loss: 0.5734175443649292\n",
            "step: 130, loss: 0.21808677911758423\n",
            "step: 140, loss: 0.32436272501945496\n",
            "step: 150, loss: 0.24977882206439972\n",
            "step: 160, loss: 0.27982521057128906\n",
            "step: 170, loss: 0.15789487957954407\n",
            "step: 180, loss: 0.3024680018424988\n",
            "step: 190, loss: 0.19228404760360718\n",
            "step: 200, loss: 0.4431131184101105\n",
            "step: 210, loss: 0.22375407814979553\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.38029782359679265, f1=0.3976470588235294, best_f1=0.3976470588235294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24731293320655823\n",
            "step: 10, loss: 0.2582995295524597\n",
            "step: 20, loss: 0.2784358859062195\n",
            "step: 30, loss: 0.18239964544773102\n",
            "step: 40, loss: 0.3413184583187103\n",
            "step: 50, loss: 0.407697856426239\n",
            "step: 60, loss: 0.2544907331466675\n",
            "step: 70, loss: 0.20386433601379395\n",
            "step: 80, loss: 0.3586225211620331\n",
            "step: 90, loss: 0.32485222816467285\n",
            "step: 100, loss: 0.34782862663269043\n",
            "step: 110, loss: 0.4785038232803345\n",
            "step: 120, loss: 0.27841663360595703\n",
            "step: 130, loss: 0.13115015625953674\n",
            "step: 140, loss: 0.33052393794059753\n",
            "step: 150, loss: 0.2578485310077667\n",
            "step: 160, loss: 0.10112304240465164\n",
            "step: 170, loss: 0.17520369589328766\n",
            "step: 180, loss: 0.28176355361938477\n",
            "step: 190, loss: 0.4980196952819824\n",
            "step: 200, loss: 0.21258173882961273\n",
            "step: 210, loss: 0.4097851514816284\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.4163027656477438, f1=0.4604519774011299, best_f1=0.4604519774011299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19420400261878967\n",
            "step: 10, loss: 0.29102039337158203\n",
            "step: 20, loss: 0.453408807516098\n",
            "step: 30, loss: 0.29599666595458984\n",
            "step: 40, loss: 0.10492093861103058\n",
            "step: 50, loss: 0.3199425935745239\n",
            "step: 60, loss: 0.07902472466230392\n",
            "step: 70, loss: 0.35806676745414734\n",
            "step: 80, loss: 0.1906835287809372\n",
            "step: 90, loss: 0.28105705976486206\n",
            "step: 100, loss: 0.05348317325115204\n",
            "step: 110, loss: 0.1959485113620758\n",
            "step: 120, loss: 0.13740362226963043\n",
            "step: 130, loss: 0.4344751238822937\n",
            "step: 140, loss: 0.39812928438186646\n",
            "step: 150, loss: 0.11797311902046204\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 160, loss: 0.1216772049665451\n",
            "step: 170, loss: 0.1205022782087326\n",
            "step: 180, loss: 0.28551191091537476\n",
            "step: 190, loss: 0.25391191244125366\n",
            "step: 200, loss: 0.09522154182195663\n",
            "step: 210, loss: 0.11896616220474243\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.4480519480519481, f1=0.4936305732484077, best_f1=0.4936305732484077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16911479830741882\n",
            "step: 10, loss: 0.12378085404634476\n",
            "step: 20, loss: 0.1695905178785324\n",
            "step: 30, loss: 0.07460375130176544\n",
            "step: 40, loss: 0.1536201387643814\n",
            "step: 50, loss: 0.14760112762451172\n",
            "step: 60, loss: 0.32510167360305786\n",
            "step: 70, loss: 0.16059136390686035\n",
            "step: 80, loss: 0.2951738238334656\n",
            "step: 90, loss: 0.31483596563339233\n",
            "step: 100, loss: 0.2559710741043091\n",
            "step: 110, loss: 0.17980021238327026\n",
            "step: 120, loss: 0.18514418601989746\n",
            "step: 130, loss: 0.12945418059825897\n",
            "step: 140, loss: 0.20625784993171692\n",
            "step: 150, loss: 0.26778650283813477\n",
            "step: 160, loss: 0.16572333872318268\n",
            "step: 170, loss: 0.18966679275035858\n",
            "step: 180, loss: 0.12279073894023895\n",
            "step: 190, loss: 0.21135112643241882\n",
            "step: 200, loss: 0.24025844037532806\n",
            "step: 210, loss: 0.21285122632980347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.45307443365695793, f1=0.5047619047619047, best_f1=0.5047619047619047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1478213667869568\n",
            "step: 10, loss: 0.17322465777397156\n",
            "step: 20, loss: 0.31569862365722656\n",
            "step: 30, loss: 0.12820275127887726\n",
            "step: 40, loss: 0.182456374168396\n",
            "step: 50, loss: 0.14041005074977875\n",
            "step: 60, loss: 0.22092635929584503\n",
            "step: 70, loss: 0.09334752708673477\n",
            "step: 80, loss: 0.1655760258436203\n",
            "step: 90, loss: 0.04880460351705551\n",
            "step: 100, loss: 0.12713396549224854\n",
            "step: 110, loss: 0.18332473933696747\n",
            "step: 120, loss: 0.08387583494186401\n",
            "step: 130, loss: 0.17144080996513367\n",
            "step: 140, loss: 0.1190577894449234\n",
            "step: 150, loss: 0.19520175457000732\n",
            "step: 160, loss: 0.07635627686977386\n",
            "step: 170, loss: 0.12421946972608566\n",
            "step: 180, loss: 0.1345585137605667\n",
            "step: 190, loss: 0.3775899112224579\n",
            "step: 200, loss: 0.12010009586811066\n",
            "step: 210, loss: 0.21630039811134338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.46183206106870234, f1=0.5154264972776769, best_f1=0.5154264972776769\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.046172965317964554\n",
            "step: 10, loss: 0.15197470784187317\n",
            "step: 20, loss: 0.32854506373405457\n",
            "step: 30, loss: 0.08678928762674332\n",
            "step: 40, loss: 0.24306301772594452\n",
            "step: 50, loss: 0.0280526764690876\n",
            "step: 60, loss: 0.28279808163642883\n",
            "step: 70, loss: 0.2421526461839676\n",
            "step: 80, loss: 0.17187076807022095\n",
            "step: 90, loss: 0.24840673804283142\n",
            "step: 100, loss: 0.22444573044776917\n",
            "step: 110, loss: 0.12041544169187546\n",
            "step: 120, loss: 0.18290762603282928\n",
            "step: 130, loss: 0.15554386377334595\n",
            "step: 140, loss: 0.16340190172195435\n",
            "step: 150, loss: 0.18163855373859406\n",
            "step: 160, loss: 0.3032277226448059\n",
            "step: 170, loss: 0.24048668146133423\n",
            "step: 180, loss: 0.17840923368930817\n",
            "step: 190, loss: 0.19248506426811218\n",
            "step: 200, loss: 0.14938265085220337\n",
            "step: 210, loss: 0.31236928701400757\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.4692179700499168, f1=0.5039872408293461, best_f1=0.5039872408293461\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:05, 448.50it/s]\n",
            "load_f1 = 0.4656616415410385\n",
            "real_f1 = 0.4542253521126761\n",
            "267it [00:00, 1352.32it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 206.91it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL0eWrGYhstu"
      },
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516b95eb-e3a3-41a8-da7a-0eef962507e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4413926303386688\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4805144965648651\n",
            "step: 20, loss: 0.26865145564079285\n",
            "step: 30, loss: 0.3644719421863556\n",
            "step: 40, loss: 0.2436416894197464\n",
            "step: 50, loss: 0.33831313252449036\n",
            "step: 60, loss: 0.48728838562965393\n",
            "step: 70, loss: 0.42347338795661926\n",
            "step: 80, loss: 0.18110929429531097\n",
            "step: 90, loss: 0.30010098218917847\n",
            "step: 100, loss: 0.46501946449279785\n",
            "step: 110, loss: 0.23515036702156067\n",
            "step: 120, loss: 0.34642869234085083\n",
            "step: 130, loss: 0.33230453729629517\n",
            "step: 140, loss: 0.23083317279815674\n",
            "step: 150, loss: 0.3090667724609375\n",
            "step: 160, loss: 0.24019461870193481\n",
            "step: 170, loss: 0.3976917266845703\n",
            "step: 180, loss: 0.15657541155815125\n",
            "step: 190, loss: 0.17245496809482574\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37430739402770996\n",
            "step: 10, loss: 0.3016575872898102\n",
            "step: 20, loss: 0.570421040058136\n",
            "step: 30, loss: 0.2445693016052246\n",
            "step: 40, loss: 0.5609502792358398\n",
            "step: 50, loss: 0.3047938048839569\n",
            "step: 60, loss: 0.4539155960083008\n",
            "step: 70, loss: 0.3372689187526703\n",
            "step: 80, loss: 0.16324910521507263\n",
            "step: 90, loss: 0.3207288980484009\n",
            "step: 100, loss: 0.26777365803718567\n",
            "step: 110, loss: 0.3936818838119507\n",
            "step: 120, loss: 0.23644253611564636\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 130, loss: 0.5109986066818237\n",
            "step: 140, loss: 0.31828364729881287\n",
            "step: 150, loss: 0.31073296070098877\n",
            "step: 160, loss: 0.31615227460861206\n",
            "step: 170, loss: 0.2402729094028473\n",
            "step: 180, loss: 0.1693158596754074\n",
            "step: 190, loss: 0.23781083524227142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38175255060195923\n",
            "step: 10, loss: 0.389522910118103\n",
            "step: 20, loss: 0.4696469008922577\n",
            "step: 30, loss: 0.29519981145858765\n",
            "step: 40, loss: 0.08817644417285919\n",
            "step: 50, loss: 0.38431045413017273\n",
            "step: 60, loss: 0.1614934802055359\n",
            "step: 70, loss: 0.3906395733356476\n",
            "step: 80, loss: 0.30236271023750305\n",
            "step: 90, loss: 0.38410353660583496\n",
            "step: 100, loss: 0.5696932673454285\n",
            "step: 110, loss: 0.6407153010368347\n",
            "step: 120, loss: 0.3765340745449066\n",
            "step: 130, loss: 0.1542743444442749\n",
            "step: 140, loss: 0.34506574273109436\n",
            "step: 150, loss: 0.3595145046710968\n",
            "step: 160, loss: 0.6040363311767578\n",
            "step: 170, loss: 0.4212009310722351\n",
            "step: 180, loss: 0.35179802775382996\n",
            "step: 190, loss: 0.1652681529521942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.2025040827436037, f1=0.20712328767123286, best_f1=0.20712328767123286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2218940258026123\n",
            "step: 10, loss: 0.21757349371910095\n",
            "step: 20, loss: 0.2887326776981354\n",
            "step: 30, loss: 0.24296733736991882\n",
            "step: 40, loss: 0.5350006818771362\n",
            "step: 50, loss: 0.23191122710704803\n",
            "step: 60, loss: 0.3610233962535858\n",
            "step: 70, loss: 0.2957395315170288\n",
            "step: 80, loss: 0.22394779324531555\n",
            "step: 90, loss: 0.15809538960456848\n",
            "step: 100, loss: 0.38834407925605774\n",
            "step: 110, loss: 0.4518400728702545\n",
            "step: 120, loss: 0.2234535962343216\n",
            "step: 130, loss: 0.417007178068161\n",
            "step: 140, loss: 0.3862205743789673\n",
            "step: 150, loss: 0.23154371976852417\n",
            "step: 160, loss: 0.2905811071395874\n",
            "step: 170, loss: 0.4059736132621765\n",
            "step: 180, loss: 0.3571479618549347\n",
            "step: 190, loss: 0.14057354629039764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.20378619153674837, f1=0.21176470588235297, best_f1=0.21176470588235297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37923434376716614\n",
            "step: 10, loss: 0.37021809816360474\n",
            "step: 20, loss: 0.14582403004169464\n",
            "step: 30, loss: 0.13055798411369324\n",
            "step: 40, loss: 0.30362001061439514\n",
            "step: 50, loss: 0.5514394640922546\n",
            "step: 60, loss: 0.24009452760219574\n",
            "step: 70, loss: 0.365360289812088\n",
            "step: 80, loss: 0.34408918023109436\n",
            "step: 90, loss: 0.2931373119354248\n",
            "step: 100, loss: 0.4262986183166504\n",
            "step: 110, loss: 0.3762189447879791\n",
            "step: 120, loss: 0.22192630171775818\n",
            "step: 130, loss: 0.5365946888923645\n",
            "step: 140, loss: 0.37808847427368164\n",
            "step: 150, loss: 0.2943319082260132\n",
            "step: 160, loss: 0.15068358182907104\n",
            "step: 170, loss: 0.3508046567440033\n",
            "step: 180, loss: 0.22977793216705322\n",
            "step: 190, loss: 0.2892818748950958\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.20642201834862384, f1=0.21602787456445996, best_f1=0.21602787456445996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29735323786735535\n",
            "step: 10, loss: 0.23742976784706116\n",
            "step: 20, loss: 0.280663400888443\n",
            "step: 30, loss: 0.43941885232925415\n",
            "step: 40, loss: 0.26666370034217834\n",
            "step: 50, loss: 0.3009784519672394\n",
            "step: 60, loss: 0.40883365273475647\n",
            "step: 70, loss: 0.27119287848472595\n",
            "step: 80, loss: 0.27377691864967346\n",
            "step: 90, loss: 0.21695590019226074\n",
            "step: 100, loss: 0.45225921273231506\n",
            "step: 110, loss: 0.23539312183856964\n",
            "step: 120, loss: 0.4526558816432953\n",
            "step: 130, loss: 0.5246230363845825\n",
            "step: 140, loss: 0.18006166815757751\n",
            "step: 150, loss: 0.38379400968551636\n",
            "step: 160, loss: 0.3828343152999878\n",
            "step: 170, loss: 0.39236143231391907\n",
            "step: 180, loss: 0.17870540916919708\n",
            "step: 190, loss: 0.31160327792167664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.21602787456445996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3142218589782715\n",
            "step: 10, loss: 0.3072943389415741\n",
            "step: 20, loss: 0.3052518665790558\n",
            "step: 30, loss: 0.18731169402599335\n",
            "step: 40, loss: 0.30792558193206787\n",
            "step: 50, loss: 0.08631190657615662\n",
            "step: 60, loss: 0.15559494495391846\n",
            "step: 70, loss: 0.15790465474128723\n",
            "step: 80, loss: 0.23475490510463715\n",
            "step: 90, loss: 0.23500661551952362\n",
            "step: 100, loss: 0.5797333717346191\n",
            "step: 110, loss: 0.4321860671043396\n",
            "step: 120, loss: 0.38392144441604614\n",
            "step: 130, loss: 0.3108298182487488\n",
            "step: 140, loss: 0.2385959029197693\n",
            "step: 150, loss: 0.29428601264953613\n",
            "step: 160, loss: 0.35843682289123535\n",
            "step: 170, loss: 0.2708812355995178\n",
            "step: 180, loss: 0.21560809016227722\n",
            "step: 190, loss: 0.3052448332309723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.20118983234180637, f1=0.20510037981551815, best_f1=0.21602787456445996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2512564957141876\n",
            "step: 10, loss: 0.37134429812431335\n",
            "step: 20, loss: 0.2319032996892929\n",
            "step: 30, loss: 0.3862839937210083\n",
            "step: 40, loss: 0.15387076139450073\n",
            "step: 50, loss: 0.3109617829322815\n",
            "step: 60, loss: 0.434845894575119\n",
            "step: 70, loss: 0.21801045536994934\n",
            "step: 80, loss: 0.3599326014518738\n",
            "step: 90, loss: 0.22286565601825714\n",
            "step: 100, loss: 0.31399673223495483\n",
            "step: 110, loss: 0.3786310851573944\n",
            "step: 120, loss: 0.35214173793792725\n",
            "step: 130, loss: 0.2866276800632477\n",
            "step: 140, loss: 0.3627394437789917\n",
            "step: 150, loss: 0.29704615473747253\n",
            "step: 160, loss: 0.16631148755550385\n",
            "step: 170, loss: 0.22174285352230072\n",
            "step: 180, loss: 0.3019081950187683\n",
            "step: 190, loss: 0.2751351296901703\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.20195439739413681, f1=0.20577027762656502, best_f1=0.21602787456445996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22878757119178772\n",
            "step: 10, loss: 0.09218954294919968\n",
            "step: 20, loss: 0.29083752632141113\n",
            "step: 30, loss: 0.17542031407356262\n",
            "step: 40, loss: 0.30269595980644226\n",
            "step: 50, loss: 0.355265736579895\n",
            "step: 60, loss: 0.364860475063324\n",
            "step: 70, loss: 0.16454458236694336\n",
            "step: 80, loss: 0.3618741035461426\n",
            "step: 90, loss: 0.7461548447608948\n",
            "step: 100, loss: 0.3387950658798218\n",
            "step: 110, loss: 0.3442023992538452\n",
            "step: 120, loss: 0.5489190816879272\n",
            "step: 130, loss: 0.2965348958969116\n",
            "step: 140, loss: 0.30421724915504456\n",
            "step: 150, loss: 0.23527655005455017\n",
            "step: 160, loss: 0.21963052451610565\n",
            "step: 170, loss: 0.518138587474823\n",
            "step: 180, loss: 0.35277804732322693\n",
            "step: 190, loss: 0.24505753815174103\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.2039911308203991, f1=0.20905533817775293, best_f1=0.21602787456445996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28708773851394653\n",
            "step: 10, loss: 0.1539716124534607\n",
            "step: 20, loss: 0.28964823484420776\n",
            "step: 30, loss: 0.3767397403717041\n",
            "step: 40, loss: 0.28177374601364136\n",
            "step: 50, loss: 0.09831574559211731\n",
            "step: 60, loss: 0.304324209690094\n",
            "step: 70, loss: 0.29745200276374817\n",
            "step: 80, loss: 0.41691216826438904\n",
            "step: 90, loss: 0.1535630077123642\n",
            "step: 100, loss: 0.2173144817352295\n",
            "step: 110, loss: 0.22118259966373444\n",
            "step: 120, loss: 0.3461056351661682\n",
            "step: 130, loss: 0.4723409116268158\n",
            "step: 140, loss: 0.3419887125492096\n",
            "step: 150, loss: 0.30455267429351807\n",
            "step: 160, loss: 0.218044251203537\n",
            "step: 170, loss: 0.3418397903442383\n",
            "step: 180, loss: 0.22086156904697418\n",
            "step: 190, loss: 0.15445150434970856\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.20378619153674837, f1=0.20840336134453782, best_f1=0.21602787456445996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5098609328269958\n",
            "step: 10, loss: 0.07829761505126953\n",
            "step: 20, loss: 0.3167053759098053\n",
            "step: 30, loss: 0.21654416620731354\n",
            "step: 40, loss: 0.07739794999361038\n",
            "step: 50, loss: 0.28905895352363586\n",
            "step: 60, loss: 0.21828536689281464\n",
            "step: 70, loss: 0.6122047901153564\n",
            "step: 80, loss: 0.3730833828449249\n",
            "step: 90, loss: 0.3931848108768463\n",
            "step: 100, loss: 0.17832955718040466\n",
            "step: 110, loss: 0.23694466054439545\n",
            "step: 120, loss: 0.2881879210472107\n",
            "step: 130, loss: 0.63383948802948\n",
            "step: 140, loss: 0.4298315942287445\n",
            "step: 150, loss: 0.30439266562461853\n",
            "step: 160, loss: 0.299383282661438\n",
            "step: 170, loss: 0.4579557180404663\n",
            "step: 180, loss: 0.2822442352771759\n",
            "step: 190, loss: 0.12251287698745728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.2027700831024931, f1=0.2120866590649943, best_f1=0.21602787456445996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3817349672317505\n",
            "step: 10, loss: 0.3012619912624359\n",
            "step: 20, loss: 0.5234796404838562\n",
            "step: 30, loss: 0.3554191291332245\n",
            "step: 40, loss: 0.2887103855609894\n",
            "step: 50, loss: 0.47995731234550476\n",
            "step: 60, loss: 0.4269101619720459\n",
            "step: 70, loss: 0.3633294105529785\n",
            "step: 80, loss: 0.3685402572154999\n",
            "step: 90, loss: 0.4445873200893402\n",
            "step: 100, loss: 0.28327664732933044\n",
            "step: 110, loss: 0.5066455006599426\n",
            "step: 120, loss: 0.16634756326675415\n",
            "step: 130, loss: 0.2302316576242447\n",
            "step: 140, loss: 0.3644101321697235\n",
            "step: 150, loss: 0.34910136461257935\n",
            "step: 160, loss: 0.22307369112968445\n",
            "step: 170, loss: 0.4798341691493988\n",
            "step: 180, loss: 0.28786352276802063\n",
            "step: 190, loss: 0.1560572236776352\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.20408163265306123, f1=0.21485714285714286, best_f1=0.21602787456445996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2955910861492157\n",
            "step: 10, loss: 0.28537318110466003\n",
            "step: 20, loss: 0.35896170139312744\n",
            "step: 30, loss: 0.3472120761871338\n",
            "step: 40, loss: 0.2251691222190857\n",
            "step: 50, loss: 0.23428891599178314\n",
            "step: 60, loss: 0.2835833728313446\n",
            "step: 70, loss: 0.15921303629875183\n",
            "step: 80, loss: 0.2945389449596405\n",
            "step: 90, loss: 0.2924869954586029\n",
            "step: 100, loss: 0.1625908464193344\n",
            "step: 110, loss: 0.5451594591140747\n",
            "step: 120, loss: 0.1595245599746704\n",
            "step: 130, loss: 0.22056256234645844\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 140, loss: 0.3025408685207367\n",
            "step: 150, loss: 0.3554525375366211\n",
            "step: 160, loss: 0.4459932744503021\n",
            "step: 170, loss: 0.10622741281986237\n",
            "step: 180, loss: 0.2262318879365921\n",
            "step: 190, loss: 0.30817049741744995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.20670391061452514, f1=0.21339387060158913, best_f1=0.21339387060158913\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28680655360221863\n",
            "step: 10, loss: 0.22061468660831451\n",
            "step: 20, loss: 0.3526844084262848\n",
            "step: 30, loss: 0.10914671421051025\n",
            "step: 40, loss: 0.30114006996154785\n",
            "step: 50, loss: 0.3510630130767822\n",
            "step: 60, loss: 0.2706139385700226\n",
            "step: 70, loss: 0.23314666748046875\n",
            "step: 80, loss: 0.36082327365875244\n",
            "step: 90, loss: 0.13997118175029755\n",
            "step: 100, loss: 0.2882431149482727\n",
            "step: 110, loss: 0.49248477816581726\n",
            "step: 120, loss: 0.28515389561653137\n",
            "step: 130, loss: 0.2903381288051605\n",
            "step: 140, loss: 0.16048182547092438\n",
            "step: 150, loss: 0.4282926619052887\n",
            "step: 160, loss: 0.39832165837287903\n",
            "step: 170, loss: 0.20947343111038208\n",
            "step: 180, loss: 0.29110050201416016\n",
            "step: 190, loss: 0.5466832518577576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.20912124582869854, f1=0.21230942970073408, best_f1=0.21230942970073408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4981628656387329\n",
            "step: 10, loss: 0.21815262734889984\n",
            "step: 20, loss: 0.2310335785150528\n",
            "step: 30, loss: 0.2890758216381073\n",
            "step: 40, loss: 0.6357709765434265\n",
            "step: 50, loss: 0.2950541079044342\n",
            "step: 60, loss: 0.17066025733947754\n",
            "step: 70, loss: 0.23292513191699982\n",
            "step: 80, loss: 0.21201612055301666\n",
            "step: 90, loss: 0.30699652433395386\n",
            "step: 100, loss: 0.33367326855659485\n",
            "step: 110, loss: 0.4111620783805847\n",
            "step: 120, loss: 0.22575266659259796\n",
            "step: 130, loss: 0.2850321829319\n",
            "step: 140, loss: 0.5083796977996826\n",
            "step: 150, loss: 0.13837766647338867\n",
            "step: 160, loss: 0.28111588954925537\n",
            "step: 170, loss: 0.7518206238746643\n",
            "step: 180, loss: 0.28419333696365356\n",
            "step: 190, loss: 0.413557767868042\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.20681818181818182, f1=0.21609195402298853, best_f1=0.21230942970073408\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 232.50it/s]\n",
            "load_f1 = 0.20798201236649805\n",
            "real_f1 = 0.20727673649393605\n",
            "733it [00:00, 3433.49it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 202.15it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW6LV4zMhstv"
      },
      "source": [
        "## DITTO TEXTUAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3an30TrShstv"
      },
      "source": [
        "### Abt-Buy - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40df0cb-1b52-4440-aa53-cc973c0c26b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5060041546821594\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4880598783493042\n",
            "step: 20, loss: 0.27633804082870483\n",
            "step: 30, loss: 0.4530740976333618\n",
            "step: 40, loss: 0.5353206992149353\n",
            "step: 50, loss: 0.3098382353782654\n",
            "step: 60, loss: 0.5936725735664368\n",
            "step: 70, loss: 0.35039153695106506\n",
            "step: 80, loss: 0.22264498472213745\n",
            "step: 90, loss: 0.21479207277297974\n",
            "step: 100, loss: 0.1643984317779541\n",
            "step: 110, loss: 0.4461592733860016\n",
            "step: 120, loss: 0.34587162733078003\n",
            "step: 130, loss: 0.3611368238925934\n",
            "step: 140, loss: 0.39522701501846313\n",
            "step: 150, loss: 0.3057967722415924\n",
            "step: 160, loss: 0.3990769386291504\n",
            "step: 170, loss: 0.3344683349132538\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.33906692266464233\n",
            "step: 10, loss: 0.4864880442619324\n",
            "step: 20, loss: 0.3176894783973694\n",
            "step: 30, loss: 0.3301257789134979\n",
            "step: 40, loss: 0.07118536531925201\n",
            "step: 50, loss: 0.470339834690094\n",
            "step: 60, loss: 0.19974327087402344\n",
            "step: 70, loss: 0.5068038702011108\n",
            "step: 80, loss: 0.23098638653755188\n",
            "step: 90, loss: 0.2421543002128601\n",
            "step: 100, loss: 0.5191940665245056\n",
            "step: 110, loss: 0.25779488682746887\n",
            "step: 120, loss: 0.2362736016511917\n",
            "step: 130, loss: 0.5618122816085815\n",
            "step: 140, loss: 0.5066244602203369\n",
            "step: 150, loss: 0.41780126094818115\n",
            "step: 160, loss: 0.44054120779037476\n",
            "step: 170, loss: 0.3534097373485565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.24251968503937008, f1=0.2396006655574043, best_f1=0.2396006655574043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5625867247581482\n",
            "step: 10, loss: 0.31542259454727173\n",
            "step: 20, loss: 0.2557789981365204\n",
            "step: 30, loss: 0.24818246066570282\n",
            "step: 40, loss: 0.355487197637558\n",
            "step: 50, loss: 0.5777066349983215\n",
            "step: 60, loss: 0.2865064740180969\n",
            "step: 70, loss: 0.24829308688640594\n",
            "step: 80, loss: 0.378502756357193\n",
            "step: 90, loss: 0.5336341857910156\n",
            "step: 100, loss: 0.289023756980896\n",
            "step: 110, loss: 0.1644231677055359\n",
            "step: 120, loss: 0.5453325510025024\n",
            "step: 130, loss: 0.483041375875473\n",
            "step: 140, loss: 0.44631874561309814\n",
            "step: 150, loss: 0.20220309495925903\n",
            "step: 160, loss: 0.1583048403263092\n",
            "step: 170, loss: 0.31653469800949097\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.23623445825932504, f1=0.23487544483985764, best_f1=0.2396006655574043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3915606439113617\n",
            "step: 10, loss: 0.5392979383468628\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.22396422922611237\n",
            "step: 30, loss: 0.3857465386390686\n",
            "step: 40, loss: 0.2529415190219879\n",
            "step: 50, loss: 0.3665289580821991\n",
            "step: 60, loss: 0.6618731617927551\n",
            "step: 70, loss: 0.31421172618865967\n",
            "step: 80, loss: 0.4765564799308777\n",
            "step: 90, loss: 0.28313636779785156\n",
            "step: 100, loss: 0.3382607698440552\n",
            "step: 110, loss: 0.4397784471511841\n",
            "step: 120, loss: 0.47207164764404297\n",
            "step: 130, loss: 0.33237650990486145\n",
            "step: 140, loss: 0.22301506996154785\n",
            "step: 150, loss: 0.7266283631324768\n",
            "step: 160, loss: 0.14205439388751984\n",
            "step: 170, loss: 0.23766571283340454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.26356589147286824, f1=0.2360034453057709, best_f1=0.2360034453057709\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3541010320186615\n",
            "step: 10, loss: 0.29018154740333557\n",
            "step: 20, loss: 0.28127333521842957\n",
            "step: 30, loss: 0.21904537081718445\n",
            "step: 40, loss: 0.2633052468299866\n",
            "step: 50, loss: 0.2359689325094223\n",
            "step: 60, loss: 0.3468209505081177\n",
            "step: 70, loss: 0.336018830537796\n",
            "step: 80, loss: 0.084278404712677\n",
            "step: 90, loss: 0.5607019662857056\n",
            "step: 100, loss: 0.24657496809959412\n",
            "step: 110, loss: 0.25955766439437866\n",
            "step: 120, loss: 0.15194636583328247\n",
            "step: 130, loss: 0.2312816083431244\n",
            "step: 140, loss: 0.19347137212753296\n",
            "step: 150, loss: 0.20031997561454773\n",
            "step: 160, loss: 0.2568568289279938\n",
            "step: 170, loss: 0.3667233884334564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.5649484536082475, f1=0.5766871165644172, best_f1=0.5766871165644172\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26289063692092896\n",
            "step: 10, loss: 0.10186257213354111\n",
            "step: 20, loss: 0.35122203826904297\n",
            "step: 30, loss: 0.1915929913520813\n",
            "step: 40, loss: 0.3449437916278839\n",
            "step: 50, loss: 0.1618650257587433\n",
            "step: 60, loss: 0.21407780051231384\n",
            "step: 70, loss: 0.1905411034822464\n",
            "step: 80, loss: 0.21861372888088226\n",
            "step: 90, loss: 0.16428956389427185\n",
            "step: 100, loss: 0.17142271995544434\n",
            "step: 110, loss: 0.281853049993515\n",
            "step: 120, loss: 0.379944771528244\n",
            "step: 130, loss: 0.3042697608470917\n",
            "step: 140, loss: 0.2596501111984253\n",
            "step: 150, loss: 0.20633552968502045\n",
            "step: 160, loss: 0.2128293365240097\n",
            "step: 170, loss: 0.11528918147087097\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.6666666666666666, f1=0.6561797752808989, best_f1=0.6561797752808989\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1229420080780983\n",
            "step: 10, loss: 0.4333944618701935\n",
            "step: 20, loss: 0.33469563722610474\n",
            "step: 30, loss: 0.3667454123497009\n",
            "step: 40, loss: 0.05341802537441254\n",
            "step: 50, loss: 0.06032676249742508\n",
            "step: 60, loss: 0.306702196598053\n",
            "step: 70, loss: 0.12256386131048203\n",
            "step: 80, loss: 0.09041278809309006\n",
            "step: 90, loss: 0.14645737409591675\n",
            "step: 100, loss: 0.11126208305358887\n",
            "step: 110, loss: 0.16639341413974762\n",
            "step: 120, loss: 0.09681151807308197\n",
            "step: 130, loss: 0.29139018058776855\n",
            "step: 140, loss: 0.06348942220211029\n",
            "step: 150, loss: 0.1614767611026764\n",
            "step: 160, loss: 0.17574194073677063\n",
            "step: 170, loss: 0.19536617398262024\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7155963302752293, f1=0.6894977168949772, best_f1=0.6894977168949772\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1413475126028061\n",
            "step: 10, loss: 0.15384052693843842\n",
            "step: 20, loss: 0.08192131668329239\n",
            "step: 30, loss: 0.01324191503226757\n",
            "step: 40, loss: 0.17788860201835632\n",
            "step: 50, loss: 0.06446538120508194\n",
            "step: 60, loss: 0.18429069221019745\n",
            "step: 70, loss: 0.03332192450761795\n",
            "step: 80, loss: 0.2670455873012543\n",
            "step: 90, loss: 0.2191222906112671\n",
            "step: 100, loss: 0.03763798251748085\n",
            "step: 110, loss: 0.16640077531337738\n",
            "step: 120, loss: 0.29674988985061646\n",
            "step: 130, loss: 0.16050498187541962\n",
            "step: 140, loss: 0.33749258518218994\n",
            "step: 150, loss: 0.15778997540473938\n",
            "step: 160, loss: 0.06282635778188705\n",
            "step: 170, loss: 0.240537628531456\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7607655502392345, f1=0.722488038277512, best_f1=0.722488038277512\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1066632941365242\n",
            "step: 10, loss: 0.03154850751161575\n",
            "step: 20, loss: 0.10756921768188477\n",
            "step: 30, loss: 0.16854019463062286\n",
            "step: 40, loss: 0.15661756694316864\n",
            "step: 50, loss: 0.02128683216869831\n",
            "step: 60, loss: 0.12955830991268158\n",
            "step: 70, loss: 0.31537124514579773\n",
            "step: 80, loss: 0.09635628014802933\n",
            "step: 90, loss: 0.162726029753685\n",
            "step: 100, loss: 0.05995015427470207\n",
            "step: 110, loss: 0.19593766331672668\n",
            "step: 120, loss: 0.05526713281869888\n",
            "step: 130, loss: 0.25423315167427063\n",
            "step: 140, loss: 0.10882455855607986\n",
            "step: 150, loss: 0.40828439593315125\n",
            "step: 160, loss: 0.04244665056467056\n",
            "step: 170, loss: 0.08900173008441925\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.7780678851174935, f1=0.7263157894736842, best_f1=0.7263157894736842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03505707532167435\n",
            "step: 10, loss: 0.04649568349123001\n",
            "step: 20, loss: 0.1632087528705597\n",
            "step: 30, loss: 0.1405094414949417\n",
            "step: 40, loss: 0.11374890804290771\n",
            "step: 50, loss: 0.11353342235088348\n",
            "step: 60, loss: 0.11993279308080673\n",
            "step: 70, loss: 0.20043043792247772\n",
            "step: 80, loss: 0.17825865745544434\n",
            "step: 90, loss: 0.1557127684354782\n",
            "step: 100, loss: 0.09331152588129044\n",
            "step: 110, loss: 0.11247048527002335\n",
            "step: 120, loss: 0.1407402753829956\n",
            "step: 130, loss: 0.04398823902010918\n",
            "step: 140, loss: 0.08143622428178787\n",
            "step: 150, loss: 0.11409679800271988\n",
            "step: 160, loss: 0.18489333987236023\n",
            "step: 170, loss: 0.17146334052085876\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7642679900744417, f1=0.7427184466019418, best_f1=0.7263157894736842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1664874106645584\n",
            "step: 10, loss: 0.022548140957951546\n",
            "step: 20, loss: 0.04947103559970856\n",
            "step: 30, loss: 0.056848201900720596\n",
            "step: 40, loss: 0.014269369654357433\n",
            "step: 50, loss: 0.07668165862560272\n",
            "step: 60, loss: 0.14694824814796448\n",
            "step: 70, loss: 0.02946287952363491\n",
            "step: 80, loss: 0.0194704532623291\n",
            "step: 90, loss: 0.253334641456604\n",
            "step: 100, loss: 0.3157424032688141\n",
            "step: 110, loss: 0.12055309861898422\n",
            "step: 120, loss: 0.029350394383072853\n",
            "step: 130, loss: 0.03994220122694969\n",
            "step: 140, loss: 0.26475125551223755\n",
            "step: 150, loss: 0.06032169982790947\n",
            "step: 160, loss: 0.12491822242736816\n",
            "step: 170, loss: 0.16680105030536652\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.7830687830687831, f1=0.7455012853470437, best_f1=0.7455012853470437\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018671540543437004\n",
            "step: 10, loss: 0.06896831840276718\n",
            "step: 20, loss: 0.06392174959182739\n",
            "step: 30, loss: 0.10455796122550964\n",
            "step: 40, loss: 0.019756287336349487\n",
            "step: 50, loss: 0.04472819343209267\n",
            "step: 60, loss: 0.05621194466948509\n",
            "step: 70, loss: 0.19448956847190857\n",
            "step: 80, loss: 0.040746744722127914\n",
            "step: 90, loss: 0.035189978778362274\n",
            "step: 100, loss: 0.018752263858914375\n",
            "step: 110, loss: 0.0605769120156765\n",
            "step: 120, loss: 0.08566481620073318\n",
            "step: 130, loss: 0.18329095840454102\n",
            "step: 140, loss: 0.020074833184480667\n",
            "step: 150, loss: 0.041084084659814835\n",
            "step: 160, loss: 0.10183383524417877\n",
            "step: 170, loss: 0.363845556974411\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.7880434782608696, f1=0.7352185089974292, best_f1=0.7352185089974292\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09657707065343857\n",
            "step: 10, loss: 0.008610248565673828\n",
            "step: 20, loss: 0.03078070655465126\n",
            "step: 30, loss: 0.21418605744838715\n",
            "step: 40, loss: 0.3043511211872101\n",
            "step: 50, loss: 0.05669766664505005\n",
            "step: 60, loss: 0.04021119698882103\n",
            "step: 70, loss: 0.22928300499916077\n",
            "step: 80, loss: 0.03957785665988922\n",
            "step: 90, loss: 0.042749933898448944\n",
            "step: 100, loss: 0.06330224871635437\n",
            "step: 110, loss: 0.03237992525100708\n",
            "step: 120, loss: 0.012598467990756035\n",
            "step: 130, loss: 0.021083615720272064\n",
            "step: 140, loss: 0.2873414158821106\n",
            "step: 150, loss: 0.05469216778874397\n",
            "step: 160, loss: 0.023392338305711746\n",
            "step: 170, loss: 0.01070152036845684\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.783289817232376, f1=0.7444168734491315, best_f1=0.7352185089974292\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10256626456975937\n",
            "step: 10, loss: 0.025750208646059036\n",
            "step: 20, loss: 0.23494774103164673\n",
            "step: 30, loss: 0.06872273981571198\n",
            "step: 40, loss: 0.05069364234805107\n",
            "step: 50, loss: 0.013733827508985996\n",
            "step: 60, loss: 0.024672530591487885\n",
            "step: 70, loss: 0.13627544045448303\n",
            "step: 80, loss: 0.01744093932211399\n",
            "step: 90, loss: 0.00846592616289854\n",
            "step: 100, loss: 0.02581178955733776\n",
            "step: 110, loss: 0.06061284616589546\n",
            "step: 120, loss: 0.12326091527938843\n",
            "step: 130, loss: 0.15177540481090546\n",
            "step: 140, loss: 0.06041615456342697\n",
            "step: 150, loss: 0.020301219075918198\n",
            "step: 160, loss: 0.01893037185072899\n",
            "step: 170, loss: 0.12213722616434097\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.7890818858560793, f1=0.7487922705314008, best_f1=0.7487922705314008\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005648382939398289\n",
            "step: 10, loss: 0.03031051903963089\n",
            "step: 20, loss: 0.309005469083786\n",
            "step: 30, loss: 0.20384754240512848\n",
            "step: 40, loss: 0.03789255768060684\n",
            "step: 50, loss: 0.0036531377118080854\n",
            "step: 60, loss: 0.021601764485239983\n",
            "step: 70, loss: 0.10686822980642319\n",
            "step: 80, loss: 0.015000078827142715\n",
            "step: 90, loss: 0.06504634767770767\n",
            "step: 100, loss: 0.09096041321754456\n",
            "step: 110, loss: 0.01298522762954235\n",
            "step: 120, loss: 0.06818254292011261\n",
            "step: 130, loss: 0.01393632311373949\n",
            "step: 140, loss: 0.0321907103061676\n",
            "step: 150, loss: 0.008761120028793812\n",
            "step: 160, loss: 0.11861822009086609\n",
            "step: 170, loss: 0.009669166058301926\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7819548872180452, f1=0.7487684729064039, best_f1=0.7487922705314008\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 313.71it/s]\n",
            "load_f1 = 0.5777777777777778\n",
            "real_f1 = 0.5382059800664453\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 196.52it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VngEb4vfhstw"
      },
      "source": [
        "## DITTO DIRTY - oK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfPaCqR4hstw"
      },
      "source": [
        "### DBLP-ACM - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff484f3-30a1-4531-9a70-60cea544cde4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5972806215286255\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.4554125666618347\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.5248098373413086\n",
            "step: 30, loss: 0.3351593315601349\n",
            "step: 40, loss: 0.35310444235801697\n",
            "step: 50, loss: 0.562762975692749\n",
            "step: 60, loss: 0.4342520534992218\n",
            "step: 70, loss: 0.29870346188545227\n",
            "step: 80, loss: 0.2749083936214447\n",
            "step: 90, loss: 0.15615807473659515\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.22101181745529175\n",
            "step: 110, loss: 0.11631473153829575\n",
            "step: 120, loss: 0.13535790145397186\n",
            "step: 130, loss: 0.04352711886167526\n",
            "step: 140, loss: 0.14842209219932556\n",
            "step: 150, loss: 0.21008417010307312\n",
            "step: 160, loss: 0.09486012160778046\n",
            "step: 170, loss: 0.12346312403678894\n",
            "step: 180, loss: 0.06370552629232407\n",
            "step: 190, loss: 0.42113324999809265\n",
            "step: 200, loss: 0.08265814930200577\n",
            "step: 210, loss: 0.06023520231246948\n",
            "step: 220, loss: 0.10516738891601562\n",
            "step: 230, loss: 0.05229602009057999\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9467969598262758, f1=0.943646408839779, best_f1=0.943646408839779\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015852469950914383\n",
            "step: 10, loss: 0.17107874155044556\n",
            "step: 20, loss: 0.0797598659992218\n",
            "step: 30, loss: 0.0694648027420044\n",
            "step: 40, loss: 0.027219654992222786\n",
            "step: 50, loss: 0.04339098185300827\n",
            "step: 60, loss: 0.046431198716163635\n",
            "step: 70, loss: 0.010068428702652454\n",
            "step: 80, loss: 0.004072179086506367\n",
            "step: 90, loss: 0.1322600543498993\n",
            "step: 100, loss: 0.2337336540222168\n",
            "step: 110, loss: 0.02892232872545719\n",
            "step: 120, loss: 0.1214178055524826\n",
            "step: 130, loss: 0.0720064789056778\n",
            "step: 140, loss: 0.06742073595523834\n",
            "step: 150, loss: 0.18896037340164185\n",
            "step: 160, loss: 0.018919581547379494\n",
            "step: 170, loss: 0.003982663620263338\n",
            "step: 180, loss: 0.061565447598695755\n",
            "step: 190, loss: 0.02862936072051525\n",
            "step: 200, loss: 0.11687719076871872\n",
            "step: 210, loss: 0.02745508961379528\n",
            "step: 220, loss: 0.013424213044345379\n",
            "step: 230, loss: 0.007205005269497633\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9537444933920706, f1=0.9481981981981982, best_f1=0.9481981981981982\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04484980180859566\n",
            "step: 10, loss: 0.0801299586892128\n",
            "step: 20, loss: 0.033526115119457245\n",
            "step: 30, loss: 0.07204493880271912\n",
            "step: 40, loss: 0.03787604346871376\n",
            "step: 50, loss: 0.05660300329327583\n",
            "step: 60, loss: 0.06313180178403854\n",
            "step: 70, loss: 0.030764926224946976\n",
            "step: 80, loss: 0.08597508817911148\n",
            "step: 90, loss: 0.011023773811757565\n",
            "step: 100, loss: 0.023636171594262123\n",
            "step: 110, loss: 0.007316816132515669\n",
            "step: 120, loss: 0.007604497950524092\n",
            "step: 130, loss: 0.015572786331176758\n",
            "step: 140, loss: 0.009905675426125526\n",
            "step: 150, loss: 0.049915749579668045\n",
            "step: 160, loss: 0.015274831093847752\n",
            "step: 170, loss: 0.009178652428090572\n",
            "step: 180, loss: 0.020248400047421455\n",
            "step: 190, loss: 0.007997553795576096\n",
            "step: 200, loss: 0.012982726097106934\n",
            "step: 210, loss: 0.0026150685735046864\n",
            "step: 220, loss: 0.04310913011431694\n",
            "step: 230, loss: 0.05444706976413727\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9511918274687856, f1=0.9457900807381776, best_f1=0.9481981981981982\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023201853036880493\n",
            "step: 10, loss: 0.017051320523023605\n",
            "step: 20, loss: 0.002251823665574193\n",
            "step: 30, loss: 0.0037937758024781942\n",
            "step: 40, loss: 0.14700503647327423\n",
            "step: 50, loss: 0.013353331945836544\n",
            "step: 60, loss: 0.014673782512545586\n",
            "step: 70, loss: 0.10764755308628082\n",
            "step: 80, loss: 0.11858736723661423\n",
            "step: 90, loss: 0.021219471469521523\n",
            "step: 100, loss: 0.003529746551066637\n",
            "step: 110, loss: 0.003484027925878763\n",
            "step: 120, loss: 0.11283724009990692\n",
            "step: 130, loss: 0.08928230404853821\n",
            "step: 140, loss: 0.008589657954871655\n",
            "step: 150, loss: 0.02713160216808319\n",
            "step: 160, loss: 0.06640433520078659\n",
            "step: 170, loss: 0.025230158120393753\n",
            "step: 180, loss: 0.1645781248807907\n",
            "step: 190, loss: 0.00574903842061758\n",
            "step: 200, loss: 0.03403308987617493\n",
            "step: 210, loss: 0.01709354855120182\n",
            "step: 220, loss: 0.0010800431482493877\n",
            "step: 230, loss: 0.0037471929099410772\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9594742606790799, f1=0.9493813273340832, best_f1=0.9493813273340832\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002146544400602579\n",
            "step: 10, loss: 0.01573726162314415\n",
            "step: 20, loss: 0.20292268693447113\n",
            "step: 30, loss: 0.005249124951660633\n",
            "step: 40, loss: 0.0049317749217152596\n",
            "step: 50, loss: 0.004551073536276817\n",
            "step: 60, loss: 0.09924184530973434\n",
            "step: 70, loss: 0.020289242267608643\n",
            "step: 80, loss: 0.018154725432395935\n",
            "step: 90, loss: 0.10495663434267044\n",
            "step: 100, loss: 0.007626193575561047\n",
            "step: 110, loss: 0.0009455239633098245\n",
            "step: 120, loss: 0.15179036557674408\n",
            "step: 130, loss: 0.04217430576682091\n",
            "step: 140, loss: 0.06058280169963837\n",
            "step: 150, loss: 0.0067688473500311375\n",
            "step: 160, loss: 0.0017841706285253167\n",
            "step: 170, loss: 0.026576131582260132\n",
            "step: 180, loss: 0.005338519345968962\n",
            "step: 190, loss: 0.034145209938287735\n",
            "step: 200, loss: 0.04190930351614952\n",
            "step: 210, loss: 0.02011852152645588\n",
            "step: 220, loss: 0.007086853962391615\n",
            "step: 230, loss: 0.030569054186344147\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9507119386637459, f1=0.9439461883408071, best_f1=0.9493813273340832\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002811812562867999\n",
            "step: 10, loss: 0.022166326642036438\n",
            "step: 20, loss: 0.003552391892299056\n",
            "step: 30, loss: 0.002602888736873865\n",
            "step: 40, loss: 0.0006794155924580991\n",
            "step: 50, loss: 0.012333580292761326\n",
            "step: 60, loss: 0.09211872518062592\n",
            "step: 70, loss: 0.10556045174598694\n",
            "step: 80, loss: 0.007049995008856058\n",
            "step: 90, loss: 0.06229085847735405\n",
            "step: 100, loss: 0.0011497262166813016\n",
            "step: 110, loss: 0.03331795707345009\n",
            "step: 120, loss: 0.0035140984691679478\n",
            "step: 130, loss: 0.0037546788807958364\n",
            "step: 140, loss: 0.0011580846039578319\n",
            "step: 150, loss: 0.0008483560523018241\n",
            "step: 160, loss: 0.0022581988014280796\n",
            "step: 170, loss: 0.0006557874730788171\n",
            "step: 180, loss: 0.030423898249864578\n",
            "step: 190, loss: 0.003678077831864357\n",
            "step: 200, loss: 0.167327418923378\n",
            "step: 210, loss: 0.00891948863863945\n",
            "step: 220, loss: 0.04202048480510712\n",
            "step: 230, loss: 0.0031306983437389135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9648924122310306, f1=0.9519450800915331, best_f1=0.9519450800915331\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006519312039017677\n",
            "step: 10, loss: 0.002227778546512127\n",
            "step: 20, loss: 0.0007307514897547662\n",
            "step: 30, loss: 0.0005470510805025697\n",
            "step: 40, loss: 0.000887205358594656\n",
            "step: 50, loss: 0.0062931375578045845\n",
            "step: 60, loss: 0.0017369026318192482\n",
            "step: 70, loss: 0.0019966335967183113\n",
            "step: 80, loss: 0.0012712046736851335\n",
            "step: 90, loss: 0.0016881362535059452\n",
            "step: 100, loss: 0.0006344317807815969\n",
            "step: 110, loss: 0.0013738642446696758\n",
            "step: 120, loss: 0.0024711876176297665\n",
            "step: 130, loss: 0.03907981887459755\n",
            "step: 140, loss: 0.0024668416008353233\n",
            "step: 150, loss: 0.1244373619556427\n",
            "step: 160, loss: 0.0007007477688603103\n",
            "step: 170, loss: 0.0548025406897068\n",
            "step: 180, loss: 0.002372225746512413\n",
            "step: 190, loss: 0.1534644067287445\n",
            "step: 200, loss: 0.031218089163303375\n",
            "step: 210, loss: 0.019186055287718773\n",
            "step: 220, loss: 0.003796808421611786\n",
            "step: 230, loss: 0.0018305518897250295\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.968609865470852, f1=0.9567198177676537, best_f1=0.9567198177676537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007072238950058818\n",
            "step: 10, loss: 0.023957695811986923\n",
            "step: 20, loss: 0.0018532982794567943\n",
            "step: 30, loss: 0.0007595912902615964\n",
            "step: 40, loss: 0.0060880049131810665\n",
            "step: 50, loss: 0.0008024809067137539\n",
            "step: 60, loss: 0.0008241672767326236\n",
            "step: 70, loss: 0.005593892186880112\n",
            "step: 80, loss: 0.0024506570771336555\n",
            "step: 90, loss: 0.000645727850496769\n",
            "step: 100, loss: 0.0016090720891952515\n",
            "step: 110, loss: 0.030996689572930336\n",
            "step: 120, loss: 0.00036232025013305247\n",
            "step: 130, loss: 0.034062597900629044\n",
            "step: 140, loss: 0.0014617738779634237\n",
            "step: 150, loss: 0.2659231424331665\n",
            "step: 160, loss: 0.002697056857869029\n",
            "step: 170, loss: 0.01428176835179329\n",
            "step: 180, loss: 0.002373988041654229\n",
            "step: 190, loss: 0.03680672124028206\n",
            "step: 200, loss: 0.011325489729642868\n",
            "step: 210, loss: 0.009013655595481396\n",
            "step: 220, loss: 0.007401086390018463\n",
            "step: 230, loss: 0.0027922876179218292\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9775280898876404, f1=0.9558323895809739, best_f1=0.9558323895809739\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017279449384659529\n",
            "step: 10, loss: 0.001193289877846837\n",
            "step: 20, loss: 0.00443172687664628\n",
            "step: 30, loss: 0.00106845295522362\n",
            "step: 40, loss: 0.003911031410098076\n",
            "step: 50, loss: 0.0010200702818110585\n",
            "step: 60, loss: 0.0006660491926595569\n",
            "step: 70, loss: 0.03224031999707222\n",
            "step: 80, loss: 0.002191763138398528\n",
            "step: 90, loss: 0.1248253583908081\n",
            "step: 100, loss: 0.0004569469019770622\n",
            "step: 110, loss: 0.000456102192401886\n",
            "step: 120, loss: 0.009236113168299198\n",
            "step: 130, loss: 0.06179096922278404\n",
            "step: 140, loss: 0.050708796828985214\n",
            "step: 150, loss: 0.01588594913482666\n",
            "step: 160, loss: 0.009270149283111095\n",
            "step: 170, loss: 0.0010469520930200815\n",
            "step: 180, loss: 0.0013047928223386407\n",
            "step: 190, loss: 0.00015724723925814033\n",
            "step: 200, loss: 0.0045041427947580814\n",
            "step: 210, loss: 0.0008097795071080327\n",
            "step: 220, loss: 0.0004351649549789727\n",
            "step: 230, loss: 0.00022701820125803351\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9807474518686297, f1=0.9540229885057471, best_f1=0.9540229885057471\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000551258388441056\n",
            "step: 10, loss: 0.0007342266035266221\n",
            "step: 20, loss: 0.0004012267745565623\n",
            "step: 30, loss: 0.00021580937027465552\n",
            "step: 40, loss: 0.02714705467224121\n",
            "step: 50, loss: 0.00012175818847026676\n",
            "step: 60, loss: 0.0006544762873090804\n",
            "step: 70, loss: 0.010140052996575832\n",
            "step: 80, loss: 0.003828460816293955\n",
            "step: 90, loss: 0.007090689614415169\n",
            "step: 100, loss: 0.00037309175240807235\n",
            "step: 110, loss: 0.011275336146354675\n",
            "step: 120, loss: 0.00039247979293577373\n",
            "step: 130, loss: 0.004659566562622786\n",
            "step: 140, loss: 0.00021647359244525433\n",
            "step: 150, loss: 0.06027532368898392\n",
            "step: 160, loss: 0.0002071074559353292\n",
            "step: 170, loss: 0.0001946765260072425\n",
            "step: 180, loss: 0.001274285139515996\n",
            "step: 190, loss: 0.0005375199834816158\n",
            "step: 200, loss: 0.000593475706409663\n",
            "step: 210, loss: 0.0014662526082247496\n",
            "step: 220, loss: 0.0011343626538291574\n",
            "step: 230, loss: 0.0006577984895557165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9821029082774049, f1=0.962962962962963, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014949965989217162\n",
            "step: 10, loss: 0.0009044166654348373\n",
            "step: 20, loss: 0.001158138969913125\n",
            "step: 30, loss: 0.0004071598523296416\n",
            "step: 40, loss: 0.00011358941264916211\n",
            "step: 50, loss: 0.0007289405330084264\n",
            "step: 60, loss: 0.001758136204443872\n",
            "step: 70, loss: 0.0003888123028445989\n",
            "step: 80, loss: 0.0016928269760683179\n",
            "step: 90, loss: 0.22079820930957794\n",
            "step: 100, loss: 0.0007576961652375758\n",
            "step: 110, loss: 0.0003939488378819078\n",
            "step: 120, loss: 0.0013944566017016768\n",
            "step: 130, loss: 0.0003101684560533613\n",
            "step: 140, loss: 0.001384776085615158\n",
            "step: 150, loss: 0.0006322851404547691\n",
            "step: 160, loss: 0.0009026431362144649\n",
            "step: 170, loss: 0.003300813026726246\n",
            "step: 180, loss: 0.0004548478464130312\n",
            "step: 190, loss: 0.0005787698901258409\n",
            "step: 200, loss: 0.003744088113307953\n",
            "step: 210, loss: 0.00034888743539340794\n",
            "step: 220, loss: 0.0004694641684181988\n",
            "step: 230, loss: 0.0007422319613397121\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.978675645342312, f1=0.963882618510158, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005826579872518778\n",
            "step: 10, loss: 0.00025363065651617944\n",
            "step: 20, loss: 0.0011427890276536345\n",
            "step: 30, loss: 0.002957334741950035\n",
            "step: 40, loss: 0.037106528878211975\n",
            "step: 50, loss: 0.0007531827432103455\n",
            "step: 60, loss: 0.0007013165741227567\n",
            "step: 70, loss: 0.0006650614668615162\n",
            "step: 80, loss: 0.00010826090874616057\n",
            "step: 90, loss: 0.001531588495709002\n",
            "step: 100, loss: 0.00010398480662843212\n",
            "step: 110, loss: 0.0011114967055618763\n",
            "step: 120, loss: 0.003264122176915407\n",
            "step: 130, loss: 0.00030788721051067114\n",
            "step: 140, loss: 0.00065907760290429\n",
            "step: 150, loss: 0.0009456360712647438\n",
            "step: 160, loss: 0.001595564535818994\n",
            "step: 170, loss: 0.0010870574042201042\n",
            "step: 180, loss: 0.0002393030736129731\n",
            "step: 190, loss: 0.000952318892814219\n",
            "step: 200, loss: 0.0002809930592775345\n",
            "step: 210, loss: 0.08111440390348434\n",
            "step: 220, loss: 0.0014133262448012829\n",
            "step: 230, loss: 0.0009357769740745425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9808773903262092, f1=0.9659090909090909, best_f1=0.962962962962963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008368974085897207\n",
            "step: 10, loss: 0.0005369903519749641\n",
            "step: 20, loss: 0.0005642111063934863\n",
            "step: 30, loss: 0.0004599870298989117\n",
            "step: 40, loss: 0.001664165174588561\n",
            "step: 50, loss: 0.0068158432841300964\n",
            "step: 60, loss: 0.000329061527736485\n",
            "step: 70, loss: 0.0017554913647472858\n",
            "step: 80, loss: 0.007073315791785717\n",
            "step: 90, loss: 0.0002172800013795495\n",
            "step: 100, loss: 0.02316305600106716\n",
            "step: 110, loss: 0.012495378032326698\n",
            "step: 120, loss: 0.002622873056679964\n",
            "step: 130, loss: 0.0014092018827795982\n",
            "step: 140, loss: 0.0001811763650039211\n",
            "step: 150, loss: 0.0022975183092057705\n",
            "step: 160, loss: 0.0015676531475037336\n",
            "step: 170, loss: 0.0007082029478624463\n",
            "step: 180, loss: 0.03766029700636864\n",
            "step: 190, loss: 0.0008156529511325061\n",
            "step: 200, loss: 0.0004624048597179353\n",
            "step: 210, loss: 0.0004143610130995512\n",
            "step: 220, loss: 0.0004450095584616065\n",
            "step: 230, loss: 0.00033535680267959833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9842696629213483, f1=0.963718820861678, best_f1=0.963718820861678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005393826868385077\n",
            "step: 10, loss: 0.002225077012553811\n",
            "step: 20, loss: 0.0015857235994189978\n",
            "step: 30, loss: 0.00047764467308297753\n",
            "step: 40, loss: 0.0015182839706540108\n",
            "step: 50, loss: 0.00015699518553446978\n",
            "step: 60, loss: 0.00022875559807289392\n",
            "step: 70, loss: 0.0004303519381210208\n",
            "step: 80, loss: 0.00023427425185218453\n",
            "step: 90, loss: 0.0004494866298045963\n",
            "step: 100, loss: 0.000599250546656549\n",
            "step: 110, loss: 0.0004988994915038347\n",
            "step: 120, loss: 0.00018544081831350923\n",
            "step: 130, loss: 0.0008301000925712287\n",
            "step: 140, loss: 0.0721735879778862\n",
            "step: 150, loss: 0.0001715696562314406\n",
            "step: 160, loss: 0.0004934826865792274\n",
            "step: 170, loss: 0.00034682286786846817\n",
            "step: 180, loss: 0.0003182046639267355\n",
            "step: 190, loss: 0.000548427109606564\n",
            "step: 200, loss: 0.0004805278149433434\n",
            "step: 210, loss: 0.0017333622090518475\n",
            "step: 220, loss: 0.001915041240863502\n",
            "step: 230, loss: 0.03151898458600044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9831271091113611, f1=0.9648127128263336, best_f1=0.963718820861678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007690976839512587\n",
            "step: 10, loss: 0.00041243276791647077\n",
            "step: 20, loss: 0.0005478262901306152\n",
            "step: 30, loss: 0.005552513990551233\n",
            "step: 40, loss: 0.0008088489412330091\n",
            "step: 50, loss: 0.0002834867627825588\n",
            "step: 60, loss: 0.0003432536032050848\n",
            "step: 70, loss: 0.005195676814764738\n",
            "step: 80, loss: 0.00019390102534089237\n",
            "step: 90, loss: 0.0007755299448035657\n",
            "step: 100, loss: 0.002522716298699379\n",
            "step: 110, loss: 0.00031732494244351983\n",
            "step: 120, loss: 0.035635825246572495\n",
            "step: 130, loss: 0.0002746782556641847\n",
            "step: 140, loss: 0.0010216940427199006\n",
            "step: 150, loss: 0.0004679904959630221\n",
            "step: 160, loss: 0.004393601790070534\n",
            "step: 170, loss: 0.00021513823594432324\n",
            "step: 180, loss: 0.0006345478468574584\n",
            "step: 190, loss: 0.00044253820669837296\n",
            "step: 200, loss: 0.008088800124824047\n",
            "step: 210, loss: 0.006295075174421072\n",
            "step: 220, loss: 0.0015342560363933444\n",
            "step: 230, loss: 0.0010856003500521183\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9831649831649831, f1=0.9625425652667423, best_f1=0.963718820861678\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 207.79it/s]\n",
            "load_f1 = 0.9808773903262092\n",
            "real_f1 = 0.9766925638179801\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 197.59it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY0y_yZuhstx"
      },
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77537de-ca09-4645-90eb-471698d9d9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 430kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 792kB/s]\n",
            "Downloading: 100% 456k/456k [00:01<00:00, 401kB/s] \n",
            "Downloading: 100% 501M/501M [00:07<00:00, 64.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6149956583976746\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.49291667342185974\n",
            "step: 20, loss: 0.31033945083618164\n",
            "step: 30, loss: 0.3375636041164398\n",
            "step: 40, loss: 0.36393681168556213\n",
            "step: 50, loss: 0.6953979134559631\n",
            "step: 60, loss: 0.35136836767196655\n",
            "step: 70, loss: 0.4533482491970062\n",
            "step: 80, loss: 0.4124554991722107\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 90, loss: 0.5335617661476135\n",
            "step: 100, loss: 0.5473364591598511\n",
            "step: 110, loss: 0.19425559043884277\n",
            "step: 120, loss: 0.20914484560489655\n",
            "step: 130, loss: 0.08789416402578354\n",
            "step: 140, loss: 0.26324889063835144\n",
            "step: 150, loss: 0.20239906013011932\n",
            "step: 160, loss: 0.21111711859703064\n",
            "step: 170, loss: 0.2714550495147705\n",
            "step: 180, loss: 0.1836307793855667\n",
            "step: 190, loss: 0.2662494480609894\n",
            "step: 200, loss: 0.2575524151325226\n",
            "step: 210, loss: 0.1345183551311493\n",
            "step: 220, loss: 0.14470162987709045\n",
            "step: 230, loss: 0.2609466314315796\n",
            "step: 240, loss: 0.08364308625459671\n",
            "step: 250, loss: 0.09783579409122467\n",
            "step: 260, loss: 0.48507264256477356\n",
            "step: 270, loss: 0.27932748198509216\n",
            "step: 280, loss: 0.06812874972820282\n",
            "step: 290, loss: 0.06668832898139954\n",
            "step: 300, loss: 0.11307282745838165\n",
            "step: 310, loss: 0.21135570108890533\n",
            "step: 320, loss: 0.1882483959197998\n",
            "step: 330, loss: 0.06027834489941597\n",
            "step: 340, loss: 0.4862723648548126\n",
            "step: 350, loss: 0.2368806153535843\n",
            "step: 360, loss: 0.006419147364795208\n",
            "step: 370, loss: 0.03125012293457985\n",
            "step: 380, loss: 0.09819621592760086\n",
            "step: 390, loss: 0.0549364909529686\n",
            "step: 400, loss: 0.10107485204935074\n",
            "step: 410, loss: 0.21242879331111908\n",
            "step: 420, loss: 0.03029101900756359\n",
            "step: 430, loss: 0.04500589519739151\n",
            "step: 440, loss: 0.08048424869775772\n",
            "step: 450, loss: 0.23590770363807678\n",
            "step: 460, loss: 0.07231741398572922\n",
            "step: 470, loss: 0.07369717955589294\n",
            "step: 480, loss: 0.17013248801231384\n",
            "step: 490, loss: 0.11939725279808044\n",
            "step: 500, loss: 0.047038041055202484\n",
            "step: 510, loss: 0.08141371607780457\n",
            "step: 520, loss: 0.10015100240707397\n",
            "step: 530, loss: 0.08539576083421707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8955091151622944, f1=0.8939802336028752, best_f1=0.8939802336028752\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1007990762591362\n",
            "step: 10, loss: 0.1627860963344574\n",
            "step: 20, loss: 0.15409424901008606\n",
            "step: 30, loss: 0.10433557629585266\n",
            "step: 40, loss: 0.17907372117042542\n",
            "step: 50, loss: 0.1050684005022049\n",
            "step: 60, loss: 0.2274089902639389\n",
            "step: 70, loss: 0.06585751473903656\n",
            "step: 80, loss: 0.06719812750816345\n",
            "step: 90, loss: 0.037091489881277084\n",
            "step: 100, loss: 0.09025919437408447\n",
            "step: 110, loss: 0.06181533634662628\n",
            "step: 120, loss: 0.1289239227771759\n",
            "step: 130, loss: 0.03606080636382103\n",
            "step: 140, loss: 0.06660792976617813\n",
            "step: 150, loss: 0.10509931296110153\n",
            "step: 160, loss: 0.04237022623419762\n",
            "step: 170, loss: 0.07856335490942001\n",
            "step: 180, loss: 0.1273140013217926\n",
            "step: 190, loss: 0.06185116246342659\n",
            "step: 200, loss: 0.24449101090431213\n",
            "step: 210, loss: 0.06823339313268661\n",
            "step: 220, loss: 0.008547027595341206\n",
            "step: 230, loss: 0.15507742762565613\n",
            "step: 240, loss: 0.02130226604640484\n",
            "step: 250, loss: 0.07968943566083908\n",
            "step: 260, loss: 0.09414280951023102\n",
            "step: 270, loss: 0.02403377927839756\n",
            "step: 280, loss: 0.09982222318649292\n",
            "step: 290, loss: 0.026100266724824905\n",
            "step: 300, loss: 0.09005782008171082\n",
            "step: 310, loss: 0.056780632585287094\n",
            "step: 320, loss: 0.06981226801872253\n",
            "step: 330, loss: 0.10006522387266159\n",
            "step: 340, loss: 0.15894697606563568\n",
            "step: 350, loss: 0.0066567156463861465\n",
            "step: 360, loss: 0.11703865230083466\n",
            "step: 370, loss: 0.08979804813861847\n",
            "step: 380, loss: 0.12046308070421219\n",
            "step: 390, loss: 0.013469541445374489\n",
            "step: 400, loss: 0.16047430038452148\n",
            "step: 410, loss: 0.10487312078475952\n",
            "step: 420, loss: 0.05280162766575813\n",
            "step: 430, loss: 0.07275494188070297\n",
            "step: 440, loss: 0.03126987814903259\n",
            "step: 450, loss: 0.05842221900820732\n",
            "step: 460, loss: 0.036339469254016876\n",
            "step: 470, loss: 0.06408408284187317\n",
            "step: 480, loss: 0.01548478938639164\n",
            "step: 490, loss: 0.09476468712091446\n",
            "step: 500, loss: 0.024283891543745995\n",
            "step: 510, loss: 0.04683101922273636\n",
            "step: 520, loss: 0.484140008687973\n",
            "step: 530, loss: 0.08369224518537521\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9294492489758761, f1=0.9216757741347906, best_f1=0.9216757741347906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16363120079040527\n",
            "step: 10, loss: 0.1373404562473297\n",
            "step: 20, loss: 0.027972718700766563\n",
            "step: 30, loss: 0.05122220516204834\n",
            "step: 40, loss: 0.04271459951996803\n",
            "step: 50, loss: 0.09446607530117035\n",
            "step: 60, loss: 0.058138709515333176\n",
            "step: 70, loss: 0.03253115341067314\n",
            "step: 80, loss: 0.1846950650215149\n",
            "step: 90, loss: 0.02808123268187046\n",
            "step: 100, loss: 0.13442876935005188\n",
            "step: 110, loss: 0.046069879084825516\n",
            "step: 120, loss: 0.07992478460073471\n",
            "step: 130, loss: 0.10410935431718826\n",
            "step: 140, loss: 0.09330940246582031\n",
            "step: 150, loss: 0.0704571008682251\n",
            "step: 160, loss: 0.08973973244428635\n",
            "step: 170, loss: 0.025375567376613617\n",
            "step: 180, loss: 0.015905600041151047\n",
            "step: 190, loss: 0.016476569697260857\n",
            "step: 200, loss: 0.008463834412395954\n",
            "step: 210, loss: 0.07018928974866867\n",
            "step: 220, loss: 0.12988807260990143\n",
            "step: 230, loss: 0.0332605354487896\n",
            "step: 240, loss: 0.06375043094158173\n",
            "step: 250, loss: 0.22071102261543274\n",
            "step: 260, loss: 0.07848740369081497\n",
            "step: 270, loss: 0.0643150806427002\n",
            "step: 280, loss: 0.015764568001031876\n",
            "step: 290, loss: 0.018997257575392723\n",
            "step: 300, loss: 0.11808480322360992\n",
            "step: 310, loss: 0.10841973125934601\n",
            "step: 320, loss: 0.04124142974615097\n",
            "step: 330, loss: 0.012318400666117668\n",
            "step: 340, loss: 0.023597804829478264\n",
            "step: 350, loss: 0.13916915655136108\n",
            "step: 360, loss: 0.013406039215624332\n",
            "step: 370, loss: 0.039764851331710815\n",
            "step: 380, loss: 0.009757510386407375\n",
            "step: 390, loss: 0.06296615302562714\n",
            "step: 400, loss: 0.05210699513554573\n",
            "step: 410, loss: 0.012043111957609653\n",
            "step: 420, loss: 0.00969256367534399\n",
            "step: 430, loss: 0.06052885949611664\n",
            "step: 440, loss: 0.2900448739528656\n",
            "step: 450, loss: 0.15378926694393158\n",
            "step: 460, loss: 0.12450769543647766\n",
            "step: 470, loss: 0.012566924095153809\n",
            "step: 480, loss: 0.17602433264255524\n",
            "step: 490, loss: 0.006511949468404055\n",
            "step: 500, loss: 0.021809810772538185\n",
            "step: 510, loss: 0.02495436556637287\n",
            "step: 520, loss: 0.08510202169418335\n",
            "step: 530, loss: 0.026674555614590645\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.925497454881999, f1=0.9178208679593721, best_f1=0.9216757741347906\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013723472133278847\n",
            "step: 10, loss: 0.011738480068743229\n",
            "step: 20, loss: 0.017798764631152153\n",
            "step: 30, loss: 0.12538231909275055\n",
            "step: 40, loss: 0.0524861142039299\n",
            "step: 50, loss: 0.05753777548670769\n",
            "step: 60, loss: 0.04041073098778725\n",
            "step: 70, loss: 0.15069158375263214\n",
            "step: 80, loss: 0.15584328770637512\n",
            "step: 90, loss: 0.03652753308415413\n",
            "step: 100, loss: 0.010986795648932457\n",
            "step: 110, loss: 0.18475604057312012\n",
            "step: 120, loss: 0.026720890775322914\n",
            "step: 130, loss: 0.18255537748336792\n",
            "step: 140, loss: 0.04978523775935173\n",
            "step: 150, loss: 0.0409136638045311\n",
            "step: 160, loss: 0.04667404666543007\n",
            "step: 170, loss: 0.061558451503515244\n",
            "step: 180, loss: 0.13289305567741394\n",
            "step: 190, loss: 0.16363807022571564\n",
            "step: 200, loss: 0.02770746499300003\n",
            "step: 210, loss: 0.006588564720004797\n",
            "step: 220, loss: 0.014420682564377785\n",
            "step: 230, loss: 0.05157316476106644\n",
            "step: 240, loss: 0.06549005210399628\n",
            "step: 250, loss: 0.15794317424297333\n",
            "step: 260, loss: 0.042120471596717834\n",
            "step: 270, loss: 0.057284072041511536\n",
            "step: 280, loss: 0.0035236021503806114\n",
            "step: 290, loss: 0.028136800974607468\n",
            "step: 300, loss: 0.003563364502042532\n",
            "step: 310, loss: 0.006643139757215977\n",
            "step: 320, loss: 0.15866853296756744\n",
            "step: 330, loss: 0.026683256030082703\n",
            "step: 340, loss: 0.03749867156147957\n",
            "step: 350, loss: 0.01993916556239128\n",
            "step: 360, loss: 0.028545618057250977\n",
            "step: 370, loss: 0.060275640338659286\n",
            "step: 380, loss: 0.06249735504388809\n",
            "step: 390, loss: 0.007875756360590458\n",
            "step: 400, loss: 0.04414300620555878\n",
            "step: 410, loss: 0.023572329431772232\n",
            "step: 420, loss: 0.015318972989916801\n",
            "step: 430, loss: 0.0191352479159832\n",
            "step: 440, loss: 0.0140862250700593\n",
            "step: 450, loss: 0.08685409277677536\n",
            "step: 460, loss: 0.03463626652956009\n",
            "step: 470, loss: 0.008673489093780518\n",
            "step: 480, loss: 0.06840655207633972\n",
            "step: 490, loss: 0.0006272522732615471\n",
            "step: 500, loss: 0.07902742922306061\n",
            "step: 510, loss: 0.0531180277466774\n",
            "step: 520, loss: 0.01423724740743637\n",
            "step: 530, loss: 0.08290515840053558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9327731092436975, f1=0.9187411930483794, best_f1=0.9187411930483794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031166696920990944\n",
            "step: 10, loss: 0.010591219179332256\n",
            "step: 20, loss: 0.024683183059096336\n",
            "step: 30, loss: 0.034465380012989044\n",
            "step: 40, loss: 0.06110921502113342\n",
            "step: 50, loss: 0.07581395655870438\n",
            "step: 60, loss: 0.022242307662963867\n",
            "step: 70, loss: 0.08546223491430283\n",
            "step: 80, loss: 0.002636958612129092\n",
            "step: 90, loss: 0.15147435665130615\n",
            "step: 100, loss: 0.1684953272342682\n",
            "step: 110, loss: 0.016494493931531906\n",
            "step: 120, loss: 0.015294533222913742\n",
            "step: 130, loss: 0.011376520618796349\n",
            "step: 140, loss: 0.060894764959812164\n",
            "step: 150, loss: 0.038217898458242416\n",
            "step: 160, loss: 0.027914853766560555\n",
            "step: 170, loss: 0.14045719802379608\n",
            "step: 180, loss: 0.01835952140390873\n",
            "step: 190, loss: 0.004067596048116684\n",
            "step: 200, loss: 0.010020517744123936\n",
            "step: 210, loss: 0.0035483406390994787\n",
            "step: 220, loss: 0.0049332138150930405\n",
            "step: 230, loss: 0.0035547546576708555\n",
            "step: 240, loss: 0.020203515887260437\n",
            "step: 250, loss: 0.09366510063409805\n",
            "step: 260, loss: 0.003417791798710823\n",
            "step: 270, loss: 0.00865147728472948\n",
            "step: 280, loss: 0.009847649373114109\n",
            "step: 290, loss: 0.012597513385117054\n",
            "step: 300, loss: 0.3683679699897766\n",
            "step: 310, loss: 0.0653829425573349\n",
            "step: 320, loss: 0.08218821883201599\n",
            "step: 330, loss: 0.03001718968153\n",
            "step: 340, loss: 0.014591231010854244\n",
            "step: 350, loss: 0.0028321545105427504\n",
            "step: 360, loss: 0.007088627200573683\n",
            "step: 370, loss: 0.028093371540308\n",
            "step: 380, loss: 0.02528325468301773\n",
            "step: 390, loss: 0.025568583980202675\n",
            "step: 400, loss: 0.03160492330789566\n",
            "step: 410, loss: 0.04111652076244354\n",
            "step: 420, loss: 0.1750514656305313\n",
            "step: 430, loss: 0.020621811971068382\n",
            "step: 440, loss: 0.006482795812189579\n",
            "step: 450, loss: 0.01434569526463747\n",
            "step: 460, loss: 0.18724249303340912\n",
            "step: 470, loss: 0.07377646118402481\n",
            "step: 480, loss: 0.03545133024454117\n",
            "step: 490, loss: 0.017069755122065544\n",
            "step: 500, loss: 0.046908408403396606\n",
            "step: 510, loss: 0.017380358651280403\n",
            "step: 520, loss: 0.20962633192539215\n",
            "step: 530, loss: 0.06119368597865105\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9319129226493746, f1=0.9187411930483794, best_f1=0.9187411930483794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.061247166246175766\n",
            "step: 10, loss: 0.02046976424753666\n",
            "step: 20, loss: 0.019171562045812607\n",
            "step: 30, loss: 0.0008161202422343194\n",
            "step: 40, loss: 0.0008995727985166013\n",
            "step: 50, loss: 0.005951219704002142\n",
            "step: 60, loss: 0.007993313483893871\n",
            "step: 70, loss: 0.002945552347227931\n",
            "step: 80, loss: 0.019846521317958832\n",
            "step: 90, loss: 0.16896434128284454\n",
            "step: 100, loss: 0.0304731335490942\n",
            "step: 110, loss: 0.012163816019892693\n",
            "step: 120, loss: 0.02763316221535206\n",
            "step: 130, loss: 0.006606435868889093\n",
            "step: 140, loss: 0.0066270302049815655\n",
            "step: 150, loss: 0.0013900543563067913\n",
            "step: 160, loss: 0.01976541429758072\n",
            "step: 170, loss: 0.004899074323475361\n",
            "step: 180, loss: 0.001443265238776803\n",
            "step: 190, loss: 0.3627907931804657\n",
            "step: 200, loss: 0.07142496854066849\n",
            "step: 210, loss: 0.014901591464877129\n",
            "step: 220, loss: 0.06860989332199097\n",
            "step: 230, loss: 0.026612555608153343\n",
            "step: 240, loss: 0.005356178618967533\n",
            "step: 250, loss: 0.04792769253253937\n",
            "step: 260, loss: 0.01973000355064869\n",
            "step: 270, loss: 0.004422212950885296\n",
            "step: 280, loss: 0.012046754360198975\n",
            "step: 290, loss: 0.0018208562396466732\n",
            "step: 300, loss: 0.0019116874318569899\n",
            "step: 310, loss: 0.12910524010658264\n",
            "step: 320, loss: 0.007166722323745489\n",
            "step: 330, loss: 0.030379561707377434\n",
            "step: 340, loss: 0.001516504562459886\n",
            "step: 350, loss: 0.06036382541060448\n",
            "step: 360, loss: 0.1407424360513687\n",
            "step: 370, loss: 0.01253095269203186\n",
            "step: 380, loss: 0.0005848266300745308\n",
            "step: 390, loss: 0.04207876697182655\n",
            "step: 400, loss: 0.003076678840443492\n",
            "step: 410, loss: 0.04541020840406418\n",
            "step: 420, loss: 0.00890765804797411\n",
            "step: 430, loss: 0.005269213113933802\n",
            "step: 440, loss: 0.003505873493850231\n",
            "step: 450, loss: 0.2072111964225769\n",
            "step: 460, loss: 0.006710894405841827\n",
            "step: 470, loss: 0.01693830080330372\n",
            "step: 480, loss: 0.014783350750803947\n",
            "step: 490, loss: 0.006568441167473793\n",
            "step: 500, loss: 0.022063355892896652\n",
            "step: 510, loss: 0.010129219852387905\n",
            "step: 520, loss: 0.005002585239708424\n",
            "step: 530, loss: 0.005949050188064575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9259962049335864, f1=0.9153515064562409, best_f1=0.9187411930483794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0073462361469864845\n",
            "step: 10, loss: 0.01571454294025898\n",
            "step: 20, loss: 0.036157265305519104\n",
            "step: 30, loss: 0.03754078224301338\n",
            "step: 40, loss: 0.00669083371758461\n",
            "step: 50, loss: 0.15207615494728088\n",
            "step: 60, loss: 0.04943373426795006\n",
            "step: 70, loss: 0.0018797715893015265\n",
            "step: 80, loss: 0.0033200865145772696\n",
            "step: 90, loss: 0.01609819382429123\n",
            "step: 100, loss: 0.014189885929226875\n",
            "step: 110, loss: 0.0014459651429206133\n",
            "step: 120, loss: 0.1699659824371338\n",
            "step: 130, loss: 0.005954301916062832\n",
            "step: 140, loss: 0.004011765122413635\n",
            "step: 150, loss: 0.0029189579654484987\n",
            "step: 160, loss: 0.00485085416585207\n",
            "step: 170, loss: 0.025249846279621124\n",
            "step: 180, loss: 0.07731613516807556\n",
            "step: 190, loss: 0.028590409085154533\n",
            "step: 200, loss: 0.0004531518497969955\n",
            "step: 210, loss: 0.0035903374664485455\n",
            "step: 220, loss: 0.005076893605291843\n",
            "step: 230, loss: 0.004562031477689743\n",
            "step: 240, loss: 0.03050367906689644\n",
            "step: 250, loss: 0.06842093169689178\n",
            "step: 260, loss: 0.011338386684656143\n",
            "step: 270, loss: 0.012906242161989212\n",
            "step: 280, loss: 0.0067929429933428764\n",
            "step: 290, loss: 0.002001001965254545\n",
            "step: 300, loss: 0.0006495245033875108\n",
            "step: 310, loss: 0.015843907371163368\n",
            "step: 320, loss: 0.1213579773902893\n",
            "step: 330, loss: 0.06239492818713188\n",
            "step: 340, loss: 0.05767865106463432\n",
            "step: 350, loss: 0.0025573582388460636\n",
            "step: 360, loss: 0.02961813285946846\n",
            "step: 370, loss: 0.08889074623584747\n",
            "step: 380, loss: 0.017654744908213615\n",
            "step: 390, loss: 0.002834367100149393\n",
            "step: 400, loss: 0.061882201582193375\n",
            "step: 410, loss: 0.002868249546736479\n",
            "step: 420, loss: 0.011381510645151138\n",
            "step: 430, loss: 0.004690353758633137\n",
            "step: 440, loss: 0.05785266309976578\n",
            "step: 450, loss: 0.02484436146914959\n",
            "step: 460, loss: 0.043180085718631744\n",
            "step: 470, loss: 0.1760336309671402\n",
            "step: 480, loss: 0.003785078413784504\n",
            "step: 490, loss: 0.006956083234399557\n",
            "step: 500, loss: 0.0025965292006731033\n",
            "step: 510, loss: 0.000906239845789969\n",
            "step: 520, loss: 0.00246328953653574\n",
            "step: 530, loss: 0.006275826599448919\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9328426862925483, f1=0.9238625812441968, best_f1=0.9238625812441968\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003383631119504571\n",
            "step: 10, loss: 0.002862591529265046\n",
            "step: 20, loss: 0.07956745475530624\n",
            "step: 30, loss: 0.010443693958222866\n",
            "step: 40, loss: 0.0008485241560265422\n",
            "step: 50, loss: 0.02073807828128338\n",
            "step: 60, loss: 0.0012124389177188277\n",
            "step: 70, loss: 0.0032604001462459564\n",
            "step: 80, loss: 0.007614240515977144\n",
            "step: 90, loss: 0.0041191186755895615\n",
            "step: 100, loss: 0.0019345306791365147\n",
            "step: 110, loss: 0.0006252852035686374\n",
            "step: 120, loss: 0.0011670986423268914\n",
            "step: 130, loss: 0.0041665309108793736\n",
            "step: 140, loss: 0.000540204462595284\n",
            "step: 150, loss: 0.0008390029543079436\n",
            "step: 160, loss: 0.0028693799395114183\n",
            "step: 170, loss: 0.06681474298238754\n",
            "step: 180, loss: 0.0006178351468406618\n",
            "step: 190, loss: 0.005187974311411381\n",
            "step: 200, loss: 0.0025763693265616894\n",
            "step: 210, loss: 0.05857887491583824\n",
            "step: 220, loss: 0.0016154606128111482\n",
            "step: 230, loss: 0.20945870876312256\n",
            "step: 240, loss: 0.06972826272249222\n",
            "step: 250, loss: 0.0016729983035475016\n",
            "step: 260, loss: 0.0022899569012224674\n",
            "step: 270, loss: 0.024076661095023155\n",
            "step: 280, loss: 0.007038275711238384\n",
            "step: 290, loss: 0.0017550303600728512\n",
            "step: 300, loss: 8.636529673822224e-05\n",
            "step: 310, loss: 0.0007058375631459057\n",
            "step: 320, loss: 0.0009053483372554183\n",
            "step: 330, loss: 0.0012769645545631647\n",
            "step: 340, loss: 0.027121735736727715\n",
            "step: 350, loss: 0.00017139024566859007\n",
            "step: 360, loss: 0.21972550451755524\n",
            "step: 370, loss: 0.1857607066631317\n",
            "step: 380, loss: 0.0004243631847202778\n",
            "step: 390, loss: 0.011023646220564842\n",
            "step: 400, loss: 0.0008448585285805166\n",
            "step: 410, loss: 0.008730070665478706\n",
            "step: 420, loss: 0.0002970362256746739\n",
            "step: 430, loss: 0.0038857413455843925\n",
            "step: 440, loss: 0.024444419890642166\n",
            "step: 450, loss: 0.0020603027660399675\n",
            "step: 460, loss: 0.0026204739697277546\n",
            "step: 470, loss: 0.05342244729399681\n",
            "step: 480, loss: 0.08212205767631531\n",
            "step: 490, loss: 0.03532245755195618\n",
            "step: 500, loss: 0.014811329543590546\n",
            "step: 510, loss: 0.0039772531017661095\n",
            "step: 520, loss: 0.00041172452620230615\n",
            "step: 530, loss: 0.00134041637647897\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9360902255639098, f1=0.9219924812030075, best_f1=0.9219924812030075\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007550226873718202\n",
            "step: 10, loss: 0.07658907026052475\n",
            "step: 20, loss: 0.014071359299123287\n",
            "step: 30, loss: 0.06836313009262085\n",
            "step: 40, loss: 0.003712432226166129\n",
            "step: 50, loss: 0.0029671103693544865\n",
            "step: 60, loss: 0.0011604587780311704\n",
            "step: 70, loss: 0.0008313973667100072\n",
            "step: 80, loss: 0.008306872099637985\n",
            "step: 90, loss: 0.040910009294748306\n",
            "step: 100, loss: 0.0005221553146839142\n",
            "step: 110, loss: 0.02820379100739956\n",
            "step: 120, loss: 0.18094663321971893\n",
            "step: 130, loss: 0.0037822872400283813\n",
            "step: 140, loss: 0.014697112143039703\n",
            "step: 150, loss: 0.007604703307151794\n",
            "step: 160, loss: 0.004449903499335051\n",
            "step: 170, loss: 0.012736300937831402\n",
            "step: 180, loss: 0.006604826543480158\n",
            "step: 190, loss: 0.1470205932855606\n",
            "step: 200, loss: 0.008633404970169067\n",
            "step: 210, loss: 0.005755546968430281\n",
            "step: 220, loss: 0.0014522764831781387\n",
            "step: 230, loss: 0.0007187661831267178\n",
            "step: 240, loss: 0.009669559076428413\n",
            "step: 250, loss: 0.08055216073989868\n",
            "step: 260, loss: 0.005136456806212664\n",
            "step: 270, loss: 0.012073181569576263\n",
            "step: 280, loss: 0.0009799712570384145\n",
            "step: 290, loss: 0.0033185824286192656\n",
            "step: 300, loss: 0.00014212164387572557\n",
            "step: 310, loss: 0.06912513822317123\n",
            "step: 320, loss: 0.0008994760573841631\n",
            "step: 330, loss: 0.03186517953872681\n",
            "step: 340, loss: 0.07904764264822006\n",
            "step: 350, loss: 0.025856604799628258\n",
            "step: 360, loss: 0.00385713460855186\n",
            "step: 370, loss: 0.004922663327306509\n",
            "step: 380, loss: 0.00246004993095994\n",
            "step: 390, loss: 0.0003457654966041446\n",
            "step: 400, loss: 0.09330029785633087\n",
            "step: 410, loss: 0.0022417400032281876\n",
            "step: 420, loss: 0.0091751329600811\n",
            "step: 430, loss: 0.023916171863675117\n",
            "step: 440, loss: 6.61923477309756e-05\n",
            "step: 450, loss: 0.01091964915394783\n",
            "step: 460, loss: 0.0004689162306021899\n",
            "step: 470, loss: 0.000756095047108829\n",
            "step: 480, loss: 9.650007268646732e-05\n",
            "step: 490, loss: 0.002571765100583434\n",
            "step: 500, loss: 0.022696755826473236\n",
            "step: 510, loss: 0.015153964050114155\n",
            "step: 520, loss: 0.020009523257613182\n",
            "step: 530, loss: 0.006845493800938129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9398148148148149, f1=0.9300797747536368, best_f1=0.9300797747536368\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07712079584598541\n",
            "step: 10, loss: 0.0009342145640403032\n",
            "step: 20, loss: 0.0073060099966824055\n",
            "step: 30, loss: 0.002729887142777443\n",
            "step: 40, loss: 0.021951425820589066\n",
            "step: 50, loss: 0.0012916064588353038\n",
            "step: 60, loss: 0.003644883166998625\n",
            "step: 70, loss: 0.0014888579025864601\n",
            "step: 80, loss: 0.1374431848526001\n",
            "step: 90, loss: 0.008188922889530659\n",
            "step: 100, loss: 0.020244134590029716\n",
            "step: 110, loss: 0.019766151905059814\n",
            "step: 120, loss: 0.001437539467588067\n",
            "step: 130, loss: 0.00026116776280105114\n",
            "step: 140, loss: 0.0008014136692509055\n",
            "step: 150, loss: 0.001133420504629612\n",
            "step: 160, loss: 0.0002842065005097538\n",
            "step: 170, loss: 0.004542922601103783\n",
            "step: 180, loss: 0.002230622572824359\n",
            "step: 190, loss: 0.0012900776928290725\n",
            "step: 200, loss: 0.0012179811019450426\n",
            "step: 210, loss: 0.006347783841192722\n",
            "step: 220, loss: 0.0009653966990299523\n",
            "step: 230, loss: 0.0003598715993575752\n",
            "step: 240, loss: 0.007757126819342375\n",
            "step: 250, loss: 0.021244650706648827\n",
            "step: 260, loss: 0.01726342737674713\n",
            "step: 270, loss: 0.0012922721216455102\n",
            "step: 280, loss: 0.026976022869348526\n",
            "step: 290, loss: 0.003421657020226121\n",
            "step: 300, loss: 0.00027842819690704346\n",
            "step: 310, loss: 0.003756972262635827\n",
            "step: 320, loss: 0.002509981393814087\n",
            "step: 330, loss: 0.12650847434997559\n",
            "step: 340, loss: 0.004255066625773907\n",
            "step: 350, loss: 0.001101645058952272\n",
            "step: 360, loss: 0.0003175330930389464\n",
            "step: 370, loss: 0.0023567217867821455\n",
            "step: 380, loss: 0.0072877597995102406\n",
            "step: 390, loss: 0.000253694859566167\n",
            "step: 400, loss: 0.0035071424208581448\n",
            "step: 410, loss: 0.0008663029875606298\n",
            "step: 420, loss: 0.0022020465694367886\n",
            "step: 430, loss: 0.0020804598461836576\n",
            "step: 440, loss: 0.00033287302358075976\n",
            "step: 450, loss: 0.05120720714330673\n",
            "step: 460, loss: 0.0026103465352207422\n",
            "step: 470, loss: 0.022233974188566208\n",
            "step: 480, loss: 0.24847544729709625\n",
            "step: 490, loss: 0.0038210095372051\n",
            "step: 500, loss: 0.0024386760778725147\n",
            "step: 510, loss: 0.00018704916874412447\n",
            "step: 520, loss: 0.004939771723002195\n",
            "step: 530, loss: 0.028408266603946686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9429622815087396, f1=0.9322191272051997, best_f1=0.9322191272051997\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013630383182317019\n",
            "step: 10, loss: 0.0023037134669721127\n",
            "step: 20, loss: 0.00022074556909501553\n",
            "step: 30, loss: 0.00019101832003798336\n",
            "step: 40, loss: 0.0005326076061464846\n",
            "step: 50, loss: 0.0004674845840781927\n",
            "step: 60, loss: 0.0017545666778460145\n",
            "step: 70, loss: 0.0038672671653330326\n",
            "step: 80, loss: 0.004872037563472986\n",
            "step: 90, loss: 0.025887761265039444\n",
            "step: 100, loss: 0.0035187348257750273\n",
            "step: 110, loss: 0.0005869740853086114\n",
            "step: 120, loss: 0.0003758080711122602\n",
            "step: 130, loss: 0.0006755513604730368\n",
            "step: 140, loss: 0.0014130283379927278\n",
            "step: 150, loss: 0.0014217214193195105\n",
            "step: 160, loss: 0.000405714672524482\n",
            "step: 170, loss: 0.0002885662252083421\n",
            "step: 180, loss: 0.00011517976236063987\n",
            "step: 190, loss: 0.0009772235061973333\n",
            "step: 200, loss: 0.0018752175383269787\n",
            "step: 210, loss: 0.0018042929004877806\n",
            "step: 220, loss: 0.10132656246423721\n",
            "step: 230, loss: 0.0006025477196089923\n",
            "step: 240, loss: 0.01793835498392582\n",
            "step: 250, loss: 0.0014097130624577403\n",
            "step: 260, loss: 0.0028914534486830235\n",
            "step: 270, loss: 0.022234657779335976\n",
            "step: 280, loss: 0.0018388121388852596\n",
            "step: 290, loss: 0.005365130957216024\n",
            "step: 300, loss: 0.009822693653404713\n",
            "step: 310, loss: 0.003006704617291689\n",
            "step: 320, loss: 0.003028400707989931\n",
            "step: 330, loss: 3.473741526249796e-05\n",
            "step: 340, loss: 0.010272473096847534\n",
            "step: 350, loss: 0.0011238326551392674\n",
            "step: 360, loss: 0.0012848349288105965\n",
            "step: 370, loss: 0.001184965600259602\n",
            "step: 380, loss: 0.009496178478002548\n",
            "step: 390, loss: 0.0033636484295129776\n",
            "step: 400, loss: 0.00034707196755334735\n",
            "step: 410, loss: 0.0009055056143552065\n",
            "step: 420, loss: 0.03075364977121353\n",
            "step: 430, loss: 0.002573159523308277\n",
            "step: 440, loss: 0.0008040462271310389\n",
            "step: 450, loss: 0.0012767196167260408\n",
            "step: 460, loss: 0.006141608115285635\n",
            "step: 470, loss: 0.00048537817201577127\n",
            "step: 480, loss: 0.003099660621955991\n",
            "step: 490, loss: 0.0003140986373182386\n",
            "step: 500, loss: 0.02946791984140873\n",
            "step: 510, loss: 0.0019926836248487234\n",
            "step: 520, loss: 0.007211483549326658\n",
            "step: 530, loss: 0.011199313215911388\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9342789598108747, f1=0.9168241965973536, best_f1=0.9322191272051997\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00533313350751996\n",
            "step: 10, loss: 0.008364317938685417\n",
            "step: 20, loss: 0.016890423372387886\n",
            "step: 30, loss: 5.8961304603144526e-05\n",
            "step: 40, loss: 0.04072338342666626\n",
            "step: 50, loss: 0.001750967581756413\n",
            "step: 60, loss: 7.409586396533996e-05\n",
            "step: 70, loss: 0.0066320206969976425\n",
            "step: 80, loss: 0.00033461753628216684\n",
            "step: 90, loss: 0.0009770009201020002\n",
            "step: 100, loss: 0.0025217835791409016\n",
            "step: 110, loss: 0.017548084259033203\n",
            "step: 120, loss: 0.06966578215360641\n",
            "step: 130, loss: 0.0009508402436040342\n",
            "step: 140, loss: 0.00014307697711046785\n",
            "step: 150, loss: 0.0024089571088552475\n",
            "step: 160, loss: 0.0067294687032699585\n",
            "step: 170, loss: 0.0003442400775384158\n",
            "step: 180, loss: 0.0001254429662367329\n",
            "step: 190, loss: 0.0005174994003027678\n",
            "step: 200, loss: 0.0017778398469090462\n",
            "step: 210, loss: 0.00020452849275898188\n",
            "step: 220, loss: 0.0007964679971337318\n",
            "step: 230, loss: 0.0009020571014843881\n",
            "step: 240, loss: 0.0012885159812867641\n",
            "step: 250, loss: 4.492099105846137e-05\n",
            "step: 260, loss: 0.018030572682619095\n",
            "step: 270, loss: 0.001343769719824195\n",
            "step: 280, loss: 0.001493748277425766\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 290, loss: 0.002471392508596182\n",
            "step: 300, loss: 0.0006738342926837504\n",
            "step: 310, loss: 0.0011102674761787057\n",
            "step: 320, loss: 0.1373155564069748\n",
            "step: 330, loss: 0.0013428680831566453\n",
            "step: 340, loss: 0.010961201973259449\n",
            "step: 350, loss: 0.00018872454529628158\n",
            "step: 360, loss: 0.0004029158444609493\n",
            "step: 370, loss: 0.0002550210920162499\n",
            "step: 380, loss: 0.00012804627476725727\n",
            "step: 390, loss: 0.0009064827463589609\n",
            "step: 400, loss: 0.0005835273186676204\n",
            "step: 410, loss: 0.0006464730831794441\n",
            "step: 420, loss: 0.00153865956235677\n",
            "step: 430, loss: 0.088473379611969\n",
            "step: 440, loss: 0.039003003388643265\n",
            "step: 450, loss: 0.048637934029102325\n",
            "step: 460, loss: 0.0005073624779470265\n",
            "step: 470, loss: 0.0004874256846960634\n",
            "step: 480, loss: 0.05564360320568085\n",
            "step: 490, loss: 0.0021508694626390934\n",
            "step: 500, loss: 5.019831223762594e-05\n",
            "step: 510, loss: 0.15473227202892303\n",
            "step: 520, loss: 0.0029149448964744806\n",
            "step: 530, loss: 0.009454556740820408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.93790546802595, f1=0.9150812064965197, best_f1=0.9322191272051997\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002845074748620391\n",
            "step: 10, loss: 0.0021921403240412474\n",
            "step: 20, loss: 0.004446128848940134\n",
            "step: 30, loss: 0.00026173132937401533\n",
            "step: 40, loss: 0.056037940084934235\n",
            "step: 50, loss: 0.0012653105659410357\n",
            "step: 60, loss: 0.0023466411512345076\n",
            "step: 70, loss: 0.0007194370264187455\n",
            "step: 80, loss: 0.001290071988478303\n",
            "step: 90, loss: 0.00026398716727271676\n",
            "step: 100, loss: 0.002941135549917817\n",
            "step: 110, loss: 0.004785514902323484\n",
            "step: 120, loss: 0.0005029794410802424\n",
            "step: 130, loss: 0.006274931598454714\n",
            "step: 140, loss: 0.0038992278277873993\n",
            "step: 150, loss: 0.003682374954223633\n",
            "step: 160, loss: 0.0007076628389768302\n",
            "step: 170, loss: 0.00035214071976952255\n",
            "step: 180, loss: 0.0023112159688025713\n",
            "step: 190, loss: 0.0006747624720446765\n",
            "step: 200, loss: 0.0001726112823234871\n",
            "step: 210, loss: 0.0004285327740944922\n",
            "step: 220, loss: 0.02533683180809021\n",
            "step: 230, loss: 0.025034157559275627\n",
            "step: 240, loss: 0.005222815088927746\n",
            "step: 250, loss: 0.007708042860031128\n",
            "step: 260, loss: 0.05289643257856369\n",
            "step: 270, loss: 0.019407758489251137\n",
            "step: 280, loss: 0.01627064123749733\n",
            "step: 290, loss: 0.010652377270162106\n",
            "step: 300, loss: 0.0013239027466624975\n",
            "step: 310, loss: 0.02971455827355385\n",
            "step: 320, loss: 0.0009159466717392206\n",
            "step: 330, loss: 0.007060125470161438\n",
            "step: 340, loss: 0.0019808150827884674\n",
            "step: 350, loss: 0.00019739984418265522\n",
            "step: 360, loss: 0.13448548316955566\n",
            "step: 370, loss: 0.0037692987825721502\n",
            "step: 380, loss: 0.0003642031515482813\n",
            "step: 390, loss: 0.008544796146452427\n",
            "step: 400, loss: 6.65643165120855e-05\n",
            "step: 410, loss: 0.001591612584888935\n",
            "step: 420, loss: 0.00032516499049961567\n",
            "step: 430, loss: 0.0017133635701611638\n",
            "step: 440, loss: 0.00027398165548220277\n",
            "step: 450, loss: 0.0005686373915523291\n",
            "step: 460, loss: 0.015769343823194504\n",
            "step: 470, loss: 0.057448141276836395\n",
            "step: 480, loss: 7.660088158445433e-05\n",
            "step: 490, loss: 7.938459020806476e-05\n",
            "step: 500, loss: 0.0002716774761211127\n",
            "step: 510, loss: 0.00040006908238865435\n",
            "step: 520, loss: 0.0008442626567557454\n",
            "step: 530, loss: 0.00017119647236540914\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9381918819188192, f1=0.9238625812441968, best_f1=0.9322191272051997\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009879304561764002\n",
            "step: 10, loss: 0.0409211628139019\n",
            "step: 20, loss: 0.0003243899845983833\n",
            "step: 30, loss: 0.002953212708234787\n",
            "step: 40, loss: 0.0007990446756593883\n",
            "step: 50, loss: 0.0018270846921950579\n",
            "step: 60, loss: 0.0007527304696850479\n",
            "step: 70, loss: 0.001102553796954453\n",
            "step: 80, loss: 0.0008122518192976713\n",
            "step: 90, loss: 2.4228465917985886e-05\n",
            "step: 100, loss: 0.0006047065835446119\n",
            "step: 110, loss: 0.00119203538633883\n",
            "step: 120, loss: 8.659171726321802e-05\n",
            "step: 130, loss: 0.00048155777039937675\n",
            "step: 140, loss: 0.00647902162745595\n",
            "step: 150, loss: 0.08935816586017609\n",
            "step: 160, loss: 0.00043896635179407895\n",
            "step: 170, loss: 0.001586464000865817\n",
            "step: 180, loss: 0.0002822738897521049\n",
            "step: 190, loss: 0.0005628187791444361\n",
            "step: 200, loss: 0.00017005595145747066\n",
            "step: 210, loss: 0.00026457649073563516\n",
            "step: 220, loss: 0.006959948223084211\n",
            "step: 230, loss: 0.013441173359751701\n",
            "step: 240, loss: 0.0005915469373576343\n",
            "step: 250, loss: 0.05626928433775902\n",
            "step: 260, loss: 0.0006088150548748672\n",
            "step: 270, loss: 0.0011654106201604009\n",
            "step: 280, loss: 0.0030765000265091658\n",
            "step: 290, loss: 0.0001300408475799486\n",
            "step: 300, loss: 0.0005098533583804965\n",
            "step: 310, loss: 0.00045507538015954196\n",
            "step: 320, loss: 9.244819375453517e-05\n",
            "step: 330, loss: 0.0023148218169808388\n",
            "step: 340, loss: 0.0002589290670584887\n",
            "step: 350, loss: 0.0001190434632007964\n",
            "step: 360, loss: 0.000567737384699285\n",
            "step: 370, loss: 0.002546025672927499\n",
            "step: 380, loss: 0.05360597372055054\n",
            "step: 390, loss: 0.0024286473635584116\n",
            "step: 400, loss: 0.000889384129550308\n",
            "step: 410, loss: 5.4397394706029445e-05\n",
            "step: 420, loss: 0.0002550631470512599\n",
            "step: 430, loss: 0.0009760463144630194\n",
            "step: 440, loss: 0.0030378533992916346\n",
            "step: 450, loss: 0.00011174689279869199\n",
            "step: 460, loss: 0.0014172218507155776\n",
            "step: 470, loss: 2.7382640837458894e-05\n",
            "step: 480, loss: 0.0002950281777884811\n",
            "step: 490, loss: 0.00014982644643168896\n",
            "step: 500, loss: 0.0016838170122355223\n",
            "step: 510, loss: 0.0006114077405072749\n",
            "step: 520, loss: 0.00040078675374388695\n",
            "step: 530, loss: 0.0042498186230659485\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.936768149882904, f1=0.9220657276995305, best_f1=0.9322191272051997\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016632068436592817\n",
            "step: 10, loss: 0.004829268902540207\n",
            "step: 20, loss: 0.00208502565510571\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 30, loss: 0.0018520207377150655\n",
            "step: 40, loss: 9.403594594914466e-05\n",
            "step: 50, loss: 0.01594553142786026\n",
            "step: 60, loss: 0.0007238123216666281\n",
            "step: 70, loss: 0.00023893029720056802\n",
            "step: 80, loss: 0.00017591517826076597\n",
            "step: 90, loss: 0.004168366547673941\n",
            "step: 100, loss: 0.00035840016789734364\n",
            "step: 110, loss: 0.1560969054698944\n",
            "step: 120, loss: 0.00015018830890767276\n",
            "step: 130, loss: 0.0009123368072323501\n",
            "step: 140, loss: 0.00011208041541976854\n",
            "step: 150, loss: 0.0001473795564379543\n",
            "step: 160, loss: 0.0007326003978960216\n",
            "step: 170, loss: 0.00016919424524530768\n",
            "step: 180, loss: 0.001167413662187755\n",
            "step: 190, loss: 0.15447324514389038\n",
            "step: 200, loss: 0.004593167454004288\n",
            "step: 210, loss: 0.0004836739390157163\n",
            "step: 220, loss: 0.0007663048454560339\n",
            "step: 230, loss: 0.0006338538369163871\n",
            "step: 240, loss: 7.419844041578472e-05\n",
            "step: 250, loss: 0.00010085565008921549\n",
            "step: 260, loss: 0.0007516191690228879\n",
            "step: 270, loss: 0.0001318736613029614\n",
            "step: 280, loss: 0.00023405755928251892\n",
            "step: 290, loss: 0.0004260671266820282\n",
            "step: 300, loss: 0.00024878286058083177\n",
            "step: 310, loss: 0.001648034667596221\n",
            "step: 320, loss: 5.8909419749397784e-05\n",
            "step: 330, loss: 0.0001733538811095059\n",
            "step: 340, loss: 0.0001818334130803123\n",
            "step: 350, loss: 0.002426623133942485\n",
            "step: 360, loss: 0.002203878480941057\n",
            "step: 370, loss: 0.0016672926722094417\n",
            "step: 380, loss: 0.0004676340613514185\n",
            "step: 390, loss: 8.984347368823364e-05\n",
            "step: 400, loss: 0.0031588857527822256\n",
            "step: 410, loss: 0.00031392942764796317\n",
            "step: 420, loss: 0.0010376228019595146\n",
            "step: 430, loss: 0.0002815687912516296\n",
            "step: 440, loss: 0.0009443257004022598\n",
            "step: 450, loss: 0.0010048376861959696\n",
            "step: 460, loss: 0.0010841755429282784\n",
            "step: 470, loss: 0.0005747207906097174\n",
            "step: 480, loss: 0.002274223603308201\n",
            "step: 490, loss: 0.0005162042798474431\n",
            "step: 500, loss: 0.0008546155295334756\n",
            "step: 510, loss: 5.557648910325952e-05\n",
            "step: 520, loss: 3.2511856261407956e-05\n",
            "step: 530, loss: 0.008819792419672012\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9372967951695309, f1=0.922071861875875, best_f1=0.9322191272051997\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:35, 163.39it/s]\n",
            "load_f1 = 0.9417206290471785\n",
            "real_f1 = 0.9404706968158745\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 144.95it/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM",
        "jeDvm9a1dIlo",
        "NJ3ExOzkeDVk",
        "M1GZmC0LgNFJ",
        "ck7uL6uPgNFK",
        "tb_EWW7DgNFL",
        "NC7Q_ekTgNFN",
        "8yLxbfdggw7_",
        "TWZ1NvUvgw8A",
        "S4v1tmXbgw8B",
        "Zbv_H8sHgw8C",
        "SSCCmtSggw8E",
        "r23AxFPnhstr",
        "iCTWC7NUhstr",
        "zW6LV4zMhstv",
        "VngEb4vfhstw"
      ],
      "name": "ADirty_90_3_5_roberta.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}