{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GLow_30_1_2_roberta.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM",
        "pw03GW7dmkqy",
        "jeDvm9a1dIlo",
        "NJ3ExOzkeDVk",
        "M1GZmC0LgNFJ",
        "ck7uL6uPgNFK",
        "tb_EWW7DgNFL",
        "NC7Q_ekTgNFN",
        "djX3yHRNgNFP",
        "10svv34hgw7-",
        "pnXzXaaYhstq"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSgSb9vUtCyX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "193609af-846e-474d-b829-0f6e6d1f3177"
      },
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 17.00 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 30.4 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 67.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 53.7 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 5.30 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 27.6 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 76.6 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.9 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 64.1 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Collecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 77.2 MB/s \n",
            "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 11.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449922 sha256=f69a5bc1622d238090b750776764c4622a52ad7eca4f4c9382d487e7f0c310cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=fd11bf7885d773c462f23010b58770e0873f7445dc8db80dd6dfe151cc87d4e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_hfO6D_uLby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948e6118-5101-4755-b1d7-5488670730aa"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10117, done.\u001b[K\n",
            "remote: Counting objects: 100% (233/233), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 10117 (delta 119), reused 152 (delta 62), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10117/10117), 14.97 MiB | 7.24 MiB/s, done.\n",
            "Resolving deltas: 100% (6927/6927), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-0ey7bfqm\n",
            "Created temporary directory: /tmp/pip-req-tracker-nb32jcqg\n",
            "Initialized build tracking at /tmp/pip-req-tracker-nb32jcqg\n",
            "Created build tracker: /tmp/pip-req-tracker-nb32jcqg\n",
            "Entered build tracker: /tmp/pip-req-tracker-nb32jcqg\n",
            "Created temporary directory: /tmp/pip-install-l75nckfo\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-lkvgebgh\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-nb32jcqg'\n",
            "    Running setup.py (path:/tmp/pip-req-build-lkvgebgh/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-cltc08fp\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-cltc08fp/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-cltc08fp/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-cltc08fp/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-cltc08fp/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-cltc08fp/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-cltc08fp/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-lkvgebgh has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-nb32jcqg'\n",
            "Created temporary directory: /tmp/pip-unpack-2xf7tz91\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-8bkkz96g\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-8bkkz96g\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-lkvgebgh/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-lkvgebgh/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-8bkkz96g\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-8bkkz96g/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298244 sha256=cf9befd191a460c5176de19968487168d899c7524c66eb94c5457b718ab4d7dd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0ey7bfqm/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-nb32jcqg'\n",
            "/content/ditto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKSZwBG_uyzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273d970c-a4ab-47a0-a5f8-9e5e1e656ccc"
      },
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 32.1 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.48-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 39.8 MB/s \n",
            "\u001b[?25hCollecting botocore==1.27.48\n",
            "  Downloading botocore-1.27.48-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 59.4 MB/s \n",
            "\u001b[?25hCollecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.48->awscli) (2.8.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.48->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.48 botocore-1.27.48 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1"
      ],
      "metadata": {
        "id": "KR84V9pFRkw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea5e0b18-2871-465b-88e1-44867ee30bc4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 18 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importa config.json"
      ],
      "metadata": {
        "id": "uAyGpUo9ifJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xawOMn6icU7",
        "outputId": "8f98ad85-e093-491a-bde1-5897783f507d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 1014, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 1014 (delta 27), reused 7 (delta 0), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (1014/1014), 254.12 MiB | 15.59 MiB/s, done.\n",
            "Resolving deltas: 100% (611/611), done.\n",
            "Checking out files: 100% (1284/1284), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove e move"
      ],
      "metadata": {
        "id": "pw03GW7dmkqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "FQVym9vwmx-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2639ddc-01eb-43bd-fa59-648acdb40862"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/GLow_30_1_2/configs.json /content/ditto/"
      ],
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA"
      ],
      "metadata": {
        "id": "bm4nohJxf9bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA STRUCTURED"
      ],
      "metadata": {
        "id": "jeDvm9a1dIlo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxLFPNvcGgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9464b949-01b1-41d2-dd73-bdc47cf90fa3"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 461kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 810kB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 514kB/s]\n",
            "Downloading: 100% 501M/501M [00:10<00:00, 49.9MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4331464469432831\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.32000000000000006, f1=0.2553191489361702, best_f1=0.2553191489361702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5009341835975647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.2828282828282828, f1=0.2692307692307693, best_f1=0.2553191489361702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.44867485761642456\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.3010752688172043, f1=0.23913043478260868, best_f1=0.2553191489361702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31462976336479187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.288659793814433, f1=0.27184466019417475, best_f1=0.2553191489361702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3513762354850769\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.28571428571428575, f1=0.26, best_f1=0.2553191489361702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2661820948123932\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.2826086956521739, f1=0.2745098039215686, best_f1=0.2553191489361702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.663905680179596\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.27906976744186046, f1=0.26262626262626265, best_f1=0.2553191489361702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5434378981590271\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.288659793814433, f1=0.27184466019417475, best_f1=0.2553191489361702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.42372992634773254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.3783783783783784, f1=0.31578947368421056, best_f1=0.31578947368421056\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.44713807106018066\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.30303030303030304, f1=0.2692307692307692, best_f1=0.31578947368421056\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4340536594390869\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.38596491228070184, f1=0.3283582089552239, best_f1=0.3283582089552239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3671049475669861\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.40740740740740744, f1=0.3582089552238806, best_f1=0.3582089552238806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.35708874464035034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.38095238095238093, f1=0.37037037037037035, best_f1=0.3582089552238806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32293468713760376\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.38095238095238093, f1=0.37037037037037035, best_f1=0.3582089552238806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3732582628726959\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.38095238095238093, f1=0.37037037037037035, best_f1=0.3582089552238806\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 132990.13it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.3333333333333333\n",
            "real_f1 = 0.391304347826087\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 145.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "FjO-q4GLeCE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "WZ7mparQevgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec84c80-96f3-46c3-c9b8-7a47e17aed24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5636836290359497\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.43069854378700256\n",
            "step: 20, loss: 0.513922929763794\n",
            "step: 30, loss: 0.3080146908760071\n",
            "step: 40, loss: 0.3103317618370056\n",
            "step: 50, loss: 0.5607637763023376\n",
            "step: 60, loss: 0.49626070261001587\n",
            "step: 70, loss: 0.374178946018219\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.1944076418876648\n",
            "step: 90, loss: 0.14213982224464417\n",
            "step: 100, loss: 0.2146715223789215\n",
            "step: 110, loss: 0.023635894060134888\n",
            "step: 120, loss: 0.07705608755350113\n",
            "step: 130, loss: 0.1254740059375763\n",
            "step: 140, loss: 0.016441432759165764\n",
            "step: 150, loss: 0.17078185081481934\n",
            "step: 160, loss: 0.01830880157649517\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 170, loss: 0.023319581523537636\n",
            "step: 180, loss: 0.05590269714593887\n",
            "step: 190, loss: 0.021930508315563202\n",
            "step: 200, loss: 0.022740643471479416\n",
            "step: 210, loss: 0.1423983871936798\n",
            "step: 220, loss: 0.41746336221694946\n",
            "step: 230, loss: 0.00829571858048439\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9797297297297298, f1=0.9806598407281, best_f1=0.9806598407281\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0934344083070755\n",
            "step: 10, loss: 0.1149289458990097\n",
            "step: 20, loss: 0.015596337616443634\n",
            "step: 30, loss: 0.005982834845781326\n",
            "step: 40, loss: 0.06713251024484634\n",
            "step: 50, loss: 0.005653634667396545\n",
            "step: 60, loss: 0.006081579253077507\n",
            "step: 70, loss: 0.005692570935934782\n",
            "step: 80, loss: 0.01865246705710888\n",
            "step: 90, loss: 0.02881813794374466\n",
            "step: 100, loss: 0.003101848531514406\n",
            "step: 110, loss: 0.003982567694038153\n",
            "step: 120, loss: 0.009093660861253738\n",
            "step: 130, loss: 0.002152681117877364\n",
            "step: 140, loss: 0.002555637853220105\n",
            "step: 150, loss: 0.09665972739458084\n",
            "step: 160, loss: 0.0020001919474452734\n",
            "step: 170, loss: 0.006850256584584713\n",
            "step: 180, loss: 0.0040092081762850285\n",
            "step: 190, loss: 0.009131614118814468\n",
            "step: 200, loss: 0.01503078918904066\n",
            "step: 210, loss: 0.0017183322925120592\n",
            "step: 220, loss: 0.0014021436218172312\n",
            "step: 230, loss: 0.0011410336010158062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9921436588103255, f1=0.9853107344632768, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01952669583261013\n",
            "step: 10, loss: 0.0012949268566444516\n",
            "step: 20, loss: 0.011695606634020805\n",
            "step: 30, loss: 0.0003469569201115519\n",
            "step: 40, loss: 0.005650754552334547\n",
            "step: 50, loss: 0.025207892060279846\n",
            "step: 60, loss: 0.0008152239606715739\n",
            "step: 70, loss: 0.00048291831626556814\n",
            "step: 80, loss: 0.008541770279407501\n",
            "step: 90, loss: 0.0003689599398057908\n",
            "step: 100, loss: 0.004371258430182934\n",
            "step: 110, loss: 0.045390233397483826\n",
            "step: 120, loss: 0.00041488101123832166\n",
            "step: 130, loss: 0.008538979105651379\n",
            "step: 140, loss: 0.0003647358389571309\n",
            "step: 150, loss: 0.0007806390058249235\n",
            "step: 160, loss: 0.0019964936655014753\n",
            "step: 170, loss: 0.002467723796144128\n",
            "step: 180, loss: 0.014069276861846447\n",
            "step: 190, loss: 0.09527404606342316\n",
            "step: 200, loss: 0.011318385601043701\n",
            "step: 210, loss: 0.00171396613586694\n",
            "step: 220, loss: 0.025595778599381447\n",
            "step: 230, loss: 0.007389153819531202\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9865168539325843, f1=0.9819819819819819, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002440342213958502\n",
            "step: 10, loss: 0.008252641186118126\n",
            "step: 20, loss: 0.0018299923976883292\n",
            "step: 30, loss: 0.000778384564910084\n",
            "step: 40, loss: 0.020243573933839798\n",
            "step: 50, loss: 0.00799773819744587\n",
            "step: 60, loss: 0.0007297112024389207\n",
            "step: 70, loss: 0.00043182290391996503\n",
            "step: 80, loss: 0.00036159378942102194\n",
            "step: 90, loss: 0.0009288639412261546\n",
            "step: 100, loss: 0.057467710226774216\n",
            "step: 110, loss: 0.0004024609224870801\n",
            "step: 120, loss: 0.008389398455619812\n",
            "step: 130, loss: 0.0661599189043045\n",
            "step: 140, loss: 0.0012534355046227574\n",
            "step: 150, loss: 0.0011560515267774463\n",
            "step: 160, loss: 0.008173671551048756\n",
            "step: 170, loss: 0.0013882371131330729\n",
            "step: 180, loss: 0.04516893997788429\n",
            "step: 190, loss: 0.0020500347018241882\n",
            "step: 200, loss: 0.013923181220889091\n",
            "step: 210, loss: 0.0005518165417015553\n",
            "step: 220, loss: 0.000408862077165395\n",
            "step: 230, loss: 0.03710102289915085\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9876543209876544, f1=0.980963045912654, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009136219508945942\n",
            "step: 10, loss: 0.00120366585906595\n",
            "step: 20, loss: 0.000935705378651619\n",
            "step: 30, loss: 0.0006995750009082258\n",
            "step: 40, loss: 0.0019429567037150264\n",
            "step: 50, loss: 0.001483504893258214\n",
            "step: 60, loss: 0.01905268058180809\n",
            "step: 70, loss: 0.0014636935666203499\n",
            "step: 80, loss: 0.0997706800699234\n",
            "step: 90, loss: 0.22843872010707855\n",
            "step: 100, loss: 0.00037724681897088885\n",
            "step: 110, loss: 0.0018034311942756176\n",
            "step: 120, loss: 0.007834569551050663\n",
            "step: 130, loss: 0.0009417987894266844\n",
            "step: 140, loss: 0.00046107705566100776\n",
            "step: 150, loss: 0.007089603692293167\n",
            "step: 160, loss: 0.00026611192151904106\n",
            "step: 170, loss: 0.06134617701172829\n",
            "step: 180, loss: 0.0012986185029149055\n",
            "step: 190, loss: 0.008961399085819721\n",
            "step: 200, loss: 0.0024369312450289726\n",
            "step: 210, loss: 0.0019396889256313443\n",
            "step: 220, loss: 0.009050303138792515\n",
            "step: 230, loss: 0.002208676654845476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9876819708846584, f1=0.9820224719101124, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005513879004865885\n",
            "step: 10, loss: 0.0015689695719629526\n",
            "step: 20, loss: 0.0013925032690167427\n",
            "step: 30, loss: 0.0007621293771080673\n",
            "step: 40, loss: 0.00042577661224640906\n",
            "step: 50, loss: 0.002328814473003149\n",
            "step: 60, loss: 0.004564226139336824\n",
            "step: 70, loss: 0.00048433500342071056\n",
            "step: 80, loss: 0.002775057451799512\n",
            "step: 90, loss: 0.01907210610806942\n",
            "step: 100, loss: 0.0007869155379012227\n",
            "step: 110, loss: 0.0025647366419434547\n",
            "step: 120, loss: 0.00033019654802046716\n",
            "step: 130, loss: 0.00033897580578923225\n",
            "step: 140, loss: 0.0029003506060689688\n",
            "step: 150, loss: 0.000366113061318174\n",
            "step: 160, loss: 0.008599048480391502\n",
            "step: 170, loss: 0.0003805147425737232\n",
            "step: 180, loss: 0.00014863445539958775\n",
            "step: 190, loss: 0.00025960011407732964\n",
            "step: 200, loss: 0.0013656769879162312\n",
            "step: 210, loss: 0.000670926587190479\n",
            "step: 220, loss: 0.0038198246620595455\n",
            "step: 230, loss: 0.005951540544629097\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9910313901345291, f1=0.9778270509977827, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017134519293904305\n",
            "step: 10, loss: 0.0007860591867938638\n",
            "step: 20, loss: 0.0003669930447358638\n",
            "step: 30, loss: 0.0002579804859124124\n",
            "step: 40, loss: 0.0010132978204637766\n",
            "step: 50, loss: 0.0007936623296700418\n",
            "step: 60, loss: 0.0005967579199932516\n",
            "step: 70, loss: 0.0005251431139186025\n",
            "step: 80, loss: 0.0007408235687762499\n",
            "step: 90, loss: 0.0003714611229952425\n",
            "step: 100, loss: 0.00044758402509614825\n",
            "step: 110, loss: 0.00038148349267430604\n",
            "step: 120, loss: 0.0003266257990617305\n",
            "step: 130, loss: 0.001133798505179584\n",
            "step: 140, loss: 0.00036884009023196995\n",
            "step: 150, loss: 0.0065875668078660965\n",
            "step: 160, loss: 0.0001435075100744143\n",
            "step: 170, loss: 0.005015556700527668\n",
            "step: 180, loss: 0.0017232041573151946\n",
            "step: 190, loss: 0.0006413991213776171\n",
            "step: 200, loss: 0.0017957298550754786\n",
            "step: 210, loss: 0.004229755140841007\n",
            "step: 220, loss: 0.00045845392742194235\n",
            "step: 230, loss: 0.0006453878013417125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9910313901345291, f1=0.9832026875699889, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005729975528083742\n",
            "step: 10, loss: 0.0015380831900984049\n",
            "step: 20, loss: 0.0016404666239395738\n",
            "step: 30, loss: 0.0017168757040053606\n",
            "step: 40, loss: 0.0077666365541517735\n",
            "step: 50, loss: 0.01044660247862339\n",
            "step: 60, loss: 0.00083203858230263\n",
            "step: 70, loss: 0.0003288064617663622\n",
            "step: 80, loss: 0.005285758525133133\n",
            "step: 90, loss: 0.0008087293244898319\n",
            "step: 100, loss: 0.0007077318732626736\n",
            "step: 110, loss: 0.018262667581439018\n",
            "step: 120, loss: 0.00030439384863711894\n",
            "step: 130, loss: 0.0007672672509215772\n",
            "step: 140, loss: 0.0018436408136039972\n",
            "step: 150, loss: 0.049270521849393845\n",
            "step: 160, loss: 0.0004067840927746147\n",
            "step: 170, loss: 0.00695757195353508\n",
            "step: 180, loss: 0.0003648429410532117\n",
            "step: 190, loss: 0.0005547809996642172\n",
            "step: 200, loss: 0.06246250495314598\n",
            "step: 210, loss: 0.0019145311089232564\n",
            "step: 220, loss: 0.0010649396572262049\n",
            "step: 230, loss: 0.0006880909204483032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9899441340782122, f1=0.9822222222222222, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00042967277113348246\n",
            "step: 10, loss: 0.0006495500565506518\n",
            "step: 20, loss: 0.0007380139431916177\n",
            "step: 30, loss: 0.000704994541592896\n",
            "step: 40, loss: 0.0005485265282914042\n",
            "step: 50, loss: 0.0006836080574430525\n",
            "step: 60, loss: 0.0005126541364006698\n",
            "step: 70, loss: 0.11372467875480652\n",
            "step: 80, loss: 0.000297837977996096\n",
            "step: 90, loss: 0.06908443570137024\n",
            "step: 100, loss: 0.0005675890133716166\n",
            "step: 110, loss: 0.0004952551680617034\n",
            "step: 120, loss: 0.023844702169299126\n",
            "step: 130, loss: 0.0006804590811952949\n",
            "step: 140, loss: 0.0007022889330983162\n",
            "step: 150, loss: 0.0033040123526006937\n",
            "step: 160, loss: 0.0008476969669573009\n",
            "step: 170, loss: 0.00041072044405154884\n",
            "step: 180, loss: 0.0006977714947424829\n",
            "step: 190, loss: 0.0002605762565508485\n",
            "step: 200, loss: 0.00039015518268570304\n",
            "step: 210, loss: 0.0008465402061119676\n",
            "step: 220, loss: 0.0005048968596383929\n",
            "step: 230, loss: 0.00028692462365143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9921612541993281, f1=0.9821428571428571, best_f1=0.9821428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004699409764725715\n",
            "step: 10, loss: 0.0007939409697428346\n",
            "step: 20, loss: 0.0004937727935612202\n",
            "step: 30, loss: 0.00029421172803267837\n",
            "step: 40, loss: 0.001576487091369927\n",
            "step: 50, loss: 0.00034454435808584094\n",
            "step: 60, loss: 0.0013583017280325294\n",
            "step: 70, loss: 0.002316203899681568\n",
            "step: 80, loss: 0.0009394121589139104\n",
            "step: 90, loss: 0.0005180130829103291\n",
            "step: 100, loss: 0.00042636407306417823\n",
            "step: 110, loss: 0.0006295401835814118\n",
            "step: 120, loss: 0.00028609350556507707\n",
            "step: 130, loss: 0.0003824271261692047\n",
            "step: 140, loss: 0.00030697902548126876\n",
            "step: 150, loss: 0.00042745377868413925\n",
            "step: 160, loss: 0.00016843312187120318\n",
            "step: 170, loss: 0.0003289133310317993\n",
            "step: 180, loss: 0.0005185601767152548\n",
            "step: 190, loss: 0.00041302863974124193\n",
            "step: 200, loss: 0.0005825061234645545\n",
            "step: 210, loss: 0.0004820358590222895\n",
            "step: 220, loss: 0.00027193102869205177\n",
            "step: 230, loss: 0.00020519556710496545\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9910514541387023, f1=0.9843749999999999, best_f1=0.9821428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018985228962264955\n",
            "step: 10, loss: 0.000431023130659014\n",
            "step: 20, loss: 0.0003494042612146586\n",
            "step: 30, loss: 0.0002906131849158555\n",
            "step: 40, loss: 6.034219768480398e-05\n",
            "step: 50, loss: 0.0001344540505670011\n",
            "step: 60, loss: 0.011202649213373661\n",
            "step: 70, loss: 0.0019135293550789356\n",
            "step: 80, loss: 0.0007382422918453813\n",
            "step: 90, loss: 0.20896124839782715\n",
            "step: 100, loss: 0.0005113906809128821\n",
            "step: 110, loss: 0.00036392174661159515\n",
            "step: 120, loss: 0.0006448577041737735\n",
            "step: 130, loss: 0.0005625479971058667\n",
            "step: 140, loss: 0.012525994330644608\n",
            "step: 150, loss: 0.0007678517140448093\n",
            "step: 160, loss: 0.0010177791118621826\n",
            "step: 170, loss: 0.0013455325970426202\n",
            "step: 180, loss: 0.0011019816156476736\n",
            "step: 190, loss: 0.0008643295732326806\n",
            "step: 200, loss: 0.0010010561672970653\n",
            "step: 210, loss: 0.0008754449663683772\n",
            "step: 220, loss: 0.0009559168829582632\n",
            "step: 230, loss: 0.000494087697006762\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9910313901345291, f1=0.9854423292273236, best_f1=0.9821428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009315382922068238\n",
            "step: 10, loss: 0.0005642649484798312\n",
            "step: 20, loss: 0.04532548412680626\n",
            "step: 30, loss: 0.045050498098134995\n",
            "step: 40, loss: 0.0009916916023939848\n",
            "step: 50, loss: 0.0007910010754130781\n",
            "step: 60, loss: 0.0008568699704483151\n",
            "step: 70, loss: 0.0006183389341458678\n",
            "step: 80, loss: 0.00028265287983231246\n",
            "step: 90, loss: 0.0026842150837183\n",
            "step: 100, loss: 0.00041957030771300197\n",
            "step: 110, loss: 0.00026856025215238333\n",
            "step: 120, loss: 0.00045762176159769297\n",
            "step: 130, loss: 0.00048285420052707195\n",
            "step: 140, loss: 0.0005821561790071428\n",
            "step: 150, loss: 0.0005865016719326377\n",
            "step: 160, loss: 0.00048681884072721004\n",
            "step: 170, loss: 0.000499196641612798\n",
            "step: 180, loss: 0.00036791712045669556\n",
            "step: 190, loss: 0.0016836603172123432\n",
            "step: 200, loss: 0.0003412653459236026\n",
            "step: 210, loss: 0.000504685565829277\n",
            "step: 220, loss: 0.01279835682362318\n",
            "step: 230, loss: 0.0023310952819883823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9910112359550561, f1=0.9843749999999999, best_f1=0.9821428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003975949657615274\n",
            "step: 10, loss: 0.0006047877832315862\n",
            "step: 20, loss: 0.0009626976097933948\n",
            "step: 30, loss: 0.00046217910130508244\n",
            "step: 40, loss: 0.000977497547864914\n",
            "step: 50, loss: 0.0011463086120784283\n",
            "step: 60, loss: 0.0006374206277541816\n",
            "step: 70, loss: 0.0005027705919928849\n",
            "step: 80, loss: 0.0005315483431331813\n",
            "step: 90, loss: 0.0006715963827446103\n",
            "step: 100, loss: 0.001206598011776805\n",
            "step: 110, loss: 0.0012335567735135555\n",
            "step: 120, loss: 0.0009680954972282052\n",
            "step: 130, loss: 0.0010188821470364928\n",
            "step: 140, loss: 0.0005813906318508089\n",
            "step: 150, loss: 0.00017532712081447244\n",
            "step: 160, loss: 0.0025833502877503633\n",
            "step: 170, loss: 0.0009308304288424551\n",
            "step: 180, loss: 0.04538977891206741\n",
            "step: 190, loss: 0.0007764690672047436\n",
            "step: 200, loss: 0.00018426212773192674\n",
            "step: 210, loss: 0.0006769606843590736\n",
            "step: 220, loss: 0.0006782260606996715\n",
            "step: 230, loss: 0.00046652305172756314\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9921259842519685, f1=0.9854423292273236, best_f1=0.9821428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00040059664752334356\n",
            "step: 10, loss: 0.0003390883794054389\n",
            "step: 20, loss: 0.00040797083056531847\n",
            "step: 30, loss: 0.0007457262836396694\n",
            "step: 40, loss: 0.0004989856388419867\n",
            "step: 50, loss: 0.00041134635102935135\n",
            "step: 60, loss: 0.0005707741365768015\n",
            "step: 70, loss: 0.0003867428167723119\n",
            "step: 80, loss: 0.0004729698412120342\n",
            "step: 90, loss: 0.0009163948125205934\n",
            "step: 100, loss: 0.0007133355247788131\n",
            "step: 110, loss: 0.0010916151804849505\n",
            "step: 120, loss: 0.00022067571990191936\n",
            "step: 130, loss: 0.001007341081276536\n",
            "step: 140, loss: 0.0004914972232654691\n",
            "step: 150, loss: 0.0002613381075207144\n",
            "step: 160, loss: 0.0005681032780557871\n",
            "step: 170, loss: 0.0006125162472017109\n",
            "step: 180, loss: 0.0007285886676982045\n",
            "step: 190, loss: 0.0004828832461498678\n",
            "step: 200, loss: 0.000733414082787931\n",
            "step: 210, loss: 0.000456993468105793\n",
            "step: 220, loss: 0.0005984374438412488\n",
            "step: 230, loss: 0.0002203651238232851\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9921259842519685, f1=0.9876543209876544, best_f1=0.9821428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025633830577135086\n",
            "step: 10, loss: 0.0005683837225660682\n",
            "step: 20, loss: 0.0005546712200157344\n",
            "step: 30, loss: 0.0006478952127508819\n",
            "step: 40, loss: 0.00042307787225581706\n",
            "step: 50, loss: 0.0005240490427240729\n",
            "step: 60, loss: 0.02721014991402626\n",
            "step: 70, loss: 0.000460839772131294\n",
            "step: 80, loss: 0.00044102611718699336\n",
            "step: 90, loss: 0.0004376518481876701\n",
            "step: 100, loss: 0.000312309741275385\n",
            "step: 110, loss: 0.0005916512454859912\n",
            "step: 120, loss: 0.02744237519800663\n",
            "step: 130, loss: 0.0003454524266999215\n",
            "step: 140, loss: 0.0008016053470782936\n",
            "step: 150, loss: 0.0004958981298841536\n",
            "step: 160, loss: 0.017098715528845787\n",
            "step: 170, loss: 0.00022963975789025426\n",
            "step: 180, loss: 0.0004943585372529924\n",
            "step: 190, loss: 0.000992717337794602\n",
            "step: 200, loss: 0.0004975451738573611\n",
            "step: 210, loss: 0.029176514595746994\n",
            "step: 220, loss: 0.0005503084976226091\n",
            "step: 230, loss: 0.000587887829169631\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9921259842519685, f1=0.9843400447427293, best_f1=0.9821428571428571\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:16, 148.59it/s]\n",
            "load_f1 = 0.9887640449438202\n",
            "real_f1 = 0.9898989898989898\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 134.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "r_G0OicNeCnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "EkIRgx40ezP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa1f3b5e-bee3-4b26-88b1-72adb069e291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.669550359249115\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4195062816143036\n",
            "step: 20, loss: 0.2939992845058441\n",
            "step: 30, loss: 0.3399136960506439\n",
            "step: 40, loss: 0.4081266522407532\n",
            "step: 50, loss: 0.5715963840484619\n",
            "step: 60, loss: 0.34054669737815857\n",
            "step: 70, loss: 0.40610283613204956\n",
            "step: 80, loss: 0.20530211925506592\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 90, loss: 0.20161287486553192\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 100, loss: 0.3761182427406311\n",
            "step: 110, loss: 0.12816035747528076\n",
            "step: 120, loss: 0.10695955902338028\n",
            "step: 130, loss: 0.11616037040948868\n",
            "step: 140, loss: 0.20506338775157928\n",
            "step: 150, loss: 0.11797469854354858\n",
            "step: 160, loss: 0.18301571905612946\n",
            "step: 170, loss: 0.060318805277347565\n",
            "step: 180, loss: 0.039870038628578186\n",
            "step: 190, loss: 0.05985328555107117\n",
            "step: 200, loss: 0.017971906810998917\n",
            "step: 210, loss: 0.10648854821920395\n",
            "step: 220, loss: 0.20889177918434143\n",
            "step: 230, loss: 0.15768173336982727\n",
            "step: 240, loss: 0.01876257173717022\n",
            "step: 250, loss: 0.08237230777740479\n",
            "step: 260, loss: 0.16984528303146362\n",
            "step: 270, loss: 0.19175264239311218\n",
            "step: 280, loss: 0.06598357856273651\n",
            "step: 290, loss: 0.09944842755794525\n",
            "step: 300, loss: 0.017650321125984192\n",
            "step: 310, loss: 0.1269105076789856\n",
            "step: 320, loss: 0.05512382462620735\n",
            "step: 330, loss: 0.08852215111255646\n",
            "step: 340, loss: 0.20543991029262543\n",
            "step: 350, loss: 0.1516328603029251\n",
            "step: 360, loss: 0.05688728764653206\n",
            "step: 370, loss: 0.1314474493265152\n",
            "step: 380, loss: 0.14652742445468903\n",
            "step: 390, loss: 0.056952305138111115\n",
            "step: 400, loss: 0.043547432869672775\n",
            "step: 410, loss: 0.30195939540863037\n",
            "step: 420, loss: 0.06389030814170837\n",
            "step: 430, loss: 0.01623852550983429\n",
            "step: 440, loss: 0.04714839905500412\n",
            "step: 450, loss: 0.068548783659935\n",
            "step: 460, loss: 0.008600003086030483\n",
            "step: 470, loss: 0.027861641719937325\n",
            "step: 480, loss: 0.19250690937042236\n",
            "step: 490, loss: 0.18659934401512146\n",
            "step: 500, loss: 0.020418405532836914\n",
            "step: 510, loss: 0.020564578473567963\n",
            "step: 520, loss: 0.050684235990047455\n",
            "step: 530, loss: 0.012641050852835178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9332096474953617, f1=0.9355586462679647, best_f1=0.9355586462679647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0687556192278862\n",
            "step: 10, loss: 0.12754717469215393\n",
            "step: 20, loss: 0.05493040010333061\n",
            "step: 30, loss: 0.09992367029190063\n",
            "step: 40, loss: 0.06392578035593033\n",
            "step: 50, loss: 0.044668033719062805\n",
            "step: 60, loss: 0.017962567508220673\n",
            "step: 70, loss: 0.01628785952925682\n",
            "step: 80, loss: 0.037765856832265854\n",
            "step: 90, loss: 0.008441504091024399\n",
            "step: 100, loss: 0.10467593371868134\n",
            "step: 110, loss: 0.02469269186258316\n",
            "step: 120, loss: 0.2304845005273819\n",
            "step: 130, loss: 0.012612725608050823\n",
            "step: 140, loss: 0.0401589460670948\n",
            "step: 150, loss: 0.010674362070858479\n",
            "step: 160, loss: 0.026987671852111816\n",
            "step: 170, loss: 0.04285478591918945\n",
            "step: 180, loss: 0.012584289535880089\n",
            "step: 190, loss: 0.02114001102745533\n",
            "step: 200, loss: 0.08666857331991196\n",
            "step: 210, loss: 0.03909960016608238\n",
            "step: 220, loss: 0.0018173570279031992\n",
            "step: 230, loss: 0.024095207452774048\n",
            "step: 240, loss: 0.014724023640155792\n",
            "step: 250, loss: 0.019231101498007774\n",
            "step: 260, loss: 0.008825830183923244\n",
            "step: 270, loss: 0.040366917848587036\n",
            "step: 280, loss: 0.06139587610960007\n",
            "step: 290, loss: 0.010247462429106236\n",
            "step: 300, loss: 0.022158939391374588\n",
            "step: 310, loss: 0.12712661921977997\n",
            "step: 320, loss: 0.07554920017719269\n",
            "step: 330, loss: 0.07542437314987183\n",
            "step: 340, loss: 0.01337869931012392\n",
            "step: 350, loss: 0.003734284546226263\n",
            "step: 360, loss: 0.08901064097881317\n",
            "step: 370, loss: 0.008839966729283333\n",
            "step: 380, loss: 0.19304247200489044\n",
            "step: 390, loss: 0.008623073808848858\n",
            "step: 400, loss: 0.02693350799381733\n",
            "step: 410, loss: 0.01432715356349945\n",
            "step: 420, loss: 0.1936599612236023\n",
            "step: 430, loss: 0.23046515882015228\n",
            "step: 440, loss: 0.039347678422927856\n",
            "step: 450, loss: 0.03466096520423889\n",
            "step: 460, loss: 0.05884310603141785\n",
            "step: 470, loss: 0.08827514946460724\n",
            "step: 480, loss: 0.01726638339459896\n",
            "step: 490, loss: 0.055991046130657196\n",
            "step: 500, loss: 0.01454133354127407\n",
            "step: 510, loss: 0.017566172406077385\n",
            "step: 520, loss: 0.4646325707435608\n",
            "step: 530, loss: 0.035747211426496506\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9511970534069981, f1=0.9475620975160993, best_f1=0.9475620975160993\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16356390714645386\n",
            "step: 10, loss: 0.042384881526231766\n",
            "step: 20, loss: 0.0052140457555651665\n",
            "step: 30, loss: 0.05095289647579193\n",
            "step: 40, loss: 0.06453626602888107\n",
            "step: 50, loss: 0.0034347069449722767\n",
            "step: 60, loss: 0.022769033908843994\n",
            "step: 70, loss: 0.018438735976815224\n",
            "step: 80, loss: 0.002785841468721628\n",
            "step: 90, loss: 0.00194695929531008\n",
            "step: 100, loss: 0.04196985438466072\n",
            "step: 110, loss: 0.040773507207632065\n",
            "step: 120, loss: 0.1142403781414032\n",
            "step: 130, loss: 0.08453156054019928\n",
            "step: 140, loss: 0.011224950663745403\n",
            "step: 150, loss: 0.012097076512873173\n",
            "step: 160, loss: 0.04621583968400955\n",
            "step: 170, loss: 0.0011180833680555224\n",
            "step: 180, loss: 0.032590966671705246\n",
            "step: 190, loss: 0.006960522383451462\n",
            "step: 200, loss: 0.06591156870126724\n",
            "step: 210, loss: 0.0651470273733139\n",
            "step: 220, loss: 0.14765852689743042\n",
            "step: 230, loss: 0.021389957517385483\n",
            "step: 240, loss: 0.03055400215089321\n",
            "step: 250, loss: 0.01744457706809044\n",
            "step: 260, loss: 0.05670371651649475\n",
            "step: 270, loss: 0.001551260007545352\n",
            "step: 280, loss: 0.006185026839375496\n",
            "step: 290, loss: 0.003927480895072222\n",
            "step: 300, loss: 0.08949033915996552\n",
            "step: 310, loss: 0.026963869109749794\n",
            "step: 320, loss: 0.11065874993801117\n",
            "step: 330, loss: 0.014538195915520191\n",
            "step: 340, loss: 0.009643030352890491\n",
            "step: 350, loss: 0.17270280420780182\n",
            "step: 360, loss: 0.026336712762713432\n",
            "step: 370, loss: 0.018356705084443092\n",
            "step: 380, loss: 0.05364491418004036\n",
            "step: 390, loss: 0.007601856254041195\n",
            "step: 400, loss: 0.07735573500394821\n",
            "step: 410, loss: 0.08098220080137253\n",
            "step: 420, loss: 0.00943084992468357\n",
            "step: 430, loss: 0.009996398352086544\n",
            "step: 440, loss: 0.08394671976566315\n",
            "step: 450, loss: 0.09143244475126266\n",
            "step: 460, loss: 0.14800699055194855\n",
            "step: 470, loss: 0.04608476907014847\n",
            "step: 480, loss: 0.02943829819560051\n",
            "step: 490, loss: 0.04709787294268608\n",
            "step: 500, loss: 0.007004184648394585\n",
            "step: 510, loss: 0.043796613812446594\n",
            "step: 520, loss: 0.0013310223584994674\n",
            "step: 530, loss: 0.014480902813374996\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9480642115203022, f1=0.9459715639810427, best_f1=0.9475620975160993\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02340666577219963\n",
            "step: 10, loss: 0.0019690911285579205\n",
            "step: 20, loss: 0.010420096106827259\n",
            "step: 30, loss: 0.13131825625896454\n",
            "step: 40, loss: 0.03305281326174736\n",
            "step: 50, loss: 0.04708680510520935\n",
            "step: 60, loss: 0.016615867614746094\n",
            "step: 70, loss: 0.025254931300878525\n",
            "step: 80, loss: 0.004709798842668533\n",
            "step: 90, loss: 0.15441565215587616\n",
            "step: 100, loss: 0.004396635107696056\n",
            "step: 110, loss: 0.018638715147972107\n",
            "step: 120, loss: 0.0005229129455983639\n",
            "step: 130, loss: 0.11067253351211548\n",
            "step: 140, loss: 0.01591203361749649\n",
            "step: 150, loss: 0.015329516492784023\n",
            "step: 160, loss: 0.006059572566300631\n",
            "step: 170, loss: 0.0648660808801651\n",
            "step: 180, loss: 0.10279133915901184\n",
            "step: 190, loss: 0.053017303347587585\n",
            "step: 200, loss: 0.020510682836174965\n",
            "step: 210, loss: 0.004000226967036724\n",
            "step: 220, loss: 0.001284438301809132\n",
            "step: 230, loss: 0.009835384786128998\n",
            "step: 240, loss: 0.03910485655069351\n",
            "step: 250, loss: 0.034824103116989136\n",
            "step: 260, loss: 0.017385758459568024\n",
            "step: 270, loss: 0.01623055338859558\n",
            "step: 280, loss: 0.0030514320824295282\n",
            "step: 290, loss: 0.1303287148475647\n",
            "step: 300, loss: 0.007417095825076103\n",
            "step: 310, loss: 0.011213990859687328\n",
            "step: 320, loss: 0.07033386081457138\n",
            "step: 330, loss: 0.035998694598674774\n",
            "step: 340, loss: 0.006538609508424997\n",
            "step: 350, loss: 0.05772673338651657\n",
            "step: 360, loss: 0.012533855624496937\n",
            "step: 370, loss: 0.004122977145016193\n",
            "step: 380, loss: 0.017292698845267296\n",
            "step: 390, loss: 0.00024471842334605753\n",
            "step: 400, loss: 0.003950178157538176\n",
            "step: 410, loss: 0.0024919966235756874\n",
            "step: 420, loss: 0.0034746762830764055\n",
            "step: 430, loss: 0.004015001934021711\n",
            "step: 440, loss: 0.006662419997155666\n",
            "step: 450, loss: 0.026603126898407936\n",
            "step: 460, loss: 0.01705736294388771\n",
            "step: 470, loss: 0.001014632172882557\n",
            "step: 480, loss: 0.01783330738544464\n",
            "step: 490, loss: 0.0024068085476756096\n",
            "step: 500, loss: 0.024160325527191162\n",
            "step: 510, loss: 0.04135288670659065\n",
            "step: 520, loss: 0.006965420208871365\n",
            "step: 530, loss: 0.08556749671697617\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9533487297921477, f1=0.9486940298507462, best_f1=0.9486940298507462\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04467804357409477\n",
            "step: 10, loss: 0.057563770562410355\n",
            "step: 20, loss: 0.04172641783952713\n",
            "step: 30, loss: 0.033350422978401184\n",
            "step: 40, loss: 0.0003324617864564061\n",
            "step: 50, loss: 0.17728659510612488\n",
            "step: 60, loss: 0.12306834012269974\n",
            "step: 70, loss: 0.0005513170617632568\n",
            "step: 80, loss: 0.022589154541492462\n",
            "step: 90, loss: 0.06275233626365662\n",
            "step: 100, loss: 0.058683570474386215\n",
            "step: 110, loss: 0.0027004978619515896\n",
            "step: 120, loss: 0.1933160275220871\n",
            "step: 130, loss: 0.011958044022321701\n",
            "step: 140, loss: 0.00862779002636671\n",
            "step: 150, loss: 0.020381532609462738\n",
            "step: 160, loss: 0.0009793051285669208\n",
            "step: 170, loss: 0.12498854100704193\n",
            "step: 180, loss: 0.04931693896651268\n",
            "step: 190, loss: 0.003401403082534671\n",
            "step: 200, loss: 0.004467909224331379\n",
            "step: 210, loss: 0.0014240809250622988\n",
            "step: 220, loss: 0.00148982263635844\n",
            "step: 230, loss: 0.000863684166688472\n",
            "step: 240, loss: 0.0019918703474104404\n",
            "step: 250, loss: 0.10464134812355042\n",
            "step: 260, loss: 0.0005124580347910523\n",
            "step: 270, loss: 0.20381274819374084\n",
            "step: 280, loss: 0.013151859864592552\n",
            "step: 290, loss: 0.025526059791445732\n",
            "step: 300, loss: 0.030765188857913017\n",
            "step: 310, loss: 0.023857057094573975\n",
            "step: 320, loss: 0.053111184388399124\n",
            "step: 330, loss: 0.0006541286129504442\n",
            "step: 340, loss: 0.035510797053575516\n",
            "step: 350, loss: 0.0008110931958071887\n",
            "step: 360, loss: 0.0002520614070817828\n",
            "step: 370, loss: 0.0011655777925625443\n",
            "step: 380, loss: 0.0008979398990049958\n",
            "step: 390, loss: 0.001857293420471251\n",
            "step: 400, loss: 0.002274904865771532\n",
            "step: 410, loss: 0.09348899126052856\n",
            "step: 420, loss: 0.14731566607952118\n",
            "step: 430, loss: 0.04085769131779671\n",
            "step: 440, loss: 0.0008031772449612617\n",
            "step: 450, loss: 0.007901903241872787\n",
            "step: 460, loss: 0.01398627832531929\n",
            "step: 470, loss: 0.04543383792042732\n",
            "step: 480, loss: 0.015551600605249405\n",
            "step: 490, loss: 0.020160026848316193\n",
            "step: 500, loss: 0.053594935685396194\n",
            "step: 510, loss: 0.0025136431213468313\n",
            "step: 520, loss: 0.09693863987922668\n",
            "step: 530, loss: 0.021171126514673233\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9524680073126143, f1=0.9537757437070938, best_f1=0.9486940298507462\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015559767372906208\n",
            "step: 10, loss: 0.0001460238272557035\n",
            "step: 20, loss: 0.00015620168414898217\n",
            "step: 30, loss: 0.0002827758726198226\n",
            "step: 40, loss: 0.011226539500057697\n",
            "step: 50, loss: 0.039318423718214035\n",
            "step: 60, loss: 0.0009237237391062081\n",
            "step: 70, loss: 0.016153786331415176\n",
            "step: 80, loss: 0.0016893476713448763\n",
            "step: 90, loss: 0.018057027831673622\n",
            "step: 100, loss: 0.0025924036744982004\n",
            "step: 110, loss: 0.006013024598360062\n",
            "step: 120, loss: 0.028325367718935013\n",
            "step: 130, loss: 0.0008111554197967052\n",
            "step: 140, loss: 0.0070214299485087395\n",
            "step: 150, loss: 0.0022917368914932013\n",
            "step: 160, loss: 0.029494550079107285\n",
            "step: 170, loss: 0.0034888293594121933\n",
            "step: 180, loss: 0.0027600079774856567\n",
            "step: 190, loss: 0.1808980256319046\n",
            "step: 200, loss: 0.008971295319497585\n",
            "step: 210, loss: 0.0070111677050590515\n",
            "step: 220, loss: 0.009839109145104885\n",
            "step: 230, loss: 0.01714484766125679\n",
            "step: 240, loss: 0.13400374352931976\n",
            "step: 250, loss: 0.020083114504814148\n",
            "step: 260, loss: 0.0033937660045921803\n",
            "step: 270, loss: 0.0037321054842323065\n",
            "step: 280, loss: 0.004512095358222723\n",
            "step: 290, loss: 0.0822344645857811\n",
            "step: 300, loss: 0.008148560300469398\n",
            "step: 310, loss: 0.013688342645764351\n",
            "step: 320, loss: 0.01998247392475605\n",
            "step: 330, loss: 0.006311905104666948\n",
            "step: 340, loss: 0.0013903798535466194\n",
            "step: 350, loss: 0.023430004715919495\n",
            "step: 360, loss: 0.03808128461241722\n",
            "step: 370, loss: 0.006418080069124699\n",
            "step: 380, loss: 0.00016715956735424697\n",
            "step: 390, loss: 0.008601007983088493\n",
            "step: 400, loss: 0.05850493162870407\n",
            "step: 410, loss: 0.00023764913203194737\n",
            "step: 420, loss: 0.009324634447693825\n",
            "step: 430, loss: 0.008406850509345531\n",
            "step: 440, loss: 0.0008680336177349091\n",
            "step: 450, loss: 0.23796777427196503\n",
            "step: 460, loss: 0.0034716606605798006\n",
            "step: 470, loss: 0.02230020798742771\n",
            "step: 480, loss: 0.026219090446829796\n",
            "step: 490, loss: 0.011444872245192528\n",
            "step: 500, loss: 0.002870525000616908\n",
            "step: 510, loss: 0.19361354410648346\n",
            "step: 520, loss: 0.0011149101192131639\n",
            "step: 530, loss: 0.0049735126085579395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9513822688274547, f1=0.942077549066539, best_f1=0.9486940298507462\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024002513382583857\n",
            "step: 10, loss: 0.001150991884060204\n",
            "step: 20, loss: 0.008263758383691311\n",
            "step: 30, loss: 0.0053455280140042305\n",
            "step: 40, loss: 0.01960626430809498\n",
            "step: 50, loss: 0.001224066480062902\n",
            "step: 60, loss: 0.006094735581427813\n",
            "step: 70, loss: 0.00031340765417553484\n",
            "step: 80, loss: 0.0018411902710795403\n",
            "step: 90, loss: 0.00014481630933005363\n",
            "step: 100, loss: 0.006927599664777517\n",
            "step: 110, loss: 0.0002047439629677683\n",
            "step: 120, loss: 0.0017184971366077662\n",
            "step: 130, loss: 0.0008099208935163915\n",
            "step: 140, loss: 0.0007172978948801756\n",
            "step: 150, loss: 0.001175579964183271\n",
            "step: 160, loss: 0.0011291262926533818\n",
            "step: 170, loss: 0.014311378821730614\n",
            "step: 180, loss: 0.059003379195928574\n",
            "step: 190, loss: 0.05411148816347122\n",
            "step: 200, loss: 0.0001410875702276826\n",
            "step: 210, loss: 0.0022910567931830883\n",
            "step: 220, loss: 0.00036353952600620687\n",
            "step: 230, loss: 0.0007825841894373298\n",
            "step: 240, loss: 0.002132267225533724\n",
            "step: 250, loss: 0.1011691614985466\n",
            "step: 260, loss: 0.003306439844891429\n",
            "step: 270, loss: 9.496649727225304e-05\n",
            "step: 280, loss: 0.00041765530477277935\n",
            "step: 290, loss: 0.0005924816941842437\n",
            "step: 300, loss: 0.00010839279275387526\n",
            "step: 310, loss: 0.009972186759114265\n",
            "step: 320, loss: 0.03820731118321419\n",
            "step: 330, loss: 0.0034091423731297255\n",
            "step: 340, loss: 0.029787657782435417\n",
            "step: 350, loss: 0.004273723345249891\n",
            "step: 360, loss: 0.003521230071783066\n",
            "step: 370, loss: 0.04983583092689514\n",
            "step: 380, loss: 0.029675347730517387\n",
            "step: 390, loss: 0.009629608131945133\n",
            "step: 400, loss: 0.015803249552845955\n",
            "step: 410, loss: 0.0008576668333262205\n",
            "step: 420, loss: 0.028660036623477936\n",
            "step: 430, loss: 0.005074556451290846\n",
            "step: 440, loss: 0.00038388444227166474\n",
            "step: 450, loss: 0.012397664599120617\n",
            "step: 460, loss: 0.0028307163156569004\n",
            "step: 470, loss: 0.12755407392978668\n",
            "step: 480, loss: 0.005657503381371498\n",
            "step: 490, loss: 0.0005980920977890491\n",
            "step: 500, loss: 0.0015261759981513023\n",
            "step: 510, loss: 0.00021696821204386652\n",
            "step: 520, loss: 0.00014839658979326487\n",
            "step: 530, loss: 0.0010932020377367735\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9522484932777004, f1=0.952513966480447, best_f1=0.9486940298507462\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025370262563228607\n",
            "step: 10, loss: 0.010684196837246418\n",
            "step: 20, loss: 0.0012657255865633488\n",
            "step: 30, loss: 0.0004954517353326082\n",
            "step: 40, loss: 0.00011114068911410868\n",
            "step: 50, loss: 0.00036434835055842996\n",
            "step: 60, loss: 0.0002223569026682526\n",
            "step: 70, loss: 0.0003195166646037251\n",
            "step: 80, loss: 0.09389106184244156\n",
            "step: 90, loss: 0.00020051162573508918\n",
            "step: 100, loss: 0.0012830322375521064\n",
            "step: 110, loss: 0.0015458350535482168\n",
            "step: 120, loss: 0.00020761150517500937\n",
            "step: 130, loss: 0.020185561850667\n",
            "step: 140, loss: 0.00014154569362290204\n",
            "step: 150, loss: 0.0029046316631138325\n",
            "step: 160, loss: 0.00034075399162247777\n",
            "step: 170, loss: 0.0010963325621560216\n",
            "step: 180, loss: 0.00598517619073391\n",
            "step: 190, loss: 0.0010206819279119372\n",
            "step: 200, loss: 0.0033908269833773375\n",
            "step: 210, loss: 0.045698996633291245\n",
            "step: 220, loss: 0.0002827791904564947\n",
            "step: 230, loss: 0.03219112008810043\n",
            "step: 240, loss: 0.0008574354578740895\n",
            "step: 250, loss: 0.0015437123365700245\n",
            "step: 260, loss: 0.004817355889827013\n",
            "step: 270, loss: 0.015982620418071747\n",
            "step: 280, loss: 0.005454178433865309\n",
            "step: 290, loss: 0.004276665393263102\n",
            "step: 300, loss: 6.406284956028685e-05\n",
            "step: 310, loss: 0.0025019622407853603\n",
            "step: 320, loss: 0.00027582378243096173\n",
            "step: 330, loss: 0.0006805849261581898\n",
            "step: 340, loss: 0.02013384737074375\n",
            "step: 350, loss: 0.002773247193545103\n",
            "step: 360, loss: 0.007388461846858263\n",
            "step: 370, loss: 0.017260082066059113\n",
            "step: 380, loss: 0.02734171412885189\n",
            "step: 390, loss: 0.0005308273830451071\n",
            "step: 400, loss: 0.0027274922467768192\n",
            "step: 410, loss: 0.0006296646315604448\n",
            "step: 420, loss: 0.00019465519289951771\n",
            "step: 430, loss: 0.0014742766506969929\n",
            "step: 440, loss: 0.00037659064400941133\n",
            "step: 450, loss: 0.0001264343736693263\n",
            "step: 460, loss: 0.0032185502350330353\n",
            "step: 470, loss: 0.03804170712828636\n",
            "step: 480, loss: 0.0008532723877578974\n",
            "step: 490, loss: 0.004311608150601387\n",
            "step: 500, loss: 0.00011860524682560936\n",
            "step: 510, loss: 0.002474299632012844\n",
            "step: 520, loss: 0.0009233971359208226\n",
            "step: 530, loss: 0.0006304102716967463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9543761638733707, f1=0.9535315985130112, best_f1=0.9535315985130112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001147114162449725\n",
            "step: 10, loss: 0.010518846102058887\n",
            "step: 20, loss: 0.0006519252783618867\n",
            "step: 30, loss: 0.07154390215873718\n",
            "step: 40, loss: 0.0002380638470640406\n",
            "step: 50, loss: 0.00014507446030620486\n",
            "step: 60, loss: 0.0007904127705842257\n",
            "step: 70, loss: 0.027700791135430336\n",
            "step: 80, loss: 0.013150237500667572\n",
            "step: 90, loss: 0.04548831656575203\n",
            "step: 100, loss: 7.69938196754083e-05\n",
            "step: 110, loss: 0.0032402167562395334\n",
            "step: 120, loss: 0.0005177208804525435\n",
            "step: 130, loss: 0.0006445215549319983\n",
            "step: 140, loss: 4.9645561375655234e-05\n",
            "step: 150, loss: 0.001089353347197175\n",
            "step: 160, loss: 0.0005838056677021086\n",
            "step: 170, loss: 0.030332330614328384\n",
            "step: 180, loss: 0.007405344396829605\n",
            "step: 190, loss: 9.408647019881755e-05\n",
            "step: 200, loss: 0.0011059982934966683\n",
            "step: 210, loss: 0.0018570753745734692\n",
            "step: 220, loss: 0.0008791141444817185\n",
            "step: 230, loss: 0.0007788304355926812\n",
            "step: 240, loss: 0.0002816363994497806\n",
            "step: 250, loss: 0.0004770267696585506\n",
            "step: 260, loss: 0.004749559331685305\n",
            "step: 270, loss: 0.004084477201104164\n",
            "step: 280, loss: 0.01785879209637642\n",
            "step: 290, loss: 0.00026055911439470947\n",
            "step: 300, loss: 0.0008275185828097165\n",
            "step: 310, loss: 0.004619607236236334\n",
            "step: 320, loss: 0.0006284329574555159\n",
            "step: 330, loss: 0.000575465674046427\n",
            "step: 340, loss: 0.00480481144040823\n",
            "step: 350, loss: 0.00986684113740921\n",
            "step: 360, loss: 0.0008401017403230071\n",
            "step: 370, loss: 0.0004876490856986493\n",
            "step: 380, loss: 0.0005797410267405212\n",
            "step: 390, loss: 0.00023557418899144977\n",
            "step: 400, loss: 0.0105748837813735\n",
            "step: 410, loss: 0.0001111429010052234\n",
            "step: 420, loss: 0.0005027704173699021\n",
            "step: 430, loss: 0.002125832485035062\n",
            "step: 440, loss: 8.203453035093844e-05\n",
            "step: 450, loss: 0.05492168292403221\n",
            "step: 460, loss: 4.733592504635453e-05\n",
            "step: 470, loss: 0.0002987525367643684\n",
            "step: 480, loss: 3.214979369658977e-05\n",
            "step: 490, loss: 0.0008258220041170716\n",
            "step: 500, loss: 0.030203012749552727\n",
            "step: 510, loss: 0.00040444309706799686\n",
            "step: 520, loss: 0.004958549980074167\n",
            "step: 530, loss: 0.01737804152071476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9553201289728237, f1=0.9532967032967032, best_f1=0.9532967032967032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0030774062033742666\n",
            "step: 10, loss: 0.0011226017959415913\n",
            "step: 20, loss: 0.00017838369240052998\n",
            "step: 30, loss: 0.001146501861512661\n",
            "step: 40, loss: 0.00022626345162279904\n",
            "step: 50, loss: 0.0004896338796243072\n",
            "step: 60, loss: 0.004625467583537102\n",
            "step: 70, loss: 0.010827296413481236\n",
            "step: 80, loss: 0.0010023119393736124\n",
            "step: 90, loss: 6.088630107115023e-05\n",
            "step: 100, loss: 0.00022892907145433128\n",
            "step: 110, loss: 0.004570616874843836\n",
            "step: 120, loss: 7.25491699995473e-05\n",
            "step: 130, loss: 0.00038083543768152595\n",
            "step: 140, loss: 0.012219498865306377\n",
            "step: 150, loss: 0.0003467446949798614\n",
            "step: 160, loss: 0.033574748784303665\n",
            "step: 170, loss: 0.0017785122618079185\n",
            "step: 180, loss: 0.0025667904410511255\n",
            "step: 190, loss: 5.691164915333502e-05\n",
            "step: 200, loss: 0.00820330623537302\n",
            "step: 210, loss: 0.020557070150971413\n",
            "step: 220, loss: 0.0006471980013884604\n",
            "step: 230, loss: 5.743445944972336e-05\n",
            "step: 240, loss: 0.000541752262506634\n",
            "step: 250, loss: 0.0004794571432285011\n",
            "step: 260, loss: 0.013725070282816887\n",
            "step: 270, loss: 7.977891073096544e-05\n",
            "step: 280, loss: 0.026254473254084587\n",
            "step: 290, loss: 0.0013546159025281668\n",
            "step: 300, loss: 0.00011964637815253809\n",
            "step: 310, loss: 0.05861320346593857\n",
            "step: 320, loss: 0.0009133177809417248\n",
            "step: 330, loss: 0.0016140304505825043\n",
            "step: 340, loss: 0.009046472609043121\n",
            "step: 350, loss: 0.0017401595832780004\n",
            "step: 360, loss: 0.00017484014097135514\n",
            "step: 370, loss: 0.0015358439413830638\n",
            "step: 380, loss: 0.0026648547500371933\n",
            "step: 390, loss: 0.0003574453585315496\n",
            "step: 400, loss: 0.0013897984754294157\n",
            "step: 410, loss: 0.013157380744814873\n",
            "step: 420, loss: 0.00023040738597046584\n",
            "step: 430, loss: 0.0002508683246560395\n",
            "step: 440, loss: 0.00022029691899660975\n",
            "step: 450, loss: 0.001618409063667059\n",
            "step: 460, loss: 0.0002027719747275114\n",
            "step: 470, loss: 0.0009160461486317217\n",
            "step: 480, loss: 0.0001019308838294819\n",
            "step: 490, loss: 0.00017919536912813783\n",
            "step: 500, loss: 0.004765987861901522\n",
            "step: 510, loss: 2.7318714273860678e-05\n",
            "step: 520, loss: 0.001030607963912189\n",
            "step: 530, loss: 0.00022451419499702752\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9529953917050692, f1=0.9547553093259464, best_f1=0.9532967032967032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.0068493288126774e-05\n",
            "step: 10, loss: 8.573394734412432e-05\n",
            "step: 20, loss: 6.82159443385899e-05\n",
            "step: 30, loss: 0.005307900253683329\n",
            "step: 40, loss: 0.00011862079554703087\n",
            "step: 50, loss: 0.000157856396981515\n",
            "step: 60, loss: 4.112156602786854e-05\n",
            "step: 70, loss: 0.0002652684925124049\n",
            "step: 80, loss: 0.002005157293751836\n",
            "step: 90, loss: 0.024153703823685646\n",
            "step: 100, loss: 0.00022978360357228667\n",
            "step: 110, loss: 0.0001138238440034911\n",
            "step: 120, loss: 0.00018001228454522789\n",
            "step: 130, loss: 4.8212528781732544e-05\n",
            "step: 140, loss: 3.568213287508115e-05\n",
            "step: 150, loss: 0.00010589459270704538\n",
            "step: 160, loss: 0.00011701000039465725\n",
            "step: 170, loss: 0.0002624545886646956\n",
            "step: 180, loss: 4.047978654853068e-05\n",
            "step: 190, loss: 0.0011332659050822258\n",
            "step: 200, loss: 3.244736581109464e-05\n",
            "step: 210, loss: 0.0009040810400620103\n",
            "step: 220, loss: 0.010305906645953655\n",
            "step: 230, loss: 2.9181112040532753e-05\n",
            "step: 240, loss: 0.004599784966558218\n",
            "step: 250, loss: 0.00013472145656123757\n",
            "step: 260, loss: 0.05461588874459267\n",
            "step: 270, loss: 0.0006704546976834536\n",
            "step: 280, loss: 0.0001890143903438002\n",
            "step: 290, loss: 0.0001860127376858145\n",
            "step: 300, loss: 0.0005093090003356338\n",
            "step: 310, loss: 0.0004896847531199455\n",
            "step: 320, loss: 0.0018580637406557798\n",
            "step: 330, loss: 6.567913806065917e-05\n",
            "step: 340, loss: 0.0012825302546843886\n",
            "step: 350, loss: 8.334620360983536e-05\n",
            "step: 360, loss: 0.00029396140598692\n",
            "step: 370, loss: 0.0005948621546849608\n",
            "step: 380, loss: 0.0023287904914468527\n",
            "step: 390, loss: 0.001567250699736178\n",
            "step: 400, loss: 0.000812023296020925\n",
            "step: 410, loss: 0.0013288523769006133\n",
            "step: 420, loss: 0.0019957772456109524\n",
            "step: 430, loss: 0.002586517482995987\n",
            "step: 440, loss: 4.44760407845024e-05\n",
            "step: 450, loss: 0.0011384085519239306\n",
            "step: 460, loss: 0.0014822555240243673\n",
            "step: 470, loss: 0.0007831510156393051\n",
            "step: 480, loss: 0.006048052571713924\n",
            "step: 490, loss: 5.888538726139814e-05\n",
            "step: 500, loss: 0.0006201117648743093\n",
            "step: 510, loss: 0.001193137839436531\n",
            "step: 520, loss: 0.00018472172087058425\n",
            "step: 530, loss: 0.002783657982945442\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.951545916012921, f1=0.9530386740331491, best_f1=0.9532967032967032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0034214735496789217\n",
            "step: 10, loss: 0.00024206048692576587\n",
            "step: 20, loss: 0.0001125814305851236\n",
            "step: 30, loss: 6.980955367907882e-05\n",
            "step: 40, loss: 0.004146753344684839\n",
            "step: 50, loss: 0.0017087050946429372\n",
            "step: 60, loss: 8.978504047263414e-05\n",
            "step: 70, loss: 0.0002404950646450743\n",
            "step: 80, loss: 0.023452162742614746\n",
            "step: 90, loss: 0.005194354336708784\n",
            "step: 100, loss: 0.0024356108624488115\n",
            "step: 110, loss: 9.011432848637924e-05\n",
            "step: 120, loss: 7.24150741007179e-05\n",
            "step: 130, loss: 3.986513547715731e-05\n",
            "step: 140, loss: 7.616660150233656e-05\n",
            "step: 150, loss: 5.274203067529015e-05\n",
            "step: 160, loss: 0.0016608929727226496\n",
            "step: 170, loss: 2.7640629923553206e-05\n",
            "step: 180, loss: 0.0003858281997963786\n",
            "step: 190, loss: 9.743691043695435e-05\n",
            "step: 200, loss: 0.0011392752639949322\n",
            "step: 210, loss: 3.8077527278801426e-05\n",
            "step: 220, loss: 3.7193618481978774e-05\n",
            "step: 230, loss: 2.3605440219398588e-05\n",
            "step: 240, loss: 0.001345172873698175\n",
            "step: 250, loss: 9.303934348281473e-05\n",
            "step: 260, loss: 4.036082100355998e-05\n",
            "step: 270, loss: 0.00755577115342021\n",
            "step: 280, loss: 0.03426589444279671\n",
            "step: 290, loss: 0.0019289086339995265\n",
            "step: 300, loss: 0.00039198281592689455\n",
            "step: 310, loss: 0.07946666330099106\n",
            "step: 320, loss: 0.0001462311192881316\n",
            "step: 330, loss: 0.0021764696575701237\n",
            "step: 340, loss: 0.0002394901675870642\n",
            "step: 350, loss: 1.7187823686981574e-05\n",
            "step: 360, loss: 0.006327333860099316\n",
            "step: 370, loss: 0.0004212836211081594\n",
            "step: 380, loss: 4.9786147428676486e-05\n",
            "step: 390, loss: 0.001842128112912178\n",
            "step: 400, loss: 3.3152580726891756e-05\n",
            "step: 410, loss: 0.00012376676022540778\n",
            "step: 420, loss: 0.001259199227206409\n",
            "step: 430, loss: 0.00047586983419023454\n",
            "step: 440, loss: 0.00038978474913164973\n",
            "step: 450, loss: 0.0001254160306416452\n",
            "step: 460, loss: 6.752913759555668e-05\n",
            "step: 470, loss: 0.003630306338891387\n",
            "step: 480, loss: 0.0002710028493311256\n",
            "step: 490, loss: 3.569387990864925e-05\n",
            "step: 500, loss: 2.7941026928601786e-05\n",
            "step: 510, loss: 0.019946400076150894\n",
            "step: 520, loss: 0.005088197998702526\n",
            "step: 530, loss: 4.232683568261564e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9495718363463369, f1=0.9444444444444445, best_f1=0.9532967032967032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.031567030120641e-05\n",
            "step: 10, loss: 0.22551682591438293\n",
            "step: 20, loss: 7.795335841365159e-05\n",
            "step: 30, loss: 2.0510933609330095e-05\n",
            "step: 40, loss: 0.0011510227341204882\n",
            "step: 50, loss: 0.002948432695120573\n",
            "step: 60, loss: 4.58327449450735e-05\n",
            "step: 70, loss: 6.227928679436445e-05\n",
            "step: 80, loss: 0.000766459503211081\n",
            "step: 90, loss: 2.8310994821367785e-05\n",
            "step: 100, loss: 0.0002443989797029644\n",
            "step: 110, loss: 0.0002382005623076111\n",
            "step: 120, loss: 3.969246245105751e-05\n",
            "step: 130, loss: 2.095727904816158e-05\n",
            "step: 140, loss: 5.280121331452392e-05\n",
            "step: 150, loss: 0.0014718429883942008\n",
            "step: 160, loss: 0.00048411579336971045\n",
            "step: 170, loss: 0.0014755136799067259\n",
            "step: 180, loss: 0.0008580776047892869\n",
            "step: 190, loss: 0.00025951830320991576\n",
            "step: 200, loss: 9.080535528482869e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 210, loss: 2.1613108401652426e-05\n",
            "step: 220, loss: 4.2084153392352164e-05\n",
            "step: 230, loss: 0.0037942638155072927\n",
            "step: 240, loss: 0.0024073394015431404\n",
            "step: 250, loss: 1.6815645722090267e-05\n",
            "step: 260, loss: 3.806740278378129e-05\n",
            "step: 270, loss: 0.0008578365086577833\n",
            "step: 280, loss: 2.8458371161832474e-05\n",
            "step: 290, loss: 1.2390036317810882e-05\n",
            "step: 300, loss: 1.4010625818627886e-05\n",
            "step: 310, loss: 8.322581561515108e-05\n",
            "step: 320, loss: 2.6928033548756503e-05\n",
            "step: 330, loss: 9.909489017445594e-05\n",
            "step: 340, loss: 0.00041548401350155473\n",
            "step: 350, loss: 1.6982601664494723e-05\n",
            "step: 360, loss: 0.10201015323400497\n",
            "step: 370, loss: 0.0005516550736501813\n",
            "step: 380, loss: 3.078629742958583e-05\n",
            "step: 390, loss: 0.00010096117330249399\n",
            "step: 400, loss: 0.00011941533739445731\n",
            "step: 410, loss: 3.072699109907262e-05\n",
            "step: 420, loss: 0.00028506311355158687\n",
            "step: 430, loss: 5.2805524319410324e-05\n",
            "step: 440, loss: 9.90174066828331e-06\n",
            "step: 450, loss: 0.001211012015119195\n",
            "step: 460, loss: 0.0007372578256763518\n",
            "step: 470, loss: 0.035070594400167465\n",
            "step: 480, loss: 2.603082248242572e-05\n",
            "step: 490, loss: 2.8949853003723547e-05\n",
            "step: 500, loss: 0.0003605267556849867\n",
            "step: 510, loss: 0.0002580881700851023\n",
            "step: 520, loss: 3.253934846725315e-05\n",
            "step: 530, loss: 2.6513698685448617e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9488151658767773, f1=0.9495238095238094, best_f1=0.9532967032967032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030081547796726227\n",
            "step: 10, loss: 0.0002087561006192118\n",
            "step: 20, loss: 1.7943859347724356e-05\n",
            "step: 30, loss: 0.00037762182182632387\n",
            "step: 40, loss: 6.881842273287475e-05\n",
            "step: 50, loss: 0.0003217355697415769\n",
            "step: 60, loss: 4.53319626103621e-05\n",
            "step: 70, loss: 4.241341957822442e-05\n",
            "step: 80, loss: 0.004285247065126896\n",
            "step: 90, loss: 1.639056245039683e-05\n",
            "step: 100, loss: 3.087978620897047e-05\n",
            "step: 110, loss: 0.00016806699568405747\n",
            "step: 120, loss: 1.190208422485739e-05\n",
            "step: 130, loss: 1.1522166460053995e-05\n",
            "step: 140, loss: 9.096758731175214e-05\n",
            "step: 150, loss: 0.00010847693192772567\n",
            "step: 160, loss: 1.655847518122755e-05\n",
            "step: 170, loss: 9.174855222227052e-05\n",
            "step: 180, loss: 0.00011422267562011257\n",
            "step: 190, loss: 4.176608854322694e-05\n",
            "step: 200, loss: 1.2516797141870484e-05\n",
            "step: 210, loss: 0.00015277510101441294\n",
            "step: 220, loss: 3.248569555580616e-05\n",
            "step: 230, loss: 0.00016213537310250103\n",
            "step: 240, loss: 0.002038288163021207\n",
            "step: 250, loss: 0.0017977439565584064\n",
            "step: 260, loss: 0.00014324979565571994\n",
            "step: 270, loss: 0.004896858241409063\n",
            "step: 280, loss: 0.00021242332877591252\n",
            "step: 290, loss: 1.7094838767661713e-05\n",
            "step: 300, loss: 2.6898240321315825e-05\n",
            "step: 310, loss: 2.4677619876456447e-05\n",
            "step: 320, loss: 2.6369181796326302e-05\n",
            "step: 330, loss: 2.8047286832588725e-05\n",
            "step: 340, loss: 1.2658360901696142e-05\n",
            "step: 350, loss: 1.0751048648671713e-05\n",
            "step: 360, loss: 4.122857717447914e-05\n",
            "step: 370, loss: 0.0009233345044776797\n",
            "step: 380, loss: 0.00017497649241704494\n",
            "step: 390, loss: 0.00050796550931409\n",
            "step: 400, loss: 0.0007565468549728394\n",
            "step: 410, loss: 1.1786441973526962e-05\n",
            "step: 420, loss: 1.2643407899304293e-05\n",
            "step: 430, loss: 6.573417340405285e-05\n",
            "step: 440, loss: 0.0006159330951049924\n",
            "step: 450, loss: 1.5280891602742486e-05\n",
            "step: 460, loss: 0.03658638894557953\n",
            "step: 470, loss: 9.365307960251812e-06\n",
            "step: 480, loss: 0.00021682700025849044\n",
            "step: 490, loss: 3.394378290977329e-05\n",
            "step: 500, loss: 0.00039023967110551894\n",
            "step: 510, loss: 0.025186873972415924\n",
            "step: 520, loss: 0.0002040304971160367\n",
            "step: 530, loss: 0.0033941329456865788\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9538891476478809, f1=0.9541454377026402, best_f1=0.9532967032967032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.1410405932110734e-05\n",
            "step: 10, loss: 0.0005627868813462555\n",
            "step: 20, loss: 0.001149081508629024\n",
            "step: 30, loss: 0.003144456073641777\n",
            "step: 40, loss: 1.192823128803866e-05\n",
            "step: 50, loss: 0.0011380136711522937\n",
            "step: 60, loss: 0.0005239741294644773\n",
            "step: 70, loss: 1.31945680550416e-05\n",
            "step: 80, loss: 1.121668810810661e-05\n",
            "step: 90, loss: 1.795484422473237e-05\n",
            "step: 100, loss: 6.638425020355498e-06\n",
            "step: 110, loss: 0.00012850186612922698\n",
            "step: 120, loss: 1.166742731584236e-05\n",
            "step: 130, loss: 1.1272554729657713e-05\n",
            "step: 140, loss: 1.3503850823326502e-05\n",
            "step: 150, loss: 1.801798316591885e-05\n",
            "step: 160, loss: 0.001582309021614492\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 1.0091659532918129e-05\n",
            "step: 180, loss: 0.03253290802240372\n",
            "step: 190, loss: 0.0031544289086014032\n",
            "step: 200, loss: 0.0015173106221482158\n",
            "step: 210, loss: 2.2942100258660503e-05\n",
            "step: 220, loss: 0.01677348092198372\n",
            "step: 230, loss: 0.0005071079358458519\n",
            "step: 240, loss: 1.4111077689449303e-05\n",
            "step: 250, loss: 1.5045608961372636e-05\n",
            "step: 260, loss: 7.87148474046262e-06\n",
            "step: 270, loss: 1.4990157978900243e-05\n",
            "step: 280, loss: 7.1674071477900725e-06\n",
            "step: 290, loss: 0.00017797671898733824\n",
            "step: 300, loss: 1.0955931429634802e-05\n",
            "step: 310, loss: 0.05381861329078674\n",
            "step: 320, loss: 1.0896364074142184e-05\n",
            "step: 330, loss: 1.260996941709891e-05\n",
            "step: 340, loss: 2.0614987079170533e-05\n",
            "step: 350, loss: 1.2054786566295661e-05\n",
            "step: 360, loss: 1.1346984138072003e-05\n",
            "step: 370, loss: 0.00046716819633729756\n",
            "step: 380, loss: 1.1767958312702831e-05\n",
            "step: 390, loss: 1.6871334082679823e-05\n",
            "step: 400, loss: 0.0007126607233658433\n",
            "step: 410, loss: 4.795116183231585e-05\n",
            "step: 420, loss: 1.0907482646871358e-05\n",
            "step: 430, loss: 7.010920398897724e-06\n",
            "step: 440, loss: 4.223778523737565e-05\n",
            "step: 450, loss: 0.004248516634106636\n",
            "step: 460, loss: 6.481935997726396e-05\n",
            "step: 470, loss: 9.6148714874289e-06\n",
            "step: 480, loss: 0.0022115036845207214\n",
            "step: 490, loss: 6.048839713912457e-05\n",
            "step: 500, loss: 0.0005536886164918542\n",
            "step: 510, loss: 1.319462444371311e-05\n",
            "step: 520, loss: 1.2583835086843465e-05\n",
            "step: 530, loss: 6.5862805058714e-06\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9531322505800465, f1=0.9542302357836338, best_f1=0.9532967032967032\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:35, 163.87it/s]\n",
            "load_f1 = 0.9552376557452701\n",
            "real_f1 = 0.9533487297921477\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 137.72it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "OdkSbOTQeC3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "x4XWqpo1e0O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9d606ea-a323-4ee0-c422-16df7ca3c5f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5082610845565796\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.41014501452445984\n",
            "step: 20, loss: 0.41079485416412354\n",
            "step: 30, loss: 0.34032168984413147\n",
            "step: 40, loss: 0.3088364601135254\n",
            "step: 50, loss: 0.46108531951904297\n",
            "step: 60, loss: 0.4913254678249359\n",
            "step: 70, loss: 0.30852770805358887\n",
            "step: 80, loss: 0.37282803654670715\n",
            "step: 90, loss: 0.29835572838783264\n",
            "step: 100, loss: 0.2358703762292862\n",
            "step: 110, loss: 0.2614438533782959\n",
            "step: 120, loss: 0.37002983689308167\n",
            "step: 130, loss: 0.242886483669281\n",
            "step: 140, loss: 0.469072550535202\n",
            "step: 150, loss: 0.39382943511009216\n",
            "step: 160, loss: 0.556541919708252\n",
            "step: 170, loss: 0.2594667375087738\n",
            "step: 180, loss: 0.36675500869750977\n",
            "step: 190, loss: 0.6922257542610168\n",
            "step: 200, loss: 0.3952532708644867\n",
            "step: 210, loss: 0.5115228891372681\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3766460716724396\n",
            "step: 10, loss: 0.2036057412624359\n",
            "step: 20, loss: 0.4983813762664795\n",
            "step: 30, loss: 0.4959862232208252\n",
            "step: 40, loss: 0.47283560037612915\n",
            "step: 50, loss: 0.2298993468284607\n",
            "step: 60, loss: 0.307115375995636\n",
            "step: 70, loss: 0.47116324305534363\n",
            "step: 80, loss: 0.31518784165382385\n",
            "step: 90, loss: 0.37581735849380493\n",
            "step: 100, loss: 0.5014696717262268\n",
            "step: 110, loss: 0.38842788338661194\n",
            "step: 120, loss: 0.23501017689704895\n",
            "step: 130, loss: 0.166880264878273\n",
            "step: 140, loss: 0.24161306023597717\n",
            "step: 150, loss: 0.4485141932964325\n",
            "step: 160, loss: 0.16808566451072693\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 0.6293199062347412\n",
            "step: 180, loss: 0.30978530645370483\n",
            "step: 190, loss: 0.3082669973373413\n",
            "step: 200, loss: 0.15426571667194366\n",
            "step: 210, loss: 0.3066663146018982\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2541778087615967\n",
            "step: 10, loss: 0.24339091777801514\n",
            "step: 20, loss: 0.4812225103378296\n",
            "step: 30, loss: 0.23416616022586823\n",
            "step: 40, loss: 0.45059701800346375\n",
            "step: 50, loss: 0.4598439931869507\n",
            "step: 60, loss: 0.48858317732810974\n",
            "step: 70, loss: 0.18350525200366974\n",
            "step: 80, loss: 0.469148725271225\n",
            "step: 90, loss: 0.23043225705623627\n",
            "step: 100, loss: 0.36992132663726807\n",
            "step: 110, loss: 0.23372003436088562\n",
            "step: 120, loss: 0.25929203629493713\n",
            "step: 130, loss: 0.15833240747451782\n",
            "step: 140, loss: 0.34718555212020874\n",
            "step: 150, loss: 0.32015788555145264\n",
            "step: 160, loss: 0.2288951426744461\n",
            "step: 170, loss: 0.3722613453865051\n",
            "step: 180, loss: 0.2512328028678894\n",
            "step: 190, loss: 0.167636439204216\n",
            "step: 200, loss: 0.25133809447288513\n",
            "step: 210, loss: 0.24972401559352875\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30829378962516785\n",
            "step: 10, loss: 0.30165228247642517\n",
            "step: 20, loss: 0.30636751651763916\n",
            "step: 30, loss: 0.2343539446592331\n",
            "step: 40, loss: 0.23516327142715454\n",
            "step: 50, loss: 0.2404763549566269\n",
            "step: 60, loss: 0.4950231611728668\n",
            "step: 70, loss: 0.25226229429244995\n",
            "step: 80, loss: 0.23952162265777588\n",
            "step: 90, loss: 0.4674607217311859\n",
            "step: 100, loss: 0.38004085421562195\n",
            "step: 110, loss: 0.6038954257965088\n",
            "step: 120, loss: 0.3750755488872528\n",
            "step: 130, loss: 0.6812676191329956\n",
            "step: 140, loss: 0.49355781078338623\n",
            "step: 150, loss: 0.3791744112968445\n",
            "step: 160, loss: 0.306303471326828\n",
            "step: 170, loss: 0.17750364542007446\n",
            "step: 180, loss: 0.08599672466516495\n",
            "step: 190, loss: 0.1716042160987854\n",
            "step: 200, loss: 0.32651910185813904\n",
            "step: 210, loss: 0.3785044252872467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3904838562011719\n",
            "step: 10, loss: 0.3164185583591461\n",
            "step: 20, loss: 0.3071117103099823\n",
            "step: 30, loss: 0.23857487738132477\n",
            "step: 40, loss: 0.44951921701431274\n",
            "step: 50, loss: 0.37788131833076477\n",
            "step: 60, loss: 0.3884945511817932\n",
            "step: 70, loss: 0.23828230798244476\n",
            "step: 80, loss: 0.44211313128471375\n",
            "step: 90, loss: 0.43223217129707336\n",
            "step: 100, loss: 0.24264521896839142\n",
            "step: 110, loss: 0.1652337908744812\n",
            "step: 120, loss: 0.23585985600948334\n",
            "step: 130, loss: 0.31361523270606995\n",
            "step: 140, loss: 0.5241647958755493\n",
            "step: 150, loss: 0.31911709904670715\n",
            "step: 160, loss: 0.2494027465581894\n",
            "step: 170, loss: 0.375164270401001\n",
            "step: 180, loss: 0.23539315164089203\n",
            "step: 190, loss: 0.5201518535614014\n",
            "step: 200, loss: 0.37371140718460083\n",
            "step: 210, loss: 0.2423170953989029\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1683349311351776\n",
            "step: 10, loss: 0.3757074475288391\n",
            "step: 20, loss: 0.3099704682826996\n",
            "step: 30, loss: 0.2392355352640152\n",
            "step: 40, loss: 0.32367056608200073\n",
            "step: 50, loss: 0.5996413826942444\n",
            "step: 60, loss: 0.3090117573738098\n",
            "step: 70, loss: 0.45106586813926697\n",
            "step: 80, loss: 0.3131057322025299\n",
            "step: 90, loss: 0.31090134382247925\n",
            "step: 100, loss: 0.31053629517555237\n",
            "step: 110, loss: 0.3834516704082489\n",
            "step: 120, loss: 0.2539282739162445\n",
            "step: 130, loss: 0.2401513159275055\n",
            "step: 140, loss: 0.38092419505119324\n",
            "step: 150, loss: 0.19356058537960052\n",
            "step: 160, loss: 0.16681864857673645\n",
            "step: 170, loss: 0.4529056251049042\n",
            "step: 180, loss: 0.38564735651016235\n",
            "step: 190, loss: 0.4572487771511078\n",
            "step: 200, loss: 0.3135196268558502\n",
            "step: 210, loss: 0.3739207684993744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.46151456236839294\n",
            "step: 10, loss: 0.3248922526836395\n",
            "step: 20, loss: 0.44124293327331543\n",
            "step: 30, loss: 0.24641208350658417\n",
            "step: 40, loss: 0.24701803922653198\n",
            "step: 50, loss: 0.3731505572795868\n",
            "step: 60, loss: 0.3142114579677582\n",
            "step: 70, loss: 0.2512874901294708\n",
            "step: 80, loss: 0.5145779848098755\n",
            "step: 90, loss: 0.38061434030532837\n",
            "step: 100, loss: 0.4424697458744049\n",
            "step: 110, loss: 0.3068847358226776\n",
            "step: 120, loss: 0.30863314867019653\n",
            "step: 130, loss: 0.4671308696269989\n",
            "step: 140, loss: 0.3143220841884613\n",
            "step: 150, loss: 0.30753737688064575\n",
            "step: 160, loss: 0.550458550453186\n",
            "step: 170, loss: 0.6056978106498718\n",
            "step: 180, loss: 0.2428511530160904\n",
            "step: 190, loss: 0.2443663626909256\n",
            "step: 200, loss: 0.31069374084472656\n",
            "step: 210, loss: 0.3132762908935547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6051820516586304\n",
            "step: 10, loss: 0.31010302901268005\n",
            "step: 20, loss: 0.2598622739315033\n",
            "step: 30, loss: 0.254655659198761\n",
            "step: 40, loss: 0.25337085127830505\n",
            "step: 50, loss: 0.17495065927505493\n",
            "step: 60, loss: 0.1897534430027008\n",
            "step: 70, loss: 0.47982820868492126\n",
            "step: 80, loss: 0.3050633668899536\n",
            "step: 90, loss: 0.37442338466644287\n",
            "step: 100, loss: 0.6068107485771179\n",
            "step: 110, loss: 0.3734467625617981\n",
            "step: 120, loss: 0.3189573884010315\n",
            "step: 130, loss: 0.1686411201953888\n",
            "step: 140, loss: 0.3146441876888275\n",
            "step: 150, loss: 0.4679372310638428\n",
            "step: 160, loss: 0.4530183970928192\n",
            "step: 170, loss: 0.5150007009506226\n",
            "step: 180, loss: 0.3105480968952179\n",
            "step: 190, loss: 0.24273933470249176\n",
            "step: 200, loss: 0.4594360291957855\n",
            "step: 210, loss: 0.382914662361145\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5756051540374756\n",
            "step: 10, loss: 0.2961004078388214\n",
            "step: 20, loss: 0.3784573972225189\n",
            "step: 30, loss: 0.16923809051513672\n",
            "step: 40, loss: 0.16293275356292725\n",
            "step: 50, loss: 0.4660361707210541\n",
            "step: 60, loss: 0.17642930150032043\n",
            "step: 70, loss: 0.3891627788543701\n",
            "step: 80, loss: 0.31580013036727905\n",
            "step: 90, loss: 0.3083171844482422\n",
            "step: 100, loss: 0.46228259801864624\n",
            "step: 110, loss: 0.5322257876396179\n",
            "step: 120, loss: 0.3763021230697632\n",
            "step: 130, loss: 0.24263499677181244\n",
            "step: 140, loss: 0.6034188866615295\n",
            "step: 150, loss: 0.11110921949148178\n",
            "step: 160, loss: 0.3781305253505707\n",
            "step: 170, loss: 0.3030232787132263\n",
            "step: 180, loss: 0.4737456440925598\n",
            "step: 190, loss: 0.3175540268421173\n",
            "step: 200, loss: 0.3077542781829834\n",
            "step: 210, loss: 0.3099668025970459\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.22597778850796718, f1=0.2816553428042001, best_f1=0.2816553428042001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.351164847612381\n",
            "step: 10, loss: 0.381942480802536\n",
            "step: 20, loss: 0.24550005793571472\n",
            "step: 30, loss: 0.1550455391407013\n",
            "step: 40, loss: 0.25981447100639343\n",
            "step: 50, loss: 0.31426572799682617\n",
            "step: 60, loss: 0.13435699045658112\n",
            "step: 70, loss: 0.29874086380004883\n",
            "step: 80, loss: 0.13243886828422546\n",
            "step: 90, loss: 0.28290486335754395\n",
            "step: 100, loss: 0.3187835216522217\n",
            "step: 110, loss: 0.15061943233013153\n",
            "step: 120, loss: 0.4355504512786865\n",
            "step: 130, loss: 0.12633110582828522\n",
            "step: 140, loss: 0.19300805032253265\n",
            "step: 150, loss: 0.2114030420780182\n",
            "step: 160, loss: 0.25058045983314514\n",
            "step: 170, loss: 0.1313183754682541\n",
            "step: 180, loss: 0.18006743490695953\n",
            "step: 190, loss: 0.1466551423072815\n",
            "step: 200, loss: 0.3535596430301666\n",
            "step: 210, loss: 0.1289663314819336\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.6515151515151516, f1=0.64, best_f1=0.64\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19307731091976166\n",
            "step: 10, loss: 0.20826908946037292\n",
            "step: 20, loss: 0.11682132631540298\n",
            "step: 30, loss: 0.055593784898519516\n",
            "step: 40, loss: 0.2365250289440155\n",
            "step: 50, loss: 0.46788302063941956\n",
            "step: 60, loss: 0.16101717948913574\n",
            "step: 70, loss: 0.14501169323921204\n",
            "step: 80, loss: 0.19457583129405975\n",
            "step: 90, loss: 0.27489715814590454\n",
            "step: 100, loss: 0.21695536375045776\n",
            "step: 110, loss: 0.24927492439746857\n",
            "step: 120, loss: 0.15846456587314606\n",
            "step: 130, loss: 0.07756292074918747\n",
            "step: 140, loss: 0.11391861736774445\n",
            "step: 150, loss: 0.26853612065315247\n",
            "step: 160, loss: 0.16390679776668549\n",
            "step: 170, loss: 0.13620741665363312\n",
            "step: 180, loss: 0.11256135255098343\n",
            "step: 190, loss: 0.398150771856308\n",
            "step: 200, loss: 0.052947964519262314\n",
            "step: 210, loss: 0.25491780042648315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.6770601336302895, f1=0.6776470588235295, best_f1=0.6776470588235295\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14702048897743225\n",
            "step: 10, loss: 0.16935555636882782\n",
            "step: 20, loss: 0.29656457901000977\n",
            "step: 30, loss: 0.1769593358039856\n",
            "step: 40, loss: 0.055412229150533676\n",
            "step: 50, loss: 0.2083292156457901\n",
            "step: 60, loss: 0.040874235332012177\n",
            "step: 70, loss: 0.205942302942276\n",
            "step: 80, loss: 0.123940110206604\n",
            "step: 90, loss: 0.18143656849861145\n",
            "step: 100, loss: 0.06122102215886116\n",
            "step: 110, loss: 0.24720287322998047\n",
            "step: 120, loss: 0.04256405681371689\n",
            "step: 130, loss: 0.26007771492004395\n",
            "step: 140, loss: 0.10919976234436035\n",
            "step: 150, loss: 0.01680767349898815\n",
            "step: 160, loss: 0.12511275708675385\n",
            "step: 170, loss: 0.12266163527965546\n",
            "step: 180, loss: 0.19064557552337646\n",
            "step: 190, loss: 0.12785100936889648\n",
            "step: 200, loss: 0.07009799778461456\n",
            "step: 210, loss: 0.08088894933462143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.6851851851851851, f1=0.6893939393939394, best_f1=0.6893939393939394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13855360448360443\n",
            "step: 10, loss: 0.05953998491168022\n",
            "step: 20, loss: 0.07486278563737869\n",
            "step: 30, loss: 0.07188326120376587\n",
            "step: 40, loss: 0.1405216008424759\n",
            "step: 50, loss: 0.21925105154514313\n",
            "step: 60, loss: 0.22647924721240997\n",
            "step: 70, loss: 0.12432003021240234\n",
            "step: 80, loss: 0.136163130402565\n",
            "step: 90, loss: 0.13873668015003204\n",
            "step: 100, loss: 0.2315378040075302\n",
            "step: 110, loss: 0.13252314925193787\n",
            "step: 120, loss: 0.20078523457050323\n",
            "step: 130, loss: 0.10363569110631943\n",
            "step: 140, loss: 0.1617993265390396\n",
            "step: 150, loss: 0.0705592930316925\n",
            "step: 160, loss: 0.11607656627893448\n",
            "step: 170, loss: 0.12234163284301758\n",
            "step: 180, loss: 0.07709937542676926\n",
            "step: 190, loss: 0.06858418881893158\n",
            "step: 200, loss: 0.11844804137945175\n",
            "step: 210, loss: 0.2002187818288803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.6909090909090909, f1=0.7193347193347193, best_f1=0.7193347193347193\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08152594417333603\n",
            "step: 10, loss: 0.11539901793003082\n",
            "step: 20, loss: 0.3017851412296295\n",
            "step: 30, loss: 0.08060431480407715\n",
            "step: 40, loss: 0.13703522086143494\n",
            "step: 50, loss: 0.1645398736000061\n",
            "step: 60, loss: 0.1231124997138977\n",
            "step: 70, loss: 0.10446176677942276\n",
            "step: 80, loss: 0.1530875861644745\n",
            "step: 90, loss: 0.021807216107845306\n",
            "step: 100, loss: 0.051577288657426834\n",
            "step: 110, loss: 0.13971391320228577\n",
            "step: 120, loss: 0.05524824559688568\n",
            "step: 130, loss: 0.13729292154312134\n",
            "step: 140, loss: 0.09429701417684555\n",
            "step: 150, loss: 0.07133185863494873\n",
            "step: 160, loss: 0.01752004772424698\n",
            "step: 170, loss: 0.060399819165468216\n",
            "step: 180, loss: 0.09173555672168732\n",
            "step: 190, loss: 0.0639820396900177\n",
            "step: 200, loss: 0.04262080788612366\n",
            "step: 210, loss: 0.10698921233415604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.6973180076628351, f1=0.7151277013752456, best_f1=0.7151277013752456\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.139412060379982\n",
            "step: 10, loss: 0.10755319893360138\n",
            "step: 20, loss: 0.10636653006076813\n",
            "step: 30, loss: 0.1354776918888092\n",
            "step: 40, loss: 0.07723593711853027\n",
            "step: 50, loss: 0.14536848664283752\n",
            "step: 60, loss: 0.18181759119033813\n",
            "step: 70, loss: 0.09151805192232132\n",
            "step: 80, loss: 0.035752762109041214\n",
            "step: 90, loss: 0.04472712054848671\n",
            "step: 100, loss: 0.06967778503894806\n",
            "step: 110, loss: 0.21804925799369812\n",
            "step: 120, loss: 0.19006860256195068\n",
            "step: 130, loss: 0.06510665267705917\n",
            "step: 140, loss: 0.09661346673965454\n",
            "step: 150, loss: 0.20495489239692688\n",
            "step: 160, loss: 0.27696332335472107\n",
            "step: 170, loss: 0.06311351805925369\n",
            "step: 180, loss: 0.051372841000556946\n",
            "step: 190, loss: 0.04197785258293152\n",
            "step: 200, loss: 0.04841308668255806\n",
            "step: 210, loss: 0.1498918980360031\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.7034220532319392, f1=0.7123287671232877, best_f1=0.7123287671232877\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:09, 234.19it/s]\n",
            "load_f1 = 0.6984732824427482\n",
            "real_f1 = 0.6895238095238096\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 138.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "ewoOK8t9eDFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HJDquM2Oe05D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0970705-2ac1-45bd-d893-4d93e565471d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4252094626426697\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5074025392532349\n",
            "step: 20, loss: 0.3230299651622772\n",
            "step: 30, loss: 0.39606431126594543\n",
            "step: 40, loss: 0.2618615925312042\n",
            "step: 50, loss: 0.3195364475250244\n",
            "step: 60, loss: 0.47071728110313416\n",
            "step: 70, loss: 0.413546621799469\n",
            "step: 80, loss: 0.16426683962345123\n",
            "step: 90, loss: 0.30158084630966187\n",
            "step: 100, loss: 0.4328615367412567\n",
            "step: 110, loss: 0.2507537603378296\n",
            "step: 120, loss: 0.34418392181396484\n",
            "step: 130, loss: 0.3042128384113312\n",
            "step: 140, loss: 0.17451795935630798\n",
            "step: 150, loss: 0.2945065200328827\n",
            "step: 160, loss: 0.231487438082695\n",
            "step: 170, loss: 0.33662137389183044\n",
            "step: 180, loss: 0.196697399020195\n",
            "step: 190, loss: 0.14246664941310883\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.4773413897280967, f1=0.49132947976878616, best_f1=0.49132947976878616\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.27278932929039\n",
            "step: 10, loss: 0.21860387921333313\n",
            "step: 20, loss: 0.43023210763931274\n",
            "step: 30, loss: 0.15768837928771973\n",
            "step: 40, loss: 0.17699164152145386\n",
            "step: 50, loss: 0.40356045961380005\n",
            "step: 60, loss: 0.397851824760437\n",
            "step: 70, loss: 0.08039404451847076\n",
            "step: 80, loss: 0.06190670281648636\n",
            "step: 90, loss: 0.1528944969177246\n",
            "step: 100, loss: 0.20612181723117828\n",
            "step: 110, loss: 0.048176735639572144\n",
            "step: 120, loss: 0.08564412593841553\n",
            "step: 130, loss: 0.09515272825956345\n",
            "step: 140, loss: 0.19008322060108185\n",
            "step: 150, loss: 0.292635977268219\n",
            "step: 160, loss: 0.08582141250371933\n",
            "step: 170, loss: 0.013442880474030972\n",
            "step: 180, loss: 0.06912176311016083\n",
            "step: 190, loss: 0.020735537633299828\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.8125, f1=0.8167539267015705, best_f1=0.8167539267015705\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10185851901769638\n",
            "step: 10, loss: 0.10436426103115082\n",
            "step: 20, loss: 0.11736597865819931\n",
            "step: 30, loss: 0.06474632769823074\n",
            "step: 40, loss: 0.028421759605407715\n",
            "step: 50, loss: 0.1393248438835144\n",
            "step: 60, loss: 0.1284089833498001\n",
            "step: 70, loss: 0.1276063174009323\n",
            "step: 80, loss: 0.1607537418603897\n",
            "step: 90, loss: 0.11273709684610367\n",
            "step: 100, loss: 0.08521458506584167\n",
            "step: 110, loss: 0.38788536190986633\n",
            "step: 120, loss: 0.23531609773635864\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 130, loss: 0.07407566905021667\n",
            "step: 140, loss: 0.08872903138399124\n",
            "step: 150, loss: 0.1525009274482727\n",
            "step: 160, loss: 0.0894496738910675\n",
            "step: 170, loss: 0.32635530829429626\n",
            "step: 180, loss: 0.3407549560070038\n",
            "step: 190, loss: 0.12127196043729782\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8290155440414507, f1=0.8373333333333334, best_f1=0.8373333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03598654270172119\n",
            "step: 10, loss: 0.09476517140865326\n",
            "step: 20, loss: 0.01533539965748787\n",
            "step: 30, loss: 0.007293581031262875\n",
            "step: 40, loss: 0.0365324467420578\n",
            "step: 50, loss: 0.10822143405675888\n",
            "step: 60, loss: 0.1456604301929474\n",
            "step: 70, loss: 0.02276284247636795\n",
            "step: 80, loss: 0.015214655548334122\n",
            "step: 90, loss: 0.024441787973046303\n",
            "step: 100, loss: 0.021319007501006126\n",
            "step: 110, loss: 0.1182490810751915\n",
            "step: 120, loss: 0.02975689433515072\n",
            "step: 130, loss: 0.01936749741435051\n",
            "step: 140, loss: 0.17964798212051392\n",
            "step: 150, loss: 0.057544928044080734\n",
            "step: 160, loss: 0.0178458821028471\n",
            "step: 170, loss: 0.05045991763472557\n",
            "step: 180, loss: 0.03741935268044472\n",
            "step: 190, loss: 0.011541126295924187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8409703504043127, f1=0.8435013262599469, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13365095853805542\n",
            "step: 10, loss: 0.030147001147270203\n",
            "step: 20, loss: 0.007504328154027462\n",
            "step: 30, loss: 0.052870240062475204\n",
            "step: 40, loss: 0.0062552583403885365\n",
            "step: 50, loss: 0.008789845742285252\n",
            "step: 60, loss: 0.032033443450927734\n",
            "step: 70, loss: 0.036843229085206985\n",
            "step: 80, loss: 0.009906049817800522\n",
            "step: 90, loss: 0.06344093382358551\n",
            "step: 100, loss: 0.03815256059169769\n",
            "step: 110, loss: 0.17039915919303894\n",
            "step: 120, loss: 0.08857668191194534\n",
            "step: 130, loss: 0.2900068759918213\n",
            "step: 140, loss: 0.028978731483221054\n",
            "step: 150, loss: 0.029581686481833458\n",
            "step: 160, loss: 0.008432740345597267\n",
            "step: 170, loss: 0.013647847808897495\n",
            "step: 180, loss: 0.02698453888297081\n",
            "step: 190, loss: 0.05724861100316048\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8503937007874015, f1=0.8435013262599469, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012633463367819786\n",
            "step: 10, loss: 0.013562025502324104\n",
            "step: 20, loss: 0.008920871652662754\n",
            "step: 30, loss: 0.025695674121379852\n",
            "step: 40, loss: 0.049520812928676605\n",
            "step: 50, loss: 0.006710301619023085\n",
            "step: 60, loss: 0.007715859916061163\n",
            "step: 70, loss: 0.011486150324344635\n",
            "step: 80, loss: 0.022138528525829315\n",
            "step: 90, loss: 0.03456222265958786\n",
            "step: 100, loss: 0.12351547181606293\n",
            "step: 110, loss: 0.0028372756205499172\n",
            "step: 120, loss: 0.0460817851126194\n",
            "step: 130, loss: 0.06439484655857086\n",
            "step: 140, loss: 0.05074948072433472\n",
            "step: 150, loss: 0.008299188688397408\n",
            "step: 160, loss: 0.013123472221195698\n",
            "step: 170, loss: 0.3707350790500641\n",
            "step: 180, loss: 0.07209734618663788\n",
            "step: 190, loss: 0.01466903556138277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8429752066115702, f1=0.8412256267409471, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004169621504843235\n",
            "step: 10, loss: 0.0035439501516520977\n",
            "step: 20, loss: 0.0030170450918376446\n",
            "step: 30, loss: 0.010395531542599201\n",
            "step: 40, loss: 0.007005246821790934\n",
            "step: 50, loss: 0.0036126987542957067\n",
            "step: 60, loss: 0.0409012995660305\n",
            "step: 70, loss: 0.0026514416094869375\n",
            "step: 80, loss: 0.0025326129980385303\n",
            "step: 90, loss: 0.02671797387301922\n",
            "step: 100, loss: 0.1574547439813614\n",
            "step: 110, loss: 0.005181537009775639\n",
            "step: 120, loss: 0.037109192460775375\n",
            "step: 130, loss: 0.08570721745491028\n",
            "step: 140, loss: 0.040938615798950195\n",
            "step: 150, loss: 0.05906103923916817\n",
            "step: 160, loss: 0.2141634076833725\n",
            "step: 170, loss: 0.16709692776203156\n",
            "step: 180, loss: 0.0175014715641737\n",
            "step: 190, loss: 0.028941353783011436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8314606741573033, f1=0.8444444444444443, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019523410126566887\n",
            "step: 10, loss: 0.01855810545384884\n",
            "step: 20, loss: 0.013874666765332222\n",
            "step: 30, loss: 0.019352877512574196\n",
            "step: 40, loss: 0.13554434478282928\n",
            "step: 50, loss: 0.010289731435477734\n",
            "step: 60, loss: 0.0073078167624771595\n",
            "step: 70, loss: 0.0014686756767332554\n",
            "step: 80, loss: 0.024355994537472725\n",
            "step: 90, loss: 0.01711278222501278\n",
            "step: 100, loss: 0.008322888053953648\n",
            "step: 110, loss: 0.0547560453414917\n",
            "step: 120, loss: 0.013408157974481583\n",
            "step: 130, loss: 0.003513638162985444\n",
            "step: 140, loss: 0.00964463409036398\n",
            "step: 150, loss: 0.001732986536808312\n",
            "step: 160, loss: 0.011767883785068989\n",
            "step: 170, loss: 0.004589461255818605\n",
            "step: 180, loss: 0.09085594117641449\n",
            "step: 190, loss: 0.002854578197002411\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.83289124668435, f1=0.8364611260053619, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008548801764845848\n",
            "step: 10, loss: 0.0012944808695465326\n",
            "step: 20, loss: 0.04322657734155655\n",
            "step: 30, loss: 0.0015926765045151114\n",
            "step: 40, loss: 0.1397431194782257\n",
            "step: 50, loss: 0.006284215487539768\n",
            "step: 60, loss: 0.09820350259542465\n",
            "step: 70, loss: 0.003507375717163086\n",
            "step: 80, loss: 0.1156996414065361\n",
            "step: 90, loss: 0.005357244051992893\n",
            "step: 100, loss: 0.003434687154367566\n",
            "step: 110, loss: 0.22340607643127441\n",
            "step: 120, loss: 0.011073538102209568\n",
            "step: 130, loss: 0.0058488138020038605\n",
            "step: 140, loss: 0.15045399963855743\n",
            "step: 150, loss: 0.0007997103966772556\n",
            "step: 160, loss: 0.003589760512113571\n",
            "step: 170, loss: 0.011303937062621117\n",
            "step: 180, loss: 0.007041095290333033\n",
            "step: 190, loss: 0.001157151535153389\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8412256267409471, f1=0.8412256267409471, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008035743376240134\n",
            "step: 10, loss: 0.000942840299103409\n",
            "step: 20, loss: 0.0020391889847815037\n",
            "step: 30, loss: 0.10636557638645172\n",
            "step: 40, loss: 0.003971171099692583\n",
            "step: 50, loss: 0.15970082581043243\n",
            "step: 60, loss: 0.020527301356196404\n",
            "step: 70, loss: 0.05681416764855385\n",
            "step: 80, loss: 0.002828879514709115\n",
            "step: 90, loss: 0.0006527628866024315\n",
            "step: 100, loss: 0.03613140061497688\n",
            "step: 110, loss: 0.07920941710472107\n",
            "step: 120, loss: 0.006256532855331898\n",
            "step: 130, loss: 0.0016778371063992381\n",
            "step: 140, loss: 0.0011884734267368913\n",
            "step: 150, loss: 0.12826530635356903\n",
            "step: 160, loss: 0.10748225450515747\n",
            "step: 170, loss: 0.0021693622693419456\n",
            "step: 180, loss: 0.03202236816287041\n",
            "step: 190, loss: 0.004479712340980768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8432432432432432, f1=0.8525469168900803, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001593693858012557\n",
            "step: 10, loss: 0.001267111860215664\n",
            "step: 20, loss: 0.0973893254995346\n",
            "step: 30, loss: 0.020176896825432777\n",
            "step: 40, loss: 0.001175709068775177\n",
            "step: 50, loss: 0.0016924610827118158\n",
            "step: 60, loss: 0.0007353745750151575\n",
            "step: 70, loss: 0.0010164682753384113\n",
            "step: 80, loss: 0.00577140599489212\n",
            "step: 90, loss: 0.009382251650094986\n",
            "step: 100, loss: 0.0022784979082643986\n",
            "step: 110, loss: 0.036565881222486496\n",
            "step: 120, loss: 0.000994706992059946\n",
            "step: 130, loss: 0.015966175124049187\n",
            "step: 140, loss: 0.001988990232348442\n",
            "step: 150, loss: 0.0046782284043729305\n",
            "step: 160, loss: 0.003353371750563383\n",
            "step: 170, loss: 0.003034257097169757\n",
            "step: 180, loss: 0.018353145569562912\n",
            "step: 190, loss: 0.00555774662643671\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8463611859838275, f1=0.8469945355191256, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030475009232759476\n",
            "step: 10, loss: 0.0024270927533507347\n",
            "step: 20, loss: 0.058719802647829056\n",
            "step: 30, loss: 0.007960342802107334\n",
            "step: 40, loss: 0.0012907544150948524\n",
            "step: 50, loss: 0.020210858434438705\n",
            "step: 60, loss: 0.01865106076002121\n",
            "step: 70, loss: 0.0071298484690487385\n",
            "step: 80, loss: 0.0036934141535311937\n",
            "step: 90, loss: 0.004567127209156752\n",
            "step: 100, loss: 0.013851313851773739\n",
            "step: 110, loss: 0.0007168222800828516\n",
            "step: 120, loss: 0.0022722952999174595\n",
            "step: 130, loss: 0.007125867530703545\n",
            "step: 140, loss: 0.0007303732563741505\n",
            "step: 150, loss: 0.016660576686263084\n",
            "step: 160, loss: 0.0004304450994823128\n",
            "step: 170, loss: 0.0011629186337813735\n",
            "step: 180, loss: 0.0007298968848772347\n",
            "step: 190, loss: 0.0006228974089026451\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.837772397094431, f1=0.8325123152709359, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01695023849606514\n",
            "step: 10, loss: 0.001617110799998045\n",
            "step: 20, loss: 0.00043749838368967175\n",
            "step: 30, loss: 0.0071540214121341705\n",
            "step: 40, loss: 0.0007878228207118809\n",
            "step: 50, loss: 0.0008321556961163878\n",
            "step: 60, loss: 0.001653266022913158\n",
            "step: 70, loss: 0.011264222674071789\n",
            "step: 80, loss: 0.006188609637320042\n",
            "step: 90, loss: 0.004468906670808792\n",
            "step: 100, loss: 0.0005590753862634301\n",
            "step: 110, loss: 0.20846860110759735\n",
            "step: 120, loss: 0.004626182373613119\n",
            "step: 130, loss: 0.001782975741662085\n",
            "step: 140, loss: 0.008234566077589989\n",
            "step: 150, loss: 0.007443190552294254\n",
            "step: 160, loss: 0.0009460397413931787\n",
            "step: 170, loss: 0.0017910751048475504\n",
            "step: 180, loss: 0.0025261654518544674\n",
            "step: 190, loss: 0.02021808736026287\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.847645429362881, f1=0.8523676880222841, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03229374811053276\n",
            "step: 10, loss: 0.006723261438310146\n",
            "step: 20, loss: 0.003136663231998682\n",
            "step: 30, loss: 0.0015901261940598488\n",
            "step: 40, loss: 0.0011281727347522974\n",
            "step: 50, loss: 0.007669389713555574\n",
            "step: 60, loss: 0.0004310845397412777\n",
            "step: 70, loss: 0.007086784113198519\n",
            "step: 80, loss: 0.000501374714076519\n",
            "step: 90, loss: 0.0005804107640869915\n",
            "step: 100, loss: 0.0005863552214577794\n",
            "step: 110, loss: 0.001139769796282053\n",
            "step: 120, loss: 0.058690570294857025\n",
            "step: 130, loss: 0.0014512748457491398\n",
            "step: 140, loss: 0.0037531298585236073\n",
            "step: 150, loss: 0.0010619205422699451\n",
            "step: 160, loss: 0.0006848113262094557\n",
            "step: 170, loss: 0.0007944187382236123\n",
            "step: 180, loss: 0.04547611624002457\n",
            "step: 190, loss: 0.007039690390229225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8488063660477454, f1=0.8693333333333334, best_f1=0.8435013262599469\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006415741518139839\n",
            "step: 10, loss: 0.001041775569319725\n",
            "step: 20, loss: 0.03229357674717903\n",
            "step: 30, loss: 0.0005574235692620277\n",
            "step: 40, loss: 0.10777389258146286\n",
            "step: 50, loss: 0.0016079837223514915\n",
            "step: 60, loss: 0.00111958768684417\n",
            "step: 70, loss: 0.0020110916811972857\n",
            "step: 80, loss: 0.006847884505987167\n",
            "step: 90, loss: 0.00812560599297285\n",
            "step: 100, loss: 0.0004088518617209047\n",
            "step: 110, loss: 0.03252663463354111\n",
            "step: 120, loss: 0.0012673686724156141\n",
            "step: 130, loss: 0.0008109759073704481\n",
            "step: 140, loss: 0.0012013016967102885\n",
            "step: 150, loss: 0.0015884976601228118\n",
            "step: 160, loss: 0.0012431311188265681\n",
            "step: 170, loss: 0.006932099349796772\n",
            "step: 180, loss: 0.0155815239995718\n",
            "step: 190, loss: 0.03169535472989082\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8503937007874015, f1=0.8713910761154856, best_f1=0.8435013262599469\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:13, 150.53it/s]\n",
            "load_f1 = 0.8061224489795918\n",
            "real_f1 = 0.7764127764127764\n",
            "733it [00:00, 3327.12it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 133.67it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA TEXTUAL"
      ],
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "R1O9a5RjeDtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "a2WpDwuee1mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c3e98c-d164-40dc-a6e2-7eb86282cd0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.49926069378852844\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4756096303462982\n",
            "step: 20, loss: 0.347630113363266\n",
            "step: 30, loss: 0.47402727603912354\n",
            "step: 40, loss: 0.49018940329551697\n",
            "step: 50, loss: 0.31942829489707947\n",
            "step: 60, loss: 0.5535739660263062\n",
            "step: 70, loss: 0.3053838610649109\n",
            "step: 80, loss: 0.23174940049648285\n",
            "step: 90, loss: 0.2313050776720047\n",
            "step: 100, loss: 0.1327790468931198\n",
            "step: 110, loss: 0.3946821093559265\n",
            "step: 120, loss: 0.3027650713920593\n",
            "step: 130, loss: 0.30798810720443726\n",
            "step: 140, loss: 0.3657178580760956\n",
            "step: 150, loss: 0.33632344007492065\n",
            "step: 160, loss: 0.36365044116973877\n",
            "step: 170, loss: 0.28018680214881897\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.28833333333333333, f1=0.3272058823529412, best_f1=0.3272058823529412\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30708634853363037\n",
            "step: 10, loss: 0.3440445065498352\n",
            "step: 20, loss: 0.2275790423154831\n",
            "step: 30, loss: 0.24108725786209106\n",
            "step: 40, loss: 0.033877041190862656\n",
            "step: 50, loss: 0.18434256315231323\n",
            "step: 60, loss: 0.23813731968402863\n",
            "step: 70, loss: 0.2275943011045456\n",
            "step: 80, loss: 0.20377005636692047\n",
            "step: 90, loss: 0.06283397227525711\n",
            "step: 100, loss: 0.2813866138458252\n",
            "step: 110, loss: 0.2002313882112503\n",
            "step: 120, loss: 0.09983202815055847\n",
            "step: 130, loss: 0.2866584062576294\n",
            "step: 140, loss: 0.23246842622756958\n",
            "step: 150, loss: 0.09602656215429306\n",
            "step: 160, loss: 0.18472698330879211\n",
            "step: 170, loss: 0.1087740883231163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7405660377358491, f1=0.7910112359550562, best_f1=0.7910112359550562\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26351621747016907\n",
            "step: 10, loss: 0.04919417202472687\n",
            "step: 20, loss: 0.021912453696131706\n",
            "step: 30, loss: 0.12992152571678162\n",
            "step: 40, loss: 0.03517081215977669\n",
            "step: 50, loss: 0.13263142108917236\n",
            "step: 60, loss: 0.06021507829427719\n",
            "step: 70, loss: 0.15045106410980225\n",
            "step: 80, loss: 0.08564194291830063\n",
            "step: 90, loss: 0.4265533983707428\n",
            "step: 100, loss: 0.06101543828845024\n",
            "step: 110, loss: 0.022755175828933716\n",
            "step: 120, loss: 0.03803986310958862\n",
            "step: 130, loss: 0.09904167801141739\n",
            "step: 140, loss: 0.21262226998806\n",
            "step: 150, loss: 0.06044546887278557\n",
            "step: 160, loss: 0.08732636272907257\n",
            "step: 170, loss: 0.07715406268835068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8071065989847717, f1=0.8321513002364066, best_f1=0.8321513002364066\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09652753919363022\n",
            "step: 10, loss: 0.06280162185430527\n",
            "step: 20, loss: 0.004275098908692598\n",
            "step: 30, loss: 0.12197096645832062\n",
            "step: 40, loss: 0.03315351903438568\n",
            "step: 50, loss: 0.0322314128279686\n",
            "step: 60, loss: 0.07832900434732437\n",
            "step: 70, loss: 0.004580034874379635\n",
            "step: 80, loss: 0.30439621210098267\n",
            "step: 90, loss: 0.15539002418518066\n",
            "step: 100, loss: 0.07640886306762695\n",
            "step: 110, loss: 0.1820768564939499\n",
            "step: 120, loss: 0.24976423382759094\n",
            "step: 130, loss: 0.11167669296264648\n",
            "step: 140, loss: 0.022021807730197906\n",
            "step: 150, loss: 0.203548401594162\n",
            "step: 160, loss: 0.010992404073476791\n",
            "step: 170, loss: 0.11981622874736786\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8353808353808354, f1=0.8717948717948718, best_f1=0.8717948717948718\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.074739970266819\n",
            "step: 10, loss: 0.05597514286637306\n",
            "step: 20, loss: 0.018657157197594643\n",
            "step: 30, loss: 0.01849561557173729\n",
            "step: 40, loss: 0.019493741914629936\n",
            "step: 50, loss: 0.013813113793730736\n",
            "step: 60, loss: 0.04961225390434265\n",
            "step: 70, loss: 0.01899242214858532\n",
            "step: 80, loss: 0.0018514448311179876\n",
            "step: 90, loss: 0.02703562192618847\n",
            "step: 100, loss: 0.02061532251536846\n",
            "step: 110, loss: 0.03787851706147194\n",
            "step: 120, loss: 0.10851456969976425\n",
            "step: 130, loss: 0.025601113215088844\n",
            "step: 140, loss: 0.061164431273937225\n",
            "step: 150, loss: 0.05133449658751488\n",
            "step: 160, loss: 0.009362460114061832\n",
            "step: 170, loss: 0.005358167923986912\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8439897698209718, f1=0.888888888888889, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12817487120628357\n",
            "step: 10, loss: 0.008000175468623638\n",
            "step: 20, loss: 0.12260451167821884\n",
            "step: 30, loss: 0.0037489261012524366\n",
            "step: 40, loss: 0.004671760369092226\n",
            "step: 50, loss: 0.011580384336411953\n",
            "step: 60, loss: 0.03274250775575638\n",
            "step: 70, loss: 0.019885145127773285\n",
            "step: 80, loss: 0.16282206773757935\n",
            "step: 90, loss: 0.1656085103750229\n",
            "step: 100, loss: 0.018498562276363373\n",
            "step: 110, loss: 0.0626349076628685\n",
            "step: 120, loss: 0.11225677281618118\n",
            "step: 130, loss: 0.009430286474525928\n",
            "step: 140, loss: 0.027920076623558998\n",
            "step: 150, loss: 0.007159445434808731\n",
            "step: 160, loss: 0.03296057879924774\n",
            "step: 170, loss: 0.011484429240226746\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8321167883211679, f1=0.8720379146919431, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03830839321017265\n",
            "step: 10, loss: 0.003632865846157074\n",
            "step: 20, loss: 0.0017514580395072699\n",
            "step: 30, loss: 0.0017876580823212862\n",
            "step: 40, loss: 0.0009967583464458585\n",
            "step: 50, loss: 0.0023168521001935005\n",
            "step: 60, loss: 0.004976989235728979\n",
            "step: 70, loss: 0.0093537587672472\n",
            "step: 80, loss: 0.032603975385427475\n",
            "step: 90, loss: 0.019449274986982346\n",
            "step: 100, loss: 0.002068551955744624\n",
            "step: 110, loss: 0.0667153000831604\n",
            "step: 120, loss: 0.01670689694583416\n",
            "step: 130, loss: 0.00747070275247097\n",
            "step: 140, loss: 0.091057188808918\n",
            "step: 150, loss: 0.11292432248592377\n",
            "step: 160, loss: 0.004289062228053808\n",
            "step: 170, loss: 0.051221564412117004\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8307692307692308, f1=0.8706467661691543, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05352722108364105\n",
            "step: 10, loss: 0.0013716445537284017\n",
            "step: 20, loss: 0.008519716560840607\n",
            "step: 30, loss: 0.000658916134852916\n",
            "step: 40, loss: 0.002000552136451006\n",
            "step: 50, loss: 0.010083350352942944\n",
            "step: 60, loss: 0.007297474425286055\n",
            "step: 70, loss: 0.034770529717206955\n",
            "step: 80, loss: 0.001875353162176907\n",
            "step: 90, loss: 0.04638725891709328\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.0007210664916783571\n",
            "step: 110, loss: 0.028107738122344017\n",
            "step: 120, loss: 0.010147488676011562\n",
            "step: 130, loss: 0.004008975811302662\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 140, loss: 0.004242307972162962\n",
            "step: 150, loss: 0.0060484749265015125\n",
            "step: 160, loss: 0.0017838955391198397\n",
            "step: 170, loss: 0.05814525857567787\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8426150121065376, f1=0.8652482269503546, best_f1=0.888888888888889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0127047598361969\n",
            "step: 10, loss: 0.005102070048451424\n",
            "step: 20, loss: 0.001470701303333044\n",
            "step: 30, loss: 0.05356398969888687\n",
            "step: 40, loss: 0.2840099334716797\n",
            "step: 50, loss: 0.017903026193380356\n",
            "step: 60, loss: 0.024330224841833115\n",
            "step: 70, loss: 0.0012317100772634149\n",
            "step: 80, loss: 0.00200811680406332\n",
            "step: 90, loss: 0.09050324559211731\n",
            "step: 100, loss: 0.03542003408074379\n",
            "step: 110, loss: 0.058240339159965515\n",
            "step: 120, loss: 0.00675318343564868\n",
            "step: 130, loss: 0.0006760115502402186\n",
            "step: 140, loss: 0.00036239164182916284\n",
            "step: 150, loss: 0.07413772493600845\n",
            "step: 160, loss: 0.004766915924847126\n",
            "step: 170, loss: 0.001246242318302393\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.855072463768116, f1=0.889423076923077, best_f1=0.889423076923077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003118712455034256\n",
            "step: 10, loss: 0.02586132101714611\n",
            "step: 20, loss: 0.03582154959440231\n",
            "step: 30, loss: 0.06471970677375793\n",
            "step: 40, loss: 0.0023538677487522364\n",
            "step: 50, loss: 0.03847627341747284\n",
            "step: 60, loss: 0.025568392127752304\n",
            "step: 70, loss: 0.0013760486617684364\n",
            "step: 80, loss: 0.038331761956214905\n",
            "step: 90, loss: 0.0005226263892836869\n",
            "step: 100, loss: 0.0031447098590433598\n",
            "step: 110, loss: 0.011809946037828922\n",
            "step: 120, loss: 0.00359911285340786\n",
            "step: 130, loss: 0.00236023822799325\n",
            "step: 140, loss: 0.004157720133662224\n",
            "step: 150, loss: 0.10392443835735321\n",
            "step: 160, loss: 0.0009486943599767983\n",
            "step: 170, loss: 0.0773770660161972\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8456057007125889, f1=0.8695652173913044, best_f1=0.889423076923077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03598199412226677\n",
            "step: 10, loss: 0.003163313725963235\n",
            "step: 20, loss: 0.003100415226072073\n",
            "step: 30, loss: 0.0017083886777982116\n",
            "step: 40, loss: 0.0012532215332612395\n",
            "step: 50, loss: 0.006100168451666832\n",
            "step: 60, loss: 0.04421263188123703\n",
            "step: 70, loss: 0.017291095107793808\n",
            "step: 80, loss: 0.058587849140167236\n",
            "step: 90, loss: 0.013011384755373001\n",
            "step: 100, loss: 0.0014127201866358519\n",
            "step: 110, loss: 0.01307592075318098\n",
            "step: 120, loss: 0.003868216648697853\n",
            "step: 130, loss: 0.015239333733916283\n",
            "step: 140, loss: 0.013930200599133968\n",
            "step: 150, loss: 0.009170117788016796\n",
            "step: 160, loss: 0.011352538131177425\n",
            "step: 170, loss: 0.02207065373659134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8426395939086295, f1=0.8731707317073171, best_f1=0.889423076923077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001437922241166234\n",
            "step: 10, loss: 0.01319440919905901\n",
            "step: 20, loss: 0.0008607681374996901\n",
            "step: 30, loss: 0.11451166868209839\n",
            "step: 40, loss: 0.0013615402858704329\n",
            "step: 50, loss: 0.000592939555644989\n",
            "step: 60, loss: 0.008976300247013569\n",
            "step: 70, loss: 0.0004596822545863688\n",
            "step: 80, loss: 0.00101671670563519\n",
            "step: 90, loss: 0.009076149202883244\n",
            "step: 100, loss: 0.0009364458383060992\n",
            "step: 110, loss: 0.021911589428782463\n",
            "step: 120, loss: 0.021371452137827873\n",
            "step: 130, loss: 0.036279868334531784\n",
            "step: 140, loss: 0.0031656718347221613\n",
            "step: 150, loss: 0.008512627333402634\n",
            "step: 160, loss: 0.044147688895463943\n",
            "step: 170, loss: 0.001724510919302702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8404255319148938, f1=0.8877551020408163, best_f1=0.889423076923077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00197223830036819\n",
            "step: 10, loss: 0.0005847982247360051\n",
            "step: 20, loss: 0.0026974081993103027\n",
            "step: 30, loss: 0.05036264285445213\n",
            "step: 40, loss: 0.001243950449861586\n",
            "step: 50, loss: 0.020432867109775543\n",
            "step: 60, loss: 0.0009814569493755698\n",
            "step: 70, loss: 0.11297915875911713\n",
            "step: 80, loss: 0.0009771873010322452\n",
            "step: 90, loss: 0.004193961154669523\n",
            "step: 100, loss: 0.011812024749815464\n",
            "step: 110, loss: 0.00046364229638129473\n",
            "step: 120, loss: 0.023969998583197594\n",
            "step: 130, loss: 0.00036663946229964495\n",
            "step: 140, loss: 0.024402089416980743\n",
            "step: 150, loss: 0.0006598714389838278\n",
            "step: 160, loss: 0.009149382822215557\n",
            "step: 170, loss: 0.0003482913307379931\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8481675392670156, f1=0.8877805486284289, best_f1=0.889423076923077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011866986751556396\n",
            "step: 10, loss: 0.0008773576118983328\n",
            "step: 20, loss: 0.001782419509254396\n",
            "step: 30, loss: 0.008389749564230442\n",
            "step: 40, loss: 0.003311724402010441\n",
            "step: 50, loss: 0.00038788188248872757\n",
            "step: 60, loss: 0.0024536247365176678\n",
            "step: 70, loss: 0.009792294353246689\n",
            "step: 80, loss: 0.0003930175444111228\n",
            "step: 90, loss: 0.00028760204440914094\n",
            "step: 100, loss: 0.0013009619433432817\n",
            "step: 110, loss: 0.021054040640592575\n",
            "step: 120, loss: 0.001643726951442659\n",
            "step: 130, loss: 0.1210077702999115\n",
            "step: 140, loss: 0.03764251247048378\n",
            "step: 150, loss: 0.03379780799150467\n",
            "step: 160, loss: 0.0007036264869384468\n",
            "step: 170, loss: 0.05244353786110878\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8514851485148516, f1=0.8782816229116946, best_f1=0.889423076923077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00035809827386401594\n",
            "step: 10, loss: 0.0006415274692699313\n",
            "step: 20, loss: 0.000515255203936249\n",
            "step: 30, loss: 0.0013322425074875355\n",
            "step: 40, loss: 0.0003719277447089553\n",
            "step: 50, loss: 0.0003192858712282032\n",
            "step: 60, loss: 0.0005823928513564169\n",
            "step: 70, loss: 0.061764393001794815\n",
            "step: 80, loss: 0.004331403877586126\n",
            "step: 90, loss: 0.009952993132174015\n",
            "step: 100, loss: 0.028038958087563515\n",
            "step: 110, loss: 0.0008500184630975127\n",
            "step: 120, loss: 0.0002946292224805802\n",
            "step: 130, loss: 0.0008964926819317043\n",
            "step: 140, loss: 0.02152494341135025\n",
            "step: 150, loss: 0.00032445768010802567\n",
            "step: 160, loss: 0.0004148848238401115\n",
            "step: 170, loss: 0.0008243889315053821\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8519480519480519, f1=0.8793969849246231, best_f1=0.889423076923077\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 223.64it/s]\n",
            "load_f1 = 0.4772413793103447\n",
            "real_f1 = 0.46469833119383824\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 134.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA DIRTY"
      ],
      "metadata": {
        "id": "6pmKonkXeD7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "bRxHd3j2eEH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "0lnLoRSEe2fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd1e4f1-da6a-4b1e-8e8e-6e9f30b99405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 520kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.39MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.82MB/s]\n",
            "Downloading: 100% 501M/501M [00:08<00:00, 57.8MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5585409998893738\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4479609727859497\n",
            "step: 20, loss: 0.47656768560409546\n",
            "step: 30, loss: 0.16349348425865173\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 40, loss: 0.12789885699748993\n",
            "step: 50, loss: 0.08718784153461456\n",
            "step: 60, loss: 0.0627484992146492\n",
            "step: 70, loss: 0.1625131368637085\n",
            "step: 80, loss: 0.02822524681687355\n",
            "step: 90, loss: 0.08259640634059906\n",
            "step: 100, loss: 0.15459725260734558\n",
            "step: 110, loss: 0.17773421108722687\n",
            "step: 120, loss: 0.037919607013463974\n",
            "step: 130, loss: 0.02520163729786873\n",
            "step: 140, loss: 0.027222732082009315\n",
            "step: 150, loss: 0.1438496857881546\n",
            "step: 160, loss: 0.011608911678195\n",
            "step: 170, loss: 0.0900685042142868\n",
            "step: 180, loss: 0.0656827837228775\n",
            "step: 190, loss: 0.3481449484825134\n",
            "step: 200, loss: 0.03562541306018829\n",
            "step: 210, loss: 0.04067953675985336\n",
            "step: 220, loss: 0.10438183695077896\n",
            "step: 230, loss: 0.0038431689608842134\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9627959413754228, f1=0.9623717217787913, best_f1=0.9623717217787913\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004411256406456232\n",
            "step: 10, loss: 0.1506527066230774\n",
            "step: 20, loss: 0.05771924555301666\n",
            "step: 30, loss: 0.012658203952014446\n",
            "step: 40, loss: 0.040094196796417236\n",
            "step: 50, loss: 0.010577218607068062\n",
            "step: 60, loss: 0.018439944833517075\n",
            "step: 70, loss: 0.0021214098669588566\n",
            "step: 80, loss: 0.013295897282660007\n",
            "step: 90, loss: 0.017625857144594193\n",
            "step: 100, loss: 0.015443358570337296\n",
            "step: 110, loss: 0.037450820207595825\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 120, loss: 0.10275203734636307\n",
            "step: 130, loss: 0.04892390966415405\n",
            "step: 140, loss: 0.009983167052268982\n",
            "step: 150, loss: 0.16328038275241852\n",
            "step: 160, loss: 0.0525825172662735\n",
            "step: 170, loss: 0.0027925886679440737\n",
            "step: 180, loss: 0.004219682887196541\n",
            "step: 190, loss: 0.016593987122178078\n",
            "step: 200, loss: 0.02729925699532032\n",
            "step: 210, loss: 0.03157010301947594\n",
            "step: 220, loss: 0.006014808546751738\n",
            "step: 230, loss: 0.010846163146197796\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9722530521642618, f1=0.9654403567447045, best_f1=0.9654403567447045\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003582240315154195\n",
            "step: 10, loss: 0.01342077273875475\n",
            "step: 20, loss: 0.00269341841340065\n",
            "step: 30, loss: 0.004133650101721287\n",
            "step: 40, loss: 0.05225486680865288\n",
            "step: 50, loss: 0.031182631850242615\n",
            "step: 60, loss: 0.012144672684371471\n",
            "step: 70, loss: 0.008460110053420067\n",
            "step: 80, loss: 0.11989393085241318\n",
            "step: 90, loss: 0.01398811861872673\n",
            "step: 100, loss: 0.003329257946461439\n",
            "step: 110, loss: 0.06142976135015488\n",
            "step: 120, loss: 0.0028871544636785984\n",
            "step: 130, loss: 0.00976966880261898\n",
            "step: 140, loss: 0.001693323371000588\n",
            "step: 150, loss: 0.011878821067512035\n",
            "step: 160, loss: 0.0013827835209667683\n",
            "step: 170, loss: 0.000985765946097672\n",
            "step: 180, loss: 0.011342584155499935\n",
            "step: 190, loss: 0.023035556077957153\n",
            "step: 200, loss: 0.014278572984039783\n",
            "step: 210, loss: 0.0077951038256287575\n",
            "step: 220, loss: 0.1006995290517807\n",
            "step: 230, loss: 0.0029380912892520428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9753914988814317, f1=0.9641255605381166, best_f1=0.9641255605381166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012008978053927422\n",
            "step: 10, loss: 0.0008990142960101366\n",
            "step: 20, loss: 0.002930688438937068\n",
            "step: 30, loss: 0.002712762448936701\n",
            "step: 40, loss: 0.06340226531028748\n",
            "step: 50, loss: 0.05131198838353157\n",
            "step: 60, loss: 0.01001882366836071\n",
            "step: 70, loss: 0.0040076253935694695\n",
            "step: 80, loss: 0.020073965191841125\n",
            "step: 90, loss: 0.089272640645504\n",
            "step: 100, loss: 0.026303503662347794\n",
            "step: 110, loss: 0.0003820470883511007\n",
            "step: 120, loss: 0.008012393489480019\n",
            "step: 130, loss: 0.013326904736459255\n",
            "step: 140, loss: 0.05245829373598099\n",
            "step: 150, loss: 0.0021482177544385195\n",
            "step: 160, loss: 0.0037831219378858805\n",
            "step: 170, loss: 0.01256315503269434\n",
            "step: 180, loss: 0.005740619730204344\n",
            "step: 190, loss: 0.0025310388300567865\n",
            "step: 200, loss: 0.03250092267990112\n",
            "step: 210, loss: 0.006964176893234253\n",
            "step: 220, loss: 0.001390937715768814\n",
            "step: 230, loss: 0.0007403510389849544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9730941704035874, f1=0.9706546275395034, best_f1=0.9641255605381166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013672136701643467\n",
            "step: 10, loss: 0.02613447979092598\n",
            "step: 20, loss: 0.038354985415935516\n",
            "step: 30, loss: 0.003283090889453888\n",
            "step: 40, loss: 0.006660830229520798\n",
            "step: 50, loss: 0.0030086126644164324\n",
            "step: 60, loss: 0.013460497371852398\n",
            "step: 70, loss: 0.002424921840429306\n",
            "step: 80, loss: 0.07296543568372726\n",
            "step: 90, loss: 0.05887584760785103\n",
            "step: 100, loss: 0.00033986152266152203\n",
            "step: 110, loss: 0.0034400280565023422\n",
            "step: 120, loss: 0.0017181371804326773\n",
            "step: 130, loss: 0.00031805472099222243\n",
            "step: 140, loss: 0.00700764637440443\n",
            "step: 150, loss: 0.015693500638008118\n",
            "step: 160, loss: 0.012623663991689682\n",
            "step: 170, loss: 0.0007929775165393949\n",
            "step: 180, loss: 0.01386401429772377\n",
            "step: 190, loss: 0.011135175824165344\n",
            "step: 200, loss: 0.007049520965665579\n",
            "step: 210, loss: 0.03770580142736435\n",
            "step: 220, loss: 0.0027998120058327913\n",
            "step: 230, loss: 0.03369409218430519\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9799107142857142, f1=0.9763779527559054, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007570014800876379\n",
            "step: 10, loss: 0.0028100779745727777\n",
            "step: 20, loss: 0.0005762066575698555\n",
            "step: 30, loss: 0.0006132987327873707\n",
            "step: 40, loss: 0.00018105271738022566\n",
            "step: 50, loss: 0.00513994786888361\n",
            "step: 60, loss: 0.0056342752650380135\n",
            "step: 70, loss: 0.18787293136119843\n",
            "step: 80, loss: 0.0010462728096172214\n",
            "step: 90, loss: 0.005076193250715733\n",
            "step: 100, loss: 0.0005193339893594384\n",
            "step: 110, loss: 0.022448336705565453\n",
            "step: 120, loss: 0.0014223720645532012\n",
            "step: 130, loss: 0.0029690980445593596\n",
            "step: 140, loss: 0.0006797860260121524\n",
            "step: 150, loss: 0.002598196268081665\n",
            "step: 160, loss: 0.0012475713156163692\n",
            "step: 170, loss: 0.001381515059620142\n",
            "step: 180, loss: 0.0013660260010510683\n",
            "step: 190, loss: 0.00048132752999663353\n",
            "step: 200, loss: 0.0009178614127449691\n",
            "step: 210, loss: 0.00016847572987899184\n",
            "step: 220, loss: 0.004426160827279091\n",
            "step: 230, loss: 0.0003791142371483147\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9785310734463276, f1=0.9750566893424036, best_f1=0.9763779527559054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007344732875935733\n",
            "step: 10, loss: 0.0003045230114366859\n",
            "step: 20, loss: 0.00033931192592717707\n",
            "step: 30, loss: 0.00010093981836689636\n",
            "step: 40, loss: 0.00022624807024840266\n",
            "step: 50, loss: 0.00023164933372754604\n",
            "step: 60, loss: 0.0002189444930991158\n",
            "step: 70, loss: 0.0004675121745094657\n",
            "step: 80, loss: 0.0003009157080668956\n",
            "step: 90, loss: 0.00021233715233393013\n",
            "step: 100, loss: 0.00016345834592357278\n",
            "step: 110, loss: 0.00017023553664330393\n",
            "step: 120, loss: 0.001230889931321144\n",
            "step: 130, loss: 0.003993373829871416\n",
            "step: 140, loss: 7.829837704775855e-05\n",
            "step: 150, loss: 0.008074553683400154\n",
            "step: 160, loss: 0.00018716484191827476\n",
            "step: 170, loss: 0.0002791076258290559\n",
            "step: 180, loss: 0.00012925911869388074\n",
            "step: 190, loss: 0.0002695170696824789\n",
            "step: 200, loss: 0.002806413220241666\n",
            "step: 210, loss: 0.002236793516203761\n",
            "step: 220, loss: 0.0012936112470924854\n",
            "step: 230, loss: 0.0035369531251490116\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9820224719101124, f1=0.9750566893424036, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004359900485724211\n",
            "step: 10, loss: 0.0049345423467457294\n",
            "step: 20, loss: 0.003347745630890131\n",
            "step: 30, loss: 0.010012284852564335\n",
            "step: 40, loss: 0.0009227698901668191\n",
            "step: 50, loss: 0.00035412723082117736\n",
            "step: 60, loss: 0.0003176293394062668\n",
            "step: 70, loss: 0.0001162242260761559\n",
            "step: 80, loss: 0.0013363598845899105\n",
            "step: 90, loss: 0.002415427239611745\n",
            "step: 100, loss: 0.000311005802359432\n",
            "step: 110, loss: 0.0010161908576264977\n",
            "step: 120, loss: 0.0010611001634970307\n",
            "step: 130, loss: 0.007979908026754856\n",
            "step: 140, loss: 0.0003854141978081316\n",
            "step: 150, loss: 0.253693163394928\n",
            "step: 160, loss: 0.00027853387291543186\n",
            "step: 170, loss: 0.013986891135573387\n",
            "step: 180, loss: 0.00039461307460442185\n",
            "step: 190, loss: 0.0005274647264741361\n",
            "step: 200, loss: 0.0018018027767539024\n",
            "step: 210, loss: 0.01855422370135784\n",
            "step: 220, loss: 0.0004608457093127072\n",
            "step: 230, loss: 0.00045684949145652354\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9810901001112348, f1=0.9709821428571428, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004806302487850189\n",
            "step: 10, loss: 0.00024840107653290033\n",
            "step: 20, loss: 0.0004803628253284842\n",
            "step: 30, loss: 0.00048474996583536267\n",
            "step: 40, loss: 0.0012099886080250144\n",
            "step: 50, loss: 0.0008150585927069187\n",
            "step: 60, loss: 0.0004297671257518232\n",
            "step: 70, loss: 0.050985757261514664\n",
            "step: 80, loss: 0.00033796942443586886\n",
            "step: 90, loss: 0.008424879983067513\n",
            "step: 100, loss: 0.0003863350721076131\n",
            "step: 110, loss: 0.0003210111754015088\n",
            "step: 120, loss: 0.028732798993587494\n",
            "step: 130, loss: 0.0018956454005092382\n",
            "step: 140, loss: 0.0003481896419543773\n",
            "step: 150, loss: 0.0004497393383644521\n",
            "step: 160, loss: 0.0008159317076206207\n",
            "step: 170, loss: 0.0020284082274883986\n",
            "step: 180, loss: 0.00049617700278759\n",
            "step: 190, loss: 0.0001623657444724813\n",
            "step: 200, loss: 0.006140848156064749\n",
            "step: 210, loss: 0.0005589410429820418\n",
            "step: 220, loss: 0.0002461228286847472\n",
            "step: 230, loss: 8.877224900061265e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9810901001112348, f1=0.9753363228699552, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019937558681704104\n",
            "step: 10, loss: 0.00022345077013596892\n",
            "step: 20, loss: 0.0001533313188701868\n",
            "step: 30, loss: 0.00013093608140479773\n",
            "step: 40, loss: 0.0007602378609590232\n",
            "step: 50, loss: 0.000685730017721653\n",
            "step: 60, loss: 0.0003090204845648259\n",
            "step: 70, loss: 0.0014558910625055432\n",
            "step: 80, loss: 0.0011195589322596788\n",
            "step: 90, loss: 0.00022846314823254943\n",
            "step: 100, loss: 0.0002665301435627043\n",
            "step: 110, loss: 0.0010948176495730877\n",
            "step: 120, loss: 0.022810062393546104\n",
            "step: 130, loss: 0.0003786641755141318\n",
            "step: 140, loss: 0.0001869612606242299\n",
            "step: 150, loss: 0.004833802580833435\n",
            "step: 160, loss: 0.0001738392311381176\n",
            "step: 170, loss: 0.0002083254512399435\n",
            "step: 180, loss: 0.0014703032793477178\n",
            "step: 190, loss: 0.0015478839632123709\n",
            "step: 200, loss: 0.0018090083030983806\n",
            "step: 210, loss: 0.0001412531128153205\n",
            "step: 220, loss: 0.0007785193738527596\n",
            "step: 230, loss: 0.00010176272189710289\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9865470852017937, f1=0.9785794813979707, best_f1=0.9785794813979707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010008020763052627\n",
            "step: 10, loss: 0.00035758124431595206\n",
            "step: 20, loss: 0.00019689268083311617\n",
            "step: 30, loss: 0.0001447526883566752\n",
            "step: 40, loss: 3.30555449181702e-05\n",
            "step: 50, loss: 0.0020088651217520237\n",
            "step: 60, loss: 0.00016182013496290892\n",
            "step: 70, loss: 0.00011693496344378218\n",
            "step: 80, loss: 0.0005417991778813303\n",
            "step: 90, loss: 0.17939352989196777\n",
            "step: 100, loss: 0.0010128978174179792\n",
            "step: 110, loss: 0.003874750342220068\n",
            "step: 120, loss: 0.00033746642293408513\n",
            "step: 130, loss: 0.00027006681193597615\n",
            "step: 140, loss: 0.0003497617435641587\n",
            "step: 150, loss: 0.00017438281793147326\n",
            "step: 160, loss: 0.0002416156348772347\n",
            "step: 170, loss: 0.001986423274502158\n",
            "step: 180, loss: 0.0016125283436849713\n",
            "step: 190, loss: 0.00011818340863101184\n",
            "step: 200, loss: 0.00025498069589957595\n",
            "step: 210, loss: 0.0007239190745167434\n",
            "step: 220, loss: 0.0003227928245905787\n",
            "step: 230, loss: 0.000369417539332062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9843400447427293, f1=0.9764309764309763, best_f1=0.9785794813979707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00048651121323928237\n",
            "step: 10, loss: 0.00040340720443055034\n",
            "step: 20, loss: 0.001043518423102796\n",
            "step: 30, loss: 0.03358427435159683\n",
            "step: 40, loss: 0.0002647133660502732\n",
            "step: 50, loss: 0.0003536874137353152\n",
            "step: 60, loss: 0.00019498106848914176\n",
            "step: 70, loss: 0.00016623202827759087\n",
            "step: 80, loss: 0.0001097179701901041\n",
            "step: 90, loss: 0.004302520304918289\n",
            "step: 100, loss: 0.00015806953888386488\n",
            "step: 110, loss: 9.565251093590632e-05\n",
            "step: 120, loss: 0.00022072248975746334\n",
            "step: 130, loss: 0.00013798322470393032\n",
            "step: 140, loss: 0.00031184169347397983\n",
            "step: 150, loss: 0.00024636846501380205\n",
            "step: 160, loss: 0.00027239619521424174\n",
            "step: 170, loss: 0.0003797845565713942\n",
            "step: 180, loss: 0.00020870944717898965\n",
            "step: 190, loss: 0.002334315562620759\n",
            "step: 200, loss: 0.00018684992392081767\n",
            "step: 210, loss: 0.0009263508836738765\n",
            "step: 220, loss: 0.015325737185776234\n",
            "step: 230, loss: 0.0002773209707811475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9865470852017937, f1=0.9785310734463276, best_f1=0.9785794813979707\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00040068512316793203\n",
            "step: 10, loss: 0.0031325866002589464\n",
            "step: 20, loss: 0.0002686090883798897\n",
            "step: 30, loss: 0.00012231399887241423\n",
            "step: 40, loss: 0.00820744689553976\n",
            "step: 50, loss: 0.00045598295400850475\n",
            "step: 60, loss: 0.00017875051707960665\n",
            "step: 70, loss: 0.00015169060498010367\n",
            "step: 80, loss: 0.0001886265235953033\n",
            "step: 90, loss: 0.00013061003119219095\n",
            "step: 100, loss: 0.0008418122888542712\n",
            "step: 110, loss: 0.0005435257335193455\n",
            "step: 120, loss: 0.0003113684360869229\n",
            "step: 130, loss: 0.00023573910584673285\n",
            "step: 140, loss: 0.000573948142118752\n",
            "step: 150, loss: 4.65833509224467e-05\n",
            "step: 160, loss: 0.005499429535120726\n",
            "step: 170, loss: 0.000349146721418947\n",
            "step: 180, loss: 0.03462295979261398\n",
            "step: 190, loss: 0.00021338442456908524\n",
            "step: 200, loss: 7.441000343533233e-05\n",
            "step: 210, loss: 0.0010786026250571012\n",
            "step: 220, loss: 0.00014953850768506527\n",
            "step: 230, loss: 0.00023951599723659456\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9876265466816648, f1=0.983050847457627, best_f1=0.983050847457627\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011430263111833483\n",
            "step: 10, loss: 0.00010706399189075455\n",
            "step: 20, loss: 0.00020018426585011184\n",
            "step: 30, loss: 0.00023634533863514662\n",
            "step: 40, loss: 0.0004001825291197747\n",
            "step: 50, loss: 0.00020967585442122072\n",
            "step: 60, loss: 0.00022105930838733912\n",
            "step: 70, loss: 0.0001723864406812936\n",
            "step: 80, loss: 0.0005684954230673611\n",
            "step: 90, loss: 0.00034186517586931586\n",
            "step: 100, loss: 0.0008838950889185071\n",
            "step: 110, loss: 0.00021265377290546894\n",
            "step: 120, loss: 7.324146281462163e-05\n",
            "step: 130, loss: 0.0013132102321833372\n",
            "step: 140, loss: 0.0001380109169986099\n",
            "step: 150, loss: 0.0002712627174332738\n",
            "step: 160, loss: 0.0001601896365173161\n",
            "step: 170, loss: 0.0010273198131471872\n",
            "step: 180, loss: 0.0001387779921060428\n",
            "step: 190, loss: 0.0001077064371202141\n",
            "step: 200, loss: 0.0006498225266113877\n",
            "step: 210, loss: 0.00013825474889017642\n",
            "step: 220, loss: 0.00014094215293880552\n",
            "step: 230, loss: 8.553636871511117e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9853768278965129, f1=0.9819004524886877, best_f1=0.983050847457627\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013541012071073055\n",
            "step: 10, loss: 0.00011475652718218043\n",
            "step: 20, loss: 0.000135682916152291\n",
            "step: 30, loss: 0.00031340529676526785\n",
            "step: 40, loss: 6.461209704866633e-05\n",
            "step: 50, loss: 0.00010024240327766165\n",
            "step: 60, loss: 0.014501716941595078\n",
            "step: 70, loss: 0.00013411165855359286\n",
            "step: 80, loss: 0.00011108470062026754\n",
            "step: 90, loss: 8.796157635515556e-05\n",
            "step: 100, loss: 6.400574784493074e-05\n",
            "step: 110, loss: 0.00011320797784719616\n",
            "step: 120, loss: 0.040704671293497086\n",
            "step: 130, loss: 7.118190114852041e-05\n",
            "step: 140, loss: 0.0001390371035085991\n",
            "step: 150, loss: 0.0003426697221584618\n",
            "step: 160, loss: 0.0001353968254989013\n",
            "step: 170, loss: 5.5468066420871764e-05\n",
            "step: 180, loss: 0.0001150181851699017\n",
            "step: 190, loss: 0.01828501932322979\n",
            "step: 200, loss: 0.00010777825809782371\n",
            "step: 210, loss: 0.00011634820839390159\n",
            "step: 220, loss: 0.004999329801648855\n",
            "step: 230, loss: 0.00013322725135367364\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.984304932735426, f1=0.9807909604519773, best_f1=0.983050847457627\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:14, 171.76it/s]\n",
            "load_f1 = 0.9865168539325843\n",
            "real_f1 = 0.9853768278965129\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:27, 161.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "sW78AaaneEUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "1Hwn5WkZe3Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ac3874-48ef-42c8-823f-b5cf8cdbead9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6220571398735046\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4626280963420868\n",
            "step: 20, loss: 0.26652851700782776\n",
            "step: 30, loss: 0.3468167185783386\n",
            "step: 40, loss: 0.2596467435359955\n",
            "step: 50, loss: 0.20150010287761688\n",
            "step: 60, loss: 0.11904998123645782\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 70, loss: 0.34612521529197693\n",
            "step: 80, loss: 0.1582440882921219\n",
            "step: 90, loss: 0.18110418319702148\n",
            "step: 100, loss: 0.20499184727668762\n",
            "step: 110, loss: 0.14274635910987854\n",
            "step: 120, loss: 0.19262324273586273\n",
            "step: 130, loss: 0.16089962422847748\n",
            "step: 140, loss: 0.1366586834192276\n",
            "step: 150, loss: 0.017306264489889145\n",
            "step: 160, loss: 0.11653464287519455\n",
            "step: 170, loss: 0.10048340260982513\n",
            "step: 180, loss: 0.03785916045308113\n",
            "step: 190, loss: 0.048510368913412094\n",
            "step: 200, loss: 0.07477514445781708\n",
            "step: 210, loss: 0.02336486056447029\n",
            "step: 220, loss: 0.14779475331306458\n",
            "step: 230, loss: 0.1710854172706604\n",
            "step: 240, loss: 0.08538270741701126\n",
            "step: 250, loss: 0.03117193654179573\n",
            "step: 260, loss: 0.09383100271224976\n",
            "step: 270, loss: 0.3879292607307434\n",
            "step: 280, loss: 0.04976237937808037\n",
            "step: 290, loss: 0.03950022533535957\n",
            "step: 300, loss: 0.10343682765960693\n",
            "step: 310, loss: 0.06501466780900955\n",
            "step: 320, loss: 0.08078210800886154\n",
            "step: 330, loss: 0.14627037942409515\n",
            "step: 340, loss: 0.2792011499404907\n",
            "step: 350, loss: 0.06782636791467667\n",
            "step: 360, loss: 0.08378702402114868\n",
            "step: 370, loss: 0.076566182076931\n",
            "step: 380, loss: 0.2617242634296417\n",
            "step: 390, loss: 0.036632537841796875\n",
            "step: 400, loss: 0.04871857166290283\n",
            "step: 410, loss: 0.33778640627861023\n",
            "step: 420, loss: 0.01085746381431818\n",
            "step: 430, loss: 0.032009370625019073\n",
            "step: 440, loss: 0.07187193632125854\n",
            "step: 450, loss: 0.043000727891922\n",
            "step: 460, loss: 0.041885558515787125\n",
            "step: 470, loss: 0.08298017829656601\n",
            "step: 480, loss: 0.1800771802663803\n",
            "step: 490, loss: 0.058772940188646317\n",
            "step: 500, loss: 0.09405848383903503\n",
            "step: 510, loss: 0.054598692804574966\n",
            "step: 520, loss: 0.01721426099538803\n",
            "step: 530, loss: 0.019758479669690132\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.937962962962963, f1=0.9339491916859123, best_f1=0.9339491916859123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022134048864245415\n",
            "step: 10, loss: 0.15479543805122375\n",
            "step: 20, loss: 0.09500069171190262\n",
            "step: 30, loss: 0.020609308034181595\n",
            "step: 40, loss: 0.06937026977539062\n",
            "step: 50, loss: 0.026062849909067154\n",
            "step: 60, loss: 0.021925345063209534\n",
            "step: 70, loss: 0.06280285865068436\n",
            "step: 80, loss: 0.03293908014893532\n",
            "step: 90, loss: 0.004362322855740786\n",
            "step: 100, loss: 0.11475042998790741\n",
            "step: 110, loss: 0.006541657727211714\n",
            "step: 120, loss: 0.0511583611369133\n",
            "step: 130, loss: 0.007238083053380251\n",
            "step: 140, loss: 0.01734072156250477\n",
            "step: 150, loss: 0.004719840362668037\n",
            "step: 160, loss: 0.023563163354992867\n",
            "step: 170, loss: 0.059913016855716705\n",
            "step: 180, loss: 0.029545947909355164\n",
            "step: 190, loss: 0.028042474761605263\n",
            "step: 200, loss: 0.12082647532224655\n",
            "step: 210, loss: 0.023798424750566483\n",
            "step: 220, loss: 0.0015553999692201614\n",
            "step: 230, loss: 0.059583425521850586\n",
            "step: 240, loss: 0.02879883535206318\n",
            "step: 250, loss: 0.03348800539970398\n",
            "step: 260, loss: 0.13612490892410278\n",
            "step: 270, loss: 0.010139341466128826\n",
            "step: 280, loss: 0.03621207922697067\n",
            "step: 290, loss: 0.05053841322660446\n",
            "step: 300, loss: 0.052870530635118484\n",
            "step: 310, loss: 0.023411480709910393\n",
            "step: 320, loss: 0.029206939041614532\n",
            "step: 330, loss: 0.04060026630759239\n",
            "step: 340, loss: 0.12559543550014496\n",
            "step: 350, loss: 0.005786160472780466\n",
            "step: 360, loss: 0.06359905004501343\n",
            "step: 370, loss: 0.010941724292933941\n",
            "step: 380, loss: 0.08406448364257812\n",
            "step: 390, loss: 0.003005056641995907\n",
            "step: 400, loss: 0.10672584921121597\n",
            "step: 410, loss: 0.04738518223166466\n",
            "step: 420, loss: 0.016387278214097023\n",
            "step: 430, loss: 0.18570218980312347\n",
            "step: 440, loss: 0.00525784632191062\n",
            "step: 450, loss: 0.06615812331438065\n",
            "step: 460, loss: 0.09186585992574692\n",
            "step: 470, loss: 0.06490188091993332\n",
            "step: 480, loss: 0.003471317468211055\n",
            "step: 490, loss: 0.03117303177714348\n",
            "step: 500, loss: 0.011477747932076454\n",
            "step: 510, loss: 0.03665812686085701\n",
            "step: 520, loss: 0.25511205196380615\n",
            "step: 530, loss: 0.10194824635982513\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.954060324825986, f1=0.9547553093259464, best_f1=0.9547553093259464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17956344783306122\n",
            "step: 10, loss: 0.09091513603925705\n",
            "step: 20, loss: 0.003375398926436901\n",
            "step: 30, loss: 0.0823824405670166\n",
            "step: 40, loss: 0.06862549483776093\n",
            "step: 50, loss: 0.009791580028831959\n",
            "step: 60, loss: 0.03050249069929123\n",
            "step: 70, loss: 0.008589518256485462\n",
            "step: 80, loss: 0.019843509420752525\n",
            "step: 90, loss: 0.004342060070484877\n",
            "step: 100, loss: 0.016434499993920326\n",
            "step: 110, loss: 0.00935788918286562\n",
            "step: 120, loss: 0.18550170958042145\n",
            "step: 130, loss: 0.04054898023605347\n",
            "step: 140, loss: 0.06226503476500511\n",
            "step: 150, loss: 0.012812057510018349\n",
            "step: 160, loss: 0.0042368280701339245\n",
            "step: 170, loss: 0.0582955926656723\n",
            "step: 180, loss: 0.012628485448658466\n",
            "step: 190, loss: 0.013984227553009987\n",
            "step: 200, loss: 0.018035244196653366\n",
            "step: 210, loss: 0.025990167632699013\n",
            "step: 220, loss: 0.16977155208587646\n",
            "step: 230, loss: 0.13693563640117645\n",
            "step: 240, loss: 0.11155788600444794\n",
            "step: 250, loss: 0.08258332312107086\n",
            "step: 260, loss: 0.146664097905159\n",
            "step: 270, loss: 0.02808353118598461\n",
            "step: 280, loss: 0.06337558478116989\n",
            "step: 290, loss: 0.04025822505354881\n",
            "step: 300, loss: 0.19724339246749878\n",
            "step: 310, loss: 0.0945829376578331\n",
            "step: 320, loss: 0.008044252172112465\n",
            "step: 330, loss: 0.010585475713014603\n",
            "step: 340, loss: 0.015756910666823387\n",
            "step: 350, loss: 0.05784076452255249\n",
            "step: 360, loss: 0.012008203193545341\n",
            "step: 370, loss: 0.04536915197968483\n",
            "step: 380, loss: 0.006822832860052586\n",
            "step: 390, loss: 0.003506760112941265\n",
            "step: 400, loss: 0.11072894185781479\n",
            "step: 410, loss: 0.024643706157803535\n",
            "step: 420, loss: 0.009783321060240269\n",
            "step: 430, loss: 0.048137810081243515\n",
            "step: 440, loss: 0.15968698263168335\n",
            "step: 450, loss: 0.009622008539736271\n",
            "step: 460, loss: 0.0807073786854744\n",
            "step: 470, loss: 0.034710489213466644\n",
            "step: 480, loss: 0.053507495671510696\n",
            "step: 490, loss: 0.04398760944604874\n",
            "step: 500, loss: 0.023576153442263603\n",
            "step: 510, loss: 0.026297202333807945\n",
            "step: 520, loss: 0.06987935304641724\n",
            "step: 530, loss: 0.022321229800581932\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9461966604823748, f1=0.95, best_f1=0.9547553093259464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022596152499318123\n",
            "step: 10, loss: 0.0028470843099057674\n",
            "step: 20, loss: 0.004982605576515198\n",
            "step: 30, loss: 0.1819055825471878\n",
            "step: 40, loss: 0.009557968936860561\n",
            "step: 50, loss: 0.04146171361207962\n",
            "step: 60, loss: 0.003965109121054411\n",
            "step: 70, loss: 0.044732943177223206\n",
            "step: 80, loss: 0.08420481532812119\n",
            "step: 90, loss: 0.0532212071120739\n",
            "step: 100, loss: 0.006689711939543486\n",
            "step: 110, loss: 0.02845119498670101\n",
            "step: 120, loss: 0.003971385303884745\n",
            "step: 130, loss: 0.05038256198167801\n",
            "step: 140, loss: 0.09192594885826111\n",
            "step: 150, loss: 0.016527103260159492\n",
            "step: 160, loss: 0.03899814933538437\n",
            "step: 170, loss: 0.04731959477066994\n",
            "step: 180, loss: 0.11199651658535004\n",
            "step: 190, loss: 0.026843639090657234\n",
            "step: 200, loss: 0.010044220834970474\n",
            "step: 210, loss: 0.0013900598278269172\n",
            "step: 220, loss: 0.030657587572932243\n",
            "step: 230, loss: 0.003312179585918784\n",
            "step: 240, loss: 0.08573035895824432\n",
            "step: 250, loss: 0.0676264837384224\n",
            "step: 260, loss: 0.017920570448040962\n",
            "step: 270, loss: 0.10164779424667358\n",
            "step: 280, loss: 0.02035265974700451\n",
            "step: 290, loss: 0.004839980974793434\n",
            "step: 300, loss: 0.001939311041496694\n",
            "step: 310, loss: 0.002564616734161973\n",
            "step: 320, loss: 0.05529644712805748\n",
            "step: 330, loss: 0.01869182474911213\n",
            "step: 340, loss: 0.005219633225351572\n",
            "step: 350, loss: 0.16672073304653168\n",
            "step: 360, loss: 0.09591741859912872\n",
            "step: 370, loss: 0.004411250352859497\n",
            "step: 380, loss: 0.03694285452365875\n",
            "step: 390, loss: 0.0012686483096331358\n",
            "step: 400, loss: 0.004529115278273821\n",
            "step: 410, loss: 0.0018910749349743128\n",
            "step: 420, loss: 0.07530365884304047\n",
            "step: 430, loss: 0.02922820672392845\n",
            "step: 440, loss: 0.0030367542058229446\n",
            "step: 450, loss: 0.009548122994601727\n",
            "step: 460, loss: 0.02021385356783867\n",
            "step: 470, loss: 0.0027961714658886194\n",
            "step: 480, loss: 0.0067482562735676765\n",
            "step: 490, loss: 0.0320211723446846\n",
            "step: 500, loss: 0.005261437967419624\n",
            "step: 510, loss: 0.020170124247670174\n",
            "step: 520, loss: 0.05375562608242035\n",
            "step: 530, loss: 0.08134731650352478\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9510032664489033, f1=0.9471733086190918, best_f1=0.9547553093259464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0034798127599060535\n",
            "step: 10, loss: 0.011980244889855385\n",
            "step: 20, loss: 0.005382912699133158\n",
            "step: 30, loss: 0.010982303880155087\n",
            "step: 40, loss: 0.0010129421716555953\n",
            "step: 50, loss: 0.17500527203083038\n",
            "step: 60, loss: 0.025476107373833656\n",
            "step: 70, loss: 0.0020603490993380547\n",
            "step: 80, loss: 0.005925409961491823\n",
            "step: 90, loss: 0.12165307253599167\n",
            "step: 100, loss: 0.056351255625486374\n",
            "step: 110, loss: 0.0028212612960487604\n",
            "step: 120, loss: 0.14273281395435333\n",
            "step: 130, loss: 0.007828240282833576\n",
            "step: 140, loss: 0.04605681821703911\n",
            "step: 150, loss: 0.06982804834842682\n",
            "step: 160, loss: 0.05199345201253891\n",
            "step: 170, loss: 0.10812518000602722\n",
            "step: 180, loss: 0.04167800024151802\n",
            "step: 190, loss: 0.028298290446400642\n",
            "step: 200, loss: 0.019513361155986786\n",
            "step: 210, loss: 0.005004600156098604\n",
            "step: 220, loss: 0.002010623924434185\n",
            "step: 230, loss: 0.0038463156670331955\n",
            "step: 240, loss: 0.00195152941159904\n",
            "step: 250, loss: 0.08381538093090057\n",
            "step: 260, loss: 0.004028339870274067\n",
            "step: 270, loss: 0.03849392384290695\n",
            "step: 280, loss: 0.0026131970807909966\n",
            "step: 290, loss: 0.0450429692864418\n",
            "step: 300, loss: 0.0209529846906662\n",
            "step: 310, loss: 0.04975868761539459\n",
            "step: 320, loss: 0.007493270561099052\n",
            "step: 330, loss: 0.08817914873361588\n",
            "step: 340, loss: 0.011285820044577122\n",
            "step: 350, loss: 0.0032367261592298746\n",
            "step: 360, loss: 0.000617857847828418\n",
            "step: 370, loss: 0.0004746159538626671\n",
            "step: 380, loss: 0.0014326982200145721\n",
            "step: 390, loss: 0.001196464290842414\n",
            "step: 400, loss: 0.01938738115131855\n",
            "step: 410, loss: 0.13612079620361328\n",
            "step: 420, loss: 0.1645815372467041\n",
            "step: 430, loss: 0.0027388387825340033\n",
            "step: 440, loss: 0.0016234948998317122\n",
            "step: 450, loss: 0.027462782338261604\n",
            "step: 460, loss: 0.19566111266613007\n",
            "step: 470, loss: 0.011519302614033222\n",
            "step: 480, loss: 0.028685076162219048\n",
            "step: 490, loss: 0.011737039312720299\n",
            "step: 500, loss: 0.0064319586381316185\n",
            "step: 510, loss: 0.004835622385144234\n",
            "step: 520, loss: 0.16866004467010498\n",
            "step: 530, loss: 0.06245416775345802\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9536178107606679, f1=0.9479981592268752, best_f1=0.9547553093259464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03516734391450882\n",
            "step: 10, loss: 0.00422839168459177\n",
            "step: 20, loss: 0.019729051738977432\n",
            "step: 30, loss: 0.0012632700381800532\n",
            "step: 40, loss: 0.042167939245700836\n",
            "step: 50, loss: 0.0015143216587603092\n",
            "step: 60, loss: 0.0396009162068367\n",
            "step: 70, loss: 0.0009823995642364025\n",
            "step: 80, loss: 0.0012967631919309497\n",
            "step: 90, loss: 0.043218113481998444\n",
            "step: 100, loss: 0.0013564530527219176\n",
            "step: 110, loss: 0.004257440101355314\n",
            "step: 120, loss: 0.025953633710741997\n",
            "step: 130, loss: 0.010816952213644981\n",
            "step: 140, loss: 0.0015460068825632334\n",
            "step: 150, loss: 0.00220690225251019\n",
            "step: 160, loss: 0.012583786621689796\n",
            "step: 170, loss: 0.0019876817241311073\n",
            "step: 180, loss: 0.07078836113214493\n",
            "step: 190, loss: 0.03999420627951622\n",
            "step: 200, loss: 0.002288335934281349\n",
            "step: 210, loss: 0.002602630527690053\n",
            "step: 220, loss: 0.02637449838221073\n",
            "step: 230, loss: 0.004642650950700045\n",
            "step: 240, loss: 0.04653489962220192\n",
            "step: 250, loss: 0.026980943977832794\n",
            "step: 260, loss: 0.0221877284348011\n",
            "step: 270, loss: 0.004451894201338291\n",
            "step: 280, loss: 0.0022932186257094145\n",
            "step: 290, loss: 0.05008920282125473\n",
            "step: 300, loss: 0.0024966192431747913\n",
            "step: 310, loss: 0.11174637079238892\n",
            "step: 320, loss: 0.0008631569216959178\n",
            "step: 330, loss: 0.0014753342838957906\n",
            "step: 340, loss: 0.001012918190099299\n",
            "step: 350, loss: 0.014894981868565083\n",
            "step: 360, loss: 0.006771457847207785\n",
            "step: 370, loss: 0.011699521914124489\n",
            "step: 380, loss: 0.0005355894099920988\n",
            "step: 390, loss: 0.008412335999310017\n",
            "step: 400, loss: 0.00040215003537014127\n",
            "step: 410, loss: 0.000543381494935602\n",
            "step: 420, loss: 0.019122034311294556\n",
            "step: 430, loss: 0.002988958265632391\n",
            "step: 440, loss: 0.0011750375851988792\n",
            "step: 450, loss: 0.11954040825366974\n",
            "step: 460, loss: 0.002261321060359478\n",
            "step: 470, loss: 0.0031018482986837626\n",
            "step: 480, loss: 0.014264988712966442\n",
            "step: 490, loss: 0.013276626355946064\n",
            "step: 500, loss: 0.003492870833724737\n",
            "step: 510, loss: 0.12209953367710114\n",
            "step: 520, loss: 0.0064332690089941025\n",
            "step: 530, loss: 0.0017771226121112704\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9493610979649787, f1=0.9456928838951311, best_f1=0.9547553093259464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001906294608488679\n",
            "step: 10, loss: 0.001226037391461432\n",
            "step: 20, loss: 0.004945417866110802\n",
            "step: 30, loss: 0.006244598422199488\n",
            "step: 40, loss: 0.009027059189975262\n",
            "step: 50, loss: 0.0074426899664103985\n",
            "step: 60, loss: 0.041007377207279205\n",
            "step: 70, loss: 0.024951757863163948\n",
            "step: 80, loss: 0.002049219561740756\n",
            "step: 90, loss: 0.006948438007384539\n",
            "step: 100, loss: 0.0024326166603714228\n",
            "step: 110, loss: 0.0008087399764917791\n",
            "step: 120, loss: 0.0005095581873320043\n",
            "step: 130, loss: 0.00035754419513978064\n",
            "step: 140, loss: 0.0012159424368292093\n",
            "step: 150, loss: 0.0007652039057575166\n",
            "step: 160, loss: 0.0016473665600642562\n",
            "step: 170, loss: 0.005938410758972168\n",
            "step: 180, loss: 0.10929665714502335\n",
            "step: 190, loss: 0.017832593992352486\n",
            "step: 200, loss: 0.001366956508718431\n",
            "step: 210, loss: 0.0074947490356862545\n",
            "step: 220, loss: 0.0007162687252275646\n",
            "step: 230, loss: 0.0005807594861835241\n",
            "step: 240, loss: 0.0013653385685756803\n",
            "step: 250, loss: 0.0013915820745751262\n",
            "step: 260, loss: 0.0016216732328757644\n",
            "step: 270, loss: 0.0012982861371710896\n",
            "step: 280, loss: 0.0027467645704746246\n",
            "step: 290, loss: 0.0026802755892276764\n",
            "step: 300, loss: 0.000506006006617099\n",
            "step: 310, loss: 0.0011124603915959597\n",
            "step: 320, loss: 0.01208737026900053\n",
            "step: 330, loss: 0.0002428942098049447\n",
            "step: 340, loss: 0.0011774091981351376\n",
            "step: 350, loss: 0.0013082429068163037\n",
            "step: 360, loss: 0.04738859459757805\n",
            "step: 370, loss: 0.0033760936930775642\n",
            "step: 380, loss: 0.009041862562298775\n",
            "step: 390, loss: 0.0072297994047403336\n",
            "step: 400, loss: 0.015507875010371208\n",
            "step: 410, loss: 0.0016729366034269333\n",
            "step: 420, loss: 0.007174242287874222\n",
            "step: 430, loss: 0.005366507451981306\n",
            "step: 440, loss: 0.000958563934545964\n",
            "step: 450, loss: 0.0021475614048540592\n",
            "step: 460, loss: 0.0007892265566624701\n",
            "step: 470, loss: 0.12394203245639801\n",
            "step: 480, loss: 0.002446886384859681\n",
            "step: 490, loss: 0.002236337633803487\n",
            "step: 500, loss: 0.0016832235269248486\n",
            "step: 510, loss: 0.0006403324659913778\n",
            "step: 520, loss: 0.0014836677582934499\n",
            "step: 530, loss: 0.0014191963709890842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9463148316651503, f1=0.9481346678798908, best_f1=0.9547553093259464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1565418392419815\n",
            "step: 10, loss: 0.059413451701402664\n",
            "step: 20, loss: 0.013411137275397778\n",
            "step: 30, loss: 0.003965911455452442\n",
            "step: 40, loss: 0.0002426074497634545\n",
            "step: 50, loss: 0.0014605928445234895\n",
            "step: 60, loss: 0.0008020767709240317\n",
            "step: 70, loss: 0.0017125883605331182\n",
            "step: 80, loss: 0.0017815626924857497\n",
            "step: 90, loss: 0.001317458925768733\n",
            "step: 100, loss: 0.002315725199878216\n",
            "step: 110, loss: 0.0006078488077037036\n",
            "step: 120, loss: 0.005550189409404993\n",
            "step: 130, loss: 0.0008765104576013982\n",
            "step: 140, loss: 0.0005289882537908852\n",
            "step: 150, loss: 0.0010677992831915617\n",
            "step: 160, loss: 0.1209186539053917\n",
            "step: 170, loss: 0.06054610759019852\n",
            "step: 180, loss: 0.002580101601779461\n",
            "step: 190, loss: 0.04683450609445572\n",
            "step: 200, loss: 0.006283028982579708\n",
            "step: 210, loss: 0.10376600921154022\n",
            "step: 220, loss: 0.003586444305256009\n",
            "step: 230, loss: 0.043931107968091965\n",
            "step: 240, loss: 0.10287030786275864\n",
            "step: 250, loss: 0.004077350255101919\n",
            "step: 260, loss: 0.0010085708927363157\n",
            "step: 270, loss: 0.015471384860575199\n",
            "step: 280, loss: 0.00028579786885529757\n",
            "step: 290, loss: 0.01366041973233223\n",
            "step: 300, loss: 0.00043416942935436964\n",
            "step: 310, loss: 0.003104287665337324\n",
            "step: 320, loss: 0.0026238851714879274\n",
            "step: 330, loss: 0.0009938105940818787\n",
            "step: 340, loss: 0.12253013998270035\n",
            "step: 350, loss: 0.0006505337660200894\n",
            "step: 360, loss: 0.022020040079951286\n",
            "step: 370, loss: 0.0033797358628362417\n",
            "step: 380, loss: 0.0015712715685367584\n",
            "step: 390, loss: 0.08217630535364151\n",
            "step: 400, loss: 0.00288764457218349\n",
            "step: 410, loss: 0.042402077466249466\n",
            "step: 420, loss: 0.001964577939361334\n",
            "step: 430, loss: 0.00398401590064168\n",
            "step: 440, loss: 0.04412742331624031\n",
            "step: 450, loss: 0.002013010438531637\n",
            "step: 460, loss: 0.005884903483092785\n",
            "step: 470, loss: 0.0866934061050415\n",
            "step: 480, loss: 0.006198082119226456\n",
            "step: 490, loss: 0.056316327303647995\n",
            "step: 500, loss: 0.004961267579346895\n",
            "step: 510, loss: 0.03961942344903946\n",
            "step: 520, loss: 0.0009605613886378706\n",
            "step: 530, loss: 0.03632698953151703\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9483960948396094, f1=0.9443166129774505, best_f1=0.9547553093259464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012928701471537352\n",
            "step: 10, loss: 0.016651207581162453\n",
            "step: 20, loss: 0.0004356523568276316\n",
            "step: 30, loss: 0.08264881372451782\n",
            "step: 40, loss: 0.0005628377548418939\n",
            "step: 50, loss: 0.009179978631436825\n",
            "step: 60, loss: 0.00420387415215373\n",
            "step: 70, loss: 0.0007739194552414119\n",
            "step: 80, loss: 0.046750858426094055\n",
            "step: 90, loss: 0.07495266199111938\n",
            "step: 100, loss: 0.0020660972222685814\n",
            "step: 110, loss: 0.018238786607980728\n",
            "step: 120, loss: 0.0009232641314156353\n",
            "step: 130, loss: 0.0005017434596084058\n",
            "step: 140, loss: 0.000580216059461236\n",
            "step: 150, loss: 0.0034552672877907753\n",
            "step: 160, loss: 0.010476961731910706\n",
            "step: 170, loss: 0.005780437029898167\n",
            "step: 180, loss: 0.0008135979878716171\n",
            "step: 190, loss: 0.00029445980908349156\n",
            "step: 200, loss: 0.00032859580824151635\n",
            "step: 210, loss: 0.0018935713451355696\n",
            "step: 220, loss: 0.022949770092964172\n",
            "step: 230, loss: 0.00022625195560976863\n",
            "step: 240, loss: 0.000181771902134642\n",
            "step: 250, loss: 0.000539391883648932\n",
            "step: 260, loss: 0.0024860436096787453\n",
            "step: 270, loss: 0.0026262367609888315\n",
            "step: 280, loss: 0.008486185222864151\n",
            "step: 290, loss: 0.0029886802658438683\n",
            "step: 300, loss: 0.0002775181201286614\n",
            "step: 310, loss: 0.33080366253852844\n",
            "step: 320, loss: 0.00036292997538112104\n",
            "step: 330, loss: 0.0011350803542882204\n",
            "step: 340, loss: 0.009422248229384422\n",
            "step: 350, loss: 0.013976921327412128\n",
            "step: 360, loss: 0.0005667972727678716\n",
            "step: 370, loss: 0.0004918525810353458\n",
            "step: 380, loss: 0.0035950487945228815\n",
            "step: 390, loss: 8.838673966238275e-05\n",
            "step: 400, loss: 0.08445454388856888\n",
            "step: 410, loss: 0.15739363431930542\n",
            "step: 420, loss: 0.00120343582239002\n",
            "step: 430, loss: 0.004156353883445263\n",
            "step: 440, loss: 0.008120032027363777\n",
            "step: 450, loss: 0.08441192656755447\n",
            "step: 460, loss: 0.003559407312422991\n",
            "step: 470, loss: 0.0012232217704877257\n",
            "step: 480, loss: 0.003307420527562499\n",
            "step: 490, loss: 0.006463410798460245\n",
            "step: 500, loss: 0.03752698749303818\n",
            "step: 510, loss: 0.00439953338354826\n",
            "step: 520, loss: 0.005391254555433989\n",
            "step: 530, loss: 0.03669652342796326\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9496268656716418, f1=0.9432558139534885, best_f1=0.9547553093259464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005709010292775929\n",
            "step: 10, loss: 0.0010689503978937864\n",
            "step: 20, loss: 0.0005848311702720821\n",
            "step: 30, loss: 0.0070200711488723755\n",
            "step: 40, loss: 0.005448784679174423\n",
            "step: 50, loss: 0.003107762197032571\n",
            "step: 60, loss: 0.0050271255895495415\n",
            "step: 70, loss: 0.001714398036710918\n",
            "step: 80, loss: 0.0008119508856907487\n",
            "step: 90, loss: 0.0041663614101707935\n",
            "step: 100, loss: 0.0017075338400900364\n",
            "step: 110, loss: 0.02083691395819187\n",
            "step: 120, loss: 0.0003212395531591028\n",
            "step: 130, loss: 0.00011241757601965219\n",
            "step: 140, loss: 0.03181580826640129\n",
            "step: 150, loss: 0.0007825085194781423\n",
            "step: 160, loss: 0.02439050003886223\n",
            "step: 170, loss: 0.00035619703703559935\n",
            "step: 180, loss: 0.0016792549286037683\n",
            "step: 190, loss: 0.002142292447388172\n",
            "step: 200, loss: 0.0004963835817761719\n",
            "step: 210, loss: 0.0017057204386219382\n",
            "step: 220, loss: 0.0036179067101329565\n",
            "step: 230, loss: 0.00040206831181421876\n",
            "step: 240, loss: 0.0015036858385428786\n",
            "step: 250, loss: 0.00969681702554226\n",
            "step: 260, loss: 0.005439200438559055\n",
            "step: 270, loss: 0.024820297956466675\n",
            "step: 280, loss: 0.003966299816966057\n",
            "step: 290, loss: 0.002805423690006137\n",
            "step: 300, loss: 0.014111703261733055\n",
            "step: 310, loss: 0.041178688406944275\n",
            "step: 320, loss: 0.05687764286994934\n",
            "step: 330, loss: 0.012340332381427288\n",
            "step: 340, loss: 0.004189128056168556\n",
            "step: 350, loss: 0.008075069636106491\n",
            "step: 360, loss: 4.865694063482806e-05\n",
            "step: 370, loss: 0.0003922129690181464\n",
            "step: 380, loss: 0.01879173144698143\n",
            "step: 390, loss: 4.8841317038750276e-05\n",
            "step: 400, loss: 0.0021038162522017956\n",
            "step: 410, loss: 0.006915663368999958\n",
            "step: 420, loss: 0.014834277331829071\n",
            "step: 430, loss: 0.0009260872611775994\n",
            "step: 440, loss: 0.00030742710805498064\n",
            "step: 450, loss: 0.0007610578904859722\n",
            "step: 460, loss: 0.004952242597937584\n",
            "step: 470, loss: 0.004858986474573612\n",
            "step: 480, loss: 0.0021205253433436155\n",
            "step: 490, loss: 0.021071085706353188\n",
            "step: 500, loss: 0.0016197210643440485\n",
            "step: 510, loss: 0.00021093557006679475\n",
            "step: 520, loss: 0.0220600888133049\n",
            "step: 530, loss: 0.005117427092045546\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9558344955834496, f1=0.9458218549127642, best_f1=0.9458218549127642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016439781757071614\n",
            "step: 10, loss: 0.0003914895060006529\n",
            "step: 20, loss: 0.0004794469859916717\n",
            "step: 30, loss: 0.0028746335301548243\n",
            "step: 40, loss: 0.00036443761200644076\n",
            "step: 50, loss: 0.07610417902469635\n",
            "step: 60, loss: 0.0011132435174658895\n",
            "step: 70, loss: 0.004063639789819717\n",
            "step: 80, loss: 0.003784486558288336\n",
            "step: 90, loss: 0.0012163665378466249\n",
            "step: 100, loss: 0.006594839505851269\n",
            "step: 110, loss: 0.0002408490254310891\n",
            "step: 120, loss: 0.00021243453375063837\n",
            "step: 130, loss: 0.004180889576673508\n",
            "step: 140, loss: 0.00014966129674576223\n",
            "step: 150, loss: 0.0002731101121753454\n",
            "step: 160, loss: 0.05095764994621277\n",
            "step: 170, loss: 0.007597845979034901\n",
            "step: 180, loss: 0.0002220384922111407\n",
            "step: 190, loss: 0.0002188889484386891\n",
            "step: 200, loss: 0.005151016637682915\n",
            "step: 210, loss: 0.002614547498524189\n",
            "step: 220, loss: 0.13640643656253815\n",
            "step: 230, loss: 0.0015495165716856718\n",
            "step: 240, loss: 0.0010790093801915646\n",
            "step: 250, loss: 0.004688588436692953\n",
            "step: 260, loss: 0.03609711304306984\n",
            "step: 270, loss: 0.0019691495690494776\n",
            "step: 280, loss: 0.0018233164446428418\n",
            "step: 290, loss: 0.0036904867738485336\n",
            "step: 300, loss: 0.0002314830489922315\n",
            "step: 310, loss: 0.024980325251817703\n",
            "step: 320, loss: 0.003821001620963216\n",
            "step: 330, loss: 3.786236993619241e-05\n",
            "step: 340, loss: 0.0074603664688766\n",
            "step: 350, loss: 0.00028261254192329943\n",
            "step: 360, loss: 0.0010572158498689532\n",
            "step: 370, loss: 0.0021233835723251104\n",
            "step: 380, loss: 0.0013037690659984946\n",
            "step: 390, loss: 0.0015027327463030815\n",
            "step: 400, loss: 0.00010335664410376921\n",
            "step: 410, loss: 0.0029785025399178267\n",
            "step: 420, loss: 0.0012662550434470177\n",
            "step: 430, loss: 0.006708802189677954\n",
            "step: 440, loss: 0.001018901588395238\n",
            "step: 450, loss: 0.001540318364277482\n",
            "step: 460, loss: 0.0003015294496435672\n",
            "step: 470, loss: 0.0012930243974551558\n",
            "step: 480, loss: 0.002696006093174219\n",
            "step: 490, loss: 0.0007854228024370968\n",
            "step: 500, loss: 0.0018123614136129618\n",
            "step: 510, loss: 0.0005118005210533738\n",
            "step: 520, loss: 0.000841824512463063\n",
            "step: 530, loss: 0.06837733089923859\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9508348794063081, f1=0.9448529411764707, best_f1=0.9458218549127642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000624591950327158\n",
            "step: 10, loss: 0.008647632785141468\n",
            "step: 20, loss: 0.001586451893672347\n",
            "step: 30, loss: 0.015103642828762531\n",
            "step: 40, loss: 0.05012885108590126\n",
            "step: 50, loss: 0.00021987977379467338\n",
            "step: 60, loss: 0.0004964562249369919\n",
            "step: 70, loss: 0.01104719191789627\n",
            "step: 80, loss: 0.0008629172225482762\n",
            "step: 90, loss: 0.03172438591718674\n",
            "step: 100, loss: 0.02026944048702717\n",
            "step: 110, loss: 0.0001529379078419879\n",
            "step: 120, loss: 0.0005139141576364636\n",
            "step: 130, loss: 0.0008794312598183751\n",
            "step: 140, loss: 6.829834455857053e-05\n",
            "step: 150, loss: 0.000943389255553484\n",
            "step: 160, loss: 0.0012610235717147589\n",
            "step: 170, loss: 0.0004164091369602829\n",
            "step: 180, loss: 0.006197658833116293\n",
            "step: 190, loss: 0.0018614373402670026\n",
            "step: 200, loss: 0.0003528919187374413\n",
            "step: 210, loss: 9.145514013653155e-06\n",
            "step: 220, loss: 0.0011119995033368468\n",
            "step: 230, loss: 5.896097354707308e-05\n",
            "step: 240, loss: 0.00044789613457396626\n",
            "step: 250, loss: 0.004048128146678209\n",
            "step: 260, loss: 0.00013035252050030977\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 270, loss: 0.002008466050028801\n",
            "step: 280, loss: 0.0010777334682643414\n",
            "step: 290, loss: 0.001029622508212924\n",
            "step: 300, loss: 0.0011699422029778361\n",
            "step: 310, loss: 0.0006424372550100088\n",
            "step: 320, loss: 0.0014755059964954853\n",
            "step: 330, loss: 0.0014346682000905275\n",
            "step: 340, loss: 0.0009914010297507048\n",
            "step: 350, loss: 3.6565939808497205e-05\n",
            "step: 360, loss: 6.173520523589104e-05\n",
            "step: 370, loss: 8.199031435651705e-05\n",
            "step: 380, loss: 0.0007370047969743609\n",
            "step: 390, loss: 0.0024229642003774643\n",
            "step: 400, loss: 3.515643402351998e-05\n",
            "step: 410, loss: 6.203104567248374e-05\n",
            "step: 420, loss: 0.00014095334336161613\n",
            "step: 430, loss: 8.322314533870667e-05\n",
            "step: 440, loss: 0.003780281636863947\n",
            "step: 450, loss: 0.002567332237958908\n",
            "step: 460, loss: 0.00011377298505976796\n",
            "step: 470, loss: 0.002898464910686016\n",
            "step: 480, loss: 0.0003913140099029988\n",
            "step: 490, loss: 0.01640947535634041\n",
            "step: 500, loss: 0.004186353180557489\n",
            "step: 510, loss: 0.020837558433413506\n",
            "step: 520, loss: 0.0011257166042923927\n",
            "step: 530, loss: 0.11627226322889328\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9512419503219871, f1=0.9495225102319236, best_f1=0.9458218549127642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006505689234472811\n",
            "step: 10, loss: 0.0006977349985390902\n",
            "step: 20, loss: 0.0003907065838575363\n",
            "step: 30, loss: 0.00011696215369738638\n",
            "step: 40, loss: 0.0010318155400454998\n",
            "step: 50, loss: 0.005226987414062023\n",
            "step: 60, loss: 0.00034906930522993207\n",
            "step: 70, loss: 0.011901194229722023\n",
            "step: 80, loss: 0.0004999330849386752\n",
            "step: 90, loss: 0.00046235238551162183\n",
            "step: 100, loss: 9.089165541809052e-05\n",
            "step: 110, loss: 0.0015975510468706489\n",
            "step: 120, loss: 0.07574913650751114\n",
            "step: 130, loss: 0.00038687250344082713\n",
            "step: 140, loss: 0.0007179511594586074\n",
            "step: 150, loss: 0.0015868607442826033\n",
            "step: 160, loss: 0.00033184251515194774\n",
            "step: 170, loss: 0.007836990989744663\n",
            "step: 180, loss: 0.002902337582781911\n",
            "step: 190, loss: 0.001974688144400716\n",
            "step: 200, loss: 0.00033332881866954267\n",
            "step: 210, loss: 2.4249078705906868e-05\n",
            "step: 220, loss: 0.0013399762101471424\n",
            "step: 230, loss: 0.0025449609383940697\n",
            "step: 240, loss: 0.0009452661033719778\n",
            "step: 250, loss: 0.0009683871758170426\n",
            "step: 260, loss: 0.00020418927306309342\n",
            "step: 270, loss: 0.010058343410491943\n",
            "step: 280, loss: 0.0010034908773377538\n",
            "step: 290, loss: 0.002223049756139517\n",
            "step: 300, loss: 0.00018128483498003334\n",
            "step: 310, loss: 0.00035597910755313933\n",
            "step: 320, loss: 0.0007305790786631405\n",
            "step: 330, loss: 0.00018105968774762005\n",
            "step: 340, loss: 0.0014498198870569468\n",
            "step: 350, loss: 0.0004764416953548789\n",
            "step: 360, loss: 0.09226123988628387\n",
            "step: 370, loss: 0.0005311286076903343\n",
            "step: 380, loss: 0.0004367568762972951\n",
            "step: 390, loss: 0.0004754708206746727\n",
            "step: 400, loss: 0.00022644612181466073\n",
            "step: 410, loss: 0.00017450182349421084\n",
            "step: 420, loss: 0.0013809505617246032\n",
            "step: 430, loss: 0.00017638357530813664\n",
            "step: 440, loss: 6.618099723709747e-05\n",
            "step: 450, loss: 0.001152503420598805\n",
            "step: 460, loss: 0.0012249370338395238\n",
            "step: 470, loss: 0.003598256967961788\n",
            "step: 480, loss: 1.4449809896177612e-05\n",
            "step: 490, loss: 7.575419294880703e-05\n",
            "step: 500, loss: 0.0015460848808288574\n",
            "step: 510, loss: 9.781679545994848e-05\n",
            "step: 520, loss: 0.00010843863128684461\n",
            "step: 530, loss: 2.1378527890192345e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9536607970342911, f1=0.9471766848816029, best_f1=0.9458218549127642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001652817358262837\n",
            "step: 10, loss: 0.005978011060506105\n",
            "step: 20, loss: 0.00011433332110755146\n",
            "step: 30, loss: 0.010863921605050564\n",
            "step: 40, loss: 0.0004823802155442536\n",
            "step: 50, loss: 0.002841286826878786\n",
            "step: 60, loss: 0.0017845446709543467\n",
            "step: 70, loss: 0.001105502131395042\n",
            "step: 80, loss: 0.00017298395687248558\n",
            "step: 90, loss: 4.5912081986898556e-05\n",
            "step: 100, loss: 0.00016160329687409103\n",
            "step: 110, loss: 3.734184429049492e-05\n",
            "step: 120, loss: 1.2363562746031675e-05\n",
            "step: 130, loss: 4.328958675614558e-05\n",
            "step: 140, loss: 0.0019504689844325185\n",
            "step: 150, loss: 0.0008165334002114832\n",
            "step: 160, loss: 0.0001196166267618537\n",
            "step: 170, loss: 0.014092711731791496\n",
            "step: 180, loss: 0.0007403240306302905\n",
            "step: 190, loss: 0.0005914652138017118\n",
            "step: 200, loss: 6.75782939651981e-05\n",
            "step: 210, loss: 0.006513470783829689\n",
            "step: 220, loss: 0.00010298299457645044\n",
            "step: 230, loss: 9.66946390690282e-05\n",
            "step: 240, loss: 0.0009150300757028162\n",
            "step: 250, loss: 0.0025746189057826996\n",
            "step: 260, loss: 0.0017730611143633723\n",
            "step: 270, loss: 0.0027364776469767094\n",
            "step: 280, loss: 0.00029093792545609176\n",
            "step: 290, loss: 4.9993948778137565e-05\n",
            "step: 300, loss: 1.5194605111901183e-05\n",
            "step: 310, loss: 0.0002776231849566102\n",
            "step: 320, loss: 2.0464078261284158e-05\n",
            "step: 330, loss: 0.0013580442173406482\n",
            "step: 340, loss: 0.0010050712153315544\n",
            "step: 350, loss: 0.0003335922956466675\n",
            "step: 360, loss: 0.0013340098084881902\n",
            "step: 370, loss: 0.0010710321366786957\n",
            "step: 380, loss: 0.0007561203674413264\n",
            "step: 390, loss: 0.00044269353384152055\n",
            "step: 400, loss: 0.0010425535729154944\n",
            "step: 410, loss: 0.0017983198631554842\n",
            "step: 420, loss: 0.007050951011478901\n",
            "step: 430, loss: 0.00987337902188301\n",
            "step: 440, loss: 0.00931684672832489\n",
            "step: 450, loss: 2.8179283617646433e-05\n",
            "step: 460, loss: 0.010247163474559784\n",
            "step: 470, loss: 4.634244305634638e-06\n",
            "step: 480, loss: 0.0012928873766213655\n",
            "step: 490, loss: 0.005279637407511473\n",
            "step: 500, loss: 0.006765799596905708\n",
            "step: 510, loss: 0.040243007242679596\n",
            "step: 520, loss: 0.00016242085257545114\n",
            "step: 530, loss: 0.007159125525504351\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9526462395543175, f1=0.948905109489051, best_f1=0.9458218549127642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004819262831006199\n",
            "step: 10, loss: 0.0025471432600170374\n",
            "step: 20, loss: 0.0020879157818853855\n",
            "step: 30, loss: 0.008020568639039993\n",
            "step: 40, loss: 1.297046856052475e-05\n",
            "step: 50, loss: 0.0021922134328633547\n",
            "step: 60, loss: 0.005796506069600582\n",
            "step: 70, loss: 0.0003122312482446432\n",
            "step: 80, loss: 0.000302230182569474\n",
            "step: 90, loss: 0.009818480350077152\n",
            "step: 100, loss: 5.3510280849877745e-05\n",
            "step: 110, loss: 0.0015000508865341544\n",
            "step: 120, loss: 5.263774801278487e-05\n",
            "step: 130, loss: 0.0002056249650195241\n",
            "step: 140, loss: 0.0042764111422002316\n",
            "step: 150, loss: 0.0003001132281497121\n",
            "step: 160, loss: 9.743366535985842e-05\n",
            "step: 170, loss: 2.488955760782119e-05\n",
            "step: 180, loss: 0.03493109345436096\n",
            "step: 190, loss: 0.0017158306436613202\n",
            "step: 200, loss: 0.001339470618404448\n",
            "step: 210, loss: 0.0007789809606038034\n",
            "step: 220, loss: 1.761884050210938e-05\n",
            "step: 230, loss: 0.0010833494598045945\n",
            "step: 240, loss: 7.208247552625835e-05\n",
            "step: 250, loss: 0.0002889104653149843\n",
            "step: 260, loss: 2.9916654966655187e-05\n",
            "step: 270, loss: 0.0007281119469553232\n",
            "step: 280, loss: 8.95677221706137e-05\n",
            "step: 290, loss: 0.0013063537189736962\n",
            "step: 300, loss: 2.1536694475798868e-05\n",
            "step: 310, loss: 0.045264311134815216\n",
            "step: 320, loss: 2.137097362719942e-05\n",
            "step: 330, loss: 0.0002801692171487957\n",
            "step: 340, loss: 5.99514132773038e-05\n",
            "step: 350, loss: 0.001502355094999075\n",
            "step: 360, loss: 9.006182517623529e-05\n",
            "step: 370, loss: 0.10051305592060089\n",
            "step: 380, loss: 0.0006785495788790286\n",
            "step: 390, loss: 0.00013380771270021796\n",
            "step: 400, loss: 0.004219475202262402\n",
            "step: 410, loss: 0.0002666882937774062\n",
            "step: 420, loss: 0.0001483502855990082\n",
            "step: 430, loss: 4.057129262946546e-05\n",
            "step: 440, loss: 0.002719244919717312\n",
            "step: 450, loss: 0.00516197644174099\n",
            "step: 460, loss: 0.06513040512800217\n",
            "step: 470, loss: 0.00012123293709009886\n",
            "step: 480, loss: 0.0031375782564282417\n",
            "step: 490, loss: 0.004530301317572594\n",
            "step: 500, loss: 0.02110281027853489\n",
            "step: 510, loss: 0.0002836326602846384\n",
            "step: 520, loss: 3.830116111203097e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 530, loss: 1.4245149941416457e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9524688509460083, f1=0.9490445859872612, best_f1=0.9458218549127642\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:27, 206.79it/s]\n",
            "load_f1 = 0.9547553093259464\n",
            "real_f1 = 0.9500687127805771\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:25, 171.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE"
      ],
      "metadata": {
        "id": "U6rVRw-HgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE STRUCTURED"
      ],
      "metadata": {
        "id": "3ifpsOJMgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7bDM3EgNFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09b9ae88-3185-46ad-ed5b-c8d230e9ae9b"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4258165955543518\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.32000000000000006, f1=0.27956989247311825, best_f1=0.27956989247311825\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4875902831554413\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.35294117647058826, f1=0.29885057471264365, best_f1=0.29885057471264365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.42487233877182007\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.3404255319148936, f1=0.2898550724637682, best_f1=0.29885057471264365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2142159640789032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.4150943396226415, f1=0.3142857142857143, best_f1=0.3142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22363892197608948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.5, f1=0.4324324324324324, best_f1=0.4324324324324324\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1681942492723465\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.6111111111111112, f1=0.44897959183673464, best_f1=0.44897959183673464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3224595785140991\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.6470588235294117, f1=0.48888888888888893, best_f1=0.48888888888888893\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22000135481357574\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6153846153846153, f1=0.5714285714285715, best_f1=0.48888888888888893\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31368744373321533\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8461538461538461, f1=0.7096774193548386, best_f1=0.7096774193548386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06923451274633408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9285714285714286, f1=0.7096774193548386, best_f1=0.7096774193548386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08553913980722427\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8666666666666666, f1=0.7058823529411764, best_f1=0.7096774193548386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13705551624298096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9285714285714286, f1=0.7142857142857143, best_f1=0.7096774193548386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009924942627549171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9285714285714286, f1=0.689655172413793, best_f1=0.7096774193548386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008849281817674637\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9285714285714286, f1=0.689655172413793, best_f1=0.7096774193548386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014872557483613491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9285714285714286, f1=0.689655172413793, best_f1=0.7096774193548386\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 115626.07it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.8571428571428571\n",
            "real_f1 = 0.8666666666666666\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:25, 173.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "zyjgIIwdgNFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c917ba7-21b3-4f96-dea8-d3f992983aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 437kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 821kB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 533kB/s]\n",
            "Downloading: 100% 501M/501M [00:11<00:00, 45.3MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5955874919891357\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4690302610397339\n",
            "step: 20, loss: 0.4641777575016022\n",
            "step: 30, loss: 0.33971107006073\n",
            "step: 40, loss: 0.3727409839630127\n",
            "step: 50, loss: 0.5801430940628052\n",
            "step: 60, loss: 0.45630067586898804\n",
            "step: 70, loss: 0.4199107587337494\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.3448459506034851\n",
            "step: 90, loss: 0.08277036249637604\n",
            "step: 100, loss: 0.22000078856945038\n",
            "step: 110, loss: 0.12458377331495285\n",
            "step: 120, loss: 0.04722507670521736\n",
            "step: 130, loss: 0.03704921901226044\n",
            "step: 140, loss: 0.011186089366674423\n",
            "step: 150, loss: 0.11368035525083542\n",
            "step: 160, loss: 0.032888878136873245\n",
            "step: 170, loss: 0.09210441261529922\n",
            "step: 180, loss: 0.024961018934845924\n",
            "step: 190, loss: 0.041316039860248566\n",
            "step: 200, loss: 0.036393534392118454\n",
            "step: 210, loss: 0.06554815918207169\n",
            "step: 220, loss: 0.08724705874919891\n",
            "step: 230, loss: 0.07631669193506241\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9603624009060023, f1=0.956221198156682, best_f1=0.956221198156682\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020556891337037086\n",
            "step: 10, loss: 0.02633609250187874\n",
            "step: 20, loss: 0.009903090074658394\n",
            "step: 30, loss: 0.01116504892706871\n",
            "step: 40, loss: 0.009992267936468124\n",
            "step: 50, loss: 0.01957814022898674\n",
            "step: 60, loss: 0.003416571067646146\n",
            "step: 70, loss: 0.022078771144151688\n",
            "step: 80, loss: 0.013870500959455967\n",
            "step: 90, loss: 0.07190696150064468\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 100, loss: 0.006477453280240297\n",
            "step: 110, loss: 0.06300286948680878\n",
            "step: 120, loss: 0.00527158472687006\n",
            "step: 130, loss: 0.007714841514825821\n",
            "step: 140, loss: 0.0055678486824035645\n",
            "step: 150, loss: 0.07547308504581451\n",
            "step: 160, loss: 0.004120172467082739\n",
            "step: 170, loss: 0.003688339376822114\n",
            "step: 180, loss: 0.006056174170225859\n",
            "step: 190, loss: 0.047307565808296204\n",
            "step: 200, loss: 0.0061811343766748905\n",
            "step: 210, loss: 0.003497451078146696\n",
            "step: 220, loss: 0.002140991622582078\n",
            "step: 230, loss: 0.0030227121897041798\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9831649831649831, f1=0.9818181818181818, best_f1=0.9818181818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004250477999448776\n",
            "step: 10, loss: 0.0025666416622698307\n",
            "step: 20, loss: 0.11866859346628189\n",
            "step: 30, loss: 0.0035621817223727703\n",
            "step: 40, loss: 0.009539962746202946\n",
            "step: 50, loss: 0.05648740753531456\n",
            "step: 60, loss: 0.03926211595535278\n",
            "step: 70, loss: 0.08898322284221649\n",
            "step: 80, loss: 0.04081130772829056\n",
            "step: 90, loss: 0.012402810156345367\n",
            "step: 100, loss: 0.0046046725474298\n",
            "step: 110, loss: 0.016821343451738358\n",
            "step: 120, loss: 0.001366067910566926\n",
            "step: 130, loss: 0.11801017820835114\n",
            "step: 140, loss: 0.004217457491904497\n",
            "step: 150, loss: 0.027972547337412834\n",
            "step: 160, loss: 0.0022552404552698135\n",
            "step: 170, loss: 0.0015904412139207125\n",
            "step: 180, loss: 0.004086094442754984\n",
            "step: 190, loss: 0.07696119695901871\n",
            "step: 200, loss: 0.034618496894836426\n",
            "step: 210, loss: 0.0026722252368927\n",
            "step: 220, loss: 0.005938081070780754\n",
            "step: 230, loss: 0.04460323229432106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9740112994350283, f1=0.9716231555051079, best_f1=0.9818181818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009669128805398941\n",
            "step: 10, loss: 0.002863065106794238\n",
            "step: 20, loss: 0.007024103309959173\n",
            "step: 30, loss: 0.0034625157713890076\n",
            "step: 40, loss: 0.07329665869474411\n",
            "step: 50, loss: 0.026890020817518234\n",
            "step: 60, loss: 0.004845838528126478\n",
            "step: 70, loss: 0.01804930903017521\n",
            "step: 80, loss: 0.0005251768161542714\n",
            "step: 90, loss: 0.0012663750676438212\n",
            "step: 100, loss: 0.001076976303011179\n",
            "step: 110, loss: 0.0005011293687857687\n",
            "step: 120, loss: 0.0017879616934806108\n",
            "step: 130, loss: 0.010495166294276714\n",
            "step: 140, loss: 0.005916884168982506\n",
            "step: 150, loss: 0.0009500043233856559\n",
            "step: 160, loss: 0.0012422187719494104\n",
            "step: 170, loss: 0.007331525441259146\n",
            "step: 180, loss: 0.12544433772563934\n",
            "step: 190, loss: 0.01656751148402691\n",
            "step: 200, loss: 0.062289007008075714\n",
            "step: 210, loss: 0.0026974196080118418\n",
            "step: 220, loss: 0.0008044826681725681\n",
            "step: 230, loss: 0.04766303300857544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9796380090497738, f1=0.976054732041049, best_f1=0.9818181818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004120255820453167\n",
            "step: 10, loss: 0.0024188184179365635\n",
            "step: 20, loss: 0.0012762685073539615\n",
            "step: 30, loss: 0.0010821949690580368\n",
            "step: 40, loss: 0.0013415392022579908\n",
            "step: 50, loss: 0.05882507935166359\n",
            "step: 60, loss: 0.04552808403968811\n",
            "step: 70, loss: 0.013037948869168758\n",
            "step: 80, loss: 0.01840035617351532\n",
            "step: 90, loss: 0.04458065330982208\n",
            "step: 100, loss: 0.0041205910965800285\n",
            "step: 110, loss: 0.0020006014965474606\n",
            "step: 120, loss: 0.003956974949687719\n",
            "step: 130, loss: 0.00037038075970485806\n",
            "step: 140, loss: 0.004647114779800177\n",
            "step: 150, loss: 0.024153875187039375\n",
            "step: 160, loss: 0.00025422521866858006\n",
            "step: 170, loss: 0.000470235594548285\n",
            "step: 180, loss: 0.00142327428329736\n",
            "step: 190, loss: 0.06302881985902786\n",
            "step: 200, loss: 0.0037392268422991037\n",
            "step: 210, loss: 0.007678322959691286\n",
            "step: 220, loss: 0.00044401647755876184\n",
            "step: 230, loss: 0.0014529322506859899\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9831649831649831, f1=0.9829738933030647, best_f1=0.9818181818181818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002636198769323528\n",
            "step: 10, loss: 0.0006736709619872272\n",
            "step: 20, loss: 0.0008040940156206489\n",
            "step: 30, loss: 0.0010576623026281595\n",
            "step: 40, loss: 0.0007133882609196007\n",
            "step: 50, loss: 0.0010999819496646523\n",
            "step: 60, loss: 0.003875043708831072\n",
            "step: 70, loss: 0.008051790297031403\n",
            "step: 80, loss: 0.001443781889975071\n",
            "step: 90, loss: 0.05221674591302872\n",
            "step: 100, loss: 0.0011296496959403157\n",
            "step: 110, loss: 0.014568598009645939\n",
            "step: 120, loss: 0.0006068822112865746\n",
            "step: 130, loss: 0.0011526194866746664\n",
            "step: 140, loss: 0.003244144609197974\n",
            "step: 150, loss: 0.0014534574002027512\n",
            "step: 160, loss: 0.005755376536399126\n",
            "step: 170, loss: 0.00044770078966394067\n",
            "step: 180, loss: 0.000655819196254015\n",
            "step: 190, loss: 0.001220107777044177\n",
            "step: 200, loss: 0.003739143256098032\n",
            "step: 210, loss: 0.005477314814925194\n",
            "step: 220, loss: 0.004475027322769165\n",
            "step: 230, loss: 0.009604964405298233\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9855072463768116, f1=0.9832402234636871, best_f1=0.9832402234636871\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0022684941068291664\n",
            "step: 10, loss: 0.0007375967688858509\n",
            "step: 20, loss: 0.0012626335956156254\n",
            "step: 30, loss: 0.0010408335365355015\n",
            "step: 40, loss: 0.0016628263983875513\n",
            "step: 50, loss: 0.0013315826654434204\n",
            "step: 60, loss: 0.001027556136250496\n",
            "step: 70, loss: 0.0010919576743617654\n",
            "step: 80, loss: 0.0073684207163751125\n",
            "step: 90, loss: 0.010210106149315834\n",
            "step: 100, loss: 0.000993130262941122\n",
            "step: 110, loss: 0.000578428793232888\n",
            "step: 120, loss: 0.0008099551778286695\n",
            "step: 130, loss: 0.0006245640688575804\n",
            "step: 140, loss: 0.0005322014912962914\n",
            "step: 150, loss: 0.009309903718531132\n",
            "step: 160, loss: 0.0022161260712891817\n",
            "step: 170, loss: 0.011709081009030342\n",
            "step: 180, loss: 0.001905698562040925\n",
            "step: 190, loss: 0.000981201184913516\n",
            "step: 200, loss: 0.0020032646134495735\n",
            "step: 210, loss: 0.0021815034560859203\n",
            "step: 220, loss: 0.0007062042132019997\n",
            "step: 230, loss: 0.0033097711857408285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9887133182844244, f1=0.9875706214689265, best_f1=0.9875706214689265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015500533627346158\n",
            "step: 10, loss: 0.0031693503260612488\n",
            "step: 20, loss: 0.0020661032758653164\n",
            "step: 30, loss: 0.0009408447658643126\n",
            "step: 40, loss: 0.0038266575429588556\n",
            "step: 50, loss: 0.0008120271377265453\n",
            "step: 60, loss: 0.0006863222806714475\n",
            "step: 70, loss: 0.00024036526156123728\n",
            "step: 80, loss: 0.0553257130086422\n",
            "step: 90, loss: 0.0004439718904905021\n",
            "step: 100, loss: 0.0006208589184097946\n",
            "step: 110, loss: 0.0016806465573608875\n",
            "step: 120, loss: 0.008634621277451515\n",
            "step: 130, loss: 0.0086146155372262\n",
            "step: 140, loss: 0.001143188215792179\n",
            "step: 150, loss: 0.054679930210113525\n",
            "step: 160, loss: 0.0016017395537346601\n",
            "step: 170, loss: 0.057289931923151016\n",
            "step: 180, loss: 0.0029831579886376858\n",
            "step: 190, loss: 0.01258473377674818\n",
            "step: 200, loss: 0.01324030477553606\n",
            "step: 210, loss: 0.0017379121854901314\n",
            "step: 220, loss: 0.0018892367370426655\n",
            "step: 230, loss: 0.0008969053160399199\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.987736900780379, f1=0.978865406006674, best_f1=0.9875706214689265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005470059113577008\n",
            "step: 10, loss: 0.0006026967312209308\n",
            "step: 20, loss: 0.0029347967356443405\n",
            "step: 30, loss: 0.002852902514860034\n",
            "step: 40, loss: 0.0013256387319415808\n",
            "step: 50, loss: 0.00824160035699606\n",
            "step: 60, loss: 0.00043902298784814775\n",
            "step: 70, loss: 0.08480183035135269\n",
            "step: 80, loss: 0.0004541474627330899\n",
            "step: 90, loss: 0.023561885580420494\n",
            "step: 100, loss: 0.000645072665065527\n",
            "step: 110, loss: 0.000586153706535697\n",
            "step: 120, loss: 0.0054922811686992645\n",
            "step: 130, loss: 0.00043028267100453377\n",
            "step: 140, loss: 0.00055472127860412\n",
            "step: 150, loss: 0.0024419957771897316\n",
            "step: 160, loss: 0.003112661885097623\n",
            "step: 170, loss: 0.00034122669603675604\n",
            "step: 180, loss: 0.00036973756505176425\n",
            "step: 190, loss: 0.00018581161566544324\n",
            "step: 200, loss: 0.00041366767254658043\n",
            "step: 210, loss: 0.020807737484574318\n",
            "step: 220, loss: 0.000704303034581244\n",
            "step: 230, loss: 0.00048410682938992977\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9887387387387387, f1=0.9864864864864865, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008887479198165238\n",
            "step: 10, loss: 0.000840427353978157\n",
            "step: 20, loss: 0.0017382071819156408\n",
            "step: 30, loss: 0.00013586987915914506\n",
            "step: 40, loss: 0.004018282983452082\n",
            "step: 50, loss: 0.0003126604715362191\n",
            "step: 60, loss: 0.0005065082223154604\n",
            "step: 70, loss: 0.0009773055789992213\n",
            "step: 80, loss: 0.0002803603420034051\n",
            "step: 90, loss: 0.00022722345602232963\n",
            "step: 100, loss: 0.00017632402887102216\n",
            "step: 110, loss: 0.0011769612319767475\n",
            "step: 120, loss: 0.00015458425332326442\n",
            "step: 130, loss: 0.0004805699863936752\n",
            "step: 140, loss: 0.0005340586067177355\n",
            "step: 150, loss: 0.008368960581719875\n",
            "step: 160, loss: 7.470963464584202e-05\n",
            "step: 170, loss: 0.00026404959498904645\n",
            "step: 180, loss: 0.0009517651633359492\n",
            "step: 190, loss: 0.00019267127208877355\n",
            "step: 200, loss: 0.0005301128840073943\n",
            "step: 210, loss: 0.003474404802545905\n",
            "step: 220, loss: 0.04213150963187218\n",
            "step: 230, loss: 0.0003587757528293878\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9888641425389755, f1=0.9799554565701558, best_f1=0.9799554565701558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006252229795791209\n",
            "step: 10, loss: 0.0005799735081382096\n",
            "step: 20, loss: 0.00028933637076988816\n",
            "step: 30, loss: 0.0004273297672625631\n",
            "step: 40, loss: 6.536702858284116e-05\n",
            "step: 50, loss: 0.00021776134963147342\n",
            "step: 60, loss: 0.017357848584651947\n",
            "step: 70, loss: 0.0011257920414209366\n",
            "step: 80, loss: 0.0057997927069664\n",
            "step: 90, loss: 0.0559091717004776\n",
            "step: 100, loss: 0.0026056719943881035\n",
            "step: 110, loss: 0.001271202228963375\n",
            "step: 120, loss: 0.003747781040146947\n",
            "step: 130, loss: 0.00019656748918350786\n",
            "step: 140, loss: 0.004176842048764229\n",
            "step: 150, loss: 0.00015357686788775027\n",
            "step: 160, loss: 0.010818121954798698\n",
            "step: 170, loss: 0.007170871365815401\n",
            "step: 180, loss: 0.001918313791975379\n",
            "step: 190, loss: 0.0011166558833792806\n",
            "step: 200, loss: 0.013269763439893723\n",
            "step: 210, loss: 0.0002727879327721894\n",
            "step: 220, loss: 0.0011068293824791908\n",
            "step: 230, loss: 0.0007349044317379594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9898989898989898, f1=0.983277591973244, best_f1=0.983277591973244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00036349432775750756\n",
            "step: 10, loss: 9.869280620478094e-05\n",
            "step: 20, loss: 0.005836003459990025\n",
            "step: 30, loss: 0.018559638410806656\n",
            "step: 40, loss: 0.0010791480308398604\n",
            "step: 50, loss: 0.002202525269240141\n",
            "step: 60, loss: 0.0008479990647174418\n",
            "step: 70, loss: 0.0001634793443372473\n",
            "step: 80, loss: 6.912565731909126e-05\n",
            "step: 90, loss: 0.006099258549511433\n",
            "step: 100, loss: 7.393283885903656e-05\n",
            "step: 110, loss: 8.08899785624817e-05\n",
            "step: 120, loss: 8.716970478417352e-05\n",
            "step: 130, loss: 0.00014701577310916036\n",
            "step: 140, loss: 0.00018365614232607186\n",
            "step: 150, loss: 0.00035106041468679905\n",
            "step: 160, loss: 0.0017551911296322942\n",
            "step: 170, loss: 0.00045835101627744734\n",
            "step: 180, loss: 0.00014616631960961968\n",
            "step: 190, loss: 0.008354021236300468\n",
            "step: 200, loss: 7.458850450348109e-05\n",
            "step: 210, loss: 0.0004535147163551301\n",
            "step: 220, loss: 0.029146188870072365\n",
            "step: 230, loss: 0.0005622134194709361\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9898989898989898, f1=0.9810055865921787, best_f1=0.983277591973244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001808500092010945\n",
            "step: 10, loss: 0.00096425402443856\n",
            "step: 20, loss: 0.00015483486640732735\n",
            "step: 30, loss: 4.634738797903992e-05\n",
            "step: 40, loss: 0.0003098914457950741\n",
            "step: 50, loss: 0.0014160798164084554\n",
            "step: 60, loss: 0.0007063522934913635\n",
            "step: 70, loss: 0.0002668453671503812\n",
            "step: 80, loss: 0.00016013158892747015\n",
            "step: 90, loss: 8.752385474508628e-05\n",
            "step: 100, loss: 0.0008512293570674956\n",
            "step: 110, loss: 0.0010622640838846564\n",
            "step: 120, loss: 0.0001473517477279529\n",
            "step: 130, loss: 8.44668538775295e-05\n",
            "step: 140, loss: 5.902122211409733e-05\n",
            "step: 150, loss: 9.959270391846076e-05\n",
            "step: 160, loss: 0.0033956014085561037\n",
            "step: 170, loss: 6.0139969718875363e-05\n",
            "step: 180, loss: 0.043594978749752045\n",
            "step: 190, loss: 9.82835772447288e-05\n",
            "step: 200, loss: 0.000239019442233257\n",
            "step: 210, loss: 0.0005006210994906723\n",
            "step: 220, loss: 7.956984336487949e-05\n",
            "step: 230, loss: 0.00027952456730417907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9898989898989898, f1=0.9831649831649831, best_f1=0.983277591973244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.598820618819445e-05\n",
            "step: 10, loss: 0.00010614440543577075\n",
            "step: 20, loss: 0.00013409584062173963\n",
            "step: 30, loss: 0.0003872533852700144\n",
            "step: 40, loss: 7.222651765914634e-05\n",
            "step: 50, loss: 6.393549119820818e-05\n",
            "step: 60, loss: 4.374955824459903e-05\n",
            "step: 70, loss: 0.00014369776181410998\n",
            "step: 80, loss: 8.147869084496051e-05\n",
            "step: 90, loss: 0.00016148117720149457\n",
            "step: 100, loss: 0.00010288268094882369\n",
            "step: 110, loss: 5.8567537053022534e-05\n",
            "step: 120, loss: 2.0678531654994003e-05\n",
            "step: 130, loss: 0.00010963861859636381\n",
            "step: 140, loss: 0.00011387930135242641\n",
            "step: 150, loss: 2.910745570261497e-05\n",
            "step: 160, loss: 4.836171865463257e-05\n",
            "step: 170, loss: 9.170830890070647e-05\n",
            "step: 180, loss: 5.407094067777507e-05\n",
            "step: 190, loss: 4.738889037980698e-05\n",
            "step: 200, loss: 0.000677631120197475\n",
            "step: 210, loss: 5.942530697211623e-05\n",
            "step: 220, loss: 5.4317599278874695e-05\n",
            "step: 230, loss: 4.1548566514393315e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9899441340782122, f1=0.9789590254706534, best_f1=0.9789590254706534\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011745515279471874\n",
            "step: 10, loss: 4.2685347580118105e-05\n",
            "step: 20, loss: 0.0019637688528746367\n",
            "step: 30, loss: 6.926732021383941e-05\n",
            "step: 40, loss: 3.484794433461502e-05\n",
            "step: 50, loss: 3.195797762600705e-05\n",
            "step: 60, loss: 0.08161837607622147\n",
            "step: 70, loss: 0.00011168471974087879\n",
            "step: 80, loss: 4.189577521174215e-05\n",
            "step: 90, loss: 6.439977005356923e-05\n",
            "step: 100, loss: 4.593627818394452e-05\n",
            "step: 110, loss: 0.00019319180864840746\n",
            "step: 120, loss: 0.0289271492511034\n",
            "step: 130, loss: 5.570628491113894e-05\n",
            "step: 140, loss: 0.0054257954470813274\n",
            "step: 150, loss: 6.241937808226794e-05\n",
            "step: 160, loss: 0.010261488147079945\n",
            "step: 170, loss: 4.3685668060788885e-05\n",
            "step: 180, loss: 8.80536827025935e-05\n",
            "step: 190, loss: 0.005491448100656271\n",
            "step: 200, loss: 0.0010489849373698235\n",
            "step: 210, loss: 0.036418456584215164\n",
            "step: 220, loss: 7.8697303251829e-05\n",
            "step: 230, loss: 0.0002826832642313093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9888392857142857, f1=0.9789590254706534, best_f1=0.9789590254706534\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:16, 148.26it/s]\n",
            "load_f1 = 0.9899441340782122\n",
            "real_f1 = 0.9899441340782122\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 138.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "ck7uL6uPgNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "YyyxG2qpgNFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6cac2a-8d7e-4907-9fd4-bf319ecb2eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6393852233886719\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.45871710777282715\n",
            "step: 20, loss: 0.3413383364677429\n",
            "step: 30, loss: 0.3538372814655304\n",
            "step: 40, loss: 0.33377131819725037\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 50, loss: 0.23129233717918396\n",
            "step: 60, loss: 0.1510351151227951\n",
            "step: 70, loss: 0.12450778484344482\n",
            "step: 80, loss: 0.11450627446174622\n",
            "step: 90, loss: 0.17277966439723969\n",
            "step: 100, loss: 0.0885126143693924\n",
            "step: 110, loss: 0.0711611658334732\n",
            "step: 120, loss: 0.17287978529930115\n",
            "step: 130, loss: 0.15182678401470184\n",
            "step: 140, loss: 0.27307480573654175\n",
            "step: 150, loss: 0.11521054804325104\n",
            "step: 160, loss: 0.06760808825492859\n",
            "step: 170, loss: 0.023297283798456192\n",
            "step: 180, loss: 0.08848536014556885\n",
            "step: 190, loss: 0.0632375180721283\n",
            "step: 200, loss: 0.017355045303702354\n",
            "step: 210, loss: 0.04129467532038689\n",
            "step: 220, loss: 0.050631675869226456\n",
            "step: 230, loss: 0.1582104116678238\n",
            "step: 240, loss: 0.018886404111981392\n",
            "step: 250, loss: 0.05359029024839401\n",
            "step: 260, loss: 0.18269577622413635\n",
            "step: 270, loss: 0.2930072546005249\n",
            "step: 280, loss: 0.04657242074608803\n",
            "step: 290, loss: 0.13502007722854614\n",
            "step: 300, loss: 0.01904098503291607\n",
            "step: 310, loss: 0.14290766417980194\n",
            "step: 320, loss: 0.08036140352487564\n",
            "step: 330, loss: 0.07947714626789093\n",
            "step: 340, loss: 0.24102698266506195\n",
            "step: 350, loss: 0.08941333740949631\n",
            "step: 360, loss: 0.11305331438779831\n",
            "step: 370, loss: 0.019047554582357407\n",
            "step: 380, loss: 0.0750788226723671\n",
            "step: 390, loss: 0.0023001760710030794\n",
            "step: 400, loss: 0.059490133076906204\n",
            "step: 410, loss: 0.25732630491256714\n",
            "step: 420, loss: 0.029567154124379158\n",
            "step: 430, loss: 0.03213764354586601\n",
            "step: 440, loss: 0.07653414458036423\n",
            "step: 450, loss: 0.02038203924894333\n",
            "step: 460, loss: 0.02132071554660797\n",
            "step: 470, loss: 0.01854737102985382\n",
            "step: 480, loss: 0.18126481771469116\n",
            "step: 490, loss: 0.18033716082572937\n",
            "step: 500, loss: 0.06701326370239258\n",
            "step: 510, loss: 0.10059145838022232\n",
            "step: 520, loss: 0.05423269420862198\n",
            "step: 530, loss: 0.005503544583916664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9390187987161852, f1=0.9376718606782769, best_f1=0.9376718606782769\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.039954256266355515\n",
            "step: 10, loss: 0.11587103456258774\n",
            "step: 20, loss: 0.02304951846599579\n",
            "step: 30, loss: 0.03951505944132805\n",
            "step: 40, loss: 0.0516178235411644\n",
            "step: 50, loss: 0.021510986611247063\n",
            "step: 60, loss: 0.014140921644866467\n",
            "step: 70, loss: 0.026270687580108643\n",
            "step: 80, loss: 0.06138930097222328\n",
            "step: 90, loss: 0.006851667538285255\n",
            "step: 100, loss: 0.0951671302318573\n",
            "step: 110, loss: 0.019204361364245415\n",
            "step: 120, loss: 0.12993451952934265\n",
            "step: 130, loss: 0.004613752476871014\n",
            "step: 140, loss: 0.08623816072940826\n",
            "step: 150, loss: 0.014469613321125507\n",
            "step: 160, loss: 0.05530261620879173\n",
            "step: 170, loss: 0.038264717906713486\n",
            "step: 180, loss: 0.004770695697516203\n",
            "step: 190, loss: 0.0060236589051783085\n",
            "step: 200, loss: 0.046573638916015625\n",
            "step: 210, loss: 0.02111927792429924\n",
            "step: 220, loss: 0.0014967219904065132\n",
            "step: 230, loss: 0.0339156836271286\n",
            "step: 240, loss: 0.05553529039025307\n",
            "step: 250, loss: 0.06976175308227539\n",
            "step: 260, loss: 0.019794225692749023\n",
            "step: 270, loss: 0.12950348854064941\n",
            "step: 280, loss: 0.20532304048538208\n",
            "step: 290, loss: 0.04247497394680977\n",
            "step: 300, loss: 0.029179979115724564\n",
            "step: 310, loss: 0.08719193935394287\n",
            "step: 320, loss: 0.02747999131679535\n",
            "step: 330, loss: 0.024532360956072807\n",
            "step: 340, loss: 0.10383787006139755\n",
            "step: 350, loss: 0.004761488176882267\n",
            "step: 360, loss: 0.04915276914834976\n",
            "step: 370, loss: 0.027890756726264954\n",
            "step: 380, loss: 0.19364336133003235\n",
            "step: 390, loss: 0.011407413519918919\n",
            "step: 400, loss: 0.011415408924221992\n",
            "step: 410, loss: 0.012774641625583172\n",
            "step: 420, loss: 0.18579673767089844\n",
            "step: 430, loss: 0.0931035727262497\n",
            "step: 440, loss: 0.008300362154841423\n",
            "step: 450, loss: 0.036841992288827896\n",
            "step: 460, loss: 0.10252797603607178\n",
            "step: 470, loss: 0.08480022847652435\n",
            "step: 480, loss: 0.014256716705858707\n",
            "step: 490, loss: 0.026239069178700447\n",
            "step: 500, loss: 0.013427411206066608\n",
            "step: 510, loss: 0.017187276855111122\n",
            "step: 520, loss: 0.34707826375961304\n",
            "step: 530, loss: 0.07706505060195923\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9452554744525548, f1=0.9432753888380604, best_f1=0.9432753888380604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12550334632396698\n",
            "step: 10, loss: 0.08208830654621124\n",
            "step: 20, loss: 0.011150674894452095\n",
            "step: 30, loss: 0.0828525498509407\n",
            "step: 40, loss: 0.039959151297807693\n",
            "step: 50, loss: 0.002144809812307358\n",
            "step: 60, loss: 0.00968286395072937\n",
            "step: 70, loss: 0.018901482224464417\n",
            "step: 80, loss: 0.008479802869260311\n",
            "step: 90, loss: 0.023887695744633675\n",
            "step: 100, loss: 0.008877323940396309\n",
            "step: 110, loss: 0.017304111272096634\n",
            "step: 120, loss: 0.08271484076976776\n",
            "step: 130, loss: 0.0320025309920311\n",
            "step: 140, loss: 0.023076513782143593\n",
            "step: 150, loss: 0.00947527028620243\n",
            "step: 160, loss: 0.017557410523295403\n",
            "step: 170, loss: 0.02061651088297367\n",
            "step: 180, loss: 0.02405078522861004\n",
            "step: 190, loss: 0.0025470827240496874\n",
            "step: 200, loss: 0.013629848137497902\n",
            "step: 210, loss: 0.041939426213502884\n",
            "step: 220, loss: 0.05706224963068962\n",
            "step: 230, loss: 0.026075106114149094\n",
            "step: 240, loss: 0.0586303249001503\n",
            "step: 250, loss: 0.037708211690187454\n",
            "step: 260, loss: 0.06342597305774689\n",
            "step: 270, loss: 0.03413647785782814\n",
            "step: 280, loss: 0.0022198217920958996\n",
            "step: 290, loss: 0.008785508573055267\n",
            "step: 300, loss: 0.14406388998031616\n",
            "step: 310, loss: 0.05977155640721321\n",
            "step: 320, loss: 0.022995179519057274\n",
            "step: 330, loss: 0.014189926907420158\n",
            "step: 340, loss: 0.012018095701932907\n",
            "step: 350, loss: 0.0620930939912796\n",
            "step: 360, loss: 0.02625163458287716\n",
            "step: 370, loss: 0.056434035301208496\n",
            "step: 380, loss: 0.03228013589978218\n",
            "step: 390, loss: 0.008691211231052876\n",
            "step: 400, loss: 0.0617409385740757\n",
            "step: 410, loss: 0.04294430837035179\n",
            "step: 420, loss: 0.018642781302332878\n",
            "step: 430, loss: 0.015440122224390507\n",
            "step: 440, loss: 0.14245043694972992\n",
            "step: 450, loss: 0.03154245764017105\n",
            "step: 460, loss: 0.10833264887332916\n",
            "step: 470, loss: 0.03981318697333336\n",
            "step: 480, loss: 0.0812232494354248\n",
            "step: 490, loss: 0.051021602004766464\n",
            "step: 500, loss: 0.0025609093718230724\n",
            "step: 510, loss: 0.01504695788025856\n",
            "step: 520, loss: 0.0019333807285875082\n",
            "step: 530, loss: 0.024972259998321533\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9450757575757576, f1=0.941398865784499, best_f1=0.9432753888380604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01426005270332098\n",
            "step: 10, loss: 0.004115786403417587\n",
            "step: 20, loss: 0.09920191019773483\n",
            "step: 30, loss: 0.09560286998748779\n",
            "step: 40, loss: 0.10654884576797485\n",
            "step: 50, loss: 0.05813773721456528\n",
            "step: 60, loss: 0.006066151894629002\n",
            "step: 70, loss: 0.014264695346355438\n",
            "step: 80, loss: 0.02722550369799137\n",
            "step: 90, loss: 0.006158432923257351\n",
            "step: 100, loss: 0.0006275466294027865\n",
            "step: 110, loss: 0.0022207119036465883\n",
            "step: 120, loss: 0.009885498322546482\n",
            "step: 130, loss: 0.10682158917188644\n",
            "step: 140, loss: 0.024648549035191536\n",
            "step: 150, loss: 0.00513097969815135\n",
            "step: 160, loss: 0.004910639487206936\n",
            "step: 170, loss: 0.007467067334800959\n",
            "step: 180, loss: 0.08645906299352646\n",
            "step: 190, loss: 0.030614472925662994\n",
            "step: 200, loss: 0.01589461974799633\n",
            "step: 210, loss: 0.050695862621068954\n",
            "step: 220, loss: 0.024128198623657227\n",
            "step: 230, loss: 0.019234998151659966\n",
            "step: 240, loss: 0.027053657919168472\n",
            "step: 250, loss: 0.08174201846122742\n",
            "step: 260, loss: 0.00041711816447786987\n",
            "step: 270, loss: 0.04659423604607582\n",
            "step: 280, loss: 0.008981358259916306\n",
            "step: 290, loss: 0.09674791991710663\n",
            "step: 300, loss: 0.01604365184903145\n",
            "step: 310, loss: 0.0033579578157514334\n",
            "step: 320, loss: 0.03938291221857071\n",
            "step: 330, loss: 0.021108446642756462\n",
            "step: 340, loss: 0.007703758310526609\n",
            "step: 350, loss: 0.14098170399665833\n",
            "step: 360, loss: 0.01264235284179449\n",
            "step: 370, loss: 0.003259707009419799\n",
            "step: 380, loss: 0.003012687200680375\n",
            "step: 390, loss: 0.0009141336777247488\n",
            "step: 400, loss: 0.018263956531882286\n",
            "step: 410, loss: 0.002954750554636121\n",
            "step: 420, loss: 0.007413115352392197\n",
            "step: 430, loss: 0.014046445488929749\n",
            "step: 440, loss: 0.022310996428132057\n",
            "step: 450, loss: 0.019011706113815308\n",
            "step: 460, loss: 0.13740412890911102\n",
            "step: 470, loss: 0.0014830526197329164\n",
            "step: 480, loss: 0.018457937985658646\n",
            "step: 490, loss: 0.0008430107263848186\n",
            "step: 500, loss: 0.010468579828739166\n",
            "step: 510, loss: 0.06549890339374542\n",
            "step: 520, loss: 0.007011455949395895\n",
            "step: 530, loss: 0.0716971904039383\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9567617295308187, f1=0.9458715596330275, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006894845282658935\n",
            "step: 10, loss: 0.016511667519807816\n",
            "step: 20, loss: 0.060154251754283905\n",
            "step: 30, loss: 0.004831334110349417\n",
            "step: 40, loss: 0.0009520570747554302\n",
            "step: 50, loss: 0.15346798300743103\n",
            "step: 60, loss: 0.010821860283613205\n",
            "step: 70, loss: 0.006570377852767706\n",
            "step: 80, loss: 0.0015588834648951888\n",
            "step: 90, loss: 0.13665801286697388\n",
            "step: 100, loss: 0.10323525220155716\n",
            "step: 110, loss: 0.002091167727485299\n",
            "step: 120, loss: 0.21949593722820282\n",
            "step: 130, loss: 0.010278608649969101\n",
            "step: 140, loss: 0.014488417655229568\n",
            "step: 150, loss: 0.016241824254393578\n",
            "step: 160, loss: 0.005079284776002169\n",
            "step: 170, loss: 0.09944386780261993\n",
            "step: 180, loss: 0.021306980401277542\n",
            "step: 190, loss: 0.008556130342185497\n",
            "step: 200, loss: 0.0043221116065979\n",
            "step: 210, loss: 0.0006801725248806179\n",
            "step: 220, loss: 0.0004178300150670111\n",
            "step: 230, loss: 0.0005928644677624106\n",
            "step: 240, loss: 0.0005327434046193957\n",
            "step: 250, loss: 0.34817227721214294\n",
            "step: 260, loss: 0.007692547515034676\n",
            "step: 270, loss: 0.007448776159435511\n",
            "step: 280, loss: 0.009665046818554401\n",
            "step: 290, loss: 0.004932183772325516\n",
            "step: 300, loss: 0.010179045610129833\n",
            "step: 310, loss: 0.19069981575012207\n",
            "step: 320, loss: 0.06876218318939209\n",
            "step: 330, loss: 0.0033854448702186346\n",
            "step: 340, loss: 0.03486699238419533\n",
            "step: 350, loss: 0.0007815364515408874\n",
            "step: 360, loss: 0.0004174456698819995\n",
            "step: 370, loss: 0.00022662866103928536\n",
            "step: 380, loss: 0.00033746781991794705\n",
            "step: 390, loss: 0.0020274824928492308\n",
            "step: 400, loss: 0.04337018355727196\n",
            "step: 410, loss: 0.03337308019399643\n",
            "step: 420, loss: 0.15771758556365967\n",
            "step: 430, loss: 0.009160324931144714\n",
            "step: 440, loss: 0.0023723687045276165\n",
            "step: 450, loss: 0.05293745920062065\n",
            "step: 460, loss: 0.057495445013046265\n",
            "step: 470, loss: 0.1284015029668808\n",
            "step: 480, loss: 0.015407413244247437\n",
            "step: 490, loss: 0.032213594764471054\n",
            "step: 500, loss: 0.005508689675480127\n",
            "step: 510, loss: 0.000786023389082402\n",
            "step: 520, loss: 0.057913850992918015\n",
            "step: 530, loss: 0.08274439722299576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9521152952115295, f1=0.9478628464067637, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018089475110173225\n",
            "step: 10, loss: 0.006251542828977108\n",
            "step: 20, loss: 0.05410291254520416\n",
            "step: 30, loss: 0.0070749069564044476\n",
            "step: 40, loss: 0.0011192196980118752\n",
            "step: 50, loss: 0.008687763474881649\n",
            "step: 60, loss: 0.005299269687384367\n",
            "step: 70, loss: 0.001070790458470583\n",
            "step: 80, loss: 0.0008452126057818532\n",
            "step: 90, loss: 0.03830242529511452\n",
            "step: 100, loss: 0.034317124634981155\n",
            "step: 110, loss: 0.010105184279382229\n",
            "step: 120, loss: 0.005142183043062687\n",
            "step: 130, loss: 0.001203284366056323\n",
            "step: 140, loss: 0.0017951298505067825\n",
            "step: 150, loss: 0.0006034184480085969\n",
            "step: 160, loss: 0.06340066343545914\n",
            "step: 170, loss: 0.004946506582200527\n",
            "step: 180, loss: 0.014907670207321644\n",
            "step: 190, loss: 0.0650499165058136\n",
            "step: 200, loss: 0.015105823054909706\n",
            "step: 210, loss: 0.004968772642314434\n",
            "step: 220, loss: 0.0032530678436160088\n",
            "step: 230, loss: 0.002883440349251032\n",
            "step: 240, loss: 0.028759203851222992\n",
            "step: 250, loss: 0.007520767860114574\n",
            "step: 260, loss: 0.0022132208105176687\n",
            "step: 270, loss: 0.0016752282390370965\n",
            "step: 280, loss: 0.030327992513775826\n",
            "step: 290, loss: 0.0005116831744089723\n",
            "step: 300, loss: 0.030261371284723282\n",
            "step: 310, loss: 0.13069309294223785\n",
            "step: 320, loss: 0.011800955981016159\n",
            "step: 330, loss: 0.03206872195005417\n",
            "step: 340, loss: 0.0008759223856031895\n",
            "step: 350, loss: 0.033877115696668625\n",
            "step: 360, loss: 0.09607788920402527\n",
            "step: 370, loss: 0.012422147206962109\n",
            "step: 380, loss: 0.0014967948663979769\n",
            "step: 390, loss: 0.0008077439852058887\n",
            "step: 400, loss: 0.04829525202512741\n",
            "step: 410, loss: 0.009135022759437561\n",
            "step: 420, loss: 0.0022809449583292007\n",
            "step: 430, loss: 9.01270832400769e-05\n",
            "step: 440, loss: 0.0003495201817713678\n",
            "step: 450, loss: 0.18234507739543915\n",
            "step: 460, loss: 0.004111988004297018\n",
            "step: 470, loss: 0.0023259753361344337\n",
            "step: 480, loss: 0.0027341400273144245\n",
            "step: 490, loss: 0.007330968976020813\n",
            "step: 500, loss: 0.0006666210247203708\n",
            "step: 510, loss: 0.2626692056655884\n",
            "step: 520, loss: 0.0010404415661469102\n",
            "step: 530, loss: 0.013437341898679733\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9494855004677268, f1=0.9497206703910613, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004799606278538704\n",
            "step: 10, loss: 0.0017407088307663798\n",
            "step: 20, loss: 0.0012143098283559084\n",
            "step: 30, loss: 0.00979443360120058\n",
            "step: 40, loss: 0.0005806173430755734\n",
            "step: 50, loss: 0.03741845861077309\n",
            "step: 60, loss: 0.017140474170446396\n",
            "step: 70, loss: 0.002465002704411745\n",
            "step: 80, loss: 0.0005121980211697519\n",
            "step: 90, loss: 3.0947845516493544e-05\n",
            "step: 100, loss: 0.0024920469149947166\n",
            "step: 110, loss: 7.06714199623093e-05\n",
            "step: 120, loss: 0.0009142194176092744\n",
            "step: 130, loss: 8.543118019588292e-05\n",
            "step: 140, loss: 0.005378863774240017\n",
            "step: 150, loss: 0.0002544877352192998\n",
            "step: 160, loss: 7.694220403209329e-05\n",
            "step: 170, loss: 0.0031708506867289543\n",
            "step: 180, loss: 0.12458009272813797\n",
            "step: 190, loss: 0.025739135220646858\n",
            "step: 200, loss: 0.0002601853047963232\n",
            "step: 210, loss: 0.013893214985728264\n",
            "step: 220, loss: 0.00014922535046935081\n",
            "step: 230, loss: 8.905708091333508e-05\n",
            "step: 240, loss: 0.00512818805873394\n",
            "step: 250, loss: 0.0031497012823820114\n",
            "step: 260, loss: 0.005235298071056604\n",
            "step: 270, loss: 0.00030556536512449384\n",
            "step: 280, loss: 0.0074585541151463985\n",
            "step: 290, loss: 0.0023819711059331894\n",
            "step: 300, loss: 0.0004428380634635687\n",
            "step: 310, loss: 0.0017090322216972709\n",
            "step: 320, loss: 0.08059095591306686\n",
            "step: 330, loss: 4.8480331315658987e-05\n",
            "step: 340, loss: 0.0014768579276278615\n",
            "step: 350, loss: 0.0018612714484333992\n",
            "step: 360, loss: 0.008517489768564701\n",
            "step: 370, loss: 0.014822135679423809\n",
            "step: 380, loss: 0.011716840788722038\n",
            "step: 390, loss: 0.01821344904601574\n",
            "step: 400, loss: 0.004775132052600384\n",
            "step: 410, loss: 9.2083471827209e-05\n",
            "step: 420, loss: 0.11122473329305649\n",
            "step: 430, loss: 0.0010149317095056176\n",
            "step: 440, loss: 0.0010387134971097112\n",
            "step: 450, loss: 0.0011472913902252913\n",
            "step: 460, loss: 0.0011045710416510701\n",
            "step: 470, loss: 0.12077246606349945\n",
            "step: 480, loss: 0.0040496033616364\n",
            "step: 490, loss: 0.00014960004773456603\n",
            "step: 500, loss: 0.0006400662241503596\n",
            "step: 510, loss: 6.839459820184857e-05\n",
            "step: 520, loss: 0.0001960444642463699\n",
            "step: 530, loss: 0.0003286800056230277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9480401093892434, f1=0.949614162505674, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000582836742978543\n",
            "step: 10, loss: 0.003934652544558048\n",
            "step: 20, loss: 4.513843305176124e-05\n",
            "step: 30, loss: 0.00013302102161105722\n",
            "step: 40, loss: 2.4589493477833457e-05\n",
            "step: 50, loss: 8.744684600969777e-05\n",
            "step: 60, loss: 9.684261749498546e-05\n",
            "step: 70, loss: 0.00022853861446492374\n",
            "step: 80, loss: 0.01927279308438301\n",
            "step: 90, loss: 0.0003004594473168254\n",
            "step: 100, loss: 0.025779075920581818\n",
            "step: 110, loss: 0.0003199505154043436\n",
            "step: 120, loss: 0.001809550914913416\n",
            "step: 130, loss: 0.0005206003552302718\n",
            "step: 140, loss: 0.06797543913125992\n",
            "step: 150, loss: 0.0026366664096713066\n",
            "step: 160, loss: 0.0004233641375321895\n",
            "step: 170, loss: 0.024718955159187317\n",
            "step: 180, loss: 0.0387042798101902\n",
            "step: 190, loss: 0.001198089448735118\n",
            "step: 200, loss: 0.002403064165264368\n",
            "step: 210, loss: 0.02318321168422699\n",
            "step: 220, loss: 0.10134989768266678\n",
            "step: 230, loss: 0.07347408682107925\n",
            "step: 240, loss: 0.017091410234570503\n",
            "step: 250, loss: 0.00022784658358432353\n",
            "step: 260, loss: 5.482304186443798e-05\n",
            "step: 270, loss: 0.023037154227495193\n",
            "step: 280, loss: 0.0001688122865743935\n",
            "step: 290, loss: 0.0007812575786374509\n",
            "step: 300, loss: 4.4771561078960076e-05\n",
            "step: 310, loss: 0.0019174072658643126\n",
            "step: 320, loss: 0.000476428511319682\n",
            "step: 330, loss: 0.014139359816908836\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 340, loss: 0.02515377476811409\n",
            "step: 350, loss: 0.004158633295446634\n",
            "step: 360, loss: 0.10361362993717194\n",
            "step: 370, loss: 0.44539186358451843\n",
            "step: 380, loss: 0.03819872811436653\n",
            "step: 390, loss: 0.02614542841911316\n",
            "step: 400, loss: 0.0019101101206615567\n",
            "step: 410, loss: 0.0005507659516297281\n",
            "step: 420, loss: 0.0002087587199639529\n",
            "step: 430, loss: 0.00036745332181453705\n",
            "step: 440, loss: 0.0009266132256016135\n",
            "step: 450, loss: 0.0004101930826436728\n",
            "step: 460, loss: 0.0020763245411217213\n",
            "step: 470, loss: 0.13874302804470062\n",
            "step: 480, loss: 0.0021501025184988976\n",
            "step: 490, loss: 0.05026206746697426\n",
            "step: 500, loss: 0.0002275587321491912\n",
            "step: 510, loss: 0.0017839488573372364\n",
            "step: 520, loss: 0.0002849060401786119\n",
            "step: 530, loss: 0.006586551200598478\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9533239038189533, f1=0.9475673122342938, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00037646963028237224\n",
            "step: 10, loss: 0.00921136699616909\n",
            "step: 20, loss: 0.0003442346060182899\n",
            "step: 30, loss: 0.044105056673288345\n",
            "step: 40, loss: 0.004437177442014217\n",
            "step: 50, loss: 0.0003465092449914664\n",
            "step: 60, loss: 0.001422392320819199\n",
            "step: 70, loss: 0.0014946432784199715\n",
            "step: 80, loss: 0.007345316931605339\n",
            "step: 90, loss: 0.06681756675243378\n",
            "step: 100, loss: 0.0008430648013018072\n",
            "step: 110, loss: 0.03372228890657425\n",
            "step: 120, loss: 0.0004585980495903641\n",
            "step: 130, loss: 0.00038072638562880456\n",
            "step: 140, loss: 0.006220564246177673\n",
            "step: 150, loss: 0.012889321893453598\n",
            "step: 160, loss: 0.00026539547252468765\n",
            "step: 170, loss: 0.0012589282123371959\n",
            "step: 180, loss: 0.00021555466810241342\n",
            "step: 190, loss: 0.0024056469555944204\n",
            "step: 200, loss: 0.00027774504269473255\n",
            "step: 210, loss: 0.0021075340919196606\n",
            "step: 220, loss: 0.0003283265687059611\n",
            "step: 230, loss: 0.0007845634827390313\n",
            "step: 240, loss: 6.394445517798886e-05\n",
            "step: 250, loss: 0.00042053681681863964\n",
            "step: 260, loss: 0.005444168113172054\n",
            "step: 270, loss: 0.00023943008272908628\n",
            "step: 280, loss: 0.008555902168154716\n",
            "step: 290, loss: 0.0002535737003199756\n",
            "step: 300, loss: 9.397242683917284e-05\n",
            "step: 310, loss: 0.13901570439338684\n",
            "step: 320, loss: 0.0001273796515306458\n",
            "step: 330, loss: 0.0002602300082799047\n",
            "step: 340, loss: 0.002579162362962961\n",
            "step: 350, loss: 0.008135637268424034\n",
            "step: 360, loss: 0.00039405806455761194\n",
            "step: 370, loss: 0.00016533327288925648\n",
            "step: 380, loss: 0.0003378003020770848\n",
            "step: 390, loss: 0.0002870618482120335\n",
            "step: 400, loss: 0.005734222941100597\n",
            "step: 410, loss: 0.0005446508293971419\n",
            "step: 420, loss: 0.0035885763354599476\n",
            "step: 430, loss: 0.009630450047552586\n",
            "step: 440, loss: 0.00035705167101696134\n",
            "step: 450, loss: 0.003599104005843401\n",
            "step: 460, loss: 0.00031685008434578776\n",
            "step: 470, loss: 0.00023147324100136757\n",
            "step: 480, loss: 0.0002489154867362231\n",
            "step: 490, loss: 0.0005008133593946695\n",
            "step: 500, loss: 0.00015575271390844136\n",
            "step: 510, loss: 0.005026591010391712\n",
            "step: 520, loss: 0.0027876710519194603\n",
            "step: 530, loss: 0.02684607356786728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9517684887459806, f1=0.9512419503219871, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003115431172773242\n",
            "step: 10, loss: 0.00034603086533024907\n",
            "step: 20, loss: 5.943283758824691e-05\n",
            "step: 30, loss: 0.00010651408229023218\n",
            "step: 40, loss: 0.000123142875963822\n",
            "step: 50, loss: 0.00011717052257154137\n",
            "step: 60, loss: 0.0007427202071994543\n",
            "step: 70, loss: 7.783887849655002e-05\n",
            "step: 80, loss: 0.00010036958701675758\n",
            "step: 90, loss: 4.311038355808705e-05\n",
            "step: 100, loss: 6.342178676277399e-05\n",
            "step: 110, loss: 0.028077254071831703\n",
            "step: 120, loss: 5.1888600864913315e-05\n",
            "step: 130, loss: 9.2546601081267e-05\n",
            "step: 140, loss: 0.00011016250937245786\n",
            "step: 150, loss: 0.00010472557914908975\n",
            "step: 160, loss: 0.21262116730213165\n",
            "step: 170, loss: 0.00023967577726580203\n",
            "step: 180, loss: 0.0005374367465265095\n",
            "step: 190, loss: 0.00039584090700373054\n",
            "step: 200, loss: 0.00023133025388233364\n",
            "step: 210, loss: 0.0029492906760424376\n",
            "step: 220, loss: 0.0004906083340756595\n",
            "step: 230, loss: 0.00026946025900542736\n",
            "step: 240, loss: 0.0005674511194229126\n",
            "step: 250, loss: 0.00011293828720226884\n",
            "step: 260, loss: 8.376520418096334e-05\n",
            "step: 270, loss: 0.0001417267048964277\n",
            "step: 280, loss: 0.015614611096680164\n",
            "step: 290, loss: 0.00019347487250342965\n",
            "step: 300, loss: 0.001911377185024321\n",
            "step: 310, loss: 0.04004118964076042\n",
            "step: 320, loss: 0.0030571434181183577\n",
            "step: 330, loss: 0.0004070887225680053\n",
            "step: 340, loss: 5.638599759549834e-05\n",
            "step: 350, loss: 0.018732817843556404\n",
            "step: 360, loss: 8.131441427394748e-05\n",
            "step: 370, loss: 0.00032578635727986693\n",
            "step: 380, loss: 0.002247593132779002\n",
            "step: 390, loss: 0.0010952689917758107\n",
            "step: 400, loss: 0.005220629740506411\n",
            "step: 410, loss: 0.0003081916074734181\n",
            "step: 420, loss: 0.00021851329074706882\n",
            "step: 430, loss: 0.0005889320163987577\n",
            "step: 440, loss: 0.0028042795602232218\n",
            "step: 450, loss: 0.0002860157983377576\n",
            "step: 460, loss: 7.04593476257287e-05\n",
            "step: 470, loss: 0.0018336394568905234\n",
            "step: 480, loss: 0.0004005461814813316\n",
            "step: 490, loss: 0.00018217235628981143\n",
            "step: 500, loss: 0.00010152104368899018\n",
            "step: 510, loss: 6.808750913478434e-05\n",
            "step: 520, loss: 0.005959969013929367\n",
            "step: 530, loss: 0.01497824490070343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9567219152854511, f1=0.9547134935304992, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02572127804160118\n",
            "step: 10, loss: 0.0008524603908881545\n",
            "step: 20, loss: 0.00042771484004333615\n",
            "step: 30, loss: 9.416484681423753e-05\n",
            "step: 40, loss: 0.00048195631825365126\n",
            "step: 50, loss: 7.286202890099958e-05\n",
            "step: 60, loss: 5.893786874366924e-05\n",
            "step: 70, loss: 0.00010221922275377437\n",
            "step: 80, loss: 0.00032938842196017504\n",
            "step: 90, loss: 0.0011420458322390914\n",
            "step: 100, loss: 0.0005534531665034592\n",
            "step: 110, loss: 7.254025695146993e-05\n",
            "step: 120, loss: 0.0002211887767771259\n",
            "step: 130, loss: 0.00014112655480857939\n",
            "step: 140, loss: 4.293692472856492e-05\n",
            "step: 150, loss: 0.00034683645935729146\n",
            "step: 160, loss: 0.00013686285819858313\n",
            "step: 170, loss: 1.8771010218188167e-05\n",
            "step: 180, loss: 3.14655335387215e-05\n",
            "step: 190, loss: 0.00013381308235693723\n",
            "step: 200, loss: 2.7997291908832267e-05\n",
            "step: 210, loss: 8.088284084806219e-05\n",
            "step: 220, loss: 0.024062521755695343\n",
            "step: 230, loss: 1.1063867532357108e-05\n",
            "step: 240, loss: 0.0029527288861572742\n",
            "step: 250, loss: 2.8251308322069235e-05\n",
            "step: 260, loss: 0.004220257978886366\n",
            "step: 270, loss: 0.0005106928292661905\n",
            "step: 280, loss: 0.0006539073656313121\n",
            "step: 290, loss: 6.81742894812487e-05\n",
            "step: 300, loss: 4.3756805098382756e-05\n",
            "step: 310, loss: 0.004604739136993885\n",
            "step: 320, loss: 5.2967789088143036e-05\n",
            "step: 330, loss: 1.7441210729884915e-05\n",
            "step: 340, loss: 0.0012854146771132946\n",
            "step: 350, loss: 0.0003717057406902313\n",
            "step: 360, loss: 0.00013897253666073084\n",
            "step: 370, loss: 0.00039916051900945604\n",
            "step: 380, loss: 0.0031142050866037607\n",
            "step: 390, loss: 0.002687429543584585\n",
            "step: 400, loss: 4.94517880724743e-05\n",
            "step: 410, loss: 0.00010352389654144645\n",
            "step: 420, loss: 0.02414967119693756\n",
            "step: 430, loss: 0.0041947634890675545\n",
            "step: 440, loss: 0.00018355606880504638\n",
            "step: 450, loss: 0.0018176486482843757\n",
            "step: 460, loss: 0.00041365044307895005\n",
            "step: 470, loss: 0.00017179574933834374\n",
            "step: 480, loss: 0.00039064980228431523\n",
            "step: 490, loss: 0.00015089756925590336\n",
            "step: 500, loss: 0.0013857929734513164\n",
            "step: 510, loss: 0.00011037083459086716\n",
            "step: 520, loss: 9.415929525857791e-05\n",
            "step: 530, loss: 0.0026155540253967047\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9543761638733707, f1=0.9551820728291317, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012128234084229916\n",
            "step: 10, loss: 0.00022444268688559532\n",
            "step: 20, loss: 0.0025570944417268038\n",
            "step: 30, loss: 0.0005887630395591259\n",
            "step: 40, loss: 7.200794789241627e-05\n",
            "step: 50, loss: 0.05248482897877693\n",
            "step: 60, loss: 3.9966140320757404e-05\n",
            "step: 70, loss: 0.02014010399580002\n",
            "step: 80, loss: 0.05867846682667732\n",
            "step: 90, loss: 0.00028072608984075487\n",
            "step: 100, loss: 0.024814235046505928\n",
            "step: 110, loss: 0.00010603245755191892\n",
            "step: 120, loss: 5.154606333235279e-05\n",
            "step: 130, loss: 5.869779488421045e-05\n",
            "step: 140, loss: 4.76104105473496e-05\n",
            "step: 150, loss: 2.8387534257490188e-05\n",
            "step: 160, loss: 0.00024162231420632452\n",
            "step: 170, loss: 4.0961229387903586e-05\n",
            "step: 180, loss: 5.009997767047025e-05\n",
            "step: 190, loss: 0.0012280174996703863\n",
            "step: 200, loss: 0.0026833969168365\n",
            "step: 210, loss: 0.00012028906348859891\n",
            "step: 220, loss: 0.00013843194756191224\n",
            "step: 230, loss: 0.00018306390848010778\n",
            "step: 240, loss: 0.002362601226195693\n",
            "step: 250, loss: 2.484623109921813e-05\n",
            "step: 260, loss: 4.036879545310512e-05\n",
            "step: 270, loss: 0.005925278645008802\n",
            "step: 280, loss: 0.0002192443935200572\n",
            "step: 290, loss: 0.0015684866812080145\n",
            "step: 300, loss: 0.003307133447378874\n",
            "step: 310, loss: 0.018003085628151894\n",
            "step: 320, loss: 0.000573617173358798\n",
            "step: 330, loss: 0.005755649413913488\n",
            "step: 340, loss: 4.514306419878267e-05\n",
            "step: 350, loss: 0.03557039797306061\n",
            "step: 360, loss: 0.0002870369062293321\n",
            "step: 370, loss: 0.0005274181021377444\n",
            "step: 380, loss: 5.5302771215792745e-05\n",
            "step: 390, loss: 0.019550610333681107\n",
            "step: 400, loss: 2.226779906777665e-05\n",
            "step: 410, loss: 0.0004008712712675333\n",
            "step: 420, loss: 0.0013434007996693254\n",
            "step: 430, loss: 0.00010873114661080763\n",
            "step: 440, loss: 0.0001675194944255054\n",
            "step: 450, loss: 0.0017873116303235292\n",
            "step: 460, loss: 3.102980917901732e-05\n",
            "step: 470, loss: 0.00791512243449688\n",
            "step: 480, loss: 0.00010626567382132635\n",
            "step: 490, loss: 2.601288906589616e-05\n",
            "step: 500, loss: 7.349738734774292e-05\n",
            "step: 510, loss: 0.0001232834765687585\n",
            "step: 520, loss: 0.00036049395566806197\n",
            "step: 530, loss: 0.001932897255755961\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.956440281030445, f1=0.9475177304964538, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.347940118052065e-05\n",
            "step: 10, loss: 9.342050179839134e-05\n",
            "step: 20, loss: 5.500163752003573e-05\n",
            "step: 30, loss: 4.474636079976335e-05\n",
            "step: 40, loss: 0.00015427089238073677\n",
            "step: 50, loss: 0.0097280815243721\n",
            "step: 60, loss: 0.00012889114441350102\n",
            "step: 70, loss: 0.004143678117543459\n",
            "step: 80, loss: 0.00039514509262517095\n",
            "step: 90, loss: 0.0020481799729168415\n",
            "step: 100, loss: 6.138846219982952e-05\n",
            "step: 110, loss: 6.489954103017226e-05\n",
            "step: 120, loss: 5.427708674687892e-05\n",
            "step: 130, loss: 6.74592811265029e-05\n",
            "step: 140, loss: 9.284985571866855e-05\n",
            "step: 150, loss: 0.0031548815313726664\n",
            "step: 160, loss: 0.00011271088442299515\n",
            "step: 170, loss: 0.007218631450086832\n",
            "step: 180, loss: 0.0003158251929562539\n",
            "step: 190, loss: 0.00045826658606529236\n",
            "step: 200, loss: 2.261535519210156e-05\n",
            "step: 210, loss: 2.64741393039003e-05\n",
            "step: 220, loss: 2.5360044674016535e-05\n",
            "step: 230, loss: 0.003024669364094734\n",
            "step: 240, loss: 0.0013787730131298304\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 250, loss: 4.77458888781257e-05\n",
            "step: 260, loss: 0.00025166181148961186\n",
            "step: 270, loss: 9.962904005078599e-05\n",
            "step: 280, loss: 2.0741577827720903e-05\n",
            "step: 290, loss: 5.807812340208329e-05\n",
            "step: 300, loss: 2.9215749236755073e-05\n",
            "step: 310, loss: 2.3196298570837826e-05\n",
            "step: 320, loss: 6.589356053154916e-05\n",
            "step: 330, loss: 8.030454773688689e-05\n",
            "step: 340, loss: 0.0022578751668334007\n",
            "step: 350, loss: 2.2500888007925823e-05\n",
            "step: 360, loss: 0.07852493971586227\n",
            "step: 370, loss: 1.4457173165283166e-05\n",
            "step: 380, loss: 2.5203409677487798e-05\n",
            "step: 390, loss: 2.9762768463115208e-05\n",
            "step: 400, loss: 6.930893869139254e-05\n",
            "step: 410, loss: 3.9370061131194234e-05\n",
            "step: 420, loss: 0.0006194551824592054\n",
            "step: 430, loss: 0.00010131680755876005\n",
            "step: 440, loss: 1.0955793186440133e-05\n",
            "step: 450, loss: 0.00011293950956314802\n",
            "step: 460, loss: 0.0009037639829330146\n",
            "step: 470, loss: 0.001989127602428198\n",
            "step: 480, loss: 9.081990356207825e-06\n",
            "step: 490, loss: 0.0007805439527146518\n",
            "step: 500, loss: 0.0006327811279334128\n",
            "step: 510, loss: 0.00020464675617404282\n",
            "step: 520, loss: 3.0249662813730538e-05\n",
            "step: 530, loss: 2.2775016986997798e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9566028931404573, f1=0.9539040451552211, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019330058712512255\n",
            "step: 10, loss: 0.00013752750237472355\n",
            "step: 20, loss: 1.8249445929541253e-05\n",
            "step: 30, loss: 0.0002495684602763504\n",
            "step: 40, loss: 2.7025065719499253e-05\n",
            "step: 50, loss: 0.00018787347653415054\n",
            "step: 60, loss: 0.00013394579582381994\n",
            "step: 70, loss: 5.439109372673556e-05\n",
            "step: 80, loss: 0.0026804928202182055\n",
            "step: 90, loss: 3.9748643757775426e-05\n",
            "step: 100, loss: 1.9382010577828623e-05\n",
            "step: 110, loss: 0.002386758103966713\n",
            "step: 120, loss: 2.6361780328443274e-05\n",
            "step: 130, loss: 1.897609035950154e-05\n",
            "step: 140, loss: 0.0010342858731746674\n",
            "step: 150, loss: 0.0001332267711404711\n",
            "step: 160, loss: 4.8359557695221156e-05\n",
            "step: 170, loss: 5.983221490168944e-05\n",
            "step: 180, loss: 7.147128053475171e-05\n",
            "step: 190, loss: 0.00010121667583007365\n",
            "step: 200, loss: 3.77962423954159e-05\n",
            "step: 210, loss: 0.047258585691452026\n",
            "step: 220, loss: 1.548171348986216e-05\n",
            "step: 230, loss: 1.7199135982082225e-05\n",
            "step: 240, loss: 0.0017055124044418335\n",
            "step: 250, loss: 0.0004008487449027598\n",
            "step: 260, loss: 8.456972864223644e-05\n",
            "step: 270, loss: 0.0019490254344418645\n",
            "step: 280, loss: 0.00021429017942864448\n",
            "step: 290, loss: 1.2944821719429456e-05\n",
            "step: 300, loss: 2.6663943572202697e-05\n",
            "step: 310, loss: 5.537292963708751e-05\n",
            "step: 320, loss: 2.2172072931425646e-05\n",
            "step: 330, loss: 6.0076647059759125e-05\n",
            "step: 340, loss: 7.913713488960639e-05\n",
            "step: 350, loss: 1.8930702935904264e-05\n",
            "step: 360, loss: 4.350050585344434e-05\n",
            "step: 370, loss: 0.00013925778330303729\n",
            "step: 380, loss: 0.00016000019968487322\n",
            "step: 390, loss: 0.005704862531274557\n",
            "step: 400, loss: 0.0019293763907626271\n",
            "step: 410, loss: 1.0322496564185712e-05\n",
            "step: 420, loss: 3.4744567528832704e-05\n",
            "step: 430, loss: 0.0002513136714696884\n",
            "step: 440, loss: 0.0005823011742904782\n",
            "step: 450, loss: 2.5173156245728023e-05\n",
            "step: 460, loss: 0.03901222348213196\n",
            "step: 470, loss: 1.520254954812117e-05\n",
            "step: 480, loss: 2.1609163013636135e-05\n",
            "step: 490, loss: 0.00016787819913588464\n",
            "step: 500, loss: 0.0002889079914893955\n",
            "step: 510, loss: 0.03301278129220009\n",
            "step: 520, loss: 0.0001809717359719798\n",
            "step: 530, loss: 0.002243814757093787\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.953759925268566, f1=0.9549295774647887, best_f1=0.9458715596330275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.0684850824181922e-05\n",
            "step: 10, loss: 0.0002981781435664743\n",
            "step: 20, loss: 0.0017600965220481157\n",
            "step: 30, loss: 0.011097321286797523\n",
            "step: 40, loss: 2.8541069696075283e-05\n",
            "step: 50, loss: 0.0031007234938442707\n",
            "step: 60, loss: 0.0002637677825987339\n",
            "step: 70, loss: 1.4457322322414257e-05\n",
            "step: 80, loss: 1.4453509720624425e-05\n",
            "step: 90, loss: 2.3563783543067984e-05\n",
            "step: 100, loss: 6.958691756153712e-06\n",
            "step: 110, loss: 0.00037485541542991996\n",
            "step: 120, loss: 1.1887041182490066e-05\n",
            "step: 130, loss: 0.00020298469462431967\n",
            "step: 140, loss: 9.777948434930295e-05\n",
            "step: 150, loss: 1.994064041355159e-05\n",
            "step: 160, loss: 1.8964396076626144e-05\n",
            "step: 170, loss: 1.6643114577163942e-05\n",
            "step: 180, loss: 1.7389122149324976e-05\n",
            "step: 190, loss: 0.00253259832970798\n",
            "step: 200, loss: 0.002353334566578269\n",
            "step: 210, loss: 2.1983805709169246e-05\n",
            "step: 220, loss: 1.4792714864597656e-05\n",
            "step: 230, loss: 0.03047296032309532\n",
            "step: 240, loss: 1.1972785614489112e-05\n",
            "step: 250, loss: 2.55831764661707e-05\n",
            "step: 260, loss: 1.2520330528786872e-05\n",
            "step: 270, loss: 1.5627185348421335e-05\n",
            "step: 280, loss: 1.4408778042707127e-05\n",
            "step: 290, loss: 2.5658700906205922e-05\n",
            "step: 300, loss: 1.8062799426843412e-05\n",
            "step: 310, loss: 0.03487420827150345\n",
            "step: 320, loss: 2.1084179024910554e-05\n",
            "step: 330, loss: 1.9598133803810924e-05\n",
            "step: 340, loss: 0.00022505418746732175\n",
            "step: 350, loss: 1.3537254744733218e-05\n",
            "step: 360, loss: 1.655091909924522e-05\n",
            "step: 370, loss: 0.00026750436518341303\n",
            "step: 380, loss: 3.5807970562018454e-05\n",
            "step: 390, loss: 4.8726134991738945e-05\n",
            "step: 400, loss: 0.002008120296522975\n",
            "step: 410, loss: 4.4884694943903014e-05\n",
            "step: 420, loss: 1.511281243438134e-05\n",
            "step: 430, loss: 6.675620625173906e-06\n",
            "step: 440, loss: 1.8409085896564648e-05\n",
            "step: 450, loss: 0.006706428714096546\n",
            "step: 460, loss: 5.608065112028271e-05\n",
            "step: 470, loss: 1.3958183444628958e-05\n",
            "step: 480, loss: 0.004676513373851776\n",
            "step: 490, loss: 0.004011091776192188\n",
            "step: 500, loss: 0.0011810172582045197\n",
            "step: 510, loss: 8.955460543802474e-06\n",
            "step: 520, loss: 5.0981834647245705e-05\n",
            "step: 530, loss: 7.525000000896398e-06\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9550301344459898, f1=0.9548627268496975, best_f1=0.9458715596330275\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:35, 161.75it/s]\n",
            "load_f1 = 0.956884561891516\n",
            "real_f1 = 0.9565619223659889\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 135.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "tb_EWW7DgNFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "-oQ7ANLogNFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a36240-6a84-4773-a8b0-e07e04c0c091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5241211652755737\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.43443790078163147\n",
            "step: 20, loss: 0.44594648480415344\n",
            "step: 30, loss: 0.33661213517189026\n",
            "step: 40, loss: 0.41665351390838623\n",
            "step: 50, loss: 0.45132264494895935\n",
            "step: 60, loss: 0.45068037509918213\n",
            "step: 70, loss: 0.2930986285209656\n",
            "step: 80, loss: 0.3785971701145172\n",
            "step: 90, loss: 0.30979102849960327\n",
            "step: 100, loss: 0.22987866401672363\n",
            "step: 110, loss: 0.24449662864208221\n",
            "step: 120, loss: 0.3515114486217499\n",
            "step: 130, loss: 0.3043532371520996\n",
            "step: 140, loss: 0.4267582893371582\n",
            "step: 150, loss: 0.30080467462539673\n",
            "step: 160, loss: 0.37002962827682495\n",
            "step: 170, loss: 0.21075378358364105\n",
            "step: 180, loss: 0.15815185010433197\n",
            "step: 190, loss: 0.38318321108818054\n",
            "step: 200, loss: 0.19629622995853424\n",
            "step: 210, loss: 0.3255671560764313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.570342205323194, f1=0.5213270142180094, best_f1=0.5213270142180094\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2634828984737396\n",
            "step: 10, loss: 0.10237053781747818\n",
            "step: 20, loss: 0.32658255100250244\n",
            "step: 30, loss: 0.44521191716194153\n",
            "step: 40, loss: 0.4504004716873169\n",
            "step: 50, loss: 0.138666570186615\n",
            "step: 60, loss: 0.41935110092163086\n",
            "step: 70, loss: 0.1288713961839676\n",
            "step: 80, loss: 0.14023590087890625\n",
            "step: 90, loss: 0.17284998297691345\n",
            "step: 100, loss: 0.4762990176677704\n",
            "step: 110, loss: 0.31396937370300293\n",
            "step: 120, loss: 0.10989823192358017\n",
            "step: 130, loss: 0.256140798330307\n",
            "step: 140, loss: 0.1453375369310379\n",
            "step: 150, loss: 0.38897812366485596\n",
            "step: 160, loss: 0.03263266012072563\n",
            "step: 170, loss: 0.2489130049943924\n",
            "step: 180, loss: 0.19307933747768402\n",
            "step: 190, loss: 0.2904796004295349\n",
            "step: 200, loss: 0.07206842303276062\n",
            "step: 210, loss: 0.11232218891382217\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6574803149606299, f1=0.6652977412731006, best_f1=0.6652977412731006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05588917061686516\n",
            "step: 10, loss: 0.03145894035696983\n",
            "step: 20, loss: 0.20823484659194946\n",
            "step: 30, loss: 0.12246821075677872\n",
            "step: 40, loss: 0.30816149711608887\n",
            "step: 50, loss: 0.12162040174007416\n",
            "step: 60, loss: 0.1946038156747818\n",
            "step: 70, loss: 0.10297788679599762\n",
            "step: 80, loss: 0.1157074049115181\n",
            "step: 90, loss: 0.15712690353393555\n",
            "step: 100, loss: 0.16049812734127045\n",
            "step: 110, loss: 0.10950452089309692\n",
            "step: 120, loss: 0.13599929213523865\n",
            "step: 130, loss: 0.2871009111404419\n",
            "step: 140, loss: 0.13442067801952362\n",
            "step: 150, loss: 0.2099481076002121\n",
            "step: 160, loss: 0.11705181002616882\n",
            "step: 170, loss: 0.2275076061487198\n",
            "step: 180, loss: 0.13025183975696564\n",
            "step: 190, loss: 0.030287671834230423\n",
            "step: 200, loss: 0.18079069256782532\n",
            "step: 210, loss: 0.17313237488269806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6783831282952547, f1=0.6678635547576302, best_f1=0.6678635547576302\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04743769019842148\n",
            "step: 10, loss: 0.09422306716442108\n",
            "step: 20, loss: 0.05190574377775192\n",
            "step: 30, loss: 0.10105524212121964\n",
            "step: 40, loss: 0.03831828013062477\n",
            "step: 50, loss: 0.16591061651706696\n",
            "step: 60, loss: 0.19013521075248718\n",
            "step: 70, loss: 0.06821215897798538\n",
            "step: 80, loss: 0.057881806045770645\n",
            "step: 90, loss: 0.07948202639818192\n",
            "step: 100, loss: 0.1631375551223755\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 110, loss: 0.2816023528575897\n",
            "step: 120, loss: 0.18412256240844727\n",
            "step: 130, loss: 0.31827449798583984\n",
            "step: 140, loss: 0.15612748265266418\n",
            "step: 150, loss: 0.08702666312456131\n",
            "step: 160, loss: 0.19290360808372498\n",
            "step: 170, loss: 0.04638296738266945\n",
            "step: 180, loss: 0.047238729894161224\n",
            "step: 190, loss: 0.1326654851436615\n",
            "step: 200, loss: 0.07990981638431549\n",
            "step: 210, loss: 0.22987307608127594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6554307116104868, f1=0.7033398821218074, best_f1=0.6678635547576302\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13682319223880768\n",
            "step: 10, loss: 0.041050445288419724\n",
            "step: 20, loss: 0.08953451365232468\n",
            "step: 30, loss: 0.005983337294310331\n",
            "step: 40, loss: 0.051840003579854965\n",
            "step: 50, loss: 0.13886597752571106\n",
            "step: 60, loss: 0.09640946984291077\n",
            "step: 70, loss: 0.06681504845619202\n",
            "step: 80, loss: 0.06793150305747986\n",
            "step: 90, loss: 0.0498322956264019\n",
            "step: 100, loss: 0.014620591886341572\n",
            "step: 110, loss: 0.1001613512635231\n",
            "step: 120, loss: 0.06906802207231522\n",
            "step: 130, loss: 0.08657343685626984\n",
            "step: 140, loss: 0.0913141518831253\n",
            "step: 150, loss: 0.11651775985956192\n",
            "step: 160, loss: 0.04729800298810005\n",
            "step: 170, loss: 0.010312489233911037\n",
            "step: 180, loss: 0.054409824311733246\n",
            "step: 190, loss: 0.1300656795501709\n",
            "step: 200, loss: 0.0542294904589653\n",
            "step: 210, loss: 0.0670556053519249\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6852173913043479, f1=0.6765799256505576, best_f1=0.6765799256505576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.037920575588941574\n",
            "step: 10, loss: 0.09935250133275986\n",
            "step: 20, loss: 0.05047175660729408\n",
            "step: 30, loss: 0.039044491946697235\n",
            "step: 40, loss: 0.038307372480630875\n",
            "step: 50, loss: 0.010777845047414303\n",
            "step: 60, loss: 0.059045661240816116\n",
            "step: 70, loss: 0.08215276151895523\n",
            "step: 80, loss: 0.034306369721889496\n",
            "step: 90, loss: 0.020267125219106674\n",
            "step: 100, loss: 0.18734398484230042\n",
            "step: 110, loss: 0.09078368544578552\n",
            "step: 120, loss: 0.018923526629805565\n",
            "step: 130, loss: 0.058542411774396896\n",
            "step: 140, loss: 0.1004919484257698\n",
            "step: 150, loss: 0.026655644178390503\n",
            "step: 160, loss: 0.06387315690517426\n",
            "step: 170, loss: 0.1156022697687149\n",
            "step: 180, loss: 0.04351004585623741\n",
            "step: 190, loss: 0.05037238076329231\n",
            "step: 200, loss: 0.10671468079090118\n",
            "step: 210, loss: 0.07325976341962814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.708955223880597, f1=0.6901960784313725, best_f1=0.6901960784313725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07021745294332504\n",
            "step: 10, loss: 0.0025733786169439554\n",
            "step: 20, loss: 0.011040602810680866\n",
            "step: 30, loss: 0.12368936836719513\n",
            "step: 40, loss: 0.02383687160909176\n",
            "step: 50, loss: 0.035166822373867035\n",
            "step: 60, loss: 0.03185218945145607\n",
            "step: 70, loss: 0.026330310851335526\n",
            "step: 80, loss: 0.02386621944606304\n",
            "step: 90, loss: 0.10420656949281693\n",
            "step: 100, loss: 0.027653688564896584\n",
            "step: 110, loss: 0.1373925507068634\n",
            "step: 120, loss: 0.02941032126545906\n",
            "step: 130, loss: 0.02546476386487484\n",
            "step: 140, loss: 0.015330677852034569\n",
            "step: 150, loss: 0.039759621024131775\n",
            "step: 160, loss: 0.3930414915084839\n",
            "step: 170, loss: 0.027254488319158554\n",
            "step: 180, loss: 0.0228051096200943\n",
            "step: 190, loss: 0.0355144627392292\n",
            "step: 200, loss: 0.017740290611982346\n",
            "step: 210, loss: 0.1583269089460373\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7018255578093306, f1=0.6972860125260961, best_f1=0.6901960784313725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05609815940260887\n",
            "step: 10, loss: 0.06729281693696976\n",
            "step: 20, loss: 0.028711974620819092\n",
            "step: 30, loss: 0.012478360906243324\n",
            "step: 40, loss: 0.02371649071574211\n",
            "step: 50, loss: 0.0010676723904907703\n",
            "step: 60, loss: 0.010317903943359852\n",
            "step: 70, loss: 0.03468482941389084\n",
            "step: 80, loss: 0.08340351283550262\n",
            "step: 90, loss: 0.07343398034572601\n",
            "step: 100, loss: 0.09481272846460342\n",
            "step: 110, loss: 0.02244846522808075\n",
            "step: 120, loss: 0.07533644139766693\n",
            "step: 130, loss: 0.004233808256685734\n",
            "step: 140, loss: 0.01771051622927189\n",
            "step: 150, loss: 0.027869828045368195\n",
            "step: 160, loss: 0.11432760953903198\n",
            "step: 170, loss: 0.09857817739248276\n",
            "step: 180, loss: 0.032830387353897095\n",
            "step: 190, loss: 0.010009906254708767\n",
            "step: 200, loss: 0.04468733072280884\n",
            "step: 210, loss: 0.17432977259159088\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6956521739130435, f1=0.6824817518248175, best_f1=0.6901960784313725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026002101600170135\n",
            "step: 10, loss: 0.04187511280179024\n",
            "step: 20, loss: 0.03138626739382744\n",
            "step: 30, loss: 0.0722200945019722\n",
            "step: 40, loss: 0.018592853099107742\n",
            "step: 50, loss: 0.016308190301060677\n",
            "step: 60, loss: 0.00044186145532876253\n",
            "step: 70, loss: 0.06491202861070633\n",
            "step: 80, loss: 0.0043890634551644325\n",
            "step: 90, loss: 0.004557423293590546\n",
            "step: 100, loss: 0.01568865403532982\n",
            "step: 110, loss: 0.0914885550737381\n",
            "step: 120, loss: 0.01677742600440979\n",
            "step: 130, loss: 0.017997978255152702\n",
            "step: 140, loss: 0.020551718771457672\n",
            "step: 150, loss: 0.056913480162620544\n",
            "step: 160, loss: 0.06069296970963478\n",
            "step: 170, loss: 0.005469661206007004\n",
            "step: 180, loss: 0.03341313451528549\n",
            "step: 190, loss: 0.0033906674943864346\n",
            "step: 200, loss: 0.04044218361377716\n",
            "step: 210, loss: 0.025959014892578125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6870229007633588, f1=0.6934865900383141, best_f1=0.6901960784313725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003310177708044648\n",
            "step: 10, loss: 0.006036111619323492\n",
            "step: 20, loss: 0.0013746211770921946\n",
            "step: 30, loss: 0.008339562453329563\n",
            "step: 40, loss: 0.0028481422923505306\n",
            "step: 50, loss: 0.005405577830970287\n",
            "step: 60, loss: 0.0022610712330788374\n",
            "step: 70, loss: 0.004080811515450478\n",
            "step: 80, loss: 0.013292292132973671\n",
            "step: 90, loss: 0.010401641950011253\n",
            "step: 100, loss: 0.14319199323654175\n",
            "step: 110, loss: 0.0023418564815074205\n",
            "step: 120, loss: 0.1213766559958458\n",
            "step: 130, loss: 0.005605265032500029\n",
            "step: 140, loss: 0.025645600631833076\n",
            "step: 150, loss: 0.024222280830144882\n",
            "step: 160, loss: 0.007702468428760767\n",
            "step: 170, loss: 0.005215305369347334\n",
            "step: 180, loss: 0.07130532711744308\n",
            "step: 190, loss: 0.01747550256550312\n",
            "step: 200, loss: 0.09820973128080368\n",
            "step: 210, loss: 0.00995662622153759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6971428571428572, f1=0.6937984496124031, best_f1=0.6901960784313725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05023317039012909\n",
            "step: 10, loss: 0.011743156239390373\n",
            "step: 20, loss: 0.08644159138202667\n",
            "step: 30, loss: 0.018042508512735367\n",
            "step: 40, loss: 0.018564675003290176\n",
            "step: 50, loss: 0.0029310635291039944\n",
            "step: 60, loss: 0.0010050747077912092\n",
            "step: 70, loss: 0.005632293410599232\n",
            "step: 80, loss: 0.0022487991955131292\n",
            "step: 90, loss: 0.1886778175830841\n",
            "step: 100, loss: 0.037185341119766235\n",
            "step: 110, loss: 0.05846056714653969\n",
            "step: 120, loss: 0.022992165759205818\n",
            "step: 130, loss: 0.003909144084900618\n",
            "step: 140, loss: 0.022169528529047966\n",
            "step: 150, loss: 0.013541724532842636\n",
            "step: 160, loss: 0.0004903601366095245\n",
            "step: 170, loss: 0.009206129238009453\n",
            "step: 180, loss: 0.0699818953871727\n",
            "step: 190, loss: 0.08750049769878387\n",
            "step: 200, loss: 0.0040380158461630344\n",
            "step: 210, loss: 0.019690051674842834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7007575757575758, f1=0.6899224806201552, best_f1=0.6901960784313725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014514211798086762\n",
            "step: 10, loss: 0.0007192039047367871\n",
            "step: 20, loss: 0.04599125310778618\n",
            "step: 30, loss: 0.0005237089935690165\n",
            "step: 40, loss: 0.00025256097433157265\n",
            "step: 50, loss: 0.0014990880154073238\n",
            "step: 60, loss: 0.01488430891185999\n",
            "step: 70, loss: 0.00655446108430624\n",
            "step: 80, loss: 0.1465451717376709\n",
            "step: 90, loss: 0.026242054998874664\n",
            "step: 100, loss: 0.007854118943214417\n",
            "step: 110, loss: 0.0005288972170092165\n",
            "step: 120, loss: 0.0001578186929691583\n",
            "step: 130, loss: 0.0017169718630611897\n",
            "step: 140, loss: 0.0018612346611917019\n",
            "step: 150, loss: 0.0002283575595356524\n",
            "step: 160, loss: 0.006571139674633741\n",
            "step: 170, loss: 0.14552699029445648\n",
            "step: 180, loss: 0.020484020933508873\n",
            "step: 190, loss: 0.01714058220386505\n",
            "step: 200, loss: 0.0027139983139932156\n",
            "step: 210, loss: 0.0014951807679608464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.7142857142857143, f1=0.6980392156862745, best_f1=0.6980392156862745\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0049660042859613895\n",
            "step: 10, loss: 0.0347110815346241\n",
            "step: 20, loss: 0.00040512337000109255\n",
            "step: 30, loss: 0.0010779881849884987\n",
            "step: 40, loss: 0.010337267071008682\n",
            "step: 50, loss: 0.07385195791721344\n",
            "step: 60, loss: 0.010021871887147427\n",
            "step: 70, loss: 0.006175587419420481\n",
            "step: 80, loss: 0.15223127603530884\n",
            "step: 90, loss: 0.026936467736959457\n",
            "step: 100, loss: 0.04365905746817589\n",
            "step: 110, loss: 0.009829921647906303\n",
            "step: 120, loss: 0.0035276366397738457\n",
            "step: 130, loss: 0.0003842586593236774\n",
            "step: 140, loss: 0.017290201038122177\n",
            "step: 150, loss: 0.001178269274532795\n",
            "step: 160, loss: 0.05317419022321701\n",
            "step: 170, loss: 0.013282526284456253\n",
            "step: 180, loss: 0.000410787935834378\n",
            "step: 190, loss: 0.0170601774007082\n",
            "step: 200, loss: 0.05095144733786583\n",
            "step: 210, loss: 0.002183050150051713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7020408163265306, f1=0.6860706860706861, best_f1=0.6980392156862745\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00023832605802454054\n",
            "step: 10, loss: 0.017756395041942596\n",
            "step: 20, loss: 0.005341432057321072\n",
            "step: 30, loss: 0.010725061409175396\n",
            "step: 40, loss: 0.007356422021985054\n",
            "step: 50, loss: 0.002100070007145405\n",
            "step: 60, loss: 0.08556415140628815\n",
            "step: 70, loss: 0.00029165888554416597\n",
            "step: 80, loss: 0.04117683321237564\n",
            "step: 90, loss: 0.0003081090108025819\n",
            "step: 100, loss: 0.0008806965197436512\n",
            "step: 110, loss: 0.0004385994398035109\n",
            "step: 120, loss: 0.00018986270879395306\n",
            "step: 130, loss: 0.019957704469561577\n",
            "step: 140, loss: 0.0004471518623176962\n",
            "step: 150, loss: 0.02866651862859726\n",
            "step: 160, loss: 0.0009634291636757553\n",
            "step: 170, loss: 0.05735566094517708\n",
            "step: 180, loss: 0.0001837316231103614\n",
            "step: 190, loss: 0.028255503624677658\n",
            "step: 200, loss: 0.12247040122747421\n",
            "step: 210, loss: 0.0014975621597841382\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6993603411513859, f1=0.6781857451403888, best_f1=0.6980392156862745\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005461539840325713\n",
            "step: 10, loss: 0.001192693947814405\n",
            "step: 20, loss: 0.003064134856685996\n",
            "step: 30, loss: 0.01980903372168541\n",
            "step: 40, loss: 0.00041402061469852924\n",
            "step: 50, loss: 0.00025247156736440957\n",
            "step: 60, loss: 0.009724412113428116\n",
            "step: 70, loss: 0.004673611838370562\n",
            "step: 80, loss: 0.06037049740552902\n",
            "step: 90, loss: 0.002158720977604389\n",
            "step: 100, loss: 0.002375432290136814\n",
            "step: 110, loss: 0.030019087716937065\n",
            "step: 120, loss: 0.0007589331362396479\n",
            "step: 130, loss: 8.192969107767567e-05\n",
            "step: 140, loss: 0.0016218675300478935\n",
            "step: 150, loss: 0.01599448174238205\n",
            "step: 160, loss: 0.0008662915206514299\n",
            "step: 170, loss: 0.005332739092409611\n",
            "step: 180, loss: 0.009677065536379814\n",
            "step: 190, loss: 0.0030041190329939127\n",
            "step: 200, loss: 0.0011892705224454403\n",
            "step: 210, loss: 0.03724248334765434\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6987951807228915, f1=0.6923076923076923, best_f1=0.6980392156862745\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:09, 234.22it/s]\n",
            "load_f1 = 0.7065637065637066\n",
            "real_f1 = 0.7131782945736435\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 133.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "iIIoASlugNFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344c29d4-176a-4a88-f6cc-05f71a783136"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 438kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 817kB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 497kB/s]\n",
            "Downloading: 100% 501M/501M [00:07<00:00, 62.9MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4918926954269409\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4712750315666199\n",
            "step: 20, loss: 0.2561512291431427\n",
            "step: 30, loss: 0.35697802901268005\n",
            "step: 40, loss: 0.2581608295440674\n",
            "step: 50, loss: 0.30532073974609375\n",
            "step: 60, loss: 0.4883239269256592\n",
            "step: 70, loss: 0.4176587760448456\n",
            "step: 80, loss: 0.16171087324619293\n",
            "step: 90, loss: 0.3027912676334381\n",
            "step: 100, loss: 0.43521955609321594\n",
            "step: 110, loss: 0.23452603816986084\n",
            "step: 120, loss: 0.34064245223999023\n",
            "step: 130, loss: 0.3139839172363281\n",
            "step: 140, loss: 0.1556825190782547\n",
            "step: 150, loss: 0.2967587411403656\n",
            "step: 160, loss: 0.18458661437034607\n",
            "step: 170, loss: 0.5266774892807007\n",
            "step: 180, loss: 0.11781255155801773\n",
            "step: 190, loss: 0.10742013901472092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.699228791773779, f1=0.7142857142857143, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3604196310043335\n",
            "step: 10, loss: 0.3765842616558075\n",
            "step: 20, loss: 0.6941676139831543\n",
            "step: 30, loss: 0.1200530081987381\n",
            "step: 40, loss: 0.24053025245666504\n",
            "step: 50, loss: 0.2727627456188202\n",
            "step: 60, loss: 0.3552014231681824\n",
            "step: 70, loss: 0.17499449849128723\n",
            "step: 80, loss: 0.08475205302238464\n",
            "step: 90, loss: 0.13960018754005432\n",
            "step: 100, loss: 0.16134727001190186\n",
            "step: 110, loss: 0.03744382783770561\n",
            "step: 120, loss: 0.041731663048267365\n",
            "step: 130, loss: 0.08669155836105347\n",
            "step: 140, loss: 0.29621821641921997\n",
            "step: 150, loss: 0.2777673006057739\n",
            "step: 160, loss: 0.1214185431599617\n",
            "step: 170, loss: 0.042276762425899506\n",
            "step: 180, loss: 0.039827801287174225\n",
            "step: 190, loss: 0.18281544744968414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.8134715025906736, f1=0.8142493638676844, best_f1=0.8142493638676844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08297470211982727\n",
            "step: 10, loss: 0.11210418492555618\n",
            "step: 20, loss: 0.01200534775853157\n",
            "step: 30, loss: 0.024622181430459023\n",
            "step: 40, loss: 0.028763655573129654\n",
            "step: 50, loss: 0.06670855730772018\n",
            "step: 60, loss: 0.09740494191646576\n",
            "step: 70, loss: 0.05311588570475578\n",
            "step: 80, loss: 0.2050590068101883\n",
            "step: 90, loss: 0.0863044261932373\n",
            "step: 100, loss: 0.04254288226366043\n",
            "step: 110, loss: 0.19869327545166016\n",
            "step: 120, loss: 0.16581328213214874\n",
            "step: 130, loss: 0.12640048563480377\n",
            "step: 140, loss: 0.05295778065919876\n",
            "step: 150, loss: 0.08115669339895248\n",
            "step: 160, loss: 0.142693892121315\n",
            "step: 170, loss: 0.18727467954158783\n",
            "step: 180, loss: 0.15022313594818115\n",
            "step: 190, loss: 0.025480041280388832\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.847457627118644, f1=0.848314606741573, best_f1=0.848314606741573\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024139581248164177\n",
            "step: 10, loss: 0.06780776381492615\n",
            "step: 20, loss: 0.08978402614593506\n",
            "step: 30, loss: 0.011460666544735432\n",
            "step: 40, loss: 0.026381174102425575\n",
            "step: 50, loss: 0.054863978177309036\n",
            "step: 60, loss: 0.007984486408531666\n",
            "step: 70, loss: 0.05622575059533119\n",
            "step: 80, loss: 0.00821415800601244\n",
            "step: 90, loss: 0.03466152399778366\n",
            "step: 100, loss: 0.00904934760183096\n",
            "step: 110, loss: 0.13390882313251495\n",
            "step: 120, loss: 0.02936028130352497\n",
            "step: 130, loss: 0.013858640566468239\n",
            "step: 140, loss: 0.10921871662139893\n",
            "step: 150, loss: 0.006890904624015093\n",
            "step: 160, loss: 0.005702361464500427\n",
            "step: 170, loss: 0.029777083545923233\n",
            "step: 180, loss: 0.15494404733181\n",
            "step: 190, loss: 0.008707121945917606\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8507042253521128, f1=0.8547008547008547, best_f1=0.8547008547008547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.019285976886749268\n",
            "step: 10, loss: 0.015189510770142078\n",
            "step: 20, loss: 0.023074183613061905\n",
            "step: 30, loss: 0.006742666009813547\n",
            "step: 40, loss: 0.01968725025653839\n",
            "step: 50, loss: 0.012022417038679123\n",
            "step: 60, loss: 0.0038200027775019407\n",
            "step: 70, loss: 0.0030179740861058235\n",
            "step: 80, loss: 0.010210159234702587\n",
            "step: 90, loss: 0.0337081141769886\n",
            "step: 100, loss: 0.0314621776342392\n",
            "step: 110, loss: 0.1575438380241394\n",
            "step: 120, loss: 0.010511073283851147\n",
            "step: 130, loss: 0.23523326218128204\n",
            "step: 140, loss: 0.015883704647421837\n",
            "step: 150, loss: 0.04496880620718002\n",
            "step: 160, loss: 0.0169965960085392\n",
            "step: 170, loss: 0.033938731998205185\n",
            "step: 180, loss: 0.025052174925804138\n",
            "step: 190, loss: 0.004301917273551226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.8254847645429362, f1=0.8459383753501402, best_f1=0.8547008547008547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020004378631711006\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 10, loss: 0.003956093452870846\n",
            "step: 20, loss: 0.03743424266576767\n",
            "step: 30, loss: 0.03053566813468933\n",
            "step: 40, loss: 0.019697025418281555\n",
            "step: 50, loss: 0.027588501572608948\n",
            "step: 60, loss: 0.015750855207443237\n",
            "step: 70, loss: 0.018344448879361153\n",
            "step: 80, loss: 0.017196182161569595\n",
            "step: 90, loss: 0.0016079619526863098\n",
            "step: 100, loss: 0.01484045572578907\n",
            "step: 110, loss: 0.0680420994758606\n",
            "step: 120, loss: 0.006002782378345728\n",
            "step: 130, loss: 0.10363998264074326\n",
            "step: 140, loss: 0.004328422714024782\n",
            "step: 150, loss: 0.0027920384891331196\n",
            "step: 160, loss: 0.015489564277231693\n",
            "step: 170, loss: 0.44796815514564514\n",
            "step: 180, loss: 0.01791338063776493\n",
            "step: 190, loss: 0.009640678763389587\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8426966292134831, f1=0.8352272727272728, best_f1=0.8547008547008547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007832088507711887\n",
            "step: 10, loss: 0.0026741756591945887\n",
            "step: 20, loss: 0.0012338673695921898\n",
            "step: 30, loss: 0.017024634405970573\n",
            "step: 40, loss: 0.0006446252227760851\n",
            "step: 50, loss: 0.0005442042020149529\n",
            "step: 60, loss: 0.005648746620863676\n",
            "step: 70, loss: 0.0011668062070384622\n",
            "step: 80, loss: 0.0005918241804465652\n",
            "step: 90, loss: 0.0007380711031146348\n",
            "step: 100, loss: 0.18310534954071045\n",
            "step: 110, loss: 0.01162942498922348\n",
            "step: 120, loss: 0.03183289244771004\n",
            "step: 130, loss: 0.014619077555835247\n",
            "step: 140, loss: 0.00565737672150135\n",
            "step: 150, loss: 0.008044583722949028\n",
            "step: 160, loss: 0.3963927626609802\n",
            "step: 170, loss: 0.01907014101743698\n",
            "step: 180, loss: 0.048856522887945175\n",
            "step: 190, loss: 0.07643114775419235\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.842391304347826, f1=0.8429752066115702, best_f1=0.8547008547008547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05769599601626396\n",
            "step: 10, loss: 0.0036962421145290136\n",
            "step: 20, loss: 0.0009401876013725996\n",
            "step: 30, loss: 0.12215866893529892\n",
            "step: 40, loss: 0.021762870252132416\n",
            "step: 50, loss: 0.024926837533712387\n",
            "step: 60, loss: 0.011331051588058472\n",
            "step: 70, loss: 0.0035463757812976837\n",
            "step: 80, loss: 0.04189949482679367\n",
            "step: 90, loss: 0.0025916637387126684\n",
            "step: 100, loss: 0.0011642187600955367\n",
            "step: 110, loss: 0.02064104750752449\n",
            "step: 120, loss: 0.019087832421064377\n",
            "step: 130, loss: 0.0013417252339422703\n",
            "step: 140, loss: 0.0031616119667887688\n",
            "step: 150, loss: 0.005573805421590805\n",
            "step: 160, loss: 0.00317584746517241\n",
            "step: 170, loss: 0.0016475007869303226\n",
            "step: 180, loss: 0.008776814676821232\n",
            "step: 190, loss: 0.000602254644036293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8390501319261214, f1=0.8709677419354839, best_f1=0.8547008547008547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007185527356341481\n",
            "step: 10, loss: 0.00033666682429611683\n",
            "step: 20, loss: 0.014126715250313282\n",
            "step: 30, loss: 0.0033174154814332724\n",
            "step: 40, loss: 0.030074112117290497\n",
            "step: 50, loss: 0.0058126444928348064\n",
            "step: 60, loss: 0.004819531459361315\n",
            "step: 70, loss: 0.0011862042592838407\n",
            "step: 80, loss: 0.009491339325904846\n",
            "step: 90, loss: 0.005912358406931162\n",
            "step: 100, loss: 0.004291401710361242\n",
            "step: 110, loss: 0.0018359945388510823\n",
            "step: 120, loss: 0.061788447201251984\n",
            "step: 130, loss: 0.008493001572787762\n",
            "step: 140, loss: 0.07157150655984879\n",
            "step: 150, loss: 0.004947511479258537\n",
            "step: 160, loss: 0.0018557360162958503\n",
            "step: 170, loss: 0.013375692069530487\n",
            "step: 180, loss: 0.0015748664736747742\n",
            "step: 190, loss: 0.00022076221648603678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8346456692913385, f1=0.8670212765957447, best_f1=0.8547008547008547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002555790124461055\n",
            "step: 10, loss: 0.0006585847586393356\n",
            "step: 20, loss: 0.0019866004586219788\n",
            "step: 30, loss: 0.008604292757809162\n",
            "step: 40, loss: 0.007657147012650967\n",
            "step: 50, loss: 0.0006530535756610334\n",
            "step: 60, loss: 0.001117998268455267\n",
            "step: 70, loss: 0.0007633338682353497\n",
            "step: 80, loss: 0.001052923733368516\n",
            "step: 90, loss: 0.0003582282515708357\n",
            "step: 100, loss: 0.00044309403165243566\n",
            "step: 110, loss: 0.00029448975692503154\n",
            "step: 120, loss: 0.0010535630863159895\n",
            "step: 130, loss: 0.029404444620013237\n",
            "step: 140, loss: 0.0003178368497174233\n",
            "step: 150, loss: 0.00036636582808569074\n",
            "step: 160, loss: 0.0025085320230573416\n",
            "step: 170, loss: 0.002864279318600893\n",
            "step: 180, loss: 0.006748618558049202\n",
            "step: 190, loss: 0.0004271866928320378\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8501362397820164, f1=0.8709677419354839, best_f1=0.8547008547008547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010061418870463967\n",
            "step: 10, loss: 0.001045320532284677\n",
            "step: 20, loss: 0.0005645817145705223\n",
            "step: 30, loss: 0.038918063044548035\n",
            "step: 40, loss: 0.00047996515058912337\n",
            "step: 50, loss: 0.000965704268310219\n",
            "step: 60, loss: 0.007685643155127764\n",
            "step: 70, loss: 0.00030771855381317437\n",
            "step: 80, loss: 0.0013072685105726123\n",
            "step: 90, loss: 0.00011949526378884912\n",
            "step: 100, loss: 0.00061986711807549\n",
            "step: 110, loss: 0.1390155702829361\n",
            "step: 120, loss: 0.00019956266623921692\n",
            "step: 130, loss: 0.017027482390403748\n",
            "step: 140, loss: 0.0013207399751991034\n",
            "step: 150, loss: 0.0010880683548748493\n",
            "step: 160, loss: 0.00013807244249619544\n",
            "step: 170, loss: 0.0004164805286563933\n",
            "step: 180, loss: 0.007359067909419537\n",
            "step: 190, loss: 0.0019046271918341517\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.8509485094850948, f1=0.854794520547945, best_f1=0.854794520547945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009981209877878428\n",
            "step: 10, loss: 0.006849199533462524\n",
            "step: 20, loss: 0.012570916675031185\n",
            "step: 30, loss: 0.09386710077524185\n",
            "step: 40, loss: 0.00036400946555659175\n",
            "step: 50, loss: 0.002133840462192893\n",
            "step: 60, loss: 0.07486637681722641\n",
            "step: 70, loss: 0.002396195661276579\n",
            "step: 80, loss: 0.000704685808159411\n",
            "step: 90, loss: 0.0006146389059722424\n",
            "step: 100, loss: 0.0008182103629224002\n",
            "step: 110, loss: 0.0032628776971250772\n",
            "step: 120, loss: 0.0003571758861653507\n",
            "step: 130, loss: 0.002284160116687417\n",
            "step: 140, loss: 0.000169659688253887\n",
            "step: 150, loss: 0.0012836985988542438\n",
            "step: 160, loss: 0.00016156755737029016\n",
            "step: 170, loss: 0.0004095155745744705\n",
            "step: 180, loss: 0.0001582552067702636\n",
            "step: 190, loss: 0.0003923574695363641\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8435013262599469, f1=0.8586666666666667, best_f1=0.854794520547945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005571513320319355\n",
            "step: 10, loss: 0.0004380170430522412\n",
            "step: 20, loss: 0.00010626792936818674\n",
            "step: 30, loss: 0.0015533509431406856\n",
            "step: 40, loss: 0.0003096687432844192\n",
            "step: 50, loss: 0.00018069504585582763\n",
            "step: 60, loss: 0.00020842376397922635\n",
            "step: 70, loss: 0.00024955280241556466\n",
            "step: 80, loss: 0.00011204877228010446\n",
            "step: 90, loss: 0.001734739518724382\n",
            "step: 100, loss: 8.455676288576797e-05\n",
            "step: 110, loss: 0.00036341301165521145\n",
            "step: 120, loss: 6.644670065725222e-05\n",
            "step: 130, loss: 0.00027083922759629786\n",
            "step: 140, loss: 0.0019030869007110596\n",
            "step: 150, loss: 0.00015497510321438313\n",
            "step: 160, loss: 0.0001684119924902916\n",
            "step: 170, loss: 0.00010332516831113026\n",
            "step: 180, loss: 0.0001420611224602908\n",
            "step: 190, loss: 0.11812934279441833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8432432432432432, f1=0.8617886178861788, best_f1=0.854794520547945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005919057875871658\n",
            "step: 10, loss: 0.0002581854350864887\n",
            "step: 20, loss: 0.0005565115134231746\n",
            "step: 30, loss: 0.003045337973162532\n",
            "step: 40, loss: 0.00018968958465848118\n",
            "step: 50, loss: 0.00011139478738186881\n",
            "step: 60, loss: 0.00021066521003376693\n",
            "step: 70, loss: 0.0003169781994074583\n",
            "step: 80, loss: 9.33187548071146e-05\n",
            "step: 90, loss: 5.671857434208505e-05\n",
            "step: 100, loss: 0.00021634675795212388\n",
            "step: 110, loss: 0.00015251168224494904\n",
            "step: 120, loss: 0.0013881551567465067\n",
            "step: 130, loss: 0.0007298632990568876\n",
            "step: 140, loss: 0.0007165850838646293\n",
            "step: 150, loss: 0.0002918803074862808\n",
            "step: 160, loss: 0.00010633642523316666\n",
            "step: 170, loss: 6.758698145858943e-05\n",
            "step: 180, loss: 0.0007274462841451168\n",
            "step: 190, loss: 0.0005422031390480697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.842391304347826, f1=0.8586956521739131, best_f1=0.854794520547945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000252085505053401\n",
            "step: 10, loss: 0.0004639169492293149\n",
            "step: 20, loss: 0.0011266060173511505\n",
            "step: 30, loss: 0.002103119622915983\n",
            "step: 40, loss: 0.0003205986868124455\n",
            "step: 50, loss: 0.00018508506764192134\n",
            "step: 60, loss: 0.00013072407455183566\n",
            "step: 70, loss: 0.00024351802130695432\n",
            "step: 80, loss: 0.00046377076068893075\n",
            "step: 90, loss: 0.0006133398856036365\n",
            "step: 100, loss: 0.001135754631832242\n",
            "step: 110, loss: 0.0003385645686648786\n",
            "step: 120, loss: 0.0013206492876634002\n",
            "step: 130, loss: 0.00018467647896613926\n",
            "step: 140, loss: 0.00011075169459218159\n",
            "step: 150, loss: 8.33084195619449e-05\n",
            "step: 160, loss: 8.549885387765244e-05\n",
            "step: 170, loss: 0.0002928194880951196\n",
            "step: 180, loss: 0.00010179041419178247\n",
            "step: 190, loss: 0.00010326293704565614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8432432432432432, f1=0.8648648648648648, best_f1=0.854794520547945\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:13, 148.68it/s]\n",
            "load_f1 = 0.8548387096774193\n",
            "real_f1 = 0.8418230563002681\n",
            "733it [00:00, 3620.52it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 133.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE TEXTUAL"
      ],
      "metadata": {
        "id": "vWkqC6MWgNFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "dtPR9KRSgNFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "62jt5GiEgNFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a6bb48-ea37-4581-95a0-9fcbcb6f6091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.49284762144088745\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5136566162109375\n",
            "step: 20, loss: 0.3395736813545227\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 30, loss: 0.3686418831348419\n",
            "step: 40, loss: 0.565153181552887\n",
            "step: 50, loss: 0.3149006962776184\n",
            "step: 60, loss: 0.6160929799079895\n",
            "step: 70, loss: 0.31300657987594604\n",
            "step: 80, loss: 0.22753013670444489\n",
            "step: 90, loss: 0.2279149740934372\n",
            "step: 100, loss: 0.17792610824108124\n",
            "step: 110, loss: 0.4275267422199249\n",
            "step: 120, loss: 0.23731297254562378\n",
            "step: 130, loss: 0.22978562116622925\n",
            "step: 140, loss: 0.1898873895406723\n",
            "step: 150, loss: 0.18379954993724823\n",
            "step: 160, loss: 0.20060628652572632\n",
            "step: 170, loss: 0.07422297447919846\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.7531172069825437, f1=0.7737226277372263, best_f1=0.7737226277372263\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16171807050704956\n",
            "step: 10, loss: 0.16105054318904877\n",
            "step: 20, loss: 0.07923094183206558\n",
            "step: 30, loss: 0.10745285451412201\n",
            "step: 40, loss: 0.018767962232232094\n",
            "step: 50, loss: 0.0535123310983181\n",
            "step: 60, loss: 0.12119244039058685\n",
            "step: 70, loss: 0.18373382091522217\n",
            "step: 80, loss: 0.17134533822536469\n",
            "step: 90, loss: 0.026194889098405838\n",
            "step: 100, loss: 0.1671024113893509\n",
            "step: 110, loss: 0.07386984676122665\n",
            "step: 120, loss: 0.06359045207500458\n",
            "step: 130, loss: 0.23563185334205627\n",
            "step: 140, loss: 0.13428816199302673\n",
            "step: 150, loss: 0.14354108273983002\n",
            "step: 160, loss: 0.07039191573858261\n",
            "step: 170, loss: 0.04048002138733864\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.8243902439024391, f1=0.8253968253968255, best_f1=0.8253968253968255\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1013185903429985\n",
            "step: 10, loss: 0.09872236847877502\n",
            "step: 20, loss: 0.08943221718072891\n",
            "step: 30, loss: 0.05231807753443718\n",
            "step: 40, loss: 0.13896073400974274\n",
            "step: 50, loss: 0.0725870355963707\n",
            "step: 60, loss: 0.025374943390488625\n",
            "step: 70, loss: 0.17499011754989624\n",
            "step: 80, loss: 0.041043587028980255\n",
            "step: 90, loss: 0.19674386084079742\n",
            "step: 100, loss: 0.0087643563747406\n",
            "step: 110, loss: 0.10757680982351303\n",
            "step: 120, loss: 0.14395231008529663\n",
            "step: 130, loss: 0.02940693497657776\n",
            "step: 140, loss: 0.23174616694450378\n",
            "step: 150, loss: 0.004877994768321514\n",
            "step: 160, loss: 0.07778275012969971\n",
            "step: 170, loss: 0.011017462238669395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8312958435207823, f1=0.8454545454545455, best_f1=0.8454545454545455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020085912197828293\n",
            "step: 10, loss: 0.019111081957817078\n",
            "step: 20, loss: 0.0009539254242554307\n",
            "step: 30, loss: 0.07429079711437225\n",
            "step: 40, loss: 0.08526033163070679\n",
            "step: 50, loss: 0.017512142658233643\n",
            "step: 60, loss: 0.035254258662462234\n",
            "step: 70, loss: 0.0007419736357405782\n",
            "step: 80, loss: 0.19125954806804657\n",
            "step: 90, loss: 0.14047522842884064\n",
            "step: 100, loss: 0.06961943954229355\n",
            "step: 110, loss: 0.18598754703998566\n",
            "step: 120, loss: 0.1354493647813797\n",
            "step: 130, loss: 0.024766314774751663\n",
            "step: 140, loss: 0.13865981996059418\n",
            "step: 150, loss: 0.10321474820375443\n",
            "step: 160, loss: 0.0006839095731265843\n",
            "step: 170, loss: 0.057376351207494736\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.830188679245283, f1=0.8311688311688312, best_f1=0.8454545454545455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01462436094880104\n",
            "step: 10, loss: 0.029134521260857582\n",
            "step: 20, loss: 0.005712189711630344\n",
            "step: 30, loss: 0.0006824751035310328\n",
            "step: 40, loss: 0.029590127989649773\n",
            "step: 50, loss: 0.004517017863690853\n",
            "step: 60, loss: 0.06331193447113037\n",
            "step: 70, loss: 0.004027754999697208\n",
            "step: 80, loss: 0.0016688397154211998\n",
            "step: 90, loss: 0.0042341104708611965\n",
            "step: 100, loss: 0.014366880059242249\n",
            "step: 110, loss: 0.05879803001880646\n",
            "step: 120, loss: 0.006094752345234156\n",
            "step: 130, loss: 0.0004208097525406629\n",
            "step: 140, loss: 0.08633052557706833\n",
            "step: 150, loss: 0.042638394981622696\n",
            "step: 160, loss: 0.003472675569355488\n",
            "step: 170, loss: 0.02779374271631241\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8350515463917526, f1=0.8753056234718827, best_f1=0.8753056234718827\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015072423964738846\n",
            "step: 10, loss: 0.017319360747933388\n",
            "step: 20, loss: 0.1253243088722229\n",
            "step: 30, loss: 0.00844449084252119\n",
            "step: 40, loss: 0.0015754579799249768\n",
            "step: 50, loss: 0.004724185448139906\n",
            "step: 60, loss: 0.011213560588657856\n",
            "step: 70, loss: 0.0244334377348423\n",
            "step: 80, loss: 0.0013873694697394967\n",
            "step: 90, loss: 0.019344763830304146\n",
            "step: 100, loss: 0.001215797383338213\n",
            "step: 110, loss: 0.007290862966328859\n",
            "step: 120, loss: 0.008295570500195026\n",
            "step: 130, loss: 0.0020958841778337955\n",
            "step: 140, loss: 0.023816658183932304\n",
            "step: 150, loss: 0.0021290890872478485\n",
            "step: 160, loss: 0.2196730375289917\n",
            "step: 170, loss: 0.024246416985988617\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8238095238095239, f1=0.874141876430206, best_f1=0.8753056234718827\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0324200838804245\n",
            "step: 10, loss: 0.03538689389824867\n",
            "step: 20, loss: 0.000934130628593266\n",
            "step: 30, loss: 0.001382271060720086\n",
            "step: 40, loss: 0.002989828819409013\n",
            "step: 50, loss: 0.0007541444501839578\n",
            "step: 60, loss: 0.007084340788424015\n",
            "step: 70, loss: 0.005917282775044441\n",
            "step: 80, loss: 0.016372084617614746\n",
            "step: 90, loss: 0.03797626122832298\n",
            "step: 100, loss: 0.00035144377034157515\n",
            "step: 110, loss: 0.004201752133667469\n",
            "step: 120, loss: 0.06733693182468414\n",
            "step: 130, loss: 0.004353213123977184\n",
            "step: 140, loss: 0.001495116506703198\n",
            "step: 150, loss: 0.1688673198223114\n",
            "step: 160, loss: 0.0052458420395851135\n",
            "step: 170, loss: 0.0458984375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8337730870712401, f1=0.8715365239294711, best_f1=0.8753056234718827\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.029308602213859558\n",
            "step: 10, loss: 0.0015620646299794316\n",
            "step: 20, loss: 0.0003018030838575214\n",
            "step: 30, loss: 0.00023605402384418994\n",
            "step: 40, loss: 0.0002272760175401345\n",
            "step: 50, loss: 0.03139115124940872\n",
            "step: 60, loss: 0.0003263259422965348\n",
            "step: 70, loss: 0.04861677810549736\n",
            "step: 80, loss: 0.000801338697783649\n",
            "step: 90, loss: 0.030205806717276573\n",
            "step: 100, loss: 0.0005440320819616318\n",
            "step: 110, loss: 0.02606586180627346\n",
            "step: 120, loss: 0.048637665808200836\n",
            "step: 130, loss: 0.0008315254235640168\n",
            "step: 140, loss: 0.027670303359627724\n",
            "step: 150, loss: 0.0017107619205489755\n",
            "step: 160, loss: 0.0018502813763916492\n",
            "step: 170, loss: 0.058890316635370255\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.8396946564885497, f1=0.8669950738916256, best_f1=0.8669950738916256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008108281530439854\n",
            "step: 10, loss: 0.01551426388323307\n",
            "step: 20, loss: 0.00750518636777997\n",
            "step: 30, loss: 0.00043830485083162785\n",
            "step: 40, loss: 0.043680932372808456\n",
            "step: 50, loss: 0.000742826028726995\n",
            "step: 60, loss: 0.010619193315505981\n",
            "step: 70, loss: 0.0008164162863977253\n",
            "step: 80, loss: 0.004296497441828251\n",
            "step: 90, loss: 0.011618314310908318\n",
            "step: 100, loss: 0.0002320275962119922\n",
            "step: 110, loss: 0.001307716709561646\n",
            "step: 120, loss: 0.014925416558980942\n",
            "step: 130, loss: 0.0002430109743727371\n",
            "step: 140, loss: 0.000711198546923697\n",
            "step: 150, loss: 0.05074002593755722\n",
            "step: 160, loss: 0.10793625563383102\n",
            "step: 170, loss: 0.052745673805475235\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8341968911917099, f1=0.8535353535353535, best_f1=0.8669950738916256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003584185615181923\n",
            "step: 10, loss: 0.0088350223377347\n",
            "step: 20, loss: 0.0009156054584309459\n",
            "step: 30, loss: 0.05248911678791046\n",
            "step: 40, loss: 0.00017958629177883267\n",
            "step: 50, loss: 0.0035935956984758377\n",
            "step: 60, loss: 0.006407937966287136\n",
            "step: 70, loss: 0.0016250057378783822\n",
            "step: 80, loss: 0.0571129210293293\n",
            "step: 90, loss: 0.001675884472206235\n",
            "step: 100, loss: 0.004340074490755796\n",
            "step: 110, loss: 0.013525545597076416\n",
            "step: 120, loss: 0.000524440489243716\n",
            "step: 130, loss: 0.003596547292545438\n",
            "step: 140, loss: 0.00016382934700232\n",
            "step: 150, loss: 0.00012729976151604205\n",
            "step: 160, loss: 0.00018251432629767805\n",
            "step: 170, loss: 0.07121390849351883\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.83248730964467, f1=0.8621553884711779, best_f1=0.8669950738916256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0032921943347901106\n",
            "step: 10, loss: 0.01923775114119053\n",
            "step: 20, loss: 0.00028066744562238455\n",
            "step: 30, loss: 0.00016752367082517594\n",
            "step: 40, loss: 0.00010239758557872847\n",
            "step: 50, loss: 0.01629292592406273\n",
            "step: 60, loss: 0.007006316911429167\n",
            "step: 70, loss: 0.0004570115124806762\n",
            "step: 80, loss: 0.03656401485204697\n",
            "step: 90, loss: 0.002089596586301923\n",
            "step: 100, loss: 0.0009314292692579329\n",
            "step: 110, loss: 8.939770486904308e-05\n",
            "step: 120, loss: 0.0007657362730242312\n",
            "step: 130, loss: 0.002084680600091815\n",
            "step: 140, loss: 0.00014474763884209096\n",
            "step: 150, loss: 9.934262197930366e-05\n",
            "step: 160, loss: 0.0013037367025390267\n",
            "step: 170, loss: 0.001722893095575273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8275862068965518, f1=0.865979381443299, best_f1=0.8669950738916256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009823222644627094\n",
            "step: 10, loss: 0.004458756651729345\n",
            "step: 20, loss: 7.04884369042702e-05\n",
            "step: 30, loss: 0.0716029554605484\n",
            "step: 40, loss: 0.00020296200818847865\n",
            "step: 50, loss: 0.0014030812308192253\n",
            "step: 60, loss: 0.00017988981562666595\n",
            "step: 70, loss: 4.372554394649342e-05\n",
            "step: 80, loss: 0.00011753395665436983\n",
            "step: 90, loss: 0.03998088464140892\n",
            "step: 100, loss: 4.184467616141774e-05\n",
            "step: 110, loss: 0.07675523310899734\n",
            "step: 120, loss: 0.01959347352385521\n",
            "step: 130, loss: 0.00020462667453102767\n",
            "step: 140, loss: 0.00016905351367313415\n",
            "step: 150, loss: 0.019222911447286606\n",
            "step: 160, loss: 0.0004955215845257044\n",
            "step: 170, loss: 5.361879084375687e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8266666666666667, f1=0.865979381443299, best_f1=0.8669950738916256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.4515956435352564e-05\n",
            "step: 10, loss: 7.389934035018086e-05\n",
            "step: 20, loss: 4.3430682126199827e-05\n",
            "step: 30, loss: 0.030032511800527573\n",
            "step: 40, loss: 6.864328315714374e-05\n",
            "step: 50, loss: 0.00043210037983953953\n",
            "step: 60, loss: 4.7450041165575385e-05\n",
            "step: 70, loss: 0.05266920477151871\n",
            "step: 80, loss: 0.0002953164221253246\n",
            "step: 90, loss: 5.080265327706002e-05\n",
            "step: 100, loss: 0.017238717526197433\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 110, loss: 0.0689442902803421\n",
            "step: 120, loss: 0.019325880333781242\n",
            "step: 130, loss: 0.04068581387400627\n",
            "step: 140, loss: 0.0020209162030369043\n",
            "step: 150, loss: 0.0008585518808104098\n",
            "step: 160, loss: 6.2140854424797e-05\n",
            "step: 170, loss: 0.0005007652216590941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8341968911917099, f1=0.8614609571788413, best_f1=0.8669950738916256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008026788011193275\n",
            "step: 10, loss: 6.861148722236976e-05\n",
            "step: 20, loss: 0.008478670381009579\n",
            "step: 30, loss: 0.001439161249436438\n",
            "step: 40, loss: 0.18608850240707397\n",
            "step: 50, loss: 0.00017022198881022632\n",
            "step: 60, loss: 0.04925263673067093\n",
            "step: 70, loss: 0.0001756456622388214\n",
            "step: 80, loss: 0.00019094889285042882\n",
            "step: 90, loss: 6.971503898967057e-05\n",
            "step: 100, loss: 0.0005402213428169489\n",
            "step: 110, loss: 0.01768549531698227\n",
            "step: 120, loss: 9.980265167541802e-05\n",
            "step: 130, loss: 7.2010989242699e-05\n",
            "step: 140, loss: 0.04181574285030365\n",
            "step: 150, loss: 0.024857331067323685\n",
            "step: 160, loss: 0.0005649306112900376\n",
            "step: 170, loss: 0.024471217766404152\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8337730870712401, f1=0.8622448979591837, best_f1=0.8669950738916256\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.947024768218398e-05\n",
            "step: 10, loss: 0.00010217940871370956\n",
            "step: 20, loss: 0.0004088866990059614\n",
            "step: 30, loss: 0.00036196320434100926\n",
            "step: 40, loss: 3.4390985092613846e-05\n",
            "step: 50, loss: 7.338202703977004e-05\n",
            "step: 60, loss: 8.402904495596886e-05\n",
            "step: 70, loss: 0.04973885416984558\n",
            "step: 80, loss: 0.00015339150559157133\n",
            "step: 90, loss: 0.010821899399161339\n",
            "step: 100, loss: 0.04638060927391052\n",
            "step: 110, loss: 8.099620754364878e-05\n",
            "step: 120, loss: 7.11971297278069e-05\n",
            "step: 130, loss: 6.626776303164661e-05\n",
            "step: 140, loss: 0.013140644878149033\n",
            "step: 150, loss: 9.910942753776908e-05\n",
            "step: 160, loss: 3.8458776543848217e-05\n",
            "step: 170, loss: 9.835214586928487e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8315789473684211, f1=0.8585858585858587, best_f1=0.8669950738916256\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:07, 266.95it/s]\n",
            "load_f1 = 0.8380462724935733\n",
            "real_f1 = 0.8396946564885497\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:27, 161.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE DIRTY"
      ],
      "metadata": {
        "id": "djX3yHRNgNFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "b011EMgogNFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "5_ai4a3YgNFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e10a1a4-4137-461a-a307-85ceef530bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5864406824111938\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4672294855117798\n",
            "step: 20, loss: 0.5058885216712952\n",
            "step: 30, loss: 0.39150887727737427\n",
            "step: 40, loss: 0.3699277341365814\n",
            "step: 50, loss: 0.6657893061637878\n",
            "step: 60, loss: 0.4706275463104248\n",
            "step: 70, loss: 0.4753074049949646\n",
            "step: 80, loss: 0.6295117735862732\n",
            "step: 90, loss: 0.43410322070121765\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.4792053699493408\n",
            "step: 110, loss: 0.4478342533111572\n",
            "step: 120, loss: 0.2719002068042755\n",
            "step: 130, loss: 0.21272537112236023\n",
            "step: 140, loss: 0.08578447997570038\n",
            "step: 150, loss: 0.3668353855609894\n",
            "step: 160, loss: 0.12683098018169403\n",
            "step: 170, loss: 0.22888582944869995\n",
            "step: 180, loss: 0.1718243807554245\n",
            "step: 190, loss: 0.20594905316829681\n",
            "step: 200, loss: 0.0514933206140995\n",
            "step: 210, loss: 0.11319036036729813\n",
            "step: 220, loss: 0.0797996073961258\n",
            "step: 230, loss: 0.008612542413175106\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9391891891891891, f1=0.9399773499433748, best_f1=0.9399773499433748\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0029355892911553383\n",
            "step: 10, loss: 0.0764969140291214\n",
            "step: 20, loss: 0.037164319306612015\n",
            "step: 30, loss: 0.055233750492334366\n",
            "step: 40, loss: 0.19538797438144684\n",
            "step: 50, loss: 0.01190901454538107\n",
            "step: 60, loss: 0.037549685686826706\n",
            "step: 70, loss: 0.030061040073633194\n",
            "step: 80, loss: 0.006378181278705597\n",
            "step: 90, loss: 0.01223571877926588\n",
            "step: 100, loss: 0.03025594726204872\n",
            "step: 110, loss: 0.0842367559671402\n",
            "step: 120, loss: 0.03906457871198654\n",
            "step: 130, loss: 0.044689495116472244\n",
            "step: 140, loss: 0.013020025566220284\n",
            "step: 150, loss: 0.13051746785640717\n",
            "step: 160, loss: 0.00977955013513565\n",
            "step: 170, loss: 0.002861675573512912\n",
            "step: 180, loss: 0.00348915858194232\n",
            "step: 190, loss: 0.003688687924295664\n",
            "step: 200, loss: 0.025760000571608543\n",
            "step: 210, loss: 0.0753767341375351\n",
            "step: 220, loss: 0.005262841936200857\n",
            "step: 230, loss: 0.028020473197102547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9587513935340021, f1=0.9572072072072072, best_f1=0.9572072072072072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03840611129999161\n",
            "step: 10, loss: 0.030476843938231468\n",
            "step: 20, loss: 0.03496444970369339\n",
            "step: 30, loss: 0.025440895929932594\n",
            "step: 40, loss: 0.05104538053274155\n",
            "step: 50, loss: 0.007793285418301821\n",
            "step: 60, loss: 0.08348522335290909\n",
            "step: 70, loss: 0.009055613540112972\n",
            "step: 80, loss: 0.05547572672367096\n",
            "step: 90, loss: 0.018772760406136513\n",
            "step: 100, loss: 0.003959941677749157\n",
            "step: 110, loss: 0.006808184087276459\n",
            "step: 120, loss: 0.009683715179562569\n",
            "step: 130, loss: 0.011872156523168087\n",
            "step: 140, loss: 0.003535831579938531\n",
            "step: 150, loss: 0.012057634070515633\n",
            "step: 160, loss: 0.008685552515089512\n",
            "step: 170, loss: 0.07435259222984314\n",
            "step: 180, loss: 0.028072498738765717\n",
            "step: 190, loss: 0.03628596290946007\n",
            "step: 200, loss: 0.0795765221118927\n",
            "step: 210, loss: 0.03838310390710831\n",
            "step: 220, loss: 0.1460040658712387\n",
            "step: 230, loss: 0.04138217121362686\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9585666293393057, f1=0.9546485260770975, best_f1=0.9572072072072072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01668514311313629\n",
            "step: 10, loss: 0.0017161463620141149\n",
            "step: 20, loss: 0.0016359000001102686\n",
            "step: 30, loss: 0.0008189461077563465\n",
            "step: 40, loss: 0.07073606550693512\n",
            "step: 50, loss: 0.1142001524567604\n",
            "step: 60, loss: 0.0393516905605793\n",
            "step: 70, loss: 0.0069188885390758514\n",
            "step: 80, loss: 0.14781318604946136\n",
            "step: 90, loss: 0.03759510815143585\n",
            "step: 100, loss: 0.012678462080657482\n",
            "step: 110, loss: 0.001296988339163363\n",
            "step: 120, loss: 0.06564320623874664\n",
            "step: 130, loss: 0.022543128579854965\n",
            "step: 140, loss: 0.0060137733817100525\n",
            "step: 150, loss: 0.0028084951918572187\n",
            "step: 160, loss: 0.013888712041079998\n",
            "step: 170, loss: 0.0023217559792101383\n",
            "step: 180, loss: 0.006110687740147114\n",
            "step: 190, loss: 0.0011084042489528656\n",
            "step: 200, loss: 0.15889444947242737\n",
            "step: 210, loss: 0.005069538950920105\n",
            "step: 220, loss: 0.0021710048895329237\n",
            "step: 230, loss: 0.004225531592965126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9722530521642618, f1=0.9663677130044843, best_f1=0.9663677130044843\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005941620096564293\n",
            "step: 10, loss: 0.00899695884436369\n",
            "step: 20, loss: 0.12240946292877197\n",
            "step: 30, loss: 0.02694738656282425\n",
            "step: 40, loss: 0.05672282353043556\n",
            "step: 50, loss: 0.10509767383337021\n",
            "step: 60, loss: 0.04067450761795044\n",
            "step: 70, loss: 0.010194257833063602\n",
            "step: 80, loss: 0.030895203351974487\n",
            "step: 90, loss: 0.03530057519674301\n",
            "step: 100, loss: 0.0018839577678591013\n",
            "step: 110, loss: 0.003716943319886923\n",
            "step: 120, loss: 0.011199990287423134\n",
            "step: 130, loss: 0.000662054109852761\n",
            "step: 140, loss: 0.0058610690757632256\n",
            "step: 150, loss: 0.0078390222042799\n",
            "step: 160, loss: 0.006523996125906706\n",
            "step: 170, loss: 0.01029851846396923\n",
            "step: 180, loss: 0.0025281510315835476\n",
            "step: 190, loss: 0.1955931931734085\n",
            "step: 200, loss: 0.04159029200673103\n",
            "step: 210, loss: 0.005956568289548159\n",
            "step: 220, loss: 0.00755816325545311\n",
            "step: 230, loss: 0.01621226966381073\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9775784753363228, f1=0.9706546275395034, best_f1=0.9706546275395034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003308493411168456\n",
            "step: 10, loss: 0.003115664469078183\n",
            "step: 20, loss: 0.00310559687204659\n",
            "step: 30, loss: 0.0018954118713736534\n",
            "step: 40, loss: 0.0009622793295420706\n",
            "step: 50, loss: 0.002973628230392933\n",
            "step: 60, loss: 0.003036019392311573\n",
            "step: 70, loss: 0.1116991639137268\n",
            "step: 80, loss: 0.003663149196654558\n",
            "step: 90, loss: 0.0038744774647057056\n",
            "step: 100, loss: 0.004451482091099024\n",
            "step: 110, loss: 0.057972900569438934\n",
            "step: 120, loss: 0.0013395363930612803\n",
            "step: 130, loss: 0.0025357494596391916\n",
            "step: 140, loss: 0.026736196130514145\n",
            "step: 150, loss: 0.001184971770271659\n",
            "step: 160, loss: 0.013954016380012035\n",
            "step: 170, loss: 0.0025075210724025965\n",
            "step: 180, loss: 0.0019979127682745457\n",
            "step: 190, loss: 0.0006413131486624479\n",
            "step: 200, loss: 0.008099248632788658\n",
            "step: 210, loss: 0.0002730035630520433\n",
            "step: 220, loss: 0.0172277744859457\n",
            "step: 230, loss: 0.0006899161962792277\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9775784753363228, f1=0.9662162162162162, best_f1=0.9706546275395034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012253789464011788\n",
            "step: 10, loss: 0.002193249762058258\n",
            "step: 20, loss: 0.000821854395326227\n",
            "step: 30, loss: 0.0004472437431104481\n",
            "step: 40, loss: 0.0018032253719866276\n",
            "step: 50, loss: 0.13582047820091248\n",
            "step: 60, loss: 0.0009459108114242554\n",
            "step: 70, loss: 0.002882922301068902\n",
            "step: 80, loss: 0.001549489563331008\n",
            "step: 90, loss: 0.04952074959874153\n",
            "step: 100, loss: 0.00043124533840455115\n",
            "step: 110, loss: 0.0005397950299084187\n",
            "step: 120, loss: 0.0033026649616658688\n",
            "step: 130, loss: 0.0020747072994709015\n",
            "step: 140, loss: 0.00038757783477194607\n",
            "step: 150, loss: 0.12565280497074127\n",
            "step: 160, loss: 0.0008905999711714685\n",
            "step: 170, loss: 0.0005821281229145825\n",
            "step: 180, loss: 0.0003367135359439999\n",
            "step: 190, loss: 0.17828933894634247\n",
            "step: 200, loss: 0.0015234255697578192\n",
            "step: 210, loss: 0.013381386175751686\n",
            "step: 220, loss: 0.0013252757489681244\n",
            "step: 230, loss: 0.003997689578682184\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.98, f1=0.9752808988764046, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014275589492172003\n",
            "step: 10, loss: 0.00798649899661541\n",
            "step: 20, loss: 0.003693838370963931\n",
            "step: 30, loss: 0.0028421450406312943\n",
            "step: 40, loss: 0.002666605170816183\n",
            "step: 50, loss: 0.001092863967642188\n",
            "step: 60, loss: 0.0013354108668863773\n",
            "step: 70, loss: 0.002832575934007764\n",
            "step: 80, loss: 0.14459992945194244\n",
            "step: 90, loss: 0.0007731081568636\n",
            "step: 100, loss: 0.005177687853574753\n",
            "step: 110, loss: 0.007411270402371883\n",
            "step: 120, loss: 0.0006322209374047816\n",
            "step: 130, loss: 0.005753112956881523\n",
            "step: 140, loss: 0.0005215908749960363\n",
            "step: 150, loss: 0.11593642830848694\n",
            "step: 160, loss: 0.0006744014681316912\n",
            "step: 170, loss: 0.07998858392238617\n",
            "step: 180, loss: 0.00038227817276492715\n",
            "step: 190, loss: 0.004153206944465637\n",
            "step: 200, loss: 0.010214678943157196\n",
            "step: 210, loss: 0.0014370951103046536\n",
            "step: 220, loss: 0.0010730077046900988\n",
            "step: 230, loss: 0.004440569784492254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9797297297297298, f1=0.9658314350797267, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012624983210116625\n",
            "step: 10, loss: 0.0007197451777756214\n",
            "step: 20, loss: 0.0011933278292417526\n",
            "step: 30, loss: 0.0007577324286103249\n",
            "step: 40, loss: 0.0016660322435200214\n",
            "step: 50, loss: 0.0006388345500454307\n",
            "step: 60, loss: 0.0006573146674782038\n",
            "step: 70, loss: 0.01390834804624319\n",
            "step: 80, loss: 0.0001943305687746033\n",
            "step: 90, loss: 0.16725124418735504\n",
            "step: 100, loss: 0.0013234992511570454\n",
            "step: 110, loss: 0.0013406537473201752\n",
            "step: 120, loss: 0.05392605811357498\n",
            "step: 130, loss: 0.008157733827829361\n",
            "step: 140, loss: 0.0009341894183307886\n",
            "step: 150, loss: 0.0005864223930984735\n",
            "step: 160, loss: 0.001342575065791607\n",
            "step: 170, loss: 0.0004489412240218371\n",
            "step: 180, loss: 0.0013778316788375378\n",
            "step: 190, loss: 0.00032786509837023914\n",
            "step: 200, loss: 0.002974630333483219\n",
            "step: 210, loss: 0.0786680057644844\n",
            "step: 220, loss: 0.00031721926643513143\n",
            "step: 230, loss: 0.0002942800347227603\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9843400447427293, f1=0.9752808988764046, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00030088244238868356\n",
            "step: 10, loss: 0.00044713489478453994\n",
            "step: 20, loss: 0.0003217718913219869\n",
            "step: 30, loss: 0.00022700949921272695\n",
            "step: 40, loss: 0.0009226341499015689\n",
            "step: 50, loss: 0.0002964045852422714\n",
            "step: 60, loss: 0.0005739384214393795\n",
            "step: 70, loss: 0.003191860858350992\n",
            "step: 80, loss: 0.000508254743181169\n",
            "step: 90, loss: 0.00045945862075313926\n",
            "step: 100, loss: 0.0004869994299951941\n",
            "step: 110, loss: 0.0012773983180522919\n",
            "step: 120, loss: 0.0004052753793075681\n",
            "step: 130, loss: 0.0049860794097185135\n",
            "step: 140, loss: 0.000566694070585072\n",
            "step: 150, loss: 0.0012553422711789608\n",
            "step: 160, loss: 0.0002300277556059882\n",
            "step: 170, loss: 0.00037283782148733735\n",
            "step: 180, loss: 0.0006664919783361256\n",
            "step: 190, loss: 0.00033402079134248197\n",
            "step: 200, loss: 0.0003525417996570468\n",
            "step: 210, loss: 0.011638772673904896\n",
            "step: 220, loss: 0.0001933225430548191\n",
            "step: 230, loss: 0.00019717254326678813\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9753363228699552, f1=0.9730337078651685, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013331176887732\n",
            "step: 10, loss: 0.0003508979280013591\n",
            "step: 20, loss: 0.00041624062578193843\n",
            "step: 30, loss: 0.0005227627116255462\n",
            "step: 40, loss: 9.08122819964774e-05\n",
            "step: 50, loss: 0.006630332209169865\n",
            "step: 60, loss: 0.0006954523269087076\n",
            "step: 70, loss: 0.006077867932617664\n",
            "step: 80, loss: 0.004651970230042934\n",
            "step: 90, loss: 0.21448299288749695\n",
            "step: 100, loss: 0.0005384894902817905\n",
            "step: 110, loss: 0.00074897485319525\n",
            "step: 120, loss: 0.0006703276303596795\n",
            "step: 130, loss: 0.0006128825480118394\n",
            "step: 140, loss: 0.0004433468566276133\n",
            "step: 150, loss: 0.0005289417458698153\n",
            "step: 160, loss: 0.0005528710316866636\n",
            "step: 170, loss: 0.0005048915045335889\n",
            "step: 180, loss: 0.0005360572249628603\n",
            "step: 190, loss: 0.0005074614309705794\n",
            "step: 200, loss: 0.024933654814958572\n",
            "step: 210, loss: 0.000377664458937943\n",
            "step: 220, loss: 0.0005265919608063996\n",
            "step: 230, loss: 0.0008215924608521163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9853768278965129, f1=0.9717514124293786, best_f1=0.9717514124293786\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000621456594672054\n",
            "step: 10, loss: 0.00031847009086050093\n",
            "step: 20, loss: 0.01555243507027626\n",
            "step: 30, loss: 0.007000530604273081\n",
            "step: 40, loss: 0.001076473854482174\n",
            "step: 50, loss: 0.0014049103483557701\n",
            "step: 60, loss: 0.000585127854719758\n",
            "step: 70, loss: 0.0006549922400154173\n",
            "step: 80, loss: 0.000244599039433524\n",
            "step: 90, loss: 0.004341338761150837\n",
            "step: 100, loss: 0.0001713510719127953\n",
            "step: 110, loss: 0.0002187417703680694\n",
            "step: 120, loss: 0.0005063881981186569\n",
            "step: 130, loss: 0.0003677325730677694\n",
            "step: 140, loss: 0.0006086240173317492\n",
            "step: 150, loss: 0.0008608310017734766\n",
            "step: 160, loss: 0.0017880230443552136\n",
            "step: 170, loss: 0.0005387351848185062\n",
            "step: 180, loss: 0.0006284458213485777\n",
            "step: 190, loss: 0.0006985347135923803\n",
            "step: 200, loss: 0.00046804509474895895\n",
            "step: 210, loss: 0.002485854085534811\n",
            "step: 220, loss: 0.006448335945606232\n",
            "step: 230, loss: 0.0003771183255594224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9843400447427293, f1=0.9741863075196409, best_f1=0.9717514124293786\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015941897872835398\n",
            "step: 10, loss: 0.0004529855796135962\n",
            "step: 20, loss: 0.00047412904677912593\n",
            "step: 30, loss: 0.00031630106968805194\n",
            "step: 40, loss: 0.00045727050746791065\n",
            "step: 50, loss: 0.0008801344083622098\n",
            "step: 60, loss: 0.00027543550822883844\n",
            "step: 70, loss: 0.0003888445207849145\n",
            "step: 80, loss: 0.0005216528079472482\n",
            "step: 90, loss: 0.0003531465190462768\n",
            "step: 100, loss: 0.0005677518784068525\n",
            "step: 110, loss: 0.0005836496711708605\n",
            "step: 120, loss: 0.0005030676838941872\n",
            "step: 130, loss: 0.0005827705026604235\n",
            "step: 140, loss: 0.0005906478618271649\n",
            "step: 150, loss: 0.00016550751752220094\n",
            "step: 160, loss: 0.003368482692167163\n",
            "step: 170, loss: 0.0005595298716798425\n",
            "step: 180, loss: 0.006491715554147959\n",
            "step: 190, loss: 0.0005921541014686227\n",
            "step: 200, loss: 0.00014840223593637347\n",
            "step: 210, loss: 0.0023741889744997025\n",
            "step: 220, loss: 0.0004636637750081718\n",
            "step: 230, loss: 0.000378135999199003\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9865470852017937, f1=0.9764309764309763, best_f1=0.9764309764309763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000366062275134027\n",
            "step: 10, loss: 0.00024179002502933145\n",
            "step: 20, loss: 0.00035247037885710597\n",
            "step: 30, loss: 0.0007127332501113415\n",
            "step: 40, loss: 0.0003793870273511857\n",
            "step: 50, loss: 0.0003675162442959845\n",
            "step: 60, loss: 0.00044609475298784673\n",
            "step: 70, loss: 0.00031894445419311523\n",
            "step: 80, loss: 0.0007763316389173269\n",
            "step: 90, loss: 0.0009775320068001747\n",
            "step: 100, loss: 0.00042196971480734646\n",
            "step: 110, loss: 0.0006659010541625321\n",
            "step: 120, loss: 0.00016849164967425168\n",
            "step: 130, loss: 0.011479475535452366\n",
            "step: 140, loss: 0.0003785051521845162\n",
            "step: 150, loss: 0.00023850525030866265\n",
            "step: 160, loss: 0.0003796009987127036\n",
            "step: 170, loss: 0.0006492396933026612\n",
            "step: 180, loss: 0.0004896075115539134\n",
            "step: 190, loss: 0.0003093595732934773\n",
            "step: 200, loss: 0.0006502457545138896\n",
            "step: 210, loss: 0.00034206395503133535\n",
            "step: 220, loss: 0.000620358856394887\n",
            "step: 230, loss: 0.003291134024038911\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9832026875699889, f1=0.9752808988764046, best_f1=0.9764309764309763\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006750317988917232\n",
            "step: 10, loss: 0.0004406913067214191\n",
            "step: 20, loss: 0.00040032234392128885\n",
            "step: 30, loss: 0.0005751324351876974\n",
            "step: 40, loss: 0.00025281813577748835\n",
            "step: 50, loss: 0.00039422648842446506\n",
            "step: 60, loss: 0.021477799862623215\n",
            "step: 70, loss: 0.0005220550228841603\n",
            "step: 80, loss: 0.0008741935016587377\n",
            "step: 90, loss: 0.0004057623737026006\n",
            "step: 100, loss: 0.00026428449200466275\n",
            "step: 110, loss: 0.0004445348458830267\n",
            "step: 120, loss: 0.008731343783438206\n",
            "step: 130, loss: 0.00022761538275517523\n",
            "step: 140, loss: 0.00302368332631886\n",
            "step: 150, loss: 0.0007705132011324167\n",
            "step: 160, loss: 0.0006009471253491938\n",
            "step: 170, loss: 0.0001703786983853206\n",
            "step: 180, loss: 0.00047722519957460463\n",
            "step: 190, loss: 0.0002943655999843031\n",
            "step: 200, loss: 0.0005854636547155678\n",
            "step: 210, loss: 0.0005724828224629164\n",
            "step: 220, loss: 0.00038807577220723033\n",
            "step: 230, loss: 0.0005725205992348492\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9843400447427293, f1=0.9753914988814317, best_f1=0.9764309764309763\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:14, 176.46it/s]\n",
            "load_f1 = 0.9853107344632768\n",
            "real_f1 = 0.9853107344632768\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:26, 163.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "h62Yut_pgNFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QGEElkeagNFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa9b8c1-e657-41c7-9c74-79dfa9bc8b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6308673620223999\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.43569323420524597\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.31038400530815125\n",
            "step: 30, loss: 0.36797842383384705\n",
            "step: 40, loss: 0.19325710833072662\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.21517646312713623\n",
            "step: 60, loss: 0.06097866967320442\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 70, loss: 0.40503713488578796\n",
            "step: 80, loss: 0.19431337714195251\n",
            "step: 90, loss: 0.12746690213680267\n",
            "step: 100, loss: 0.2724410593509674\n",
            "step: 110, loss: 0.14815251529216766\n",
            "step: 120, loss: 0.3090895116329193\n",
            "step: 130, loss: 0.12149252742528915\n",
            "step: 140, loss: 0.23006704449653625\n",
            "step: 150, loss: 0.051231611520051956\n",
            "step: 160, loss: 0.146855428814888\n",
            "step: 170, loss: 0.06972761452198029\n",
            "step: 180, loss: 0.09554887562990189\n",
            "step: 190, loss: 0.02751384675502777\n",
            "step: 200, loss: 0.055464331060647964\n",
            "step: 210, loss: 0.04688764363527298\n",
            "step: 220, loss: 0.005443306174129248\n",
            "step: 230, loss: 0.29195162653923035\n",
            "step: 240, loss: 0.029493732377886772\n",
            "step: 250, loss: 0.049972716718912125\n",
            "step: 260, loss: 0.07151364535093307\n",
            "step: 270, loss: 0.4306575059890747\n",
            "step: 280, loss: 0.047997813671827316\n",
            "step: 290, loss: 0.02799965813755989\n",
            "step: 300, loss: 0.025748562067747116\n",
            "step: 310, loss: 0.07244131714105606\n",
            "step: 320, loss: 0.04656913876533508\n",
            "step: 330, loss: 0.18431295454502106\n",
            "step: 340, loss: 0.4503173232078552\n",
            "step: 350, loss: 0.10649530589580536\n",
            "step: 360, loss: 0.13415491580963135\n",
            "step: 370, loss: 0.12923528254032135\n",
            "step: 380, loss: 0.274933785200119\n",
            "step: 390, loss: 0.054872315376996994\n",
            "step: 400, loss: 0.08395784348249435\n",
            "step: 410, loss: 0.2393614500761032\n",
            "step: 420, loss: 0.007686366327106953\n",
            "step: 430, loss: 0.024318452924489975\n",
            "step: 440, loss: 0.17436207830905914\n",
            "step: 450, loss: 0.012102697044610977\n",
            "step: 460, loss: 0.07848821580410004\n",
            "step: 470, loss: 0.0769619271159172\n",
            "step: 480, loss: 0.18159887194633484\n",
            "step: 490, loss: 0.05256090313196182\n",
            "step: 500, loss: 0.017573906108736992\n",
            "step: 510, loss: 0.04050683230161667\n",
            "step: 520, loss: 0.09293679893016815\n",
            "step: 530, loss: 0.0033101520966738462\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9518916394208314, f1=0.9424964936886395, best_f1=0.9424964936886395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0738675594329834\n",
            "step: 10, loss: 0.05036786571145058\n",
            "step: 20, loss: 0.00645051384344697\n",
            "step: 30, loss: 0.0633639544248581\n",
            "step: 40, loss: 0.060459405183792114\n",
            "step: 50, loss: 0.04458734765648842\n",
            "step: 60, loss: 0.04440968111157417\n",
            "step: 70, loss: 0.016063973307609558\n",
            "step: 80, loss: 0.09594271332025528\n",
            "step: 90, loss: 0.009374584071338177\n",
            "step: 100, loss: 0.1923980563879013\n",
            "step: 110, loss: 0.016581088304519653\n",
            "step: 120, loss: 0.02482917159795761\n",
            "step: 130, loss: 0.0023901090025901794\n",
            "step: 140, loss: 0.04226371645927429\n",
            "step: 150, loss: 0.015108383260667324\n",
            "step: 160, loss: 0.008420602418482304\n",
            "step: 170, loss: 0.014014222659170628\n",
            "step: 180, loss: 0.00892513059079647\n",
            "step: 190, loss: 0.012908535078167915\n",
            "step: 200, loss: 0.1992916464805603\n",
            "step: 210, loss: 0.014450805261731148\n",
            "step: 220, loss: 0.002359607256948948\n",
            "step: 230, loss: 0.13295944035053253\n",
            "step: 240, loss: 0.05381322652101517\n",
            "step: 250, loss: 0.01812053844332695\n",
            "step: 260, loss: 0.041068170219659805\n",
            "step: 270, loss: 0.04929543286561966\n",
            "step: 280, loss: 0.0505107045173645\n",
            "step: 290, loss: 0.02322838269174099\n",
            "step: 300, loss: 0.04535858333110809\n",
            "step: 310, loss: 0.033525533974170685\n",
            "step: 320, loss: 0.06535109877586365\n",
            "step: 330, loss: 0.014839465729892254\n",
            "step: 340, loss: 0.036901552230119705\n",
            "step: 350, loss: 0.0037129404954612255\n",
            "step: 360, loss: 0.1259613335132599\n",
            "step: 370, loss: 0.00282351765781641\n",
            "step: 380, loss: 0.11557578295469284\n",
            "step: 390, loss: 0.00666729686781764\n",
            "step: 400, loss: 0.1331208199262619\n",
            "step: 410, loss: 0.015676338225603104\n",
            "step: 420, loss: 0.007326530292630196\n",
            "step: 430, loss: 0.18981020152568817\n",
            "step: 440, loss: 0.012914525344967842\n",
            "step: 450, loss: 0.020758071914315224\n",
            "step: 460, loss: 0.04323064163327217\n",
            "step: 470, loss: 0.02431623637676239\n",
            "step: 480, loss: 0.0044629801996052265\n",
            "step: 490, loss: 0.07814677059650421\n",
            "step: 500, loss: 0.011991379782557487\n",
            "step: 510, loss: 0.05082060024142265\n",
            "step: 520, loss: 0.31806400418281555\n",
            "step: 530, loss: 0.07300733774900436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.9516728624535317, f1=0.9481481481481482, best_f1=0.9424964936886395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1969938725233078\n",
            "step: 10, loss: 0.01746227592229843\n",
            "step: 20, loss: 0.008026674389839172\n",
            "step: 30, loss: 0.041320960968732834\n",
            "step: 40, loss: 0.08147258311510086\n",
            "step: 50, loss: 0.004698708187788725\n",
            "step: 60, loss: 0.00934132281690836\n",
            "step: 70, loss: 0.0030974787659943104\n",
            "step: 80, loss: 0.1496574729681015\n",
            "step: 90, loss: 0.006509755272418261\n",
            "step: 100, loss: 0.0063647194765508175\n",
            "step: 110, loss: 0.012028896249830723\n",
            "step: 120, loss: 0.22453761100769043\n",
            "step: 130, loss: 0.08099406212568283\n",
            "step: 140, loss: 0.033413175493478775\n",
            "step: 150, loss: 0.018241234123706818\n",
            "step: 160, loss: 0.02718506008386612\n",
            "step: 170, loss: 0.03737135976552963\n",
            "step: 180, loss: 0.013694380410015583\n",
            "step: 190, loss: 0.0099398298189044\n",
            "step: 200, loss: 0.048155587166547775\n",
            "step: 210, loss: 0.06689327955245972\n",
            "step: 220, loss: 0.12564922869205475\n",
            "step: 230, loss: 0.015880800783634186\n",
            "step: 240, loss: 0.0898575484752655\n",
            "step: 250, loss: 0.041469186544418335\n",
            "step: 260, loss: 0.16470444202423096\n",
            "step: 270, loss: 0.002316553145647049\n",
            "step: 280, loss: 0.009670387953519821\n",
            "step: 290, loss: 0.008912729099392891\n",
            "step: 300, loss: 0.18574920296669006\n",
            "step: 310, loss: 0.12520167231559753\n",
            "step: 320, loss: 0.018825778737664223\n",
            "step: 330, loss: 0.007442031987011433\n",
            "step: 340, loss: 0.006998666562139988\n",
            "step: 350, loss: 0.11664871871471405\n",
            "step: 360, loss: 0.015456832945346832\n",
            "step: 370, loss: 0.023954197764396667\n",
            "step: 380, loss: 0.007666466757655144\n",
            "step: 390, loss: 0.004197297152131796\n",
            "step: 400, loss: 0.14983384311199188\n",
            "step: 410, loss: 0.01468430645763874\n",
            "step: 420, loss: 0.0058500515297055244\n",
            "step: 430, loss: 0.015423450618982315\n",
            "step: 440, loss: 0.06812790781259537\n",
            "step: 450, loss: 0.00583431962877512\n",
            "step: 460, loss: 0.09759047627449036\n",
            "step: 470, loss: 0.00983015913516283\n",
            "step: 480, loss: 0.10810495913028717\n",
            "step: 490, loss: 0.09497496485710144\n",
            "step: 500, loss: 0.012970101088285446\n",
            "step: 510, loss: 0.05249495431780815\n",
            "step: 520, loss: 0.005117934662848711\n",
            "step: 530, loss: 0.00511776190251112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9487895716945998, f1=0.949767441860465, best_f1=0.9424964936886395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01209260430186987\n",
            "step: 10, loss: 0.0014721167972311378\n",
            "step: 20, loss: 0.0026009015273302794\n",
            "step: 30, loss: 0.14859648048877716\n",
            "step: 40, loss: 0.008315942250192165\n",
            "step: 50, loss: 0.02545805461704731\n",
            "step: 60, loss: 0.004579576663672924\n",
            "step: 70, loss: 0.07145723700523376\n",
            "step: 80, loss: 0.12415937334299088\n",
            "step: 90, loss: 0.09496353566646576\n",
            "step: 100, loss: 0.0034639015793800354\n",
            "step: 110, loss: 0.07776093482971191\n",
            "step: 120, loss: 0.008029796183109283\n",
            "step: 130, loss: 0.025666438043117523\n",
            "step: 140, loss: 0.0033968379721045494\n",
            "step: 150, loss: 0.011359786614775658\n",
            "step: 160, loss: 0.17571556568145752\n",
            "step: 170, loss: 0.025224804878234863\n",
            "step: 180, loss: 0.01799466274678707\n",
            "step: 190, loss: 0.02128271386027336\n",
            "step: 200, loss: 0.09989558160305023\n",
            "step: 210, loss: 0.006545179057866335\n",
            "step: 220, loss: 0.00901031494140625\n",
            "step: 230, loss: 0.035840462893247604\n",
            "step: 240, loss: 0.008238530717790127\n",
            "step: 250, loss: 0.018823275342583656\n",
            "step: 260, loss: 0.029169170185923576\n",
            "step: 270, loss: 0.23327863216400146\n",
            "step: 280, loss: 0.011640479788184166\n",
            "step: 290, loss: 0.02557649463415146\n",
            "step: 300, loss: 0.010134106501936913\n",
            "step: 310, loss: 0.0024708115961402655\n",
            "step: 320, loss: 0.010847293771803379\n",
            "step: 330, loss: 0.0168544240295887\n",
            "step: 340, loss: 0.0015978452283889055\n",
            "step: 350, loss: 0.01616368442773819\n",
            "step: 360, loss: 0.06111973896622658\n",
            "step: 370, loss: 0.0033126724883913994\n",
            "step: 380, loss: 0.020252499729394913\n",
            "step: 390, loss: 0.0005427083233371377\n",
            "step: 400, loss: 0.011656171642243862\n",
            "step: 410, loss: 0.0385783351957798\n",
            "step: 420, loss: 0.02702363394200802\n",
            "step: 430, loss: 0.04048171639442444\n",
            "step: 440, loss: 0.01890764944255352\n",
            "step: 450, loss: 0.00567725021392107\n",
            "step: 460, loss: 0.007562275510281324\n",
            "step: 470, loss: 0.0016374834813177586\n",
            "step: 480, loss: 0.02250698022544384\n",
            "step: 490, loss: 0.005043595097959042\n",
            "step: 500, loss: 0.008654621429741383\n",
            "step: 510, loss: 0.007703820243477821\n",
            "step: 520, loss: 0.04928872361779213\n",
            "step: 530, loss: 0.14072829484939575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9519365375641624, f1=0.9482517482517483, best_f1=0.9482517482517483\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004916697274893522\n",
            "step: 10, loss: 0.019446689635515213\n",
            "step: 20, loss: 0.02122274599969387\n",
            "step: 30, loss: 0.0020791904535144567\n",
            "step: 40, loss: 0.0005674568237736821\n",
            "step: 50, loss: 0.0848168358206749\n",
            "step: 60, loss: 0.0035145424772053957\n",
            "step: 70, loss: 0.0009934817207977176\n",
            "step: 80, loss: 0.003456264268606901\n",
            "step: 90, loss: 0.11218377202749252\n",
            "step: 100, loss: 0.01066327653825283\n",
            "step: 110, loss: 0.010713606141507626\n",
            "step: 120, loss: 0.07801385223865509\n",
            "step: 130, loss: 0.004202988930046558\n",
            "step: 140, loss: 0.0015798771055415273\n",
            "step: 150, loss: 0.004577525891363621\n",
            "step: 160, loss: 0.006194422021508217\n",
            "step: 170, loss: 0.023508165031671524\n",
            "step: 180, loss: 0.005699211731553078\n",
            "step: 190, loss: 0.002337457612156868\n",
            "step: 200, loss: 0.002857899060472846\n",
            "step: 210, loss: 0.001485330518335104\n",
            "step: 220, loss: 0.0047107962891459465\n",
            "step: 230, loss: 0.0030272628646343946\n",
            "step: 240, loss: 0.0055137332528829575\n",
            "step: 250, loss: 0.1497051864862442\n",
            "step: 260, loss: 0.0063744401559233665\n",
            "step: 270, loss: 0.0021297512575984\n",
            "step: 280, loss: 0.0037640128284692764\n",
            "step: 290, loss: 0.08428675681352615\n",
            "step: 300, loss: 0.11093876510858536\n",
            "step: 310, loss: 0.054367322474718094\n",
            "step: 320, loss: 0.006717141717672348\n",
            "step: 330, loss: 0.00037178193451836705\n",
            "step: 340, loss: 0.020792139694094658\n",
            "step: 350, loss: 0.006240156479179859\n",
            "step: 360, loss: 0.00029532972257584333\n",
            "step: 370, loss: 0.000612519565038383\n",
            "step: 380, loss: 0.00027885858435183764\n",
            "step: 390, loss: 0.021052150055766106\n",
            "step: 400, loss: 0.000427313701948151\n",
            "step: 410, loss: 0.16802366077899933\n",
            "step: 420, loss: 0.19368864595890045\n",
            "step: 430, loss: 0.040965672582387924\n",
            "step: 440, loss: 0.004065255168825388\n",
            "step: 450, loss: 0.007826893590390682\n",
            "step: 460, loss: 0.003473056945949793\n",
            "step: 470, loss: 0.0028377496637403965\n",
            "step: 480, loss: 0.014983846805989742\n",
            "step: 490, loss: 0.0033458829857409\n",
            "step: 500, loss: 0.01566234976053238\n",
            "step: 510, loss: 0.0008016335195861757\n",
            "step: 520, loss: 0.08796550333499908\n",
            "step: 530, loss: 0.007036186754703522\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9542056074766355, f1=0.9478098788443615, best_f1=0.9478098788443615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003657157998532057\n",
            "step: 10, loss: 0.000356143107637763\n",
            "step: 20, loss: 0.009094388224184513\n",
            "step: 30, loss: 0.00037679189699701965\n",
            "step: 40, loss: 0.0015813471982255578\n",
            "step: 50, loss: 0.000773474806919694\n",
            "step: 60, loss: 0.02460172399878502\n",
            "step: 70, loss: 0.0011512935161590576\n",
            "step: 80, loss: 0.00015917209384497255\n",
            "step: 90, loss: 0.0013325521722435951\n",
            "step: 100, loss: 0.0003388152690604329\n",
            "step: 110, loss: 0.0004036948084831238\n",
            "step: 120, loss: 0.214355930685997\n",
            "step: 130, loss: 0.005893392488360405\n",
            "step: 140, loss: 0.03680785000324249\n",
            "step: 150, loss: 0.00448536267504096\n",
            "step: 160, loss: 0.0012102194596081972\n",
            "step: 170, loss: 0.0020677123684436083\n",
            "step: 180, loss: 0.001305433688685298\n",
            "step: 190, loss: 0.09838757663965225\n",
            "step: 200, loss: 0.02287481166422367\n",
            "step: 210, loss: 0.00963826384395361\n",
            "step: 220, loss: 0.0015264999819919467\n",
            "step: 230, loss: 0.08293716609477997\n",
            "step: 240, loss: 0.0037337818648666143\n",
            "step: 250, loss: 0.004678620025515556\n",
            "step: 260, loss: 0.0009501404128968716\n",
            "step: 270, loss: 0.00280697550624609\n",
            "step: 280, loss: 0.0014769320841878653\n",
            "step: 290, loss: 0.00023959616373758763\n",
            "step: 300, loss: 0.027728606015443802\n",
            "step: 310, loss: 0.004312832839787006\n",
            "step: 320, loss: 9.664978279033676e-05\n",
            "step: 330, loss: 0.001434721751138568\n",
            "step: 340, loss: 0.0004493607266340405\n",
            "step: 350, loss: 0.0019025527872145176\n",
            "step: 360, loss: 0.010060708969831467\n",
            "step: 370, loss: 0.00843252707272768\n",
            "step: 380, loss: 0.0013709128834307194\n",
            "step: 390, loss: 0.011005222797393799\n",
            "step: 400, loss: 0.005657361354678869\n",
            "step: 410, loss: 0.0055653732270002365\n",
            "step: 420, loss: 0.0024859735276550055\n",
            "step: 430, loss: 0.020834337919950485\n",
            "step: 440, loss: 0.004902938846498728\n",
            "step: 450, loss: 0.26720482110977173\n",
            "step: 460, loss: 0.0030119153670966625\n",
            "step: 470, loss: 0.0022923494689166546\n",
            "step: 480, loss: 0.004021611530333757\n",
            "step: 490, loss: 0.006235372740775347\n",
            "step: 500, loss: 0.001790455775335431\n",
            "step: 510, loss: 0.01487276703119278\n",
            "step: 520, loss: 0.000401974655687809\n",
            "step: 530, loss: 0.000539807544555515\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9475620975160993, f1=0.9463056447911886, best_f1=0.9478098788443615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001137280254624784\n",
            "step: 10, loss: 0.0010780749144032598\n",
            "step: 20, loss: 0.0077348859049379826\n",
            "step: 30, loss: 0.004580912180244923\n",
            "step: 40, loss: 0.0036565798800438643\n",
            "step: 50, loss: 0.0020408127456903458\n",
            "step: 60, loss: 0.0007005097577348351\n",
            "step: 70, loss: 0.0006498401053249836\n",
            "step: 80, loss: 0.0011583241866901517\n",
            "step: 90, loss: 0.00023149534536059946\n",
            "step: 100, loss: 0.0745650976896286\n",
            "step: 110, loss: 5.0394712161505595e-05\n",
            "step: 120, loss: 0.0002558788692113012\n",
            "step: 130, loss: 5.400013105827384e-05\n",
            "step: 140, loss: 0.00031956835300661623\n",
            "step: 150, loss: 0.0004290039069019258\n",
            "step: 160, loss: 0.00022837256256025285\n",
            "step: 170, loss: 0.002018974395468831\n",
            "step: 180, loss: 0.056814443320035934\n",
            "step: 190, loss: 0.002079145982861519\n",
            "step: 200, loss: 0.0004078653873875737\n",
            "step: 210, loss: 0.0010045812232419848\n",
            "step: 220, loss: 0.0269811749458313\n",
            "step: 230, loss: 0.002211672253906727\n",
            "step: 240, loss: 0.010484179481863976\n",
            "step: 250, loss: 0.003312348620966077\n",
            "step: 260, loss: 0.001456087571568787\n",
            "step: 270, loss: 0.0030296901240944862\n",
            "step: 280, loss: 0.15975072979927063\n",
            "step: 290, loss: 0.00892794132232666\n",
            "step: 300, loss: 0.0011195573024451733\n",
            "step: 310, loss: 0.0008138550911098719\n",
            "step: 320, loss: 0.001499865553341806\n",
            "step: 330, loss: 0.006233831401914358\n",
            "step: 340, loss: 0.004258513916283846\n",
            "step: 350, loss: 0.0034214917104691267\n",
            "step: 360, loss: 0.02636951394379139\n",
            "step: 370, loss: 0.0021061657462269068\n",
            "step: 380, loss: 0.011744106188416481\n",
            "step: 390, loss: 0.008245899342000484\n",
            "step: 400, loss: 0.07183753699064255\n",
            "step: 410, loss: 0.009181962348520756\n",
            "step: 420, loss: 0.009563294239342213\n",
            "step: 430, loss: 0.00099075585603714\n",
            "step: 440, loss: 0.006052012555301189\n",
            "step: 450, loss: 0.0015360265970230103\n",
            "step: 460, loss: 0.003475508652627468\n",
            "step: 470, loss: 0.14813120663166046\n",
            "step: 480, loss: 0.002503459807485342\n",
            "step: 490, loss: 0.0012548526283353567\n",
            "step: 500, loss: 0.003459368832409382\n",
            "step: 510, loss: 0.00034298087120987475\n",
            "step: 520, loss: 0.00024334182671736926\n",
            "step: 530, loss: 0.00395832397043705\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9521597770552717, f1=0.9483480688692415, best_f1=0.9478098788443615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001447233371436596\n",
            "step: 10, loss: 0.001269850879907608\n",
            "step: 20, loss: 0.0002533468941692263\n",
            "step: 30, loss: 0.000167755366419442\n",
            "step: 40, loss: 0.00016892075655050576\n",
            "step: 50, loss: 0.00018623536743689328\n",
            "step: 60, loss: 0.0015564290806651115\n",
            "step: 70, loss: 6.464306352427229e-05\n",
            "step: 80, loss: 0.0012089599622413516\n",
            "step: 90, loss: 4.889049887424335e-05\n",
            "step: 100, loss: 8.321314817294478e-05\n",
            "step: 110, loss: 0.005489380098879337\n",
            "step: 120, loss: 0.001007336424663663\n",
            "step: 130, loss: 0.026784269139170647\n",
            "step: 140, loss: 3.7156129110371694e-05\n",
            "step: 150, loss: 3.204270979040302e-05\n",
            "step: 160, loss: 4.9585480155656114e-05\n",
            "step: 170, loss: 0.004867951851338148\n",
            "step: 180, loss: 4.482060467125848e-05\n",
            "step: 190, loss: 0.0005438131047412753\n",
            "step: 200, loss: 0.002585367998108268\n",
            "step: 210, loss: 0.05480944365262985\n",
            "step: 220, loss: 0.0010884612565860152\n",
            "step: 230, loss: 0.11977387219667435\n",
            "step: 240, loss: 0.04903800040483475\n",
            "step: 250, loss: 0.008979900740087032\n",
            "step: 260, loss: 0.00025016459403559566\n",
            "step: 270, loss: 0.022727767005562782\n",
            "step: 280, loss: 0.00022106819960754365\n",
            "step: 290, loss: 0.011901774443686008\n",
            "step: 300, loss: 0.00018407529569230974\n",
            "step: 310, loss: 0.04288572072982788\n",
            "step: 320, loss: 0.0014662302564829588\n",
            "step: 330, loss: 0.0002380964724579826\n",
            "step: 340, loss: 0.151763916015625\n",
            "step: 350, loss: 6.139175820862874e-05\n",
            "step: 360, loss: 0.10120277106761932\n",
            "step: 370, loss: 0.09579125046730042\n",
            "step: 380, loss: 0.00011133834777865559\n",
            "step: 390, loss: 0.005405360832810402\n",
            "step: 400, loss: 0.0121269216760993\n",
            "step: 410, loss: 0.00036743044620379806\n",
            "step: 420, loss: 0.0009908305946737528\n",
            "step: 430, loss: 0.001958580454811454\n",
            "step: 440, loss: 0.0028717503882944584\n",
            "step: 450, loss: 0.0013057797914370894\n",
            "step: 460, loss: 0.001537178992293775\n",
            "step: 470, loss: 0.037059154361486435\n",
            "step: 480, loss: 0.0023323907516896725\n",
            "step: 490, loss: 0.021672174334526062\n",
            "step: 500, loss: 0.00010182870755670592\n",
            "step: 510, loss: 0.0023651698138564825\n",
            "step: 520, loss: 6.994188152020797e-05\n",
            "step: 530, loss: 0.0008263495401479304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.946124763705104, f1=0.93989588263133, best_f1=0.9478098788443615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0064577399753034115\n",
            "step: 10, loss: 0.0006118848687037826\n",
            "step: 20, loss: 0.00011104997247457504\n",
            "step: 30, loss: 0.006580287124961615\n",
            "step: 40, loss: 0.0007584752165712416\n",
            "step: 50, loss: 0.0031687652226537466\n",
            "step: 60, loss: 0.0013020389014855027\n",
            "step: 70, loss: 0.1555641144514084\n",
            "step: 80, loss: 0.0023967125453054905\n",
            "step: 90, loss: 0.1736822873353958\n",
            "step: 100, loss: 0.012760825455188751\n",
            "step: 110, loss: 0.011028088629245758\n",
            "step: 120, loss: 0.00859018787741661\n",
            "step: 130, loss: 0.0031699263490736485\n",
            "step: 140, loss: 0.0002796142944134772\n",
            "step: 150, loss: 0.0006402014987543225\n",
            "step: 160, loss: 0.08748070895671844\n",
            "step: 170, loss: 0.008200306445360184\n",
            "step: 180, loss: 0.004315913189202547\n",
            "step: 190, loss: 0.0059159849770367146\n",
            "step: 200, loss: 0.0006222578813321888\n",
            "step: 210, loss: 0.0009797952370718122\n",
            "step: 220, loss: 0.00017380835197400302\n",
            "step: 230, loss: 0.00024585373466834426\n",
            "step: 240, loss: 0.0011344046797603369\n",
            "step: 250, loss: 0.0005374219617806375\n",
            "step: 260, loss: 0.006231850944459438\n",
            "step: 270, loss: 0.00031572673469781876\n",
            "step: 280, loss: 0.0177789144217968\n",
            "step: 290, loss: 0.0001958147477125749\n",
            "step: 300, loss: 0.00015530374366790056\n",
            "step: 310, loss: 0.01841351017355919\n",
            "step: 320, loss: 0.006964339409023523\n",
            "step: 330, loss: 0.00012656519538722932\n",
            "step: 340, loss: 0.0015568897360935807\n",
            "step: 350, loss: 0.001259908196516335\n",
            "step: 360, loss: 9.478615538682789e-05\n",
            "step: 370, loss: 0.0008822914096526802\n",
            "step: 380, loss: 0.00020660746667999774\n",
            "step: 390, loss: 7.464051304850727e-05\n",
            "step: 400, loss: 0.029060937464237213\n",
            "step: 410, loss: 0.00013325043255463243\n",
            "step: 420, loss: 6.200792995514348e-05\n",
            "step: 430, loss: 0.011351041495800018\n",
            "step: 440, loss: 0.0012722366955131292\n",
            "step: 450, loss: 0.0021104076877236366\n",
            "step: 460, loss: 0.029491418972611427\n",
            "step: 470, loss: 0.00252247741445899\n",
            "step: 480, loss: 0.0005065232398919761\n",
            "step: 490, loss: 0.0191957950592041\n",
            "step: 500, loss: 8.341016655322164e-05\n",
            "step: 510, loss: 0.0016530401771888137\n",
            "step: 520, loss: 0.0020203380845487118\n",
            "step: 530, loss: 0.00039349248982034624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9520260829063809, f1=0.9475638051044084, best_f1=0.9478098788443615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000232384743867442\n",
            "step: 10, loss: 0.0009778446983546019\n",
            "step: 20, loss: 6.400573329301551e-05\n",
            "step: 30, loss: 0.1347949057817459\n",
            "step: 40, loss: 5.103125295136124e-05\n",
            "step: 50, loss: 0.0007414236897602677\n",
            "step: 60, loss: 0.004144247155636549\n",
            "step: 70, loss: 7.781755994074047e-05\n",
            "step: 80, loss: 0.00036734389141201973\n",
            "step: 90, loss: 3.6783359973924235e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.010577631182968616\n",
            "step: 110, loss: 0.001716743572615087\n",
            "step: 120, loss: 1.7764870790415443e-05\n",
            "step: 130, loss: 0.013953517191112041\n",
            "step: 140, loss: 0.0003464514738880098\n",
            "step: 150, loss: 0.01544242538511753\n",
            "step: 160, loss: 0.007144998759031296\n",
            "step: 170, loss: 0.00015214121958706528\n",
            "step: 180, loss: 0.00013144683907739818\n",
            "step: 190, loss: 8.354224701179191e-05\n",
            "step: 200, loss: 0.0001911805447889492\n",
            "step: 210, loss: 0.00672936299815774\n",
            "step: 220, loss: 0.0525520034134388\n",
            "step: 230, loss: 0.00513831153512001\n",
            "step: 240, loss: 0.0007496240432374179\n",
            "step: 250, loss: 0.0008378430502489209\n",
            "step: 260, loss: 0.0007113841129466891\n",
            "step: 270, loss: 0.00030740309739485383\n",
            "step: 280, loss: 0.014598353765904903\n",
            "step: 290, loss: 0.009483563713729382\n",
            "step: 300, loss: 0.0006392970099113882\n",
            "step: 310, loss: 0.003294179681688547\n",
            "step: 320, loss: 0.038833197206258774\n",
            "step: 330, loss: 0.002236047061160207\n",
            "step: 340, loss: 0.00024279800709336996\n",
            "step: 350, loss: 0.0007600177195854485\n",
            "step: 360, loss: 2.5777932023629546e-05\n",
            "step: 370, loss: 0.004900245927274227\n",
            "step: 380, loss: 0.0014093102654442191\n",
            "step: 390, loss: 8.944082946982235e-05\n",
            "step: 400, loss: 0.022080153226852417\n",
            "step: 410, loss: 0.0001337386784143746\n",
            "step: 420, loss: 3.8174650399014354e-05\n",
            "step: 430, loss: 0.00017234978440683335\n",
            "step: 440, loss: 0.006883623544126749\n",
            "step: 450, loss: 0.00013761752052232623\n",
            "step: 460, loss: 0.00024172222765628248\n",
            "step: 470, loss: 0.0010906317038461566\n",
            "step: 480, loss: 0.00012904875620733947\n",
            "step: 490, loss: 0.03078516200184822\n",
            "step: 500, loss: 0.013986513018608093\n",
            "step: 510, loss: 0.030563656240701675\n",
            "step: 520, loss: 0.0013254535151645541\n",
            "step: 530, loss: 0.0008102715946733952\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9552789303826649, f1=0.954001839926403, best_f1=0.954001839926403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.3166883693193085e-05\n",
            "step: 10, loss: 8.089934271993116e-05\n",
            "step: 20, loss: 3.0678824259666726e-05\n",
            "step: 30, loss: 2.5412937247892842e-05\n",
            "step: 40, loss: 3.3327734854537994e-05\n",
            "step: 50, loss: 6.514543201774359e-05\n",
            "step: 60, loss: 9.644082456361502e-05\n",
            "step: 70, loss: 2.6027546482509933e-05\n",
            "step: 80, loss: 0.00026465795235708356\n",
            "step: 90, loss: 0.0004823790804948658\n",
            "step: 100, loss: 0.00015715854533482343\n",
            "step: 110, loss: 2.787448829622008e-05\n",
            "step: 120, loss: 3.0549970688298345e-05\n",
            "step: 130, loss: 2.2339541828841902e-05\n",
            "step: 140, loss: 2.2879710741108283e-05\n",
            "step: 150, loss: 8.368113776668906e-05\n",
            "step: 160, loss: 5.115247768117115e-05\n",
            "step: 170, loss: 2.408651744190138e-05\n",
            "step: 180, loss: 1.4912073311279528e-05\n",
            "step: 190, loss: 9.518402657704428e-05\n",
            "step: 200, loss: 2.1401083358796313e-05\n",
            "step: 210, loss: 0.00012639422493521124\n",
            "step: 220, loss: 0.003022936172783375\n",
            "step: 230, loss: 0.00010741753067122772\n",
            "step: 240, loss: 0.0014444788685068488\n",
            "step: 250, loss: 8.83759930729866e-05\n",
            "step: 260, loss: 0.0025655822828412056\n",
            "step: 270, loss: 0.002044414635747671\n",
            "step: 280, loss: 0.0017095282673835754\n",
            "step: 290, loss: 2.2451153199654073e-05\n",
            "step: 300, loss: 0.00011055065260734409\n",
            "step: 310, loss: 0.00034029193921014667\n",
            "step: 320, loss: 0.00023461251112166792\n",
            "step: 330, loss: 1.2881828297395259e-05\n",
            "step: 340, loss: 9.965147910406813e-05\n",
            "step: 350, loss: 0.005340413190424442\n",
            "step: 360, loss: 5.8788242313312367e-05\n",
            "step: 370, loss: 0.0033893154468387365\n",
            "step: 380, loss: 0.000664274615701288\n",
            "step: 390, loss: 0.001312736072577536\n",
            "step: 400, loss: 6.335452781058848e-05\n",
            "step: 410, loss: 0.0014519660035148263\n",
            "step: 420, loss: 8.863040420692414e-05\n",
            "step: 430, loss: 0.001650368794798851\n",
            "step: 440, loss: 4.323290704633109e-05\n",
            "step: 450, loss: 0.000900769722647965\n",
            "step: 460, loss: 0.00021563390328083187\n",
            "step: 470, loss: 0.0006186337559483945\n",
            "step: 480, loss: 0.0005702904309146106\n",
            "step: 490, loss: 3.402995935175568e-05\n",
            "step: 500, loss: 0.0005883724661543965\n",
            "step: 510, loss: 8.531938510714099e-05\n",
            "step: 520, loss: 0.0004733514797408134\n",
            "step: 530, loss: 0.0001898399059427902\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9526044110746129, f1=0.947022972339428, best_f1=0.954001839926403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.890802640351467e-05\n",
            "step: 10, loss: 0.000330991402734071\n",
            "step: 20, loss: 0.0006777825765311718\n",
            "step: 30, loss: 0.00044123135739937425\n",
            "step: 40, loss: 0.0009755212813615799\n",
            "step: 50, loss: 9.361568118038122e-06\n",
            "step: 60, loss: 1.5076003364811186e-05\n",
            "step: 70, loss: 8.55972248245962e-05\n",
            "step: 80, loss: 0.00018311498570255935\n",
            "step: 90, loss: 0.0002795106265693903\n",
            "step: 100, loss: 0.010454729199409485\n",
            "step: 110, loss: 0.00010699187987484038\n",
            "step: 120, loss: 0.0006403147126547992\n",
            "step: 130, loss: 0.000853267905768007\n",
            "step: 140, loss: 0.0004197692614980042\n",
            "step: 150, loss: 0.000795080151874572\n",
            "step: 160, loss: 0.00033833985798992217\n",
            "step: 170, loss: 2.0883149772998877e-05\n",
            "step: 180, loss: 6.89533626427874e-05\n",
            "step: 190, loss: 0.00017601842409931123\n",
            "step: 200, loss: 0.0015529276570305228\n",
            "step: 210, loss: 8.073101344052702e-05\n",
            "step: 220, loss: 2.7222315111430362e-05\n",
            "step: 230, loss: 2.076324562949594e-05\n",
            "step: 240, loss: 0.001244921120814979\n",
            "step: 250, loss: 1.026301924866857e-05\n",
            "step: 260, loss: 1.5694207831984386e-05\n",
            "step: 270, loss: 0.0015083079924806952\n",
            "step: 280, loss: 2.1955765987513587e-05\n",
            "step: 290, loss: 0.003655735868960619\n",
            "step: 300, loss: 0.0016886297380551696\n",
            "step: 310, loss: 4.1681760194478557e-05\n",
            "step: 320, loss: 1.294889170821989e-05\n",
            "step: 330, loss: 0.008599316701292992\n",
            "step: 340, loss: 8.18490661913529e-05\n",
            "step: 350, loss: 0.000701542419847101\n",
            "step: 360, loss: 6.587599636986852e-05\n",
            "step: 370, loss: 0.0004419710021466017\n",
            "step: 380, loss: 0.0005477808299474418\n",
            "step: 390, loss: 0.015924610197544098\n",
            "step: 400, loss: 1.605122997716535e-05\n",
            "step: 410, loss: 0.0002806921547744423\n",
            "step: 420, loss: 0.0004538317152764648\n",
            "step: 430, loss: 0.0005064163124188781\n",
            "step: 440, loss: 0.025324298068881035\n",
            "step: 450, loss: 0.0010763033060356975\n",
            "step: 460, loss: 4.9709280574461445e-05\n",
            "step: 470, loss: 0.0015031594084575772\n",
            "step: 480, loss: 0.0002321185020264238\n",
            "step: 490, loss: 6.381536513799801e-05\n",
            "step: 500, loss: 2.286247217853088e-05\n",
            "step: 510, loss: 0.0011744432849809527\n",
            "step: 520, loss: 0.002464430406689644\n",
            "step: 530, loss: 6.664067041128874e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9522041763341067, f1=0.9540441176470589, best_f1=0.954001839926403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.2397559657983948e-05\n",
            "step: 10, loss: 1.2658157174882945e-05\n",
            "step: 20, loss: 6.428502092603594e-05\n",
            "step: 30, loss: 8.683564374223351e-06\n",
            "step: 40, loss: 2.6881774829234928e-05\n",
            "step: 50, loss: 0.0018193774158135056\n",
            "step: 60, loss: 1.2628371223399881e-05\n",
            "step: 70, loss: 6.366005982272327e-05\n",
            "step: 80, loss: 2.0432124074432068e-05\n",
            "step: 90, loss: 1.9396840798435733e-05\n",
            "step: 100, loss: 9.667024642112665e-06\n",
            "step: 110, loss: 1.0370986274210736e-05\n",
            "step: 120, loss: 9.655846042733174e-06\n",
            "step: 130, loss: 2.1484116587089375e-05\n",
            "step: 140, loss: 1.1186905794602353e-05\n",
            "step: 150, loss: 0.00035840310738421977\n",
            "step: 160, loss: 0.0004430263361427933\n",
            "step: 170, loss: 0.0008699870086275041\n",
            "step: 180, loss: 1.0941039363387972e-05\n",
            "step: 190, loss: 2.010073876590468e-05\n",
            "step: 200, loss: 3.793941141339019e-05\n",
            "step: 210, loss: 8.445155799563508e-06\n",
            "step: 220, loss: 8.4935772974859e-06\n",
            "step: 230, loss: 0.0007196219521574676\n",
            "step: 240, loss: 0.00038628862239420414\n",
            "step: 250, loss: 0.0003889111685566604\n",
            "step: 260, loss: 0.0022256290540099144\n",
            "step: 270, loss: 0.00042234474676661193\n",
            "step: 280, loss: 1.084046107280301e-05\n",
            "step: 290, loss: 8.847452590998728e-06\n",
            "step: 300, loss: 0.0021009768825024366\n",
            "step: 310, loss: 3.964909410569817e-05\n",
            "step: 320, loss: 0.0002921436389442533\n",
            "step: 330, loss: 1.481152321503032e-05\n",
            "step: 340, loss: 0.002715568756684661\n",
            "step: 350, loss: 6.616066002607113e-06\n",
            "step: 360, loss: 0.09116315096616745\n",
            "step: 370, loss: 7.551097951363772e-06\n",
            "step: 380, loss: 1.5921348676783964e-05\n",
            "step: 390, loss: 7.76717843109509e-06\n",
            "step: 400, loss: 8.530975173925981e-05\n",
            "step: 410, loss: 1.5794774299138226e-05\n",
            "step: 420, loss: 1.160012288892176e-05\n",
            "step: 430, loss: 0.00048183900071308017\n",
            "step: 440, loss: 6.77998013998149e-06\n",
            "step: 450, loss: 0.00045950859203003347\n",
            "step: 460, loss: 0.001629882724955678\n",
            "step: 470, loss: 0.0015398190589621663\n",
            "step: 480, loss: 1.113428697863128e-05\n",
            "step: 490, loss: 2.384902472840622e-05\n",
            "step: 500, loss: 8.4600278569269e-06\n",
            "step: 510, loss: 7.953425665618852e-06\n",
            "step: 520, loss: 6.485690391855314e-06\n",
            "step: 530, loss: 2.5203398763551377e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9542668552569542, f1=0.9454887218045113, best_f1=0.954001839926403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008040149696171284\n",
            "step: 10, loss: 0.00021541249589063227\n",
            "step: 20, loss: 7.852844646549784e-06\n",
            "step: 30, loss: 0.0005984738236293197\n",
            "step: 40, loss: 0.0005216557765379548\n",
            "step: 50, loss: 0.002680788515135646\n",
            "step: 60, loss: 0.01136599201709032\n",
            "step: 70, loss: 8.720824553165585e-06\n",
            "step: 80, loss: 1.2006335964542814e-05\n",
            "step: 90, loss: 6.120619673311012e-06\n",
            "step: 100, loss: 8.668662303534802e-06\n",
            "step: 110, loss: 9.581343874742743e-06\n",
            "step: 120, loss: 8.013036676857155e-06\n",
            "step: 130, loss: 0.0012040807632729411\n",
            "step: 140, loss: 0.00038695454713888466\n",
            "step: 150, loss: 0.0010603846749290824\n",
            "step: 160, loss: 9.141725968220271e-06\n",
            "step: 170, loss: 7.968328645802103e-06\n",
            "step: 180, loss: 1.1655970411084127e-05\n",
            "step: 190, loss: 0.0002068728645099327\n",
            "step: 200, loss: 1.0598338121781126e-05\n",
            "step: 210, loss: 8.873532351572067e-06\n",
            "step: 220, loss: 8.38555206428282e-06\n",
            "step: 230, loss: 1.5212859580060467e-05\n",
            "step: 240, loss: 0.0012172601418569684\n",
            "step: 250, loss: 0.0009293803013861179\n",
            "step: 260, loss: 2.2897049348102883e-05\n",
            "step: 270, loss: 0.00020711969409603626\n",
            "step: 280, loss: 0.004669290967285633\n",
            "step: 290, loss: 7.407142402371392e-05\n",
            "step: 300, loss: 2.6488602088647895e-05\n",
            "step: 310, loss: 6.522944204334635e-06\n",
            "step: 320, loss: 5.192090611672029e-05\n",
            "step: 330, loss: 0.0012344964779913425\n",
            "step: 340, loss: 0.00081878702621907\n",
            "step: 350, loss: 7.308968633878976e-06\n",
            "step: 360, loss: 8.948037248046603e-06\n",
            "step: 370, loss: 0.0005526047316379845\n",
            "step: 380, loss: 0.0006971347611397505\n",
            "step: 390, loss: 8.402786625083536e-05\n",
            "step: 400, loss: 0.004320893436670303\n",
            "step: 410, loss: 2.7670941562973894e-05\n",
            "step: 420, loss: 1.2237187547725625e-05\n",
            "step: 430, loss: 8.243995580414776e-06\n",
            "step: 440, loss: 3.3976273698499426e-05\n",
            "step: 450, loss: 7.536208158853697e-06\n",
            "step: 460, loss: 0.010626988485455513\n",
            "step: 470, loss: 8.694743883097544e-06\n",
            "step: 480, loss: 6.91037212163792e-06\n",
            "step: 490, loss: 7.599537639180198e-06\n",
            "step: 500, loss: 0.0016802866011857986\n",
            "step: 510, loss: 0.000705496349837631\n",
            "step: 520, loss: 0.0008164438768289983\n",
            "step: 530, loss: 0.0010670417686924338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9525598872710193, f1=0.947022972339428, best_f1=0.954001839926403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.001845344551839e-06\n",
            "step: 10, loss: 0.002109113149344921\n",
            "step: 20, loss: 0.0017679572338238358\n",
            "step: 30, loss: 0.0011326548410579562\n",
            "step: 40, loss: 1.1995241948170587e-05\n",
            "step: 50, loss: 0.00033224659273400903\n",
            "step: 60, loss: 0.0013862629421055317\n",
            "step: 70, loss: 1.1186804840690456e-05\n",
            "step: 80, loss: 7.461691893695388e-06\n",
            "step: 90, loss: 7.908720363047905e-06\n",
            "step: 100, loss: 5.092450464871945e-06\n",
            "step: 110, loss: 0.00016557784692849964\n",
            "step: 120, loss: 7.163683676481014e-06\n",
            "step: 130, loss: 1.5283912944141775e-05\n",
            "step: 140, loss: 6.392555405909661e-06\n",
            "step: 150, loss: 1.0657926395651884e-05\n",
            "step: 160, loss: 5.923180651734583e-06\n",
            "step: 170, loss: 2.6609292035573162e-05\n",
            "step: 180, loss: 1.5239778804243542e-05\n",
            "step: 190, loss: 0.0013399645686149597\n",
            "step: 200, loss: 0.0007309236680157483\n",
            "step: 210, loss: 1.6862937627593055e-05\n",
            "step: 220, loss: 7.95714913692791e-06\n",
            "step: 230, loss: 9.257265446649399e-06\n",
            "step: 240, loss: 1.065411288436735e-05\n",
            "step: 250, loss: 3.3261447242693976e-05\n",
            "step: 260, loss: 8.251407962234225e-06\n",
            "step: 270, loss: 7.696392458456103e-06\n",
            "step: 280, loss: 7.1375834522768855e-06\n",
            "step: 290, loss: 0.0010131036397069693\n",
            "step: 300, loss: 6.765069883840624e-06\n",
            "step: 310, loss: 0.002486619632691145\n",
            "step: 320, loss: 7.174865459091961e-06\n",
            "step: 330, loss: 0.00023471990425605327\n",
            "step: 340, loss: 8.787886145000812e-06\n",
            "step: 350, loss: 0.0011184990871697664\n",
            "step: 360, loss: 0.00033916591200977564\n",
            "step: 370, loss: 2.114215021720156e-05\n",
            "step: 380, loss: 1.1387945960450452e-05\n",
            "step: 390, loss: 1.681150206422899e-05\n",
            "step: 400, loss: 0.0015349796740338206\n",
            "step: 410, loss: 7.565997293568216e-06\n",
            "step: 420, loss: 2.144686004612595e-05\n",
            "step: 430, loss: 5.673581654264126e-06\n",
            "step: 440, loss: 6.400011443474796e-06\n",
            "step: 450, loss: 0.001916887704282999\n",
            "step: 460, loss: 0.0007026362582109869\n",
            "step: 470, loss: 6.8544877649401315e-06\n",
            "step: 480, loss: 0.0024021996650844812\n",
            "step: 490, loss: 6.651429430348799e-05\n",
            "step: 500, loss: 0.001253367867320776\n",
            "step: 510, loss: 6.582532023458043e-06\n",
            "step: 520, loss: 2.2537675249623135e-05\n",
            "step: 530, loss: 6.4000082602433395e-06\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9534992954438704, f1=0.9480580252690688, best_f1=0.954001839926403\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:27, 208.31it/s]\n",
            "load_f1 = 0.955637707948244\n",
            "real_f1 = 0.9541454377026402\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:25, 171.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DK"
      ],
      "metadata": {
        "id": "10svv34hgw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "IJORCv6SsqPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa1986b4-bde4-4d25-e7e7-bd1a3f77b0d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=5b6bfa33c3e216d7810e580e25742abd48c44e7ec6a2bfc8c64bb27c279b9001\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mz33kr3v/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK STRUCTURED"
      ],
      "metadata": {
        "id": "pdNk8ikFgw7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCvdP9vMgw7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ee42e2b-ccc6-42ce-e1ba-2b3444e1dc1b"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 501M/501M [00:07<00:00, 67.0MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4614316523075104\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.29545454545454547, f1=0.25316455696202533, best_f1=0.25316455696202533\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4841925799846649\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.3333333333333333, f1=0.28571428571428575, best_f1=0.28571428571428575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4196210205554962\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.30434782608695654, f1=0.2857142857142857, best_f1=0.28571428571428575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3357836902141571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5517241379310344, f1=0.55, best_f1=0.55\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2502564787864685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7586206896551724, f1=0.6060606060606061, best_f1=0.6060606060606061\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1268058866262436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8571428571428571, f1=0.7142857142857143, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31151723861694336\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8387096774193549, f1=0.7741935483870968, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1505563110113144\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8571428571428571, f1=0.6923076923076924, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0709352195262909\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8, f1=0.5454545454545454, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28781262040138245\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8571428571428571, f1=0.8275862068965518, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12085378915071487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.8666666666666666, f1=0.7142857142857143, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016904689371585846\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8666666666666666, f1=0.7999999999999999, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011594644747674465\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9032258064516129, f1=0.8125000000000001, best_f1=0.8125000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018098028376698494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8750000000000001, f1=0.8125000000000001, best_f1=0.8125000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018990593031048775\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8750000000000001, f1=0.8125000000000001, best_f1=0.8125000000000001\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 126301.01it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.8148148148148148\n",
            "real_f1 = 0.8571428571428571\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 144.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "6VIiiAcAgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01967a36-fed9-4d75-9bd2-edb7ce6951ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5486199855804443\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4379050135612488\n",
            "step: 20, loss: 0.5638028383255005\n",
            "step: 30, loss: 0.28011828660964966\n",
            "step: 40, loss: 0.32004544138908386\n",
            "step: 50, loss: 0.5847017168998718\n",
            "step: 60, loss: 0.17498408257961273\n",
            "step: 70, loss: 0.17107851803302765\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 80, loss: 0.19831036031246185\n",
            "step: 90, loss: 0.09304876625537872\n",
            "step: 100, loss: 0.15195319056510925\n",
            "step: 110, loss: 0.11054126173257828\n",
            "step: 120, loss: 0.04825717210769653\n",
            "step: 130, loss: 0.022747119888663292\n",
            "step: 140, loss: 0.11145834624767303\n",
            "step: 150, loss: 0.3215790092945099\n",
            "step: 160, loss: 0.005533230025321245\n",
            "step: 170, loss: 0.014871135354042053\n",
            "step: 180, loss: 0.029402492567896843\n",
            "step: 190, loss: 0.03340177237987518\n",
            "step: 200, loss: 0.051542237401008606\n",
            "step: 210, loss: 0.13507050275802612\n",
            "step: 220, loss: 0.03600883111357689\n",
            "step: 230, loss: 0.06422343850135803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9581497797356827, f1=0.9640449438202248, best_f1=0.9640449438202248\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007114645559340715\n",
            "step: 10, loss: 0.041623689234256744\n",
            "step: 20, loss: 0.013920962810516357\n",
            "step: 30, loss: 0.002968662418425083\n",
            "step: 40, loss: 0.021019408479332924\n",
            "step: 50, loss: 0.001455108867958188\n",
            "step: 60, loss: 0.002296861493960023\n",
            "step: 70, loss: 0.01942306011915207\n",
            "step: 80, loss: 0.015762396156787872\n",
            "step: 90, loss: 0.009715475142002106\n",
            "step: 100, loss: 0.027826771140098572\n",
            "step: 110, loss: 0.015392077155411243\n",
            "step: 120, loss: 0.00791031401604414\n",
            "step: 130, loss: 0.020223628729581833\n",
            "step: 140, loss: 0.07389072328805923\n",
            "step: 150, loss: 0.0655662789940834\n",
            "step: 160, loss: 0.029934007674455643\n",
            "step: 170, loss: 0.011761181987822056\n",
            "step: 180, loss: 0.0075485678389668465\n",
            "step: 190, loss: 0.025617631152272224\n",
            "step: 200, loss: 0.022288886830210686\n",
            "step: 210, loss: 0.19178904592990875\n",
            "step: 220, loss: 0.006476634182035923\n",
            "step: 230, loss: 0.0016499991761520505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9752252252252253, f1=0.9715585893060296, best_f1=0.9715585893060296\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.035693731158971786\n",
            "step: 10, loss: 0.0010554929031059146\n",
            "step: 20, loss: 0.020802441984415054\n",
            "step: 30, loss: 0.006472621113061905\n",
            "step: 40, loss: 0.027606049552559853\n",
            "step: 50, loss: 0.012383121065795422\n",
            "step: 60, loss: 0.0010325743351131678\n",
            "step: 70, loss: 0.0006682200473733246\n",
            "step: 80, loss: 0.0002820360823534429\n",
            "step: 90, loss: 0.008438198827207088\n",
            "step: 100, loss: 0.05131042003631592\n",
            "step: 110, loss: 0.04826737940311432\n",
            "step: 120, loss: 0.006789391860365868\n",
            "step: 130, loss: 0.0742112472653389\n",
            "step: 140, loss: 0.0005811208975501359\n",
            "step: 150, loss: 0.04257219284772873\n",
            "step: 160, loss: 0.0009515952551737428\n",
            "step: 170, loss: 0.005331980995833874\n",
            "step: 180, loss: 0.0010838761227205396\n",
            "step: 190, loss: 0.04877743870019913\n",
            "step: 200, loss: 0.04398578777909279\n",
            "step: 210, loss: 0.006549449171870947\n",
            "step: 220, loss: 0.0012147566303610802\n",
            "step: 230, loss: 0.008772236295044422\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9809203142536477, f1=0.9809203142536477, best_f1=0.9809203142536477\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004123429302126169\n",
            "step: 10, loss: 0.0015987810911610723\n",
            "step: 20, loss: 0.0021157325245440006\n",
            "step: 30, loss: 0.0005847717402502894\n",
            "step: 40, loss: 0.011873300187289715\n",
            "step: 50, loss: 0.058289844542741776\n",
            "step: 60, loss: 0.006974917370826006\n",
            "step: 70, loss: 0.019257228821516037\n",
            "step: 80, loss: 0.0006333499914035201\n",
            "step: 90, loss: 0.0013088133418932557\n",
            "step: 100, loss: 0.0011200614972040057\n",
            "step: 110, loss: 0.0013258361723273993\n",
            "step: 120, loss: 0.11508958041667938\n",
            "step: 130, loss: 0.001826587482355535\n",
            "step: 140, loss: 0.0009541245526634157\n",
            "step: 150, loss: 0.0016960144275799394\n",
            "step: 160, loss: 0.0005347529659047723\n",
            "step: 170, loss: 0.013206207193434238\n",
            "step: 180, loss: 0.06501984596252441\n",
            "step: 190, loss: 0.0009509860537946224\n",
            "step: 200, loss: 0.012304456904530525\n",
            "step: 210, loss: 0.0035930145531892776\n",
            "step: 220, loss: 0.0002975761017296463\n",
            "step: 230, loss: 0.04388988018035889\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9810479375696767, f1=0.9820627802690582, best_f1=0.9820627802690582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002884666435420513\n",
            "step: 10, loss: 0.0044276961125433445\n",
            "step: 20, loss: 0.10127949714660645\n",
            "step: 30, loss: 0.002395081566646695\n",
            "step: 40, loss: 0.0017663058824837208\n",
            "step: 50, loss: 0.0006880878354422748\n",
            "step: 60, loss: 0.0007924935780465603\n",
            "step: 70, loss: 0.001327293342910707\n",
            "step: 80, loss: 0.004649042151868343\n",
            "step: 90, loss: 0.004200930241495371\n",
            "step: 100, loss: 0.00037288962630555034\n",
            "step: 110, loss: 0.0026481139939278364\n",
            "step: 120, loss: 0.05914699658751488\n",
            "step: 130, loss: 0.004172125365585089\n",
            "step: 140, loss: 0.0006725971470586956\n",
            "step: 150, loss: 0.0034267278388142586\n",
            "step: 160, loss: 0.0003175045712850988\n",
            "step: 170, loss: 0.0050877295434474945\n",
            "step: 180, loss: 0.0013265195302665234\n",
            "step: 190, loss: 0.005805641878396273\n",
            "step: 200, loss: 0.0010958677157759666\n",
            "step: 210, loss: 0.002229186473414302\n",
            "step: 220, loss: 0.0012071311939507723\n",
            "step: 230, loss: 0.0038100476376712322\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9876819708846584, f1=0.9808773903262092, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010762031888589263\n",
            "step: 10, loss: 0.00493033230304718\n",
            "step: 20, loss: 0.0034238097723573446\n",
            "step: 30, loss: 0.0010448605753481388\n",
            "step: 40, loss: 0.0005313542787916958\n",
            "step: 50, loss: 0.00044498094939626753\n",
            "step: 60, loss: 0.02698398008942604\n",
            "step: 70, loss: 0.013154859654605389\n",
            "step: 80, loss: 0.003296618815511465\n",
            "step: 90, loss: 0.012728327885270119\n",
            "step: 100, loss: 0.0016372036188840866\n",
            "step: 110, loss: 0.029165096580982208\n",
            "step: 120, loss: 0.0021639359183609486\n",
            "step: 130, loss: 0.004923987202346325\n",
            "step: 140, loss: 0.0017505364958196878\n",
            "step: 150, loss: 0.00820532999932766\n",
            "step: 160, loss: 0.03518545255064964\n",
            "step: 170, loss: 0.0011029258603230119\n",
            "step: 180, loss: 0.0013960336800664663\n",
            "step: 190, loss: 0.0010833840351551771\n",
            "step: 200, loss: 0.004145604558289051\n",
            "step: 210, loss: 0.0006735408678650856\n",
            "step: 220, loss: 0.012010536156594753\n",
            "step: 230, loss: 0.001313297776505351\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.984304932735426, f1=0.9819819819819819, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024908732157200575\n",
            "step: 10, loss: 0.0009355123620480299\n",
            "step: 20, loss: 0.0007847769884392619\n",
            "step: 30, loss: 0.0016213557682931423\n",
            "step: 40, loss: 0.00226286961697042\n",
            "step: 50, loss: 0.0010195986833423376\n",
            "step: 60, loss: 0.0012345591094344854\n",
            "step: 70, loss: 0.0006756382645107806\n",
            "step: 80, loss: 0.000695135910063982\n",
            "step: 90, loss: 0.024783369153738022\n",
            "step: 100, loss: 0.07317198812961578\n",
            "step: 110, loss: 0.0031408420763909817\n",
            "step: 120, loss: 0.0008009958546608686\n",
            "step: 130, loss: 0.004083039239048958\n",
            "step: 140, loss: 0.0001720525324344635\n",
            "step: 150, loss: 0.007926827296614647\n",
            "step: 160, loss: 0.00039955557440407574\n",
            "step: 170, loss: 0.0019254212966188788\n",
            "step: 180, loss: 0.0006386422901414335\n",
            "step: 190, loss: 0.00101001956500113\n",
            "step: 200, loss: 0.0067207589745521545\n",
            "step: 210, loss: 0.20378194749355316\n",
            "step: 220, loss: 0.0018700516084209085\n",
            "step: 230, loss: 0.005335867404937744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9852104664391355, f1=0.977116704805492, best_f1=0.9808773903262092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005114930681884289\n",
            "step: 10, loss: 0.008537685498595238\n",
            "step: 20, loss: 0.002347043016925454\n",
            "step: 30, loss: 0.0017840116051957011\n",
            "step: 40, loss: 0.0022208362352102995\n",
            "step: 50, loss: 0.006706735584884882\n",
            "step: 60, loss: 0.0008110355702228844\n",
            "step: 70, loss: 0.00043892176472581923\n",
            "step: 80, loss: 0.0003884669567923993\n",
            "step: 90, loss: 0.02945110946893692\n",
            "step: 100, loss: 0.0004261407011654228\n",
            "step: 110, loss: 0.003343351650983095\n",
            "step: 120, loss: 0.000379729550331831\n",
            "step: 130, loss: 0.016893625259399414\n",
            "step: 140, loss: 0.0004212422645650804\n",
            "step: 150, loss: 0.12189753353595734\n",
            "step: 160, loss: 0.0005191604141145945\n",
            "step: 170, loss: 0.028549835085868835\n",
            "step: 180, loss: 0.00043541949708014727\n",
            "step: 190, loss: 0.004033468663692474\n",
            "step: 200, loss: 0.004449853673577309\n",
            "step: 210, loss: 0.02673056721687317\n",
            "step: 220, loss: 0.0021082167513668537\n",
            "step: 230, loss: 0.001020737225189805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9898074745186863, f1=0.9864253393665158, best_f1=0.9864253393665158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005304568912833929\n",
            "step: 10, loss: 0.00310192396864295\n",
            "step: 20, loss: 0.0010616440558806062\n",
            "step: 30, loss: 0.0008653425611555576\n",
            "step: 40, loss: 0.0007109635043889284\n",
            "step: 50, loss: 0.002130882814526558\n",
            "step: 60, loss: 0.0007586394203826785\n",
            "step: 70, loss: 0.07874690741300583\n",
            "step: 80, loss: 0.00025509498664177954\n",
            "step: 90, loss: 0.08734161406755447\n",
            "step: 100, loss: 0.007981612347066402\n",
            "step: 110, loss: 0.00027999337180517614\n",
            "step: 120, loss: 0.04198422655463219\n",
            "step: 130, loss: 0.00048716345918364823\n",
            "step: 140, loss: 0.0005204659537412226\n",
            "step: 150, loss: 0.004538453184068203\n",
            "step: 160, loss: 0.0007722312584519386\n",
            "step: 170, loss: 0.0002867799485102296\n",
            "step: 180, loss: 0.0015933022368699312\n",
            "step: 190, loss: 0.0002348864945815876\n",
            "step: 200, loss: 0.0003268294676672667\n",
            "step: 210, loss: 0.0009920424781739712\n",
            "step: 220, loss: 0.00039500428829342127\n",
            "step: 230, loss: 0.0003223066742066294\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9887133182844244, f1=0.9852774631936579, best_f1=0.9864253393665158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004518468340393156\n",
            "step: 10, loss: 0.0007750988588668406\n",
            "step: 20, loss: 0.00039909896440804005\n",
            "step: 30, loss: 0.00028244819259271026\n",
            "step: 40, loss: 0.001337121007964015\n",
            "step: 50, loss: 0.0007385819335468113\n",
            "step: 60, loss: 0.004566386342048645\n",
            "step: 70, loss: 0.0035776845179498196\n",
            "step: 80, loss: 0.0003516036958899349\n",
            "step: 90, loss: 0.00026056531351059675\n",
            "step: 100, loss: 0.0002719162730500102\n",
            "step: 110, loss: 0.0006423068698495626\n",
            "step: 120, loss: 0.00029931747121736407\n",
            "step: 130, loss: 0.0004332715179771185\n",
            "step: 140, loss: 0.00027557360590435565\n",
            "step: 150, loss: 0.0005306251696310937\n",
            "step: 160, loss: 0.00017746428784448653\n",
            "step: 170, loss: 0.00035345120704732835\n",
            "step: 180, loss: 0.0006814555381424725\n",
            "step: 190, loss: 0.00042249064426869154\n",
            "step: 200, loss: 0.000688976957462728\n",
            "step: 210, loss: 0.0004824287025257945\n",
            "step: 220, loss: 0.007654554210603237\n",
            "step: 230, loss: 0.00027195236179977655\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9898534385569334, f1=0.9864864864864865, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019075747695751488\n",
            "step: 10, loss: 0.00044288806384429336\n",
            "step: 20, loss: 0.0003718246298376471\n",
            "step: 30, loss: 0.0004358105652499944\n",
            "step: 40, loss: 0.0007440616027452052\n",
            "step: 50, loss: 0.00035191542701795697\n",
            "step: 60, loss: 0.019040580838918686\n",
            "step: 70, loss: 0.009934410452842712\n",
            "step: 80, loss: 0.00026719519519247115\n",
            "step: 90, loss: 0.18992963433265686\n",
            "step: 100, loss: 0.0023874747566878796\n",
            "step: 110, loss: 0.00097014190396294\n",
            "step: 120, loss: 0.0007563233375549316\n",
            "step: 130, loss: 0.0005548063199967146\n",
            "step: 140, loss: 0.0018708814168348908\n",
            "step: 150, loss: 0.0006585908122360706\n",
            "step: 160, loss: 0.006727053318172693\n",
            "step: 170, loss: 0.0028557872865349054\n",
            "step: 180, loss: 0.0013758688000962138\n",
            "step: 190, loss: 0.0004951037699356675\n",
            "step: 200, loss: 0.006208289414644241\n",
            "step: 210, loss: 0.0013242131099104881\n",
            "step: 220, loss: 0.005393852014094591\n",
            "step: 230, loss: 0.0003092741244472563\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9876543209876544, f1=0.9820224719101124, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004860307672061026\n",
            "step: 10, loss: 0.00027960806619375944\n",
            "step: 20, loss: 0.01110239140689373\n",
            "step: 30, loss: 0.028035828843712807\n",
            "step: 40, loss: 0.0008895836654119194\n",
            "step: 50, loss: 0.004181985277682543\n",
            "step: 60, loss: 0.0006642970838584006\n",
            "step: 70, loss: 0.0005801856750622392\n",
            "step: 80, loss: 0.00015724792319815606\n",
            "step: 90, loss: 0.002079753205180168\n",
            "step: 100, loss: 0.00024086012854240835\n",
            "step: 110, loss: 0.00016168606816790998\n",
            "step: 120, loss: 0.002297869883477688\n",
            "step: 130, loss: 0.0005445068818517029\n",
            "step: 140, loss: 0.0008747664978727698\n",
            "step: 150, loss: 0.00034440140007063746\n",
            "step: 160, loss: 0.0004343477776274085\n",
            "step: 170, loss: 0.0020990464836359024\n",
            "step: 180, loss: 0.00033674348378553987\n",
            "step: 190, loss: 0.0038387873210012913\n",
            "step: 200, loss: 0.000280263222521171\n",
            "step: 210, loss: 0.0004719517019111663\n",
            "step: 220, loss: 0.013761937618255615\n",
            "step: 230, loss: 0.0004055506724398583\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9876543209876544, f1=0.9875706214689265, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011024451814591885\n",
            "step: 10, loss: 0.00023307382070925087\n",
            "step: 20, loss: 0.001554465270601213\n",
            "step: 30, loss: 0.00031155371107161045\n",
            "step: 40, loss: 0.0013811031822115183\n",
            "step: 50, loss: 0.003976375330239534\n",
            "step: 60, loss: 0.00038428764673881233\n",
            "step: 70, loss: 0.0013868095120415092\n",
            "step: 80, loss: 0.0013007005909457803\n",
            "step: 90, loss: 0.00044551517930813134\n",
            "step: 100, loss: 0.001742169144563377\n",
            "step: 110, loss: 0.0012951522367075086\n",
            "step: 120, loss: 0.0007940015057101846\n",
            "step: 130, loss: 0.0002557167608756572\n",
            "step: 140, loss: 0.0003720156382769346\n",
            "step: 150, loss: 7.863094651838765e-05\n",
            "step: 160, loss: 0.0012081917375326157\n",
            "step: 170, loss: 0.0027467021718621254\n",
            "step: 180, loss: 0.03714153915643692\n",
            "step: 190, loss: 0.0003365858574397862\n",
            "step: 200, loss: 6.197447510203347e-05\n",
            "step: 210, loss: 0.002510341349989176\n",
            "step: 220, loss: 0.0003319628303870559\n",
            "step: 230, loss: 0.0003017802373506129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9887387387387387, f1=0.9831271091113611, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011720426118699834\n",
            "step: 10, loss: 9.700866212369874e-05\n",
            "step: 20, loss: 0.00018062637536786497\n",
            "step: 30, loss: 0.000420232187025249\n",
            "step: 40, loss: 9.596822928870097e-05\n",
            "step: 50, loss: 0.00021942002058494836\n",
            "step: 60, loss: 0.00025822402676567435\n",
            "step: 70, loss: 0.0001701847795629874\n",
            "step: 80, loss: 0.0005922128912061453\n",
            "step: 90, loss: 0.0026235547848045826\n",
            "step: 100, loss: 0.0012331211473792791\n",
            "step: 110, loss: 0.0002669971436262131\n",
            "step: 120, loss: 4.992468166165054e-05\n",
            "step: 130, loss: 0.00021439902775455266\n",
            "step: 140, loss: 0.0002227427321486175\n",
            "step: 150, loss: 4.923560481984168e-05\n",
            "step: 160, loss: 0.0004889875999651849\n",
            "step: 170, loss: 0.00024461362045258284\n",
            "step: 180, loss: 0.00014828750863671303\n",
            "step: 190, loss: 0.0001804341736715287\n",
            "step: 200, loss: 0.0009600965422578156\n",
            "step: 210, loss: 0.00015999893366824836\n",
            "step: 220, loss: 0.00018485941109247506\n",
            "step: 230, loss: 9.395687811775133e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9876543209876544, f1=0.9842696629213483, best_f1=0.9864864864864865\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0176670141518116\n",
            "step: 10, loss: 0.0002765968383755535\n",
            "step: 20, loss: 0.0021470363717526197\n",
            "step: 30, loss: 0.0001607035956112668\n",
            "step: 40, loss: 0.000250336219323799\n",
            "step: 50, loss: 0.00013635223149321973\n",
            "step: 60, loss: 0.058985378593206406\n",
            "step: 70, loss: 0.0003145758237224072\n",
            "step: 80, loss: 0.00013582811516243964\n",
            "step: 90, loss: 0.00020932205370627344\n",
            "step: 100, loss: 9.981414768844843e-05\n",
            "step: 110, loss: 0.0001835139119066298\n",
            "step: 120, loss: 0.02850107103586197\n",
            "step: 130, loss: 0.00014108780305832624\n",
            "step: 140, loss: 0.0070040347054600716\n",
            "step: 150, loss: 0.00017452608153689653\n",
            "step: 160, loss: 0.010714912787079811\n",
            "step: 170, loss: 6.284717528615147e-05\n",
            "step: 180, loss: 0.002314545912668109\n",
            "step: 190, loss: 0.00032210751669481397\n",
            "step: 200, loss: 0.0014550368068739772\n",
            "step: 210, loss: 0.011503443121910095\n",
            "step: 220, loss: 0.00015882670413702726\n",
            "step: 230, loss: 0.00017558957915753126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9876819708846584, f1=0.9842696629213483, best_f1=0.9864864864864865\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:17, 144.60it/s]\n",
            "load_f1 = 0.9898762654668166\n",
            "real_f1 = 0.9865470852017937\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:33, 130.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "S4v1tmXbgw8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "qUUIV1IBgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2d6b3e-6cf3-4eb3-b8f8-a440e4392025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6188238859176636\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.45748141407966614\n",
            "step: 20, loss: 0.3040255904197693\n",
            "step: 30, loss: 0.3328167498111725\n",
            "step: 40, loss: 0.33275845646858215\n",
            "step: 50, loss: 0.3507930636405945\n",
            "step: 60, loss: 0.09721757471561432\n",
            "step: 70, loss: 0.3080408573150635\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.10292492061853409\n",
            "step: 90, loss: 0.213617205619812\n",
            "step: 100, loss: 0.1406605839729309\n",
            "step: 110, loss: 0.09398798644542694\n",
            "step: 120, loss: 0.14416372776031494\n",
            "step: 130, loss: 0.11904973536729813\n",
            "step: 140, loss: 0.15404047071933746\n",
            "step: 150, loss: 0.08746249973773956\n",
            "step: 160, loss: 0.17654626071453094\n",
            "step: 170, loss: 0.04822002351284027\n",
            "step: 180, loss: 0.07108522951602936\n",
            "step: 190, loss: 0.06202555075287819\n",
            "step: 200, loss: 0.022978434339165688\n",
            "step: 210, loss: 0.03428840637207031\n",
            "step: 220, loss: 0.18238967657089233\n",
            "step: 230, loss: 0.1626981645822525\n",
            "step: 240, loss: 0.04193751513957977\n",
            "step: 250, loss: 0.07087667286396027\n",
            "step: 260, loss: 0.23446007072925568\n",
            "step: 270, loss: 0.3180544078350067\n",
            "step: 280, loss: 0.06264682859182358\n",
            "step: 290, loss: 0.13361482322216034\n",
            "step: 300, loss: 0.02772160992026329\n",
            "step: 310, loss: 0.13553541898727417\n",
            "step: 320, loss: 0.12579916417598724\n",
            "step: 330, loss: 0.16848525404930115\n",
            "step: 340, loss: 0.5909309387207031\n",
            "step: 350, loss: 0.12115619331598282\n",
            "step: 360, loss: 0.06721813231706619\n",
            "step: 370, loss: 0.07620566338300705\n",
            "step: 380, loss: 0.1867814064025879\n",
            "step: 390, loss: 0.021803978830575943\n",
            "step: 400, loss: 0.053443364799022675\n",
            "step: 410, loss: 0.25571051239967346\n",
            "step: 420, loss: 0.011972647160291672\n",
            "step: 430, loss: 0.016778474673628807\n",
            "step: 440, loss: 0.05709170922636986\n",
            "step: 450, loss: 0.09803558140993118\n",
            "step: 460, loss: 0.021734731271862984\n",
            "step: 470, loss: 0.03950239717960358\n",
            "step: 480, loss: 0.16749480366706848\n",
            "step: 490, loss: 0.17489252984523773\n",
            "step: 500, loss: 0.02054774947464466\n",
            "step: 510, loss: 0.06687886267900467\n",
            "step: 520, loss: 0.11958061903715134\n",
            "step: 530, loss: 0.007810610346496105\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9409576940957695, f1=0.9424326833797586, best_f1=0.9424326833797586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.033059101551771164\n",
            "step: 10, loss: 0.17456857860088348\n",
            "step: 20, loss: 0.054417677223682404\n",
            "step: 30, loss: 0.05688678100705147\n",
            "step: 40, loss: 0.07831283658742905\n",
            "step: 50, loss: 0.14657726883888245\n",
            "step: 60, loss: 0.05059288069605827\n",
            "step: 70, loss: 0.029589150100946426\n",
            "step: 80, loss: 0.027354411780834198\n",
            "step: 90, loss: 0.003969786688685417\n",
            "step: 100, loss: 0.10496693104505539\n",
            "step: 110, loss: 0.01528165489435196\n",
            "step: 120, loss: 0.16002368927001953\n",
            "step: 130, loss: 0.04230896756052971\n",
            "step: 140, loss: 0.03785058483481407\n",
            "step: 150, loss: 0.013276764191687107\n",
            "step: 160, loss: 0.03784399852156639\n",
            "step: 170, loss: 0.05956389755010605\n",
            "step: 180, loss: 0.035180315375328064\n",
            "step: 190, loss: 0.06114790216088295\n",
            "step: 200, loss: 0.19222237169742584\n",
            "step: 210, loss: 0.06273642182350159\n",
            "step: 220, loss: 0.002632268937304616\n",
            "step: 230, loss: 0.016064785420894623\n",
            "step: 240, loss: 0.1399652659893036\n",
            "step: 250, loss: 0.0374675951898098\n",
            "step: 260, loss: 0.017415577545762062\n",
            "step: 270, loss: 0.01779817044734955\n",
            "step: 280, loss: 0.05948847532272339\n",
            "step: 290, loss: 0.052440956234931946\n",
            "step: 300, loss: 0.024114197120070457\n",
            "step: 310, loss: 0.039753515273332596\n",
            "step: 320, loss: 0.049869731068611145\n",
            "step: 330, loss: 0.00801790226250887\n",
            "step: 340, loss: 0.03901410847902298\n",
            "step: 350, loss: 0.0049805608578026295\n",
            "step: 360, loss: 0.05084194242954254\n",
            "step: 370, loss: 0.0010369475930929184\n",
            "step: 380, loss: 0.2754332423210144\n",
            "step: 390, loss: 0.014988232403993607\n",
            "step: 400, loss: 0.02658967860043049\n",
            "step: 410, loss: 0.12238039821386337\n",
            "step: 420, loss: 0.05011267587542534\n",
            "step: 430, loss: 0.2315954864025116\n",
            "step: 440, loss: 0.009157788008451462\n",
            "step: 450, loss: 0.061011169105768204\n",
            "step: 460, loss: 0.03923499584197998\n",
            "step: 470, loss: 0.026957562193274498\n",
            "step: 480, loss: 0.00202798075042665\n",
            "step: 490, loss: 0.06501718610525131\n",
            "step: 500, loss: 0.008139543235301971\n",
            "step: 510, loss: 0.009283533319830894\n",
            "step: 520, loss: 0.3753072917461395\n",
            "step: 530, loss: 0.019952522590756416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9507593189139439, f1=0.9442139234670355, best_f1=0.9442139234670355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07189963012933731\n",
            "step: 10, loss: 0.04243973270058632\n",
            "step: 20, loss: 0.00730513921007514\n",
            "step: 30, loss: 0.13067932426929474\n",
            "step: 40, loss: 0.06018224358558655\n",
            "step: 50, loss: 0.006223083008080721\n",
            "step: 60, loss: 0.010922790504992008\n",
            "step: 70, loss: 0.006947140675038099\n",
            "step: 80, loss: 0.0017147164326161146\n",
            "step: 90, loss: 0.0041458806954324245\n",
            "step: 100, loss: 0.03271936625242233\n",
            "step: 110, loss: 0.03179367631673813\n",
            "step: 120, loss: 0.13798177242279053\n",
            "step: 130, loss: 0.04851031303405762\n",
            "step: 140, loss: 0.007455938030034304\n",
            "step: 150, loss: 0.010710753500461578\n",
            "step: 160, loss: 0.005040369462221861\n",
            "step: 170, loss: 0.002562546404078603\n",
            "step: 180, loss: 0.03645377606153488\n",
            "step: 190, loss: 0.004483216442167759\n",
            "step: 200, loss: 0.03842606395483017\n",
            "step: 210, loss: 0.04905683919787407\n",
            "step: 220, loss: 0.026550976559519768\n",
            "step: 230, loss: 0.03160906210541725\n",
            "step: 240, loss: 0.06851746886968613\n",
            "step: 250, loss: 0.08844010531902313\n",
            "step: 260, loss: 0.09310203045606613\n",
            "step: 270, loss: 0.0035310874227434397\n",
            "step: 280, loss: 0.0009889926295727491\n",
            "step: 290, loss: 0.0062406109645962715\n",
            "step: 300, loss: 0.06403665989637375\n",
            "step: 310, loss: 0.0674070417881012\n",
            "step: 320, loss: 0.19015629589557648\n",
            "step: 330, loss: 0.00406182836741209\n",
            "step: 340, loss: 0.0062524303793907166\n",
            "step: 350, loss: 0.12119261920452118\n",
            "step: 360, loss: 0.010217337869107723\n",
            "step: 370, loss: 0.0209105983376503\n",
            "step: 380, loss: 0.007122586946934462\n",
            "step: 390, loss: 0.005853199865669012\n",
            "step: 400, loss: 0.09227554500102997\n",
            "step: 410, loss: 0.01566622406244278\n",
            "step: 420, loss: 0.024919239804148674\n",
            "step: 430, loss: 0.006246154196560383\n",
            "step: 440, loss: 0.18947918713092804\n",
            "step: 450, loss: 0.15538521111011505\n",
            "step: 460, loss: 0.034029316157102585\n",
            "step: 470, loss: 0.1295909434556961\n",
            "step: 480, loss: 0.060047004371881485\n",
            "step: 490, loss: 0.025902867317199707\n",
            "step: 500, loss: 0.02155821956694126\n",
            "step: 510, loss: 0.02491661347448826\n",
            "step: 520, loss: 0.012842914089560509\n",
            "step: 530, loss: 0.01292046532034874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9549632352941175, f1=0.9471264367816092, best_f1=0.9471264367816092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02345062606036663\n",
            "step: 10, loss: 0.005692279897630215\n",
            "step: 20, loss: 0.060018137097358704\n",
            "step: 30, loss: 0.0103401318192482\n",
            "step: 40, loss: 0.0370112806558609\n",
            "step: 50, loss: 0.10564721375703812\n",
            "step: 60, loss: 0.008157031610608101\n",
            "step: 70, loss: 0.002214468317106366\n",
            "step: 80, loss: 0.0018171854317188263\n",
            "step: 90, loss: 0.043084483593702316\n",
            "step: 100, loss: 0.0030698482878506184\n",
            "step: 110, loss: 0.00461615389212966\n",
            "step: 120, loss: 0.002855336293578148\n",
            "step: 130, loss: 0.1411115974187851\n",
            "step: 140, loss: 0.03690319135785103\n",
            "step: 150, loss: 0.027547761797904968\n",
            "step: 160, loss: 0.011743666604161263\n",
            "step: 170, loss: 0.013012785464525223\n",
            "step: 180, loss: 0.17111563682556152\n",
            "step: 190, loss: 0.025553347542881966\n",
            "step: 200, loss: 0.013533015735447407\n",
            "step: 210, loss: 0.0038756721187382936\n",
            "step: 220, loss: 0.0003411217185202986\n",
            "step: 230, loss: 0.029655465856194496\n",
            "step: 240, loss: 0.007787054870277643\n",
            "step: 250, loss: 0.12703044712543488\n",
            "step: 260, loss: 0.006427394226193428\n",
            "step: 270, loss: 0.03510052338242531\n",
            "step: 280, loss: 0.0054396032355725765\n",
            "step: 290, loss: 0.055475618690252304\n",
            "step: 300, loss: 0.0012506042839959264\n",
            "step: 310, loss: 0.0021630297414958477\n",
            "step: 320, loss: 0.16588819026947021\n",
            "step: 330, loss: 0.006201880984008312\n",
            "step: 340, loss: 0.009691094979643822\n",
            "step: 350, loss: 0.09562762081623077\n",
            "step: 360, loss: 0.02844015136361122\n",
            "step: 370, loss: 0.007719664368778467\n",
            "step: 380, loss: 0.009729469195008278\n",
            "step: 390, loss: 0.0005497772945091128\n",
            "step: 400, loss: 0.008740723133087158\n",
            "step: 410, loss: 0.0012100358726456761\n",
            "step: 420, loss: 0.010457322932779789\n",
            "step: 430, loss: 0.015325370244681835\n",
            "step: 440, loss: 0.06431291252374649\n",
            "step: 450, loss: 0.02455870434641838\n",
            "step: 460, loss: 0.04026792570948601\n",
            "step: 470, loss: 0.0006724843988195062\n",
            "step: 480, loss: 0.041558846831321716\n",
            "step: 490, loss: 0.004745832644402981\n",
            "step: 500, loss: 0.04204915463924408\n",
            "step: 510, loss: 0.05200415849685669\n",
            "step: 520, loss: 0.002886503469198942\n",
            "step: 530, loss: 0.10207132995128632\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9563994374120957, f1=0.9477154969382948, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010380018502473831\n",
            "step: 10, loss: 0.02398526296019554\n",
            "step: 20, loss: 0.0029220760334283113\n",
            "step: 30, loss: 0.054797977209091187\n",
            "step: 40, loss: 0.0013292721705511212\n",
            "step: 50, loss: 0.13931629061698914\n",
            "step: 60, loss: 0.048388246446847916\n",
            "step: 70, loss: 0.001942927367053926\n",
            "step: 80, loss: 0.00042368806316517293\n",
            "step: 90, loss: 0.051640138030052185\n",
            "step: 100, loss: 0.06672630459070206\n",
            "step: 110, loss: 0.017427289858460426\n",
            "step: 120, loss: 0.054812829941511154\n",
            "step: 130, loss: 0.007838202640414238\n",
            "step: 140, loss: 0.02014477178454399\n",
            "step: 150, loss: 0.044347744435071945\n",
            "step: 160, loss: 0.017334651201963425\n",
            "step: 170, loss: 0.09315066039562225\n",
            "step: 180, loss: 0.015187532640993595\n",
            "step: 190, loss: 0.003763003973290324\n",
            "step: 200, loss: 0.015732398256659508\n",
            "step: 210, loss: 0.03903349116444588\n",
            "step: 220, loss: 0.000925532600376755\n",
            "step: 230, loss: 0.0005532811628654599\n",
            "step: 240, loss: 0.015244237147271633\n",
            "step: 250, loss: 0.1014786809682846\n",
            "step: 260, loss: 0.0012565483339130878\n",
            "step: 270, loss: 0.02631022222340107\n",
            "step: 280, loss: 0.06597919762134552\n",
            "step: 290, loss: 0.013319611549377441\n",
            "step: 300, loss: 0.012093092314898968\n",
            "step: 310, loss: 0.014143078587949276\n",
            "step: 320, loss: 0.11228140443563461\n",
            "step: 330, loss: 0.0018806520383805037\n",
            "step: 340, loss: 0.0508219450712204\n",
            "step: 350, loss: 0.0014481086982414126\n",
            "step: 360, loss: 0.0007891635177657008\n",
            "step: 370, loss: 0.009198804385960102\n",
            "step: 380, loss: 0.0017277891747653484\n",
            "step: 390, loss: 0.004841525107622147\n",
            "step: 400, loss: 0.0007683911244384944\n",
            "step: 410, loss: 0.02110128290951252\n",
            "step: 420, loss: 0.21311385929584503\n",
            "step: 430, loss: 0.003793608397245407\n",
            "step: 440, loss: 0.0014735382283106446\n",
            "step: 450, loss: 0.01623423583805561\n",
            "step: 460, loss: 0.05506276339292526\n",
            "step: 470, loss: 0.0343208983540535\n",
            "step: 480, loss: 0.004263011272996664\n",
            "step: 490, loss: 0.004151578526943922\n",
            "step: 500, loss: 0.03644121438264847\n",
            "step: 510, loss: 0.0022269829642027617\n",
            "step: 520, loss: 0.09822910279035568\n",
            "step: 530, loss: 0.013785106129944324\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9520260829063809, f1=0.9505135387488329, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06449758261442184\n",
            "step: 10, loss: 0.0002565899631008506\n",
            "step: 20, loss: 0.00034448117366991937\n",
            "step: 30, loss: 0.0010343100875616074\n",
            "step: 40, loss: 0.0004939345526508987\n",
            "step: 50, loss: 0.003402572125196457\n",
            "step: 60, loss: 0.019526753574609756\n",
            "step: 70, loss: 0.0010615199571475387\n",
            "step: 80, loss: 0.0002999919524881989\n",
            "step: 90, loss: 0.00031363009475171566\n",
            "step: 100, loss: 0.2830575108528137\n",
            "step: 110, loss: 0.0018841004930436611\n",
            "step: 120, loss: 0.01083350833505392\n",
            "step: 130, loss: 0.0006775836227461696\n",
            "step: 140, loss: 0.007083290722221136\n",
            "step: 150, loss: 0.00506451353430748\n",
            "step: 160, loss: 0.15477026998996735\n",
            "step: 170, loss: 0.00559987174347043\n",
            "step: 180, loss: 0.005231175571680069\n",
            "step: 190, loss: 0.027040984481573105\n",
            "step: 200, loss: 0.015718627721071243\n",
            "step: 210, loss: 0.0006985141080804169\n",
            "step: 220, loss: 0.0012459808494895697\n",
            "step: 230, loss: 0.024120589718222618\n",
            "step: 240, loss: 0.200161412358284\n",
            "step: 250, loss: 0.05861228331923485\n",
            "step: 260, loss: 0.00765560707077384\n",
            "step: 270, loss: 0.007972870022058487\n",
            "step: 280, loss: 0.0042240675538778305\n",
            "step: 290, loss: 0.0003857699630316347\n",
            "step: 300, loss: 0.0027206146623939276\n",
            "step: 310, loss: 0.06508932262659073\n",
            "step: 320, loss: 0.00023344634973909706\n",
            "step: 330, loss: 0.0061560943722724915\n",
            "step: 340, loss: 0.0006731277680955827\n",
            "step: 350, loss: 0.0102081298828125\n",
            "step: 360, loss: 0.031234135851264\n",
            "step: 370, loss: 0.01470436342060566\n",
            "step: 380, loss: 0.001047411235049367\n",
            "step: 390, loss: 0.002805189462378621\n",
            "step: 400, loss: 0.0023612407967448235\n",
            "step: 410, loss: 0.004474494140595198\n",
            "step: 420, loss: 0.016530033200979233\n",
            "step: 430, loss: 0.003555819159373641\n",
            "step: 440, loss: 0.07105087488889694\n",
            "step: 450, loss: 0.2142612189054489\n",
            "step: 460, loss: 0.010401374660432339\n",
            "step: 470, loss: 0.0006703269318677485\n",
            "step: 480, loss: 0.0024548026267439127\n",
            "step: 490, loss: 0.004490723833441734\n",
            "step: 500, loss: 0.0036146328784525394\n",
            "step: 510, loss: 0.02795296348631382\n",
            "step: 520, loss: 0.004556834697723389\n",
            "step: 530, loss: 0.015245635993778706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9473193473193473, f1=0.948162111215834, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008070646785199642\n",
            "step: 10, loss: 0.0029796359594911337\n",
            "step: 20, loss: 0.009754146449267864\n",
            "step: 30, loss: 0.011287808418273926\n",
            "step: 40, loss: 0.004972292575985193\n",
            "step: 50, loss: 0.002676036674529314\n",
            "step: 60, loss: 0.004410037770867348\n",
            "step: 70, loss: 0.0005269805551506579\n",
            "step: 80, loss: 0.00048718886682763696\n",
            "step: 90, loss: 0.00023537129163742065\n",
            "step: 100, loss: 0.0006920526502653956\n",
            "step: 110, loss: 0.00013712586951442063\n",
            "step: 120, loss: 0.00029671602533198893\n",
            "step: 130, loss: 0.0003290344320703298\n",
            "step: 140, loss: 0.00030165398493409157\n",
            "step: 150, loss: 0.0020676793064922094\n",
            "step: 160, loss: 0.0003404525632504374\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 0.007817627862095833\n",
            "step: 180, loss: 0.12247568368911743\n",
            "step: 190, loss: 0.013522186316549778\n",
            "step: 200, loss: 0.0009051637607626617\n",
            "step: 210, loss: 0.0007638518000021577\n",
            "step: 220, loss: 0.00011251930118305609\n",
            "step: 230, loss: 0.0006780902622267604\n",
            "step: 240, loss: 0.03138977289199829\n",
            "step: 250, loss: 0.07515682280063629\n",
            "step: 260, loss: 0.0019447029335424304\n",
            "step: 270, loss: 0.00048791273729875684\n",
            "step: 280, loss: 0.004603539127856493\n",
            "step: 290, loss: 0.00019849147065542638\n",
            "step: 300, loss: 0.0001199152902700007\n",
            "step: 310, loss: 0.0042349048890173435\n",
            "step: 320, loss: 0.09647223353385925\n",
            "step: 330, loss: 0.00023675763804931194\n",
            "step: 340, loss: 0.001238823402673006\n",
            "step: 350, loss: 0.0006439696298912168\n",
            "step: 360, loss: 0.003867338178679347\n",
            "step: 370, loss: 0.01015226636081934\n",
            "step: 380, loss: 0.0009061670280061662\n",
            "step: 390, loss: 0.006730799563229084\n",
            "step: 400, loss: 0.0017704892670735717\n",
            "step: 410, loss: 0.00015457463450729847\n",
            "step: 420, loss: 0.06306565552949905\n",
            "step: 430, loss: 0.00016010957187972963\n",
            "step: 440, loss: 0.0012321901740506291\n",
            "step: 450, loss: 0.00028740064590238035\n",
            "step: 460, loss: 0.0001949865254573524\n",
            "step: 470, loss: 0.1706565022468567\n",
            "step: 480, loss: 0.009257360361516476\n",
            "step: 490, loss: 0.00021235315944068134\n",
            "step: 500, loss: 0.0015529809752479196\n",
            "step: 510, loss: 0.0005912821507081389\n",
            "step: 520, loss: 0.0011286578373983502\n",
            "step: 530, loss: 0.00040313063072972\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9435370975268316, f1=0.939679547596607, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008437586948275566\n",
            "step: 10, loss: 0.020124971866607666\n",
            "step: 20, loss: 0.0005280346958898008\n",
            "step: 30, loss: 0.00021927498164586723\n",
            "step: 40, loss: 0.0003085437056142837\n",
            "step: 50, loss: 0.00034391070948913693\n",
            "step: 60, loss: 0.00031891666003502905\n",
            "step: 70, loss: 0.0006460344884544611\n",
            "step: 80, loss: 0.0015380613040179014\n",
            "step: 90, loss: 0.0005343104712665081\n",
            "step: 100, loss: 0.00589198200032115\n",
            "step: 110, loss: 0.0003160380001645535\n",
            "step: 120, loss: 0.001767642330378294\n",
            "step: 130, loss: 0.01313470397144556\n",
            "step: 140, loss: 0.00028173584723845124\n",
            "step: 150, loss: 0.0571293942630291\n",
            "step: 160, loss: 0.06328719109296799\n",
            "step: 170, loss: 0.05656825006008148\n",
            "step: 180, loss: 0.02682131715118885\n",
            "step: 190, loss: 0.025991789996623993\n",
            "step: 200, loss: 0.0033382843248546124\n",
            "step: 210, loss: 0.0670192763209343\n",
            "step: 220, loss: 0.00015577803424093872\n",
            "step: 230, loss: 0.045480914413928986\n",
            "step: 240, loss: 0.022954629734158516\n",
            "step: 250, loss: 0.0005414113402366638\n",
            "step: 260, loss: 0.0011736316373571754\n",
            "step: 270, loss: 0.0004545266565401107\n",
            "step: 280, loss: 0.0016076307510957122\n",
            "step: 290, loss: 0.0009620212367735803\n",
            "step: 300, loss: 8.468722808174789e-05\n",
            "step: 310, loss: 0.0007176075014285743\n",
            "step: 320, loss: 0.0008919679094105959\n",
            "step: 330, loss: 0.0004064972454216331\n",
            "step: 340, loss: 0.17547692358493805\n",
            "step: 350, loss: 0.00039885807200334966\n",
            "step: 360, loss: 0.04921136051416397\n",
            "step: 370, loss: 0.007586474530398846\n",
            "step: 380, loss: 0.000386832223739475\n",
            "step: 390, loss: 0.013486308045685291\n",
            "step: 400, loss: 0.002161305397748947\n",
            "step: 410, loss: 0.004333114717155695\n",
            "step: 420, loss: 0.001308559556491673\n",
            "step: 430, loss: 0.0031588515266776085\n",
            "step: 440, loss: 0.0012820437550544739\n",
            "step: 450, loss: 0.0004804680065717548\n",
            "step: 460, loss: 0.01131447684019804\n",
            "step: 470, loss: 0.05943167582154274\n",
            "step: 480, loss: 0.13640868663787842\n",
            "step: 490, loss: 0.004261863883584738\n",
            "step: 500, loss: 0.0012658372288569808\n",
            "step: 510, loss: 0.002355037024244666\n",
            "step: 520, loss: 0.0009746336727403104\n",
            "step: 530, loss: 0.00033603451447561383\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9477093937991671, f1=0.9471221338324755, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021047668997198343\n",
            "step: 10, loss: 0.01636907458305359\n",
            "step: 20, loss: 0.004620634485036135\n",
            "step: 30, loss: 0.05008329823613167\n",
            "step: 40, loss: 0.00027787324506789446\n",
            "step: 50, loss: 0.0016003185883164406\n",
            "step: 60, loss: 0.0010111292358487844\n",
            "step: 70, loss: 0.16467006504535675\n",
            "step: 80, loss: 0.015855703502893448\n",
            "step: 90, loss: 0.06405754387378693\n",
            "step: 100, loss: 0.0010241646086797118\n",
            "step: 110, loss: 0.008951550349593163\n",
            "step: 120, loss: 0.00024088460486382246\n",
            "step: 130, loss: 0.00024775968631729484\n",
            "step: 140, loss: 0.005431056953966618\n",
            "step: 150, loss: 0.0001399677712470293\n",
            "step: 160, loss: 0.0001741543528623879\n",
            "step: 170, loss: 0.005344468168914318\n",
            "step: 180, loss: 0.00011775989696616307\n",
            "step: 190, loss: 7.666827877983451e-05\n",
            "step: 200, loss: 0.00014068535529077053\n",
            "step: 210, loss: 7.094401371432468e-05\n",
            "step: 220, loss: 0.0018172778654843569\n",
            "step: 230, loss: 0.001145757851190865\n",
            "step: 240, loss: 7.611604814883322e-05\n",
            "step: 250, loss: 0.0007545856642536819\n",
            "step: 260, loss: 0.0015253758756443858\n",
            "step: 270, loss: 0.03462657332420349\n",
            "step: 280, loss: 0.017108792439103127\n",
            "step: 290, loss: 0.00022297730902209878\n",
            "step: 300, loss: 0.0010304481256753206\n",
            "step: 310, loss: 0.02480117417871952\n",
            "step: 320, loss: 0.03599517047405243\n",
            "step: 330, loss: 0.0005009883316233754\n",
            "step: 340, loss: 0.0013541245134547353\n",
            "step: 350, loss: 0.022439874708652496\n",
            "step: 360, loss: 0.0001855390873970464\n",
            "step: 370, loss: 0.019887734204530716\n",
            "step: 380, loss: 0.023680374026298523\n",
            "step: 390, loss: 0.00028919108444824815\n",
            "step: 400, loss: 0.0008365868707187474\n",
            "step: 410, loss: 0.0034764311276376247\n",
            "step: 420, loss: 0.00016383503680117428\n",
            "step: 430, loss: 0.005185004323720932\n",
            "step: 440, loss: 0.031185034662485123\n",
            "step: 450, loss: 0.046942081302404404\n",
            "step: 460, loss: 0.0001352698600385338\n",
            "step: 470, loss: 0.0028069124091416597\n",
            "step: 480, loss: 0.00011694486602209508\n",
            "step: 490, loss: 0.0004014337609987706\n",
            "step: 500, loss: 0.0027923877350986004\n",
            "step: 510, loss: 0.0012577230809256434\n",
            "step: 520, loss: 0.088225357234478\n",
            "step: 530, loss: 0.0747952088713646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9496470588235294, f1=0.9479659413434247, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007014806033112109\n",
            "step: 10, loss: 0.002577441046014428\n",
            "step: 20, loss: 0.00040976126911118627\n",
            "step: 30, loss: 0.0011690394021570683\n",
            "step: 40, loss: 0.0003812774666585028\n",
            "step: 50, loss: 0.0006909450166858733\n",
            "step: 60, loss: 0.00911769736558199\n",
            "step: 70, loss: 0.00010493527224753052\n",
            "step: 80, loss: 7.551432645414025e-05\n",
            "step: 90, loss: 7.174295751610771e-05\n",
            "step: 100, loss: 0.003206136403605342\n",
            "step: 110, loss: 0.013619635254144669\n",
            "step: 120, loss: 0.0011409753933548927\n",
            "step: 130, loss: 0.0016711876960471272\n",
            "step: 140, loss: 0.00011767403339035809\n",
            "step: 150, loss: 0.0008023447589948773\n",
            "step: 160, loss: 0.03826112672686577\n",
            "step: 170, loss: 3.542984268278815e-05\n",
            "step: 180, loss: 0.00016866724763531238\n",
            "step: 190, loss: 3.149216354358941e-05\n",
            "step: 200, loss: 8.136266842484474e-05\n",
            "step: 210, loss: 0.010190131142735481\n",
            "step: 220, loss: 8.395307668251917e-05\n",
            "step: 230, loss: 4.098728459211998e-05\n",
            "step: 240, loss: 5.79413135710638e-05\n",
            "step: 250, loss: 0.0017425876576453447\n",
            "step: 260, loss: 0.0007276409887708724\n",
            "step: 270, loss: 7.572754111606628e-05\n",
            "step: 280, loss: 0.015420781448483467\n",
            "step: 290, loss: 0.00011207479838049039\n",
            "step: 300, loss: 0.0004883162910118699\n",
            "step: 310, loss: 0.07383985072374344\n",
            "step: 320, loss: 0.0011426148703321815\n",
            "step: 330, loss: 0.004420161712914705\n",
            "step: 340, loss: 0.0005616036360152066\n",
            "step: 350, loss: 0.002624941524118185\n",
            "step: 360, loss: 3.946623473893851e-05\n",
            "step: 370, loss: 0.00022589584114030004\n",
            "step: 380, loss: 0.0051671895198524\n",
            "step: 390, loss: 2.3189300918602385e-05\n",
            "step: 400, loss: 4.60761075373739e-05\n",
            "step: 410, loss: 0.006458848714828491\n",
            "step: 420, loss: 2.8031621695845388e-05\n",
            "step: 430, loss: 2.6071100364788435e-05\n",
            "step: 440, loss: 0.002945445943623781\n",
            "step: 450, loss: 0.00035425741225481033\n",
            "step: 460, loss: 2.96298494504299e-05\n",
            "step: 470, loss: 0.00046683649998158216\n",
            "step: 480, loss: 0.00010980905062751845\n",
            "step: 490, loss: 4.872853241977282e-05\n",
            "step: 500, loss: 0.008138788864016533\n",
            "step: 510, loss: 2.1442159777507186e-05\n",
            "step: 520, loss: 0.0006214614841155708\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 530, loss: 0.00038371802656911314\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.942008486562942, f1=0.9374999999999999, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.9816015739925206e-05\n",
            "step: 10, loss: 0.3764384090900421\n",
            "step: 20, loss: 0.00017152755754068494\n",
            "step: 30, loss: 0.0004777610010933131\n",
            "step: 40, loss: 0.015898585319519043\n",
            "step: 50, loss: 0.0001435368467355147\n",
            "step: 60, loss: 6.0286674852250144e-05\n",
            "step: 70, loss: 9.210805001202971e-05\n",
            "step: 80, loss: 4.474955494515598e-05\n",
            "step: 90, loss: 0.0008260383037850261\n",
            "step: 100, loss: 0.00012061007873853669\n",
            "step: 110, loss: 3.842134901788086e-05\n",
            "step: 120, loss: 8.503460412612185e-05\n",
            "step: 130, loss: 4.120293306186795e-05\n",
            "step: 140, loss: 4.398930832394399e-05\n",
            "step: 150, loss: 0.00047737525892443955\n",
            "step: 160, loss: 0.00026376370806246996\n",
            "step: 170, loss: 6.29658970865421e-05\n",
            "step: 180, loss: 4.379880920168944e-05\n",
            "step: 190, loss: 0.0012511418899521232\n",
            "step: 200, loss: 5.220578168518841e-05\n",
            "step: 210, loss: 0.0002128887572325766\n",
            "step: 220, loss: 0.025775039568543434\n",
            "step: 230, loss: 9.158676402876154e-05\n",
            "step: 240, loss: 0.005341660231351852\n",
            "step: 250, loss: 5.963830335531384e-05\n",
            "step: 260, loss: 0.0055381907150149345\n",
            "step: 270, loss: 6.286869756877422e-05\n",
            "step: 280, loss: 0.00012285906996112317\n",
            "step: 290, loss: 6.81313467794098e-05\n",
            "step: 300, loss: 0.00041704505565576255\n",
            "step: 310, loss: 0.15517404675483704\n",
            "step: 320, loss: 7.846687367418781e-05\n",
            "step: 330, loss: 0.0009084987686946988\n",
            "step: 340, loss: 0.0010820202296599746\n",
            "step: 350, loss: 0.00013436190783977509\n",
            "step: 360, loss: 0.0028909833636134863\n",
            "step: 370, loss: 0.0015088807558640838\n",
            "step: 380, loss: 0.0070296465419232845\n",
            "step: 390, loss: 0.011647133156657219\n",
            "step: 400, loss: 0.005936469417065382\n",
            "step: 410, loss: 0.002117990516126156\n",
            "step: 420, loss: 0.04392467811703682\n",
            "step: 430, loss: 0.002883083885535598\n",
            "step: 440, loss: 0.0010379973100498319\n",
            "step: 450, loss: 0.0018686945550143719\n",
            "step: 460, loss: 0.019250454381108284\n",
            "step: 470, loss: 0.0022918167524039745\n",
            "step: 480, loss: 0.0001944709656527266\n",
            "step: 490, loss: 0.00010341098095523193\n",
            "step: 500, loss: 0.000850627722684294\n",
            "step: 510, loss: 4.927669942844659e-05\n",
            "step: 520, loss: 0.0002610509400255978\n",
            "step: 530, loss: 0.05955510586500168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9466292134831461, f1=0.9392523364485983, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008812992833554745\n",
            "step: 10, loss: 0.0006308113224804401\n",
            "step: 20, loss: 0.0035843069199472666\n",
            "step: 30, loss: 5.853292896063067e-05\n",
            "step: 40, loss: 0.00011043123959098011\n",
            "step: 50, loss: 2.2429287128034048e-05\n",
            "step: 60, loss: 2.299577317899093e-05\n",
            "step: 70, loss: 0.0005116612883284688\n",
            "step: 80, loss: 6.057493374100886e-05\n",
            "step: 90, loss: 0.0011630698572844267\n",
            "step: 100, loss: 0.0038907055277377367\n",
            "step: 110, loss: 3.985870353062637e-05\n",
            "step: 120, loss: 3.067931538680568e-05\n",
            "step: 130, loss: 3.877494600601494e-05\n",
            "step: 140, loss: 3.929268132196739e-05\n",
            "step: 150, loss: 2.5363700842717662e-05\n",
            "step: 160, loss: 6.730639870511368e-05\n",
            "step: 170, loss: 2.4306173145305365e-05\n",
            "step: 180, loss: 1.9751090803765692e-05\n",
            "step: 190, loss: 0.0005125731695443392\n",
            "step: 200, loss: 1.5921534213703126e-05\n",
            "step: 210, loss: 0.0005739190382882953\n",
            "step: 220, loss: 0.0005363432574085891\n",
            "step: 230, loss: 1.861865894170478e-05\n",
            "step: 240, loss: 6.764557474525645e-05\n",
            "step: 250, loss: 1.545227860333398e-05\n",
            "step: 260, loss: 1.8223514416604303e-05\n",
            "step: 270, loss: 0.004238460678607225\n",
            "step: 280, loss: 2.8721293347189203e-05\n",
            "step: 290, loss: 0.000376155658159405\n",
            "step: 300, loss: 0.0012169636320322752\n",
            "step: 310, loss: 1.8108163203578442e-05\n",
            "step: 320, loss: 0.0005206165369600058\n",
            "step: 330, loss: 0.0014170859940350056\n",
            "step: 340, loss: 8.304492803290486e-05\n",
            "step: 350, loss: 1.7631304217502475e-05\n",
            "step: 360, loss: 0.0007572183385491371\n",
            "step: 370, loss: 0.00037608976708725095\n",
            "step: 380, loss: 0.019075307995080948\n",
            "step: 390, loss: 0.0020156302489340305\n",
            "step: 400, loss: 0.0032607438042759895\n",
            "step: 410, loss: 0.00017646861670073122\n",
            "step: 420, loss: 2.9948070732643828e-05\n",
            "step: 430, loss: 0.00021012680372223258\n",
            "step: 440, loss: 0.00032252378878183663\n",
            "step: 450, loss: 0.001024215598590672\n",
            "step: 460, loss: 0.00018814006762113422\n",
            "step: 470, loss: 0.003924350254237652\n",
            "step: 480, loss: 0.0005458163213916123\n",
            "step: 490, loss: 6.0163332818774506e-05\n",
            "step: 500, loss: 0.00015391860506497324\n",
            "step: 510, loss: 0.0014517720555886626\n",
            "step: 520, loss: 0.0014538965187966824\n",
            "step: 530, loss: 4.7028577682795e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9503219871205152, f1=0.9441391941391942, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.7499458155944012e-05\n",
            "step: 10, loss: 0.00017219525761902332\n",
            "step: 20, loss: 4.1071809391723946e-05\n",
            "step: 30, loss: 2.31001504289452e-05\n",
            "step: 40, loss: 0.00022112017904873937\n",
            "step: 50, loss: 0.003380298614501953\n",
            "step: 60, loss: 3.179003397235647e-05\n",
            "step: 70, loss: 0.00011800362699432299\n",
            "step: 80, loss: 6.754929199814796e-05\n",
            "step: 90, loss: 3.296356226201169e-05\n",
            "step: 100, loss: 3.2161318813450634e-05\n",
            "step: 110, loss: 3.2020929211284965e-05\n",
            "step: 120, loss: 3.1019619200378656e-05\n",
            "step: 130, loss: 2.0998955733375624e-05\n",
            "step: 140, loss: 8.791017899056897e-05\n",
            "step: 150, loss: 0.003807740518823266\n",
            "step: 160, loss: 6.164106889627874e-05\n",
            "step: 170, loss: 0.0027608382515609264\n",
            "step: 180, loss: 3.573531284928322e-05\n",
            "step: 190, loss: 0.0002241883339593187\n",
            "step: 200, loss: 3.3222695492440835e-05\n",
            "step: 210, loss: 2.6098556190845557e-05\n",
            "step: 220, loss: 2.9509539672289975e-05\n",
            "step: 230, loss: 0.0013955894391983747\n",
            "step: 240, loss: 0.0027726064436137676\n",
            "step: 250, loss: 5.6116732594091445e-05\n",
            "step: 260, loss: 2.4172784833353944e-05\n",
            "step: 270, loss: 0.0003150624979753047\n",
            "step: 280, loss: 3.0851260817144066e-05\n",
            "step: 290, loss: 1.2133162272220943e-05\n",
            "step: 300, loss: 1.9628201698651537e-05\n",
            "step: 310, loss: 3.01513027807232e-05\n",
            "step: 320, loss: 2.0078914531040937e-05\n",
            "step: 330, loss: 3.042281423404347e-05\n",
            "step: 340, loss: 0.0012546611251309514\n",
            "step: 350, loss: 1.942330527526792e-05\n",
            "step: 360, loss: 0.0770670473575592\n",
            "step: 370, loss: 7.491639553336427e-05\n",
            "step: 380, loss: 0.00430266885086894\n",
            "step: 390, loss: 2.7003206923836842e-05\n",
            "step: 400, loss: 2.3763044737279415e-05\n",
            "step: 410, loss: 2.7256697649136186e-05\n",
            "step: 420, loss: 1.4971733435231727e-05\n",
            "step: 430, loss: 0.0001123886468121782\n",
            "step: 440, loss: 1.6696287275408395e-05\n",
            "step: 450, loss: 2.6214251192868687e-05\n",
            "step: 460, loss: 0.08442994952201843\n",
            "step: 470, loss: 0.00239218445494771\n",
            "step: 480, loss: 1.2524316844064742e-05\n",
            "step: 490, loss: 6.015750841470435e-05\n",
            "step: 500, loss: 0.001820391626097262\n",
            "step: 510, loss: 5.524485095520504e-05\n",
            "step: 520, loss: 1.3720072274736594e-05\n",
            "step: 530, loss: 9.538013546261936e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.947119924457035, f1=0.9450757575757576, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0037154420278966427\n",
            "step: 10, loss: 0.0008719497127458453\n",
            "step: 20, loss: 1.585835707373917e-05\n",
            "step: 30, loss: 0.00014942199049983174\n",
            "step: 40, loss: 0.0001473964366596192\n",
            "step: 50, loss: 0.0015951166860759258\n",
            "step: 60, loss: 3.562959318514913e-05\n",
            "step: 70, loss: 0.00024274663883261383\n",
            "step: 80, loss: 0.003663405077531934\n",
            "step: 90, loss: 1.959090877790004e-05\n",
            "step: 100, loss: 9.678499191068113e-05\n",
            "step: 110, loss: 9.918327123159543e-05\n",
            "step: 120, loss: 4.1952469473471865e-05\n",
            "step: 130, loss: 2.027597656706348e-05\n",
            "step: 140, loss: 0.00013527962437365204\n",
            "step: 150, loss: 0.00032609381014481187\n",
            "step: 160, loss: 0.000692447938490659\n",
            "step: 170, loss: 0.0010887796524912119\n",
            "step: 180, loss: 2.832934660546016e-05\n",
            "step: 190, loss: 0.0009356682421639562\n",
            "step: 200, loss: 2.4928624043241143e-05\n",
            "step: 210, loss: 0.00021719779761042446\n",
            "step: 220, loss: 4.214257933199406e-05\n",
            "step: 230, loss: 3.9947462937561795e-05\n",
            "step: 240, loss: 0.0012061366578564048\n",
            "step: 250, loss: 0.0021969410590827465\n",
            "step: 260, loss: 8.351437281817198e-05\n",
            "step: 270, loss: 0.008815444074571133\n",
            "step: 280, loss: 7.018545875325799e-05\n",
            "step: 290, loss: 0.0002889694878831506\n",
            "step: 300, loss: 2.3170710846898146e-05\n",
            "step: 310, loss: 0.0008389412541873753\n",
            "step: 320, loss: 2.5604431357351132e-05\n",
            "step: 330, loss: 2.6195135433226824e-05\n",
            "step: 340, loss: 4.87511460960377e-05\n",
            "step: 350, loss: 1.6227100786636584e-05\n",
            "step: 360, loss: 0.0022247692104429007\n",
            "step: 370, loss: 0.014658615924417973\n",
            "step: 380, loss: 0.0002517457469366491\n",
            "step: 390, loss: 0.00012442884326446801\n",
            "step: 400, loss: 0.0017855444457381964\n",
            "step: 410, loss: 1.6509999113623053e-05\n",
            "step: 420, loss: 9.111255349125713e-05\n",
            "step: 430, loss: 0.0010732341324910522\n",
            "step: 440, loss: 0.0001860000629676506\n",
            "step: 450, loss: 3.193964221281931e-05\n",
            "step: 460, loss: 0.03624311834573746\n",
            "step: 470, loss: 1.293408695346443e-05\n",
            "step: 480, loss: 2.148683779523708e-05\n",
            "step: 490, loss: 6.667490379186347e-05\n",
            "step: 500, loss: 0.0018363037379458547\n",
            "step: 510, loss: 0.009300176054239273\n",
            "step: 520, loss: 1.8405884475214407e-05\n",
            "step: 530, loss: 0.0008268359815701842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9488503050211168, f1=0.9487058823529413, best_f1=0.9477154969382948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 1.4401758562598843e-05\n",
            "step: 10, loss: 0.0018461951985955238\n",
            "step: 20, loss: 0.0015286379493772984\n",
            "step: 30, loss: 0.015216663479804993\n",
            "step: 40, loss: 4.6451070375042036e-05\n",
            "step: 50, loss: 0.0009819279657676816\n",
            "step: 60, loss: 9.32088223635219e-05\n",
            "step: 70, loss: 5.7386434491490945e-05\n",
            "step: 80, loss: 2.6711819373304024e-05\n",
            "step: 90, loss: 1.8789460227708332e-05\n",
            "step: 100, loss: 1.2945248272444587e-05\n",
            "step: 110, loss: 0.016038939356803894\n",
            "step: 120, loss: 2.1318568542483263e-05\n",
            "step: 130, loss: 1.3861646039003972e-05\n",
            "step: 140, loss: 0.0003929108497686684\n",
            "step: 150, loss: 1.2058648280799389e-05\n",
            "step: 160, loss: 2.7672474971041083e-05\n",
            "step: 170, loss: 8.356612670468166e-05\n",
            "step: 180, loss: 2.052950185316149e-05\n",
            "step: 190, loss: 0.0010083713568747044\n",
            "step: 200, loss: 0.0007061536889523268\n",
            "step: 210, loss: 6.961877079447731e-05\n",
            "step: 220, loss: 1.5582601918140426e-05\n",
            "step: 230, loss: 0.00024820491671562195\n",
            "step: 240, loss: 1.2475841685954947e-05\n",
            "step: 250, loss: 1.7694463167572394e-05\n",
            "step: 260, loss: 0.00015099119627848268\n",
            "step: 270, loss: 1.1958052709815092e-05\n",
            "step: 280, loss: 1.0050760465674102e-05\n",
            "step: 290, loss: 8.792382868705317e-05\n",
            "step: 300, loss: 1.9236744265072048e-05\n",
            "step: 310, loss: 0.03371824324131012\n",
            "step: 320, loss: 2.024649984377902e-05\n",
            "step: 330, loss: 1.4956879567762371e-05\n",
            "step: 340, loss: 8.009768498595804e-05\n",
            "step: 350, loss: 0.00038502892130054533\n",
            "step: 360, loss: 2.7527672500582412e-05\n",
            "step: 370, loss: 4.009467374999076e-05\n",
            "step: 380, loss: 1.5407562386826612e-05\n",
            "step: 390, loss: 4.3644013203447685e-05\n",
            "step: 400, loss: 0.002609915565699339\n",
            "step: 410, loss: 0.0007372673135250807\n",
            "step: 420, loss: 3.6043689760845155e-05\n",
            "step: 430, loss: 1.0281720278726425e-05\n",
            "step: 440, loss: 1.909530874399934e-05\n",
            "step: 450, loss: 0.0028191180899739265\n",
            "step: 460, loss: 0.00036892638308927417\n",
            "step: 470, loss: 1.4375424143509008e-05\n",
            "step: 480, loss: 0.003328696358948946\n",
            "step: 490, loss: 6.681108789052814e-05\n",
            "step: 500, loss: 0.0011639485601335764\n",
            "step: 510, loss: 1.5891635484877042e-05\n",
            "step: 520, loss: 4.1191797208739445e-05\n",
            "step: 530, loss: 1.059833994077053e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9493908153701968, f1=0.949343339587242, best_f1=0.9477154969382948\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:37, 152.32it/s]\n",
            "load_f1 = 0.9534776600644865\n",
            "real_f1 = 0.9512308406874129\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 133.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "oqkZ1fXggw8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f6b2908-ce3d-4518-aa41-1c86eca81f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5041724443435669\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5760746002197266\n",
            "step: 20, loss: 0.46412545442581177\n",
            "step: 30, loss: 0.3226455748081207\n",
            "step: 40, loss: 0.3146035969257355\n",
            "step: 50, loss: 0.4929715096950531\n",
            "step: 60, loss: 0.4681403934955597\n",
            "step: 70, loss: 0.35566043853759766\n",
            "step: 80, loss: 0.4526368975639343\n",
            "step: 90, loss: 0.2777218222618103\n",
            "step: 100, loss: 0.2480766773223877\n",
            "step: 110, loss: 0.24330869317054749\n",
            "step: 120, loss: 0.41404128074645996\n",
            "step: 130, loss: 0.2567008137702942\n",
            "step: 140, loss: 0.4382891058921814\n",
            "step: 150, loss: 0.3898755609989166\n",
            "step: 160, loss: 0.4944058060646057\n",
            "step: 170, loss: 0.23716744780540466\n",
            "step: 180, loss: 0.3988436460494995\n",
            "step: 190, loss: 0.6158481240272522\n",
            "step: 200, loss: 0.409505158662796\n",
            "step: 210, loss: 0.511177659034729\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3803889751434326\n",
            "step: 10, loss: 0.2133331596851349\n",
            "step: 20, loss: 0.48640596866607666\n",
            "step: 30, loss: 0.5276857614517212\n",
            "step: 40, loss: 0.4443977475166321\n",
            "step: 50, loss: 0.21547985076904297\n",
            "step: 60, loss: 0.3103888928890228\n",
            "step: 70, loss: 0.430831640958786\n",
            "step: 80, loss: 0.31116199493408203\n",
            "step: 90, loss: 0.38683223724365234\n",
            "step: 100, loss: 0.47620055079460144\n",
            "step: 110, loss: 0.3898858428001404\n",
            "step: 120, loss: 0.23715466260910034\n",
            "step: 130, loss: 0.1761653870344162\n",
            "step: 140, loss: 0.2526911497116089\n",
            "step: 150, loss: 0.4479692578315735\n",
            "step: 160, loss: 0.16844049096107483\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 0.6233566999435425\n",
            "step: 180, loss: 0.3290555477142334\n",
            "step: 190, loss: 0.3259986639022827\n",
            "step: 200, loss: 0.15076567232608795\n",
            "step: 210, loss: 0.30630677938461304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2611106336116791\n",
            "step: 10, loss: 0.2384774535894394\n",
            "step: 20, loss: 0.48235952854156494\n",
            "step: 30, loss: 0.25398707389831543\n",
            "step: 40, loss: 0.43871983885765076\n",
            "step: 50, loss: 0.47228479385375977\n",
            "step: 60, loss: 0.5022034645080566\n",
            "step: 70, loss: 0.20385396480560303\n",
            "step: 80, loss: 0.45383796095848083\n",
            "step: 90, loss: 0.24406981468200684\n",
            "step: 100, loss: 0.3856942653656006\n",
            "step: 110, loss: 0.23358485102653503\n",
            "step: 120, loss: 0.23806773126125336\n",
            "step: 130, loss: 0.16249431669712067\n",
            "step: 140, loss: 0.37773597240448\n",
            "step: 150, loss: 0.3169133961200714\n",
            "step: 160, loss: 0.21113988757133484\n",
            "step: 170, loss: 0.38343143463134766\n",
            "step: 180, loss: 0.2540951669216156\n",
            "step: 190, loss: 0.16343431174755096\n",
            "step: 200, loss: 0.2340717315673828\n",
            "step: 210, loss: 0.2419489026069641\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3118242919445038\n",
            "step: 10, loss: 0.30330562591552734\n",
            "step: 20, loss: 0.30985352396965027\n",
            "step: 30, loss: 0.237383633852005\n",
            "step: 40, loss: 0.23581571877002716\n",
            "step: 50, loss: 0.23299133777618408\n",
            "step: 60, loss: 0.4967680275440216\n",
            "step: 70, loss: 0.25881603360176086\n",
            "step: 80, loss: 0.23811142146587372\n",
            "step: 90, loss: 0.4474259912967682\n",
            "step: 100, loss: 0.3627628982067108\n",
            "step: 110, loss: 0.5860417485237122\n",
            "step: 120, loss: 0.3857390284538269\n",
            "step: 130, loss: 0.6821143627166748\n",
            "step: 140, loss: 0.49662959575653076\n",
            "step: 150, loss: 0.3863489329814911\n",
            "step: 160, loss: 0.3254215121269226\n",
            "step: 170, loss: 0.18035130202770233\n",
            "step: 180, loss: 0.08816344290971756\n",
            "step: 190, loss: 0.16446711122989655\n",
            "step: 200, loss: 0.317981094121933\n",
            "step: 210, loss: 0.3802613914012909\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38856327533721924\n",
            "step: 10, loss: 0.32288745045661926\n",
            "step: 20, loss: 0.32495057582855225\n",
            "step: 30, loss: 0.23966842889785767\n",
            "step: 40, loss: 0.4384290277957916\n",
            "step: 50, loss: 0.38378390669822693\n",
            "step: 60, loss: 0.38105857372283936\n",
            "step: 70, loss: 0.24496370553970337\n",
            "step: 80, loss: 0.45227083563804626\n",
            "step: 90, loss: 0.4331541359424591\n",
            "step: 100, loss: 0.2434973120689392\n",
            "step: 110, loss: 0.15915241837501526\n",
            "step: 120, loss: 0.22988271713256836\n",
            "step: 130, loss: 0.3169121742248535\n",
            "step: 140, loss: 0.5308547616004944\n",
            "step: 150, loss: 0.31218358874320984\n",
            "step: 160, loss: 0.24974587559700012\n",
            "step: 170, loss: 0.3843604028224945\n",
            "step: 180, loss: 0.24003081023693085\n",
            "step: 190, loss: 0.5310019254684448\n",
            "step: 200, loss: 0.38769400119781494\n",
            "step: 210, loss: 0.2459791749715805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16266445815563202\n",
            "step: 10, loss: 0.38178667426109314\n",
            "step: 20, loss: 0.30915138125419617\n",
            "step: 30, loss: 0.2449016124010086\n",
            "step: 40, loss: 0.31889083981513977\n",
            "step: 50, loss: 0.6135328412055969\n",
            "step: 60, loss: 0.3136651813983917\n",
            "step: 70, loss: 0.4336855113506317\n",
            "step: 80, loss: 0.311686247587204\n",
            "step: 90, loss: 0.31193703413009644\n",
            "step: 100, loss: 0.3157855272293091\n",
            "step: 110, loss: 0.3762960135936737\n",
            "step: 120, loss: 0.24778659641742706\n",
            "step: 130, loss: 0.2360650897026062\n",
            "step: 140, loss: 0.37968653440475464\n",
            "step: 150, loss: 0.18871928751468658\n",
            "step: 160, loss: 0.16579945385456085\n",
            "step: 170, loss: 0.4601098895072937\n",
            "step: 180, loss: 0.3770511746406555\n",
            "step: 190, loss: 0.45443665981292725\n",
            "step: 200, loss: 0.30551791191101074\n",
            "step: 210, loss: 0.3796556890010834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4500158131122589\n",
            "step: 10, loss: 0.31283923983573914\n",
            "step: 20, loss: 0.4407801330089569\n",
            "step: 30, loss: 0.24646779894828796\n",
            "step: 40, loss: 0.24704232811927795\n",
            "step: 50, loss: 0.37914079427719116\n",
            "step: 60, loss: 0.3170207440853119\n",
            "step: 70, loss: 0.2524263262748718\n",
            "step: 80, loss: 0.5110667943954468\n",
            "step: 90, loss: 0.37351059913635254\n",
            "step: 100, loss: 0.4387146532535553\n",
            "step: 110, loss: 0.30531415343284607\n",
            "step: 120, loss: 0.31798070669174194\n",
            "step: 130, loss: 0.47109851241111755\n",
            "step: 140, loss: 0.3137548863887787\n",
            "step: 150, loss: 0.3130759596824646\n",
            "step: 160, loss: 0.5521305203437805\n",
            "step: 170, loss: 0.618061363697052\n",
            "step: 180, loss: 0.23477229475975037\n",
            "step: 190, loss: 0.23835596442222595\n",
            "step: 200, loss: 0.3125579357147217\n",
            "step: 210, loss: 0.31427887082099915\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6080752611160278\n",
            "step: 10, loss: 0.31781715154647827\n",
            "step: 20, loss: 0.24929334223270416\n",
            "step: 30, loss: 0.24800190329551697\n",
            "step: 40, loss: 0.2410147488117218\n",
            "step: 50, loss: 0.16584770381450653\n",
            "step: 60, loss: 0.16983480751514435\n",
            "step: 70, loss: 0.45943328738212585\n",
            "step: 80, loss: 0.30769720673561096\n",
            "step: 90, loss: 0.3851092457771301\n",
            "step: 100, loss: 0.6102311611175537\n",
            "step: 110, loss: 0.37461456656455994\n",
            "step: 120, loss: 0.30152660608291626\n",
            "step: 130, loss: 0.1708136647939682\n",
            "step: 140, loss: 0.3168551027774811\n",
            "step: 150, loss: 0.4711753726005554\n",
            "step: 160, loss: 0.44375962018966675\n",
            "step: 170, loss: 0.5304307341575623\n",
            "step: 180, loss: 0.3129090666770935\n",
            "step: 190, loss: 0.2296237349510193\n",
            "step: 200, loss: 0.45537886023521423\n",
            "step: 210, loss: 0.38395410776138306\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5961818099021912\n",
            "step: 10, loss: 0.28420498967170715\n",
            "step: 20, loss: 0.38480904698371887\n",
            "step: 30, loss: 0.17202965915203094\n",
            "step: 40, loss: 0.16779232025146484\n",
            "step: 50, loss: 0.45096951723098755\n",
            "step: 60, loss: 0.17190766334533691\n",
            "step: 70, loss: 0.38450485467910767\n",
            "step: 80, loss: 0.30622780323028564\n",
            "step: 90, loss: 0.32051917910575867\n",
            "step: 100, loss: 0.46216824650764465\n",
            "step: 110, loss: 0.5290734767913818\n",
            "step: 120, loss: 0.38639986515045166\n",
            "step: 130, loss: 0.24259072542190552\n",
            "step: 140, loss: 0.5970783233642578\n",
            "step: 150, loss: 0.11306048929691315\n",
            "step: 160, loss: 0.3782874047756195\n",
            "step: 170, loss: 0.3169347941875458\n",
            "step: 180, loss: 0.4573005139827728\n",
            "step: 190, loss: 0.31084802746772766\n",
            "step: 200, loss: 0.31131434440612793\n",
            "step: 210, loss: 0.30659669637680054\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.306844562292099\n",
            "step: 10, loss: 0.3826238214969635\n",
            "step: 20, loss: 0.25994354486465454\n",
            "step: 30, loss: 0.17355534434318542\n",
            "step: 40, loss: 0.3121204972267151\n",
            "step: 50, loss: 0.5112889409065247\n",
            "step: 60, loss: 0.24636225402355194\n",
            "step: 70, loss: 0.3845391273498535\n",
            "step: 80, loss: 0.1636137068271637\n",
            "step: 90, loss: 0.38423317670822144\n",
            "step: 100, loss: 0.46097037196159363\n",
            "step: 110, loss: 0.17404741048812866\n",
            "step: 120, loss: 0.4508204162120819\n",
            "step: 130, loss: 0.24040956795215607\n",
            "step: 140, loss: 0.3726884722709656\n",
            "step: 150, loss: 0.24766044318675995\n",
            "step: 160, loss: 0.3831023573875427\n",
            "step: 170, loss: 0.24886077642440796\n",
            "step: 180, loss: 0.3160536289215088\n",
            "step: 190, loss: 0.24149826169013977\n",
            "step: 200, loss: 0.6418158411979675\n",
            "step: 210, loss: 0.2511022984981537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3166930079460144\n",
            "step: 10, loss: 0.31459182500839233\n",
            "step: 20, loss: 0.3901274800300598\n",
            "step: 30, loss: 0.16756044328212738\n",
            "step: 40, loss: 0.4468727707862854\n",
            "step: 50, loss: 0.4329097270965576\n",
            "step: 60, loss: 0.44220489263534546\n",
            "step: 70, loss: 0.17682139575481415\n",
            "step: 80, loss: 0.5222845673561096\n",
            "step: 90, loss: 0.3908003568649292\n",
            "step: 100, loss: 0.5137608647346497\n",
            "step: 110, loss: 0.44887274503707886\n",
            "step: 120, loss: 0.44418230652809143\n",
            "step: 130, loss: 0.24418407678604126\n",
            "step: 140, loss: 0.31310388445854187\n",
            "step: 150, loss: 0.31894001364707947\n",
            "step: 160, loss: 0.1715497076511383\n",
            "step: 170, loss: 0.3132527768611908\n",
            "step: 180, loss: 0.23998220264911652\n",
            "step: 190, loss: 0.4468246400356293\n",
            "step: 200, loss: 0.1798144280910492\n",
            "step: 210, loss: 0.3758852779865265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24808506667613983\n",
            "step: 10, loss: 0.30906951427459717\n",
            "step: 20, loss: 0.46283408999443054\n",
            "step: 30, loss: 0.3114384710788727\n",
            "step: 40, loss: 0.17242968082427979\n",
            "step: 50, loss: 0.45090898871421814\n",
            "step: 60, loss: 0.11018909513950348\n",
            "step: 70, loss: 0.44754862785339355\n",
            "step: 80, loss: 0.31638020277023315\n",
            "step: 90, loss: 0.390511155128479\n",
            "step: 100, loss: 0.12397093325853348\n",
            "step: 110, loss: 0.2420874536037445\n",
            "step: 120, loss: 0.2459345906972885\n",
            "step: 130, loss: 0.4610857665538788\n",
            "step: 140, loss: 0.3796297609806061\n",
            "step: 150, loss: 0.0994202271103859\n",
            "step: 160, loss: 0.24181769788265228\n",
            "step: 170, loss: 0.2402547299861908\n",
            "step: 180, loss: 0.3151618242263794\n",
            "step: 190, loss: 0.3818625509738922\n",
            "step: 200, loss: 0.17403188347816467\n",
            "step: 210, loss: 0.3139626979827881\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3114413321018219\n",
            "step: 10, loss: 0.2374451905488968\n",
            "step: 20, loss: 0.31465575098991394\n",
            "step: 30, loss: 0.24646402895450592\n",
            "step: 40, loss: 0.25011345744132996\n",
            "step: 50, loss: 0.24861694872379303\n",
            "step: 60, loss: 0.5764137506484985\n",
            "step: 70, loss: 0.4433172643184662\n",
            "step: 80, loss: 0.38308677077293396\n",
            "step: 90, loss: 0.24670444428920746\n",
            "step: 100, loss: 0.3796460032463074\n",
            "step: 110, loss: 0.24519012868404388\n",
            "step: 120, loss: 0.30535081028938293\n",
            "step: 130, loss: 0.31033751368522644\n",
            "step: 140, loss: 0.16543924808502197\n",
            "step: 150, loss: 0.31239476799964905\n",
            "step: 160, loss: 0.5401768684387207\n",
            "step: 170, loss: 0.2417943775653839\n",
            "step: 180, loss: 0.2387385219335556\n",
            "step: 190, loss: 0.24519082903862\n",
            "step: 200, loss: 0.38060498237609863\n",
            "step: 210, loss: 0.37421509623527527\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24273380637168884\n",
            "step: 10, loss: 0.3824155330657959\n",
            "step: 20, loss: 0.5109807252883911\n",
            "step: 30, loss: 0.24638235569000244\n",
            "step: 40, loss: 0.32907241582870483\n",
            "step: 50, loss: 0.25099894404411316\n",
            "step: 60, loss: 0.379548043012619\n",
            "step: 70, loss: 0.24632269144058228\n",
            "step: 80, loss: 0.3163859248161316\n",
            "step: 90, loss: 0.17871858179569244\n",
            "step: 100, loss: 0.2423788458108902\n",
            "step: 110, loss: 0.44332605600357056\n",
            "step: 120, loss: 0.24852672219276428\n",
            "step: 130, loss: 0.3109099864959717\n",
            "step: 140, loss: 0.31016817688941956\n",
            "step: 150, loss: 0.31566542387008667\n",
            "step: 160, loss: 0.1778554618358612\n",
            "step: 170, loss: 0.45246753096580505\n",
            "step: 180, loss: 0.2411431223154068\n",
            "step: 190, loss: 0.3858364224433899\n",
            "step: 200, loss: 0.17004847526550293\n",
            "step: 210, loss: 0.4496334493160248\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2425592690706253\n",
            "step: 10, loss: 0.2450638860464096\n",
            "step: 20, loss: 0.5192540884017944\n",
            "step: 30, loss: 0.24554231762886047\n",
            "step: 40, loss: 0.38560301065444946\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.1735723465681076\n",
            "step: 60, loss: 0.30145877599716187\n",
            "step: 70, loss: 0.45520392060279846\n",
            "step: 80, loss: 0.2399044781923294\n",
            "step: 90, loss: 0.452409565448761\n",
            "step: 100, loss: 0.3116222321987152\n",
            "step: 110, loss: 0.3115505576133728\n",
            "step: 120, loss: 0.45081785321235657\n",
            "step: 130, loss: 0.17127947509288788\n",
            "step: 140, loss: 0.3101363778114319\n",
            "step: 150, loss: 0.5316551327705383\n",
            "step: 160, loss: 0.37952762842178345\n",
            "step: 170, loss: 0.3787423372268677\n",
            "step: 180, loss: 0.308510959148407\n",
            "step: 190, loss: 0.31464287638664246\n",
            "step: 200, loss: 0.31146353483200073\n",
            "step: 210, loss: 0.4577195644378662\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:09, 230.31it/s]\n",
            "load_f1 = 0.18519984170953702\n",
            "real_f1 = 0.18519984170953702\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:34, 129.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "nXvTChDGgw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884760e2-5918-4b76-ef14-4fc5831aa6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.46847236156463623\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4115792512893677\n",
            "step: 20, loss: 0.25278806686401367\n",
            "step: 30, loss: 0.3987118601799011\n",
            "step: 40, loss: 0.24234120547771454\n",
            "step: 50, loss: 0.3330713212490082\n",
            "step: 60, loss: 0.49866411089897156\n",
            "step: 70, loss: 0.4431764483451843\n",
            "step: 80, loss: 0.15188930928707123\n",
            "step: 90, loss: 0.31428030133247375\n",
            "step: 100, loss: 0.4199192225933075\n",
            "step: 110, loss: 0.25078678131103516\n",
            "step: 120, loss: 0.31907281279563904\n",
            "step: 130, loss: 0.3459508717060089\n",
            "step: 140, loss: 0.17237292230129242\n",
            "step: 150, loss: 0.32132771611213684\n",
            "step: 160, loss: 0.22802986204624176\n",
            "step: 170, loss: 0.365928590297699\n",
            "step: 180, loss: 0.17943677306175232\n",
            "step: 190, loss: 0.15548226237297058\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.39442339539527893\n",
            "step: 10, loss: 0.34057876467704773\n",
            "step: 20, loss: 0.6717296242713928\n",
            "step: 30, loss: 0.25412312150001526\n",
            "step: 40, loss: 0.5475385785102844\n",
            "step: 50, loss: 0.33722439408302307\n",
            "step: 60, loss: 0.4413849413394928\n",
            "step: 70, loss: 0.3136940002441406\n",
            "step: 80, loss: 0.1808326542377472\n",
            "step: 90, loss: 0.2864428758621216\n",
            "step: 100, loss: 0.15660053491592407\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 110, loss: 0.20496827363967896\n",
            "step: 120, loss: 0.17856065928936005\n",
            "step: 130, loss: 0.1341363489627838\n",
            "step: 140, loss: 0.19769425690174103\n",
            "step: 150, loss: 0.29242628812789917\n",
            "step: 160, loss: 0.2163134664297104\n",
            "step: 170, loss: 0.04637683928012848\n",
            "step: 180, loss: 0.036936867982149124\n",
            "step: 190, loss: 0.09708469361066818\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6975609756097562, f1=0.7004830917874396, best_f1=0.7004830917874396\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18086282908916473\n",
            "step: 10, loss: 0.18451061844825745\n",
            "step: 20, loss: 0.20310895144939423\n",
            "step: 30, loss: 0.13614994287490845\n",
            "step: 40, loss: 0.1388663947582245\n",
            "step: 50, loss: 0.07965706288814545\n",
            "step: 60, loss: 0.12410826981067657\n",
            "step: 70, loss: 0.1578894853591919\n",
            "step: 80, loss: 0.05592035502195358\n",
            "step: 90, loss: 0.1582416594028473\n",
            "step: 100, loss: 0.16150324046611786\n",
            "step: 110, loss: 0.30355599522590637\n",
            "step: 120, loss: 0.20854546129703522\n",
            "step: 130, loss: 0.03865088149905205\n",
            "step: 140, loss: 0.09753090143203735\n",
            "step: 150, loss: 0.1500217616558075\n",
            "step: 160, loss: 0.11065662652254105\n",
            "step: 170, loss: 0.3195893168449402\n",
            "step: 180, loss: 0.13831143081188202\n",
            "step: 190, loss: 0.09389946609735489\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8241758241758241, f1=0.8176795580110496, best_f1=0.8176795580110496\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01914484240114689\n",
            "step: 10, loss: 0.11206596344709396\n",
            "step: 20, loss: 0.03542495518922806\n",
            "step: 30, loss: 0.013778485357761383\n",
            "step: 40, loss: 0.040788255631923676\n",
            "step: 50, loss: 0.03343485668301582\n",
            "step: 60, loss: 0.151172935962677\n",
            "step: 70, loss: 0.11115764081478119\n",
            "step: 80, loss: 0.023004934191703796\n",
            "step: 90, loss: 0.008660678751766682\n",
            "step: 100, loss: 0.07500115782022476\n",
            "step: 110, loss: 0.09339321404695511\n",
            "step: 120, loss: 0.029536087065935135\n",
            "step: 130, loss: 0.03077978640794754\n",
            "step: 140, loss: 0.22203421592712402\n",
            "step: 150, loss: 0.013278120197355747\n",
            "step: 160, loss: 0.021513523533940315\n",
            "step: 170, loss: 0.037084586918354034\n",
            "step: 180, loss: 0.054670657962560654\n",
            "step: 190, loss: 0.015557521022856236\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8262108262108262, f1=0.8158640226628895, best_f1=0.8158640226628895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2945418357849121\n",
            "step: 10, loss: 0.1293381005525589\n",
            "step: 20, loss: 0.03902410343289375\n",
            "step: 30, loss: 0.010798677802085876\n",
            "step: 40, loss: 0.03664006292819977\n",
            "step: 50, loss: 0.00929232593625784\n",
            "step: 60, loss: 0.0028067012317478657\n",
            "step: 70, loss: 0.017530910670757294\n",
            "step: 80, loss: 0.017480066046118736\n",
            "step: 90, loss: 0.20033012330532074\n",
            "step: 100, loss: 0.12418468296527863\n",
            "step: 110, loss: 0.30116522312164307\n",
            "step: 120, loss: 0.0363299697637558\n",
            "step: 130, loss: 0.32366645336151123\n",
            "step: 140, loss: 0.03341720253229141\n",
            "step: 150, loss: 0.06713426113128662\n",
            "step: 160, loss: 0.10145779699087143\n",
            "step: 170, loss: 0.01770769990980625\n",
            "step: 180, loss: 0.01584768109023571\n",
            "step: 190, loss: 0.1548897922039032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8410256410256409, f1=0.7958656330749353, best_f1=0.7958656330749353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026647381484508514\n",
            "step: 10, loss: 0.02006879262626171\n",
            "step: 20, loss: 0.023508938029408455\n",
            "step: 30, loss: 0.03958183154463768\n",
            "step: 40, loss: 0.011271040886640549\n",
            "step: 50, loss: 0.009938502684235573\n",
            "step: 60, loss: 0.011587628163397312\n",
            "step: 70, loss: 0.007776594255119562\n",
            "step: 80, loss: 0.006697839591652155\n",
            "step: 90, loss: 0.006057559046894312\n",
            "step: 100, loss: 0.030241243541240692\n",
            "step: 110, loss: 0.010850438848137856\n",
            "step: 120, loss: 0.018137061968445778\n",
            "step: 130, loss: 0.15069922804832458\n",
            "step: 140, loss: 0.03271099925041199\n",
            "step: 150, loss: 0.10286326706409454\n",
            "step: 160, loss: 0.15831461548805237\n",
            "step: 170, loss: 0.30506443977355957\n",
            "step: 180, loss: 0.04873563349246979\n",
            "step: 190, loss: 0.011439301073551178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8365650969529086, f1=0.8328767123287673, best_f1=0.7958656330749353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014616257511079311\n",
            "step: 10, loss: 0.002826160052791238\n",
            "step: 20, loss: 0.0013083535013720393\n",
            "step: 30, loss: 0.07986722886562347\n",
            "step: 40, loss: 0.007554213982075453\n",
            "step: 50, loss: 0.0028622036334127188\n",
            "step: 60, loss: 0.05793379992246628\n",
            "step: 70, loss: 0.0034682455006986856\n",
            "step: 80, loss: 0.007118529174476862\n",
            "step: 90, loss: 0.004257110878825188\n",
            "step: 100, loss: 0.2275584638118744\n",
            "step: 110, loss: 0.08180779963731766\n",
            "step: 120, loss: 0.03298849239945412\n",
            "step: 130, loss: 0.017613181844353676\n",
            "step: 140, loss: 0.03495578095316887\n",
            "step: 150, loss: 0.017190175130963326\n",
            "step: 160, loss: 0.2438485026359558\n",
            "step: 170, loss: 0.0700046494603157\n",
            "step: 180, loss: 0.011344973929226398\n",
            "step: 190, loss: 0.05285221338272095\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8191489361702128, f1=0.8333333333333334, best_f1=0.7958656330749353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012341393157839775\n",
            "step: 10, loss: 0.0025229353923350573\n",
            "step: 20, loss: 0.0011171952355653048\n",
            "step: 30, loss: 0.002647532382979989\n",
            "step: 40, loss: 0.07916822284460068\n",
            "step: 50, loss: 0.03030943125486374\n",
            "step: 60, loss: 0.009069283492863178\n",
            "step: 70, loss: 0.001902374206110835\n",
            "step: 80, loss: 0.01921258494257927\n",
            "step: 90, loss: 0.002786843106150627\n",
            "step: 100, loss: 0.0009462088928557932\n",
            "step: 110, loss: 0.05603628233075142\n",
            "step: 120, loss: 0.0017545379232615232\n",
            "step: 130, loss: 0.0025458973832428455\n",
            "step: 140, loss: 0.0016760111320763826\n",
            "step: 150, loss: 0.00809592753648758\n",
            "step: 160, loss: 0.00162902707234025\n",
            "step: 170, loss: 0.0010482539655640721\n",
            "step: 180, loss: 0.18499746918678284\n",
            "step: 190, loss: 0.00129341846331954\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8168316831683167, f1=0.8418367346938775, best_f1=0.7958656330749353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0023974638897925615\n",
            "step: 10, loss: 0.0006520007736980915\n",
            "step: 20, loss: 0.0033706151880323887\n",
            "step: 30, loss: 0.006080552004277706\n",
            "step: 40, loss: 0.020642917603254318\n",
            "step: 50, loss: 0.02332977205514908\n",
            "step: 60, loss: 0.0021090267691761255\n",
            "step: 70, loss: 0.0008931256597861648\n",
            "step: 80, loss: 0.022474534809589386\n",
            "step: 90, loss: 0.005330640822649002\n",
            "step: 100, loss: 0.2595476508140564\n",
            "step: 110, loss: 0.0019356028642505407\n",
            "step: 120, loss: 0.039438147097826004\n",
            "step: 130, loss: 0.0027646494563668966\n",
            "step: 140, loss: 0.010507187806069851\n",
            "step: 150, loss: 0.003366640768945217\n",
            "step: 160, loss: 0.0012045946205034852\n",
            "step: 170, loss: 0.0011147403856739402\n",
            "step: 180, loss: 0.0038143189158290625\n",
            "step: 190, loss: 0.0006192188593558967\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8232323232323232, f1=0.835509138381201, best_f1=0.7958656330749353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010528888087719679\n",
            "step: 10, loss: 0.00039832296897657216\n",
            "step: 20, loss: 0.14058014750480652\n",
            "step: 30, loss: 0.11279873549938202\n",
            "step: 40, loss: 0.005860275123268366\n",
            "step: 50, loss: 0.0009736421634443104\n",
            "step: 60, loss: 0.0007933145971037447\n",
            "step: 70, loss: 0.001491026021540165\n",
            "step: 80, loss: 0.001766063505783677\n",
            "step: 90, loss: 0.001974229235202074\n",
            "step: 100, loss: 0.0016441134503111243\n",
            "step: 110, loss: 0.00393324252218008\n",
            "step: 120, loss: 0.0013563059037551284\n",
            "step: 130, loss: 0.0007615129579789937\n",
            "step: 140, loss: 0.001618041074834764\n",
            "step: 150, loss: 0.007270730100572109\n",
            "step: 160, loss: 0.001909139333292842\n",
            "step: 170, loss: 0.0007197754457592964\n",
            "step: 180, loss: 0.002033962169662118\n",
            "step: 190, loss: 0.01179587934166193\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8244680851063829, f1=0.8364611260053619, best_f1=0.7958656330749353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005586084444075823\n",
            "step: 10, loss: 0.0004581343091558665\n",
            "step: 20, loss: 0.0024623223580420017\n",
            "step: 30, loss: 0.0019119747448712587\n",
            "step: 40, loss: 0.0012783717829734087\n",
            "step: 50, loss: 0.000694502901751548\n",
            "step: 60, loss: 0.0018231788417324424\n",
            "step: 70, loss: 0.0007630238542333245\n",
            "step: 80, loss: 0.053856294602155685\n",
            "step: 90, loss: 0.00028822096646763384\n",
            "step: 100, loss: 0.0008112555951811373\n",
            "step: 110, loss: 0.0900740772485733\n",
            "step: 120, loss: 0.000719624396879226\n",
            "step: 130, loss: 0.0015355248469859362\n",
            "step: 140, loss: 0.012341001071035862\n",
            "step: 150, loss: 0.0032894352916628122\n",
            "step: 160, loss: 0.003829773049801588\n",
            "step: 170, loss: 0.000639323377981782\n",
            "step: 180, loss: 0.002005456481128931\n",
            "step: 190, loss: 0.0003895942645613104\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.8431876606683805, f1=0.84375, best_f1=0.84375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003988250973634422\n",
            "step: 10, loss: 0.0033305136021226645\n",
            "step: 20, loss: 0.0009184475056827068\n",
            "step: 30, loss: 0.001074274186976254\n",
            "step: 40, loss: 0.0030125800985842943\n",
            "step: 50, loss: 0.0014122631400823593\n",
            "step: 60, loss: 0.11075753718614578\n",
            "step: 70, loss: 0.00660345796495676\n",
            "step: 80, loss: 0.0013840642059221864\n",
            "step: 90, loss: 0.0008655470446683466\n",
            "step: 100, loss: 0.0009214226738549769\n",
            "step: 110, loss: 0.006240652408450842\n",
            "step: 120, loss: 0.0004540653608273715\n",
            "step: 130, loss: 0.003048297483474016\n",
            "step: 140, loss: 0.00026682051247917116\n",
            "step: 150, loss: 0.0013332534581422806\n",
            "step: 160, loss: 0.0007127244607545435\n",
            "step: 170, loss: 0.00048141900333575904\n",
            "step: 180, loss: 0.00023646761837881058\n",
            "step: 190, loss: 0.00038997404044494033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8364611260053619, f1=0.8406593406593407, best_f1=0.84375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006560764741152525\n",
            "step: 10, loss: 0.0017428739229217172\n",
            "step: 20, loss: 0.00019283870642539114\n",
            "step: 30, loss: 0.0019952713046222925\n",
            "step: 40, loss: 0.004327396862208843\n",
            "step: 50, loss: 0.001434607314877212\n",
            "step: 60, loss: 0.0700850859284401\n",
            "step: 70, loss: 0.0009389185579493642\n",
            "step: 80, loss: 0.000752704800106585\n",
            "step: 90, loss: 0.0002151427761418745\n",
            "step: 100, loss: 0.0003272911417298019\n",
            "step: 110, loss: 0.004590901546180248\n",
            "step: 120, loss: 0.00034889098606072366\n",
            "step: 130, loss: 0.0004872263816650957\n",
            "step: 140, loss: 0.009172727353870869\n",
            "step: 150, loss: 0.00031474410207010806\n",
            "step: 160, loss: 0.0011071108747273684\n",
            "step: 170, loss: 0.00022637276560999453\n",
            "step: 180, loss: 0.0004288263153284788\n",
            "step: 190, loss: 0.11701346188783646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8409703504043127, f1=0.8324022346368716, best_f1=0.84375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016918780747801065\n",
            "step: 10, loss: 0.01138338539749384\n",
            "step: 20, loss: 0.014475353062152863\n",
            "step: 30, loss: 0.0003854901297017932\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 40, loss: 0.000356510397978127\n",
            "step: 50, loss: 0.0003567979147192091\n",
            "step: 60, loss: 0.0003618735645432025\n",
            "step: 70, loss: 0.0003009666397701949\n",
            "step: 80, loss: 0.0007518581696785986\n",
            "step: 90, loss: 0.0013826419599354267\n",
            "step: 100, loss: 0.00011932936467928812\n",
            "step: 110, loss: 0.00011864330008393154\n",
            "step: 120, loss: 0.0008615721017122269\n",
            "step: 130, loss: 0.00019052106654271483\n",
            "step: 140, loss: 0.0002612588868942112\n",
            "step: 150, loss: 0.00040054417331703007\n",
            "step: 160, loss: 0.00016518999473191798\n",
            "step: 170, loss: 0.000199367044842802\n",
            "step: 180, loss: 0.00039204375934787095\n",
            "step: 190, loss: 0.0005193006945773959\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8409703504043127, f1=0.8509485094850948, best_f1=0.84375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01730506680905819\n",
            "step: 10, loss: 0.00013116448826622218\n",
            "step: 20, loss: 0.003324739634990692\n",
            "step: 30, loss: 0.00019445974612608552\n",
            "step: 40, loss: 0.00044488071580417454\n",
            "step: 50, loss: 0.01978858932852745\n",
            "step: 60, loss: 0.0010715678799897432\n",
            "step: 70, loss: 0.0007549096480943263\n",
            "step: 80, loss: 0.0002680705802049488\n",
            "step: 90, loss: 0.0038753501139581203\n",
            "step: 100, loss: 0.008258750662207603\n",
            "step: 110, loss: 0.0003865629551000893\n",
            "step: 120, loss: 0.0002965984749607742\n",
            "step: 130, loss: 0.00028002492035739124\n",
            "step: 140, loss: 0.00022580068616662174\n",
            "step: 150, loss: 0.0001387081720167771\n",
            "step: 160, loss: 0.00014912104234099388\n",
            "step: 170, loss: 0.00024052814114838839\n",
            "step: 180, loss: 0.0005970289348624647\n",
            "step: 190, loss: 0.0005443572299554944\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8342245989304813, f1=0.8509485094850948, best_f1=0.84375\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:14, 139.18it/s]\n",
            "load_f1 = 0.7916666666666666\n",
            "real_f1 = 0.7857142857142857\n",
            "733it [00:00, 2583.07it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:33, 131.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK TEXTUAL"
      ],
      "metadata": {
        "id": "SSCCmtSggw8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "OAbIZQYfgw8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b00ec4-ee8e-4e57-8d8c-10036f2a7a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5249601602554321\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5497928261756897\n",
            "step: 20, loss: 0.351473331451416\n",
            "step: 30, loss: 0.3982040584087372\n",
            "step: 40, loss: 0.6531309485435486\n",
            "step: 50, loss: 0.3058395981788635\n",
            "step: 60, loss: 0.5454572439193726\n",
            "step: 70, loss: 0.32871755957603455\n",
            "step: 80, loss: 0.23036383092403412\n",
            "step: 90, loss: 0.2341386079788208\n",
            "step: 100, loss: 0.151798814535141\n",
            "step: 110, loss: 0.3993971347808838\n",
            "step: 120, loss: 0.3290098011493683\n",
            "step: 130, loss: 0.28678056597709656\n",
            "step: 140, loss: 0.37784543633461\n",
            "step: 150, loss: 0.30124741792678833\n",
            "step: 160, loss: 0.4085273742675781\n",
            "step: 170, loss: 0.32289448380470276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.24733475479744135, f1=0.2648401826484018, best_f1=0.2648401826484018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30023038387298584\n",
            "step: 10, loss: 0.4237901568412781\n",
            "step: 20, loss: 0.24592894315719604\n",
            "step: 30, loss: 0.29630425572395325\n",
            "step: 40, loss: 0.2149616926908493\n",
            "step: 50, loss: 0.16563168168067932\n",
            "step: 60, loss: 0.08697736263275146\n",
            "step: 70, loss: 0.2655879259109497\n",
            "step: 80, loss: 0.3065975308418274\n",
            "step: 90, loss: 0.09868501126766205\n",
            "step: 100, loss: 0.23029345273971558\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 110, loss: 0.2423168122768402\n",
            "step: 120, loss: 0.16240696609020233\n",
            "step: 130, loss: 0.21638941764831543\n",
            "step: 140, loss: 0.18939340114593506\n",
            "step: 150, loss: 0.11011567711830139\n",
            "step: 160, loss: 0.15785230696201324\n",
            "step: 170, loss: 0.02424352429807186\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7927107061503416, f1=0.7775377969762419, best_f1=0.7775377969762419\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24926508963108063\n",
            "step: 10, loss: 0.0770546942949295\n",
            "step: 20, loss: 0.1011771485209465\n",
            "step: 30, loss: 0.11170203238725662\n",
            "step: 40, loss: 0.13045582175254822\n",
            "step: 50, loss: 0.20202551782131195\n",
            "step: 60, loss: 0.042885877192020416\n",
            "step: 70, loss: 0.14096154272556305\n",
            "step: 80, loss: 0.026692185550928116\n",
            "step: 90, loss: 0.43607962131500244\n",
            "step: 100, loss: 0.01649686135351658\n",
            "step: 110, loss: 0.02083825320005417\n",
            "step: 120, loss: 0.06812901049852371\n",
            "step: 130, loss: 0.2071593850851059\n",
            "step: 140, loss: 0.10489823669195175\n",
            "step: 150, loss: 0.012388534843921661\n",
            "step: 160, loss: 0.05916867405176163\n",
            "step: 170, loss: 0.03635726496577263\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8275862068965517, f1=0.8510638297872338, best_f1=0.8510638297872338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04138745367527008\n",
            "step: 10, loss: 0.0050291339866817\n",
            "step: 20, loss: 0.12619049847126007\n",
            "step: 30, loss: 0.1932460516691208\n",
            "step: 40, loss: 0.02667800895869732\n",
            "step: 50, loss: 0.029154254123568535\n",
            "step: 60, loss: 0.1279328316450119\n",
            "step: 70, loss: 0.0022752524819225073\n",
            "step: 80, loss: 0.124204121530056\n",
            "step: 90, loss: 0.13153356313705444\n",
            "step: 100, loss: 0.0752420574426651\n",
            "step: 110, loss: 0.22493118047714233\n",
            "step: 120, loss: 0.10672719031572342\n",
            "step: 130, loss: 0.03223070129752159\n",
            "step: 140, loss: 0.07547540962696075\n",
            "step: 150, loss: 0.07491065561771393\n",
            "step: 160, loss: 0.003172406228259206\n",
            "step: 170, loss: 0.14800181984901428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8300970873786409, f1=0.859122401847575, best_f1=0.859122401847575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008608822710812092\n",
            "step: 10, loss: 0.06692766398191452\n",
            "step: 20, loss: 0.07176415622234344\n",
            "step: 30, loss: 0.01643216609954834\n",
            "step: 40, loss: 0.10107723623514175\n",
            "step: 50, loss: 0.013041311874985695\n",
            "step: 60, loss: 0.05108681693673134\n",
            "step: 70, loss: 0.007237757556140423\n",
            "step: 80, loss: 0.001257374999113381\n",
            "step: 90, loss: 0.021047376096248627\n",
            "step: 100, loss: 0.029879016801714897\n",
            "step: 110, loss: 0.04678644239902496\n",
            "step: 120, loss: 0.017711380496621132\n",
            "step: 130, loss: 0.004139070399105549\n",
            "step: 140, loss: 0.01957232505083084\n",
            "step: 150, loss: 0.02404184639453888\n",
            "step: 160, loss: 0.0026144927833229303\n",
            "step: 170, loss: 0.0063479505479335785\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.8207792207792208, f1=0.8676470588235294, best_f1=0.859122401847575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13430653512477875\n",
            "step: 10, loss: 0.027685604989528656\n",
            "step: 20, loss: 0.0718703493475914\n",
            "step: 30, loss: 0.004349185153841972\n",
            "step: 40, loss: 0.0008456888608634472\n",
            "step: 50, loss: 0.0033081634901463985\n",
            "step: 60, loss: 0.03153763338923454\n",
            "step: 70, loss: 0.007740318309515715\n",
            "step: 80, loss: 0.004386541433632374\n",
            "step: 90, loss: 0.09875376522541046\n",
            "step: 100, loss: 0.027656545862555504\n",
            "step: 110, loss: 0.027118466794490814\n",
            "step: 120, loss: 0.018277736380696297\n",
            "step: 130, loss: 0.005712701473385096\n",
            "step: 140, loss: 0.010577969253063202\n",
            "step: 150, loss: 0.09469900280237198\n",
            "step: 160, loss: 0.08332762122154236\n",
            "step: 170, loss: 0.005218413658440113\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8522167487684729, f1=0.8738317757009346, best_f1=0.8738317757009346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04046444222331047\n",
            "step: 10, loss: 0.005959186237305403\n",
            "step: 20, loss: 0.01164967380464077\n",
            "step: 30, loss: 0.00802281592041254\n",
            "step: 40, loss: 0.015076511539518833\n",
            "step: 50, loss: 0.0004776997084263712\n",
            "step: 60, loss: 0.027882792055606842\n",
            "step: 70, loss: 0.006263351533561945\n",
            "step: 80, loss: 0.008603023365139961\n",
            "step: 90, loss: 0.0727853998541832\n",
            "step: 100, loss: 0.0007897292380221188\n",
            "step: 110, loss: 0.013930447399616241\n",
            "step: 120, loss: 0.005298496223986149\n",
            "step: 130, loss: 0.0020485869608819485\n",
            "step: 140, loss: 0.0010326574556529522\n",
            "step: 150, loss: 0.06305106729269028\n",
            "step: 160, loss: 0.004041845910251141\n",
            "step: 170, loss: 0.018704991787672043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8259740259740259, f1=0.8676470588235294, best_f1=0.8738317757009346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05324633792042732\n",
            "step: 10, loss: 0.004341420251876116\n",
            "step: 20, loss: 0.0014787534018978477\n",
            "step: 30, loss: 0.0003672220336738974\n",
            "step: 40, loss: 0.0004409459070302546\n",
            "step: 50, loss: 0.009924323298037052\n",
            "step: 60, loss: 0.0015466201584786177\n",
            "step: 70, loss: 0.005059476010501385\n",
            "step: 80, loss: 0.01227517519146204\n",
            "step: 90, loss: 0.052096568048000336\n",
            "step: 100, loss: 0.0002523709845263511\n",
            "step: 110, loss: 0.24808524549007416\n",
            "step: 120, loss: 0.03241337090730667\n",
            "step: 130, loss: 0.0017387925181537867\n",
            "step: 140, loss: 0.007868831045925617\n",
            "step: 150, loss: 0.006447015330195427\n",
            "step: 160, loss: 0.06742282211780548\n",
            "step: 170, loss: 0.08587995171546936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.828125, f1=0.8710462287104622, best_f1=0.8738317757009346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007912680506706238\n",
            "step: 10, loss: 0.007838549092411995\n",
            "step: 20, loss: 0.0073853530921041965\n",
            "step: 30, loss: 0.01232126448303461\n",
            "step: 40, loss: 0.003198971739038825\n",
            "step: 50, loss: 0.002621992025524378\n",
            "step: 60, loss: 0.02086629532277584\n",
            "step: 70, loss: 0.0022360251750797033\n",
            "step: 80, loss: 0.0011136432876810431\n",
            "step: 90, loss: 0.0009522187174297869\n",
            "step: 100, loss: 0.02453494258224964\n",
            "step: 110, loss: 0.12995027005672455\n",
            "step: 120, loss: 0.04685494303703308\n",
            "step: 130, loss: 0.0011424445547163486\n",
            "step: 140, loss: 0.0004720531578641385\n",
            "step: 150, loss: 0.04525764286518097\n",
            "step: 160, loss: 0.0025086114183068275\n",
            "step: 170, loss: 0.010014726780354977\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8298969072164948, f1=0.8641975308641976, best_f1=0.8738317757009346\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04038283973932266\n",
            "step: 10, loss: 0.0005129383644089103\n",
            "step: 20, loss: 0.053680580109357834\n",
            "step: 30, loss: 0.07247604429721832\n",
            "step: 40, loss: 0.002271861769258976\n",
            "step: 50, loss: 0.029350392520427704\n",
            "step: 60, loss: 0.01180072408169508\n",
            "step: 70, loss: 0.0011072501074522734\n",
            "step: 80, loss: 0.04036116227507591\n",
            "step: 90, loss: 0.0002308545372216031\n",
            "step: 100, loss: 0.008553266525268555\n",
            "step: 110, loss: 0.09055469930171967\n",
            "step: 120, loss: 0.004424441140145063\n",
            "step: 130, loss: 0.015338076278567314\n",
            "step: 140, loss: 0.0004361289902590215\n",
            "step: 150, loss: 0.00014488125452771783\n",
            "step: 160, loss: 0.0005711041740141809\n",
            "step: 170, loss: 0.04130905494093895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.8549618320610687, f1=0.8782816229116946, best_f1=0.8782816229116946\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01620177924633026\n",
            "step: 10, loss: 0.00343704828992486\n",
            "step: 20, loss: 0.00013123148528393358\n",
            "step: 30, loss: 0.005841887556016445\n",
            "step: 40, loss: 0.00013685256999451667\n",
            "step: 50, loss: 0.0767478495836258\n",
            "step: 60, loss: 0.0087158577516675\n",
            "step: 70, loss: 0.0004456837777979672\n",
            "step: 80, loss: 0.029096094891428947\n",
            "step: 90, loss: 0.012621023692190647\n",
            "step: 100, loss: 0.0019184760749340057\n",
            "step: 110, loss: 0.00019952333241235465\n",
            "step: 120, loss: 0.000598473590798676\n",
            "step: 130, loss: 0.015750307589769363\n",
            "step: 140, loss: 0.0002816115156747401\n",
            "step: 150, loss: 0.00010800376912811771\n",
            "step: 160, loss: 0.007136739324778318\n",
            "step: 170, loss: 0.0058453381061553955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8513853904282115, f1=0.8646080760095013, best_f1=0.8782816229116946\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007168698124587536\n",
            "step: 10, loss: 0.00011836887279059738\n",
            "step: 20, loss: 0.00012179333134554327\n",
            "step: 30, loss: 0.05279269441962242\n",
            "step: 40, loss: 0.00011945598089369014\n",
            "step: 50, loss: 0.0001018785042106174\n",
            "step: 60, loss: 0.0003291392349638045\n",
            "step: 70, loss: 9.893744572764263e-05\n",
            "step: 80, loss: 0.00013888526882510632\n",
            "step: 90, loss: 0.007895654067397118\n",
            "step: 100, loss: 0.0008704950450919569\n",
            "step: 110, loss: 0.028484143316745758\n",
            "step: 120, loss: 0.04059790074825287\n",
            "step: 130, loss: 0.0049604992382228374\n",
            "step: 140, loss: 0.0001251690846402198\n",
            "step: 150, loss: 0.008855103515088558\n",
            "step: 160, loss: 0.00540638854727149\n",
            "step: 170, loss: 0.007926074787974358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.858611825192802, f1=0.8695652173913043, best_f1=0.8695652173913043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007048183120787144\n",
            "step: 10, loss: 0.0002149652864318341\n",
            "step: 20, loss: 0.00030577636789530516\n",
            "step: 30, loss: 0.02018013782799244\n",
            "step: 40, loss: 0.00016009205137379467\n",
            "step: 50, loss: 0.005400970112532377\n",
            "step: 60, loss: 0.00012462426093406975\n",
            "step: 70, loss: 0.04960281401872635\n",
            "step: 80, loss: 0.001833549002185464\n",
            "step: 90, loss: 0.001456701778806746\n",
            "step: 100, loss: 0.049019020050764084\n",
            "step: 110, loss: 0.0006198078626766801\n",
            "step: 120, loss: 0.030600933358073235\n",
            "step: 130, loss: 7.855131843825802e-05\n",
            "step: 140, loss: 0.002261360874399543\n",
            "step: 150, loss: 0.00012189291010145098\n",
            "step: 160, loss: 0.0002370007277932018\n",
            "step: 170, loss: 8.613472891738638e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8324324324324324, f1=0.8706467661691543, best_f1=0.8695652173913043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011060868389904499\n",
            "step: 10, loss: 0.00023988615430425853\n",
            "step: 20, loss: 0.007028400897979736\n",
            "step: 30, loss: 0.014924558810889721\n",
            "step: 40, loss: 0.00012607777898665518\n",
            "step: 50, loss: 0.00024511016090400517\n",
            "step: 60, loss: 0.001121705980040133\n",
            "step: 70, loss: 0.00014673813711851835\n",
            "step: 80, loss: 0.00022274126240517944\n",
            "step: 90, loss: 7.640466355951503e-05\n",
            "step: 100, loss: 0.00015337896184064448\n",
            "step: 110, loss: 0.04126952961087227\n",
            "step: 120, loss: 0.0002180957089876756\n",
            "step: 130, loss: 0.06163746118545532\n",
            "step: 140, loss: 0.07146011292934418\n",
            "step: 150, loss: 0.02199455164372921\n",
            "step: 160, loss: 8.232030813815072e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 0.015758957713842392\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8321167883211679, f1=0.8577981651376146, best_f1=0.8695652173913043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 6.385643791873008e-05\n",
            "step: 10, loss: 0.00016293123189825565\n",
            "step: 20, loss: 0.00010364706395193934\n",
            "step: 30, loss: 0.00013828417286276817\n",
            "step: 40, loss: 6.556610605912283e-05\n",
            "step: 50, loss: 8.666322537465021e-05\n",
            "step: 60, loss: 0.003160474356263876\n",
            "step: 70, loss: 0.04081973060965538\n",
            "step: 80, loss: 8.605829498264939e-05\n",
            "step: 90, loss: 0.007154646795243025\n",
            "step: 100, loss: 0.01997974142432213\n",
            "step: 110, loss: 8.509570034220815e-05\n",
            "step: 120, loss: 7.331358938245103e-05\n",
            "step: 130, loss: 0.00010119762009708211\n",
            "step: 140, loss: 0.008532600477337837\n",
            "step: 150, loss: 6.153212598292157e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 160, loss: 5.798197162221186e-05\n",
            "step: 170, loss: 0.00023958242672961205\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.837696335078534, f1=0.8753056234718827, best_f1=0.8695652173913043\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:09, 209.61it/s]\n",
            "load_f1 = 0.5584415584415584\n",
            "real_f1 = 0.5102639296187683\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 138.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK DIRTY"
      ],
      "metadata": {
        "id": "5HZE1zMQgw8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "9jg7qrOQgw8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab3afc1e-3c5d-475a-a526-153b7941569d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5891004800796509\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4428415894508362\n",
            "step: 20, loss: 0.4634675681591034\n",
            "step: 30, loss: 0.29766735434532166\n",
            "step: 40, loss: 0.23626431822776794\n",
            "step: 50, loss: 0.18549221754074097\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 60, loss: 0.13551673293113708\n",
            "step: 70, loss: 0.20551268756389618\n",
            "step: 80, loss: 0.029445283114910126\n",
            "step: 90, loss: 0.07949160039424896\n",
            "step: 100, loss: 0.2006663829088211\n",
            "step: 110, loss: 0.10148154944181442\n",
            "step: 120, loss: 0.009249869734048843\n",
            "step: 130, loss: 0.1814500093460083\n",
            "step: 140, loss: 0.028119364753365517\n",
            "step: 150, loss: 0.14013755321502686\n",
            "step: 160, loss: 0.06851822137832642\n",
            "step: 170, loss: 0.09632544964551926\n",
            "step: 180, loss: 0.045067332684993744\n",
            "step: 190, loss: 0.007620927412062883\n",
            "step: 200, loss: 0.027575917541980743\n",
            "step: 210, loss: 0.039289116859436035\n",
            "step: 220, loss: 0.10101057589054108\n",
            "step: 230, loss: 0.006602437235414982\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9620535714285715, f1=0.9641255605381166, best_f1=0.9641255605381166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0036253745201975107\n",
            "step: 10, loss: 0.1983962506055832\n",
            "step: 20, loss: 0.01037950161844492\n",
            "step: 30, loss: 0.00415817042812705\n",
            "step: 40, loss: 0.06492824852466583\n",
            "step: 50, loss: 0.014919349923729897\n",
            "step: 60, loss: 0.026814274489879608\n",
            "step: 70, loss: 0.05164596810936928\n",
            "step: 80, loss: 0.008856951259076595\n",
            "step: 90, loss: 0.06641816347837448\n",
            "step: 100, loss: 0.017478352412581444\n",
            "step: 110, loss: 0.02337072417140007\n",
            "step: 120, loss: 0.026949914172291756\n",
            "step: 130, loss: 0.02513563632965088\n",
            "step: 140, loss: 0.004224635660648346\n",
            "step: 150, loss: 0.07606562227010727\n",
            "step: 160, loss: 0.01325895544141531\n",
            "step: 170, loss: 0.004440372809767723\n",
            "step: 180, loss: 0.002453444991260767\n",
            "step: 190, loss: 0.0009248614078387618\n",
            "step: 200, loss: 0.03147772327065468\n",
            "step: 210, loss: 0.02760155312716961\n",
            "step: 220, loss: 0.0029916574712842703\n",
            "step: 230, loss: 0.0015887339832261205\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9668141592920354, f1=0.9686800894854586, best_f1=0.9686800894854586\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.035498328506946564\n",
            "step: 10, loss: 0.0032076395582407713\n",
            "step: 20, loss: 0.007234751712530851\n",
            "step: 30, loss: 0.00674245273694396\n",
            "step: 40, loss: 0.06870704889297485\n",
            "step: 50, loss: 0.008735097013413906\n",
            "step: 60, loss: 0.017099829390645027\n",
            "step: 70, loss: 0.017080791294574738\n",
            "step: 80, loss: 0.1486699879169464\n",
            "step: 90, loss: 0.07265950739383698\n",
            "step: 100, loss: 0.006721820216625929\n",
            "step: 110, loss: 0.07573940604925156\n",
            "step: 120, loss: 0.06219755858182907\n",
            "step: 130, loss: 0.010115080513060093\n",
            "step: 140, loss: 0.0036489630583673716\n",
            "step: 150, loss: 0.15978164970874786\n",
            "step: 160, loss: 0.010922531597316265\n",
            "step: 170, loss: 0.0017902874387800694\n",
            "step: 180, loss: 0.008422212675213814\n",
            "step: 190, loss: 0.006630973424762487\n",
            "step: 200, loss: 0.012403870932757854\n",
            "step: 210, loss: 0.008531699888408184\n",
            "step: 220, loss: 0.12689681351184845\n",
            "step: 230, loss: 0.009828914888203144\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9744160177975528, f1=0.9597315436241611, best_f1=0.9597315436241611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0057143461890518665\n",
            "step: 10, loss: 0.0010646753944456577\n",
            "step: 20, loss: 0.0016974890604615211\n",
            "step: 30, loss: 0.0014588473131880164\n",
            "step: 40, loss: 0.02156807854771614\n",
            "step: 50, loss: 0.015929076820611954\n",
            "step: 60, loss: 0.009189709089696407\n",
            "step: 70, loss: 0.05522925779223442\n",
            "step: 80, loss: 0.13632002472877502\n",
            "step: 90, loss: 0.09825877845287323\n",
            "step: 100, loss: 0.020606568083167076\n",
            "step: 110, loss: 0.0012541520409286022\n",
            "step: 120, loss: 0.007805908564478159\n",
            "step: 130, loss: 0.0024078215938061476\n",
            "step: 140, loss: 0.0015886551700532436\n",
            "step: 150, loss: 0.002876848680898547\n",
            "step: 160, loss: 0.017413249239325523\n",
            "step: 170, loss: 0.0010747454361990094\n",
            "step: 180, loss: 0.11092819273471832\n",
            "step: 190, loss: 0.004078340716660023\n",
            "step: 200, loss: 0.1314510554075241\n",
            "step: 210, loss: 0.0035671719815582037\n",
            "step: 220, loss: 0.0021524287294596434\n",
            "step: 230, loss: 0.007914496585726738\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9761634506242906, f1=0.952819332566168, best_f1=0.952819332566168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010829759994521737\n",
            "step: 10, loss: 0.004440495278686285\n",
            "step: 20, loss: 0.029126213863492012\n",
            "step: 30, loss: 0.0019506918033584952\n",
            "step: 40, loss: 0.01193302683532238\n",
            "step: 50, loss: 0.0008438787772320211\n",
            "step: 60, loss: 0.0031580915674567223\n",
            "step: 70, loss: 0.003522673388943076\n",
            "step: 80, loss: 0.01807571016252041\n",
            "step: 90, loss: 0.015798307955265045\n",
            "step: 100, loss: 0.000808258424513042\n",
            "step: 110, loss: 0.007214678451418877\n",
            "step: 120, loss: 0.0036555312108248472\n",
            "step: 130, loss: 0.0005219049635343254\n",
            "step: 140, loss: 0.0027873183134943247\n",
            "step: 150, loss: 0.01793934777379036\n",
            "step: 160, loss: 0.0017516508232802153\n",
            "step: 170, loss: 0.0010052468860521913\n",
            "step: 180, loss: 0.003512146882712841\n",
            "step: 190, loss: 0.20424439013004303\n",
            "step: 200, loss: 0.006665156688541174\n",
            "step: 210, loss: 0.01335895899683237\n",
            "step: 220, loss: 0.005130135454237461\n",
            "step: 230, loss: 0.028948096558451653\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9776785714285714, f1=0.976324689966178, best_f1=0.976324689966178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015278315404430032\n",
            "step: 10, loss: 0.0023248200304806232\n",
            "step: 20, loss: 0.0006222590454854071\n",
            "step: 30, loss: 0.00030007632449269295\n",
            "step: 40, loss: 0.00020919748931191862\n",
            "step: 50, loss: 0.0002578684943728149\n",
            "step: 60, loss: 0.0014273461420089006\n",
            "step: 70, loss: 0.1589675396680832\n",
            "step: 80, loss: 0.0020750188268721104\n",
            "step: 90, loss: 0.0017382418736815453\n",
            "step: 100, loss: 0.0005998389096930623\n",
            "step: 110, loss: 0.004022924695163965\n",
            "step: 120, loss: 0.00038224601303227246\n",
            "step: 130, loss: 0.008364596404135227\n",
            "step: 140, loss: 0.0031908873934298754\n",
            "step: 150, loss: 0.0001671111094765365\n",
            "step: 160, loss: 0.005482666660100222\n",
            "step: 170, loss: 0.0010069309500977397\n",
            "step: 180, loss: 0.004411119967699051\n",
            "step: 190, loss: 0.0064531066454946995\n",
            "step: 200, loss: 0.015462805517017841\n",
            "step: 210, loss: 0.001850625965744257\n",
            "step: 220, loss: 0.0033054444938898087\n",
            "step: 230, loss: 0.0008609648793935776\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9741863075196409, f1=0.9718785151856018, best_f1=0.976324689966178\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004914556629955769\n",
            "step: 10, loss: 0.0005320276250131428\n",
            "step: 20, loss: 0.00023416461772285402\n",
            "step: 30, loss: 0.00030409314786083996\n",
            "step: 40, loss: 0.04811355471611023\n",
            "step: 50, loss: 0.0007355944253504276\n",
            "step: 60, loss: 0.0007041132193990052\n",
            "step: 70, loss: 0.003106695832684636\n",
            "step: 80, loss: 0.0008642858592793345\n",
            "step: 90, loss: 0.00040511126280762255\n",
            "step: 100, loss: 0.0006495658308267593\n",
            "step: 110, loss: 0.001138328923843801\n",
            "step: 120, loss: 0.022057119756937027\n",
            "step: 130, loss: 0.0014097915263846517\n",
            "step: 140, loss: 0.0007362765609286726\n",
            "step: 150, loss: 0.058781642466783524\n",
            "step: 160, loss: 0.0003337115340400487\n",
            "step: 170, loss: 0.025798119604587555\n",
            "step: 180, loss: 0.0003010093350894749\n",
            "step: 190, loss: 0.0007631039479747415\n",
            "step: 200, loss: 0.006116360425949097\n",
            "step: 210, loss: 0.0003821042482741177\n",
            "step: 220, loss: 0.00048808014253154397\n",
            "step: 230, loss: 0.0013953268062323332\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9798206278026906, f1=0.971815107102593, best_f1=0.971815107102593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014882689574733377\n",
            "step: 10, loss: 0.0018609149847179651\n",
            "step: 20, loss: 0.001990771619603038\n",
            "step: 30, loss: 0.0015951694222167134\n",
            "step: 40, loss: 0.0011442156974226236\n",
            "step: 50, loss: 0.0006412086077034473\n",
            "step: 60, loss: 0.0006997503223828971\n",
            "step: 70, loss: 0.0003310233587399125\n",
            "step: 80, loss: 0.00044280063593760133\n",
            "step: 90, loss: 0.0003780114057008177\n",
            "step: 100, loss: 0.0008858554065227509\n",
            "step: 110, loss: 0.0003886317426804453\n",
            "step: 120, loss: 0.00035839667543768883\n",
            "step: 130, loss: 0.01156017929315567\n",
            "step: 140, loss: 0.00020445416157599539\n",
            "step: 150, loss: 0.056388191878795624\n",
            "step: 160, loss: 0.0002156477712560445\n",
            "step: 170, loss: 0.01623695157468319\n",
            "step: 180, loss: 0.0001538364012958482\n",
            "step: 190, loss: 0.0001575517817400396\n",
            "step: 200, loss: 0.03159397095441818\n",
            "step: 210, loss: 0.0007717291591688991\n",
            "step: 220, loss: 0.0007326498162001371\n",
            "step: 230, loss: 0.0009151221602223814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9766925638179801, f1=0.9675977653631285, best_f1=0.971815107102593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038278046995401382\n",
            "step: 10, loss: 0.0064010475762188435\n",
            "step: 20, loss: 0.0032481495290994644\n",
            "step: 30, loss: 0.0021557325962930918\n",
            "step: 40, loss: 0.009794346988201141\n",
            "step: 50, loss: 0.002338527701795101\n",
            "step: 60, loss: 0.0010956758633255959\n",
            "step: 70, loss: 0.013362137600779533\n",
            "step: 80, loss: 0.0016519823111593723\n",
            "step: 90, loss: 0.04379939287900925\n",
            "step: 100, loss: 0.0006704549305140972\n",
            "step: 110, loss: 0.0005591055378317833\n",
            "step: 120, loss: 0.03551703318953514\n",
            "step: 130, loss: 0.028837701305747032\n",
            "step: 140, loss: 0.0012275754706934094\n",
            "step: 150, loss: 0.0014038683148100972\n",
            "step: 160, loss: 0.0007273431983776391\n",
            "step: 170, loss: 0.0001976129860850051\n",
            "step: 180, loss: 0.002684136386960745\n",
            "step: 190, loss: 0.000644797517452389\n",
            "step: 200, loss: 0.0002057481906376779\n",
            "step: 210, loss: 0.025904683396220207\n",
            "step: 220, loss: 0.00020837862393818796\n",
            "step: 230, loss: 9.978960588341579e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9787709497206705, f1=0.9751131221719457, best_f1=0.971815107102593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017394627502653748\n",
            "step: 10, loss: 0.00025687809102237225\n",
            "step: 20, loss: 0.0001778873265720904\n",
            "step: 30, loss: 8.97091013030149e-05\n",
            "step: 40, loss: 0.00028913840651512146\n",
            "step: 50, loss: 6.486084748758003e-05\n",
            "step: 60, loss: 0.00039192254189401865\n",
            "step: 70, loss: 0.004916551988571882\n",
            "step: 80, loss: 0.0007155667990446091\n",
            "step: 90, loss: 0.00046688527800142765\n",
            "step: 100, loss: 0.00036978020216338336\n",
            "step: 110, loss: 0.00035555599606595933\n",
            "step: 120, loss: 0.0003048541839234531\n",
            "step: 130, loss: 0.0004438565520104021\n",
            "step: 140, loss: 0.0037969325203448534\n",
            "step: 150, loss: 0.0008670431561768055\n",
            "step: 160, loss: 0.00014421960804611444\n",
            "step: 170, loss: 0.0002488672616891563\n",
            "step: 180, loss: 0.004291073884814978\n",
            "step: 190, loss: 0.0004972670576535165\n",
            "step: 200, loss: 0.0007240180275402963\n",
            "step: 210, loss: 0.005784698761999607\n",
            "step: 220, loss: 0.003027047263458371\n",
            "step: 230, loss: 0.00024255330208688974\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9820627802690582, f1=0.9739524348810873, best_f1=0.9739524348810873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002391464076936245\n",
            "step: 10, loss: 0.0018384744180366397\n",
            "step: 20, loss: 0.000451504485681653\n",
            "step: 30, loss: 0.000374637427739799\n",
            "step: 40, loss: 0.00016023332136683166\n",
            "step: 50, loss: 0.0009864246239885688\n",
            "step: 60, loss: 0.0006355256773531437\n",
            "step: 70, loss: 0.0005152865196578205\n",
            "step: 80, loss: 0.00029110207105986774\n",
            "step: 90, loss: 0.20147472620010376\n",
            "step: 100, loss: 0.000587838760111481\n",
            "step: 110, loss: 0.002618216909468174\n",
            "step: 120, loss: 0.0005651566316373646\n",
            "step: 130, loss: 0.0006341198459267616\n",
            "step: 140, loss: 0.0003781883278861642\n",
            "step: 150, loss: 0.000592827913351357\n",
            "step: 160, loss: 0.0004492932348512113\n",
            "step: 170, loss: 0.003424809779971838\n",
            "step: 180, loss: 0.00032383896177634597\n",
            "step: 190, loss: 0.00027041995781473815\n",
            "step: 200, loss: 0.00035056297201663256\n",
            "step: 210, loss: 0.000403938174713403\n",
            "step: 220, loss: 0.0012928287032991648\n",
            "step: 230, loss: 0.00026508228620514274\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9798657718120806, f1=0.9718785151856018, best_f1=0.9739524348810873\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005542777944356203\n",
            "step: 10, loss: 0.0002912183990702033\n",
            "step: 20, loss: 0.0009708424331620336\n",
            "step: 30, loss: 0.017302537336945534\n",
            "step: 40, loss: 0.00037916662404313684\n",
            "step: 50, loss: 0.0034838425926864147\n",
            "step: 60, loss: 0.0003872636298183352\n",
            "step: 70, loss: 0.0003271688474342227\n",
            "step: 80, loss: 0.00011766517854994163\n",
            "step: 90, loss: 0.0003388210607226938\n",
            "step: 100, loss: 0.0003573241410776973\n",
            "step: 110, loss: 0.00016035832231864333\n",
            "step: 120, loss: 0.00028134521562606096\n",
            "step: 130, loss: 0.00041629947372712195\n",
            "step: 140, loss: 0.000864670320879668\n",
            "step: 150, loss: 0.0008827264537103474\n",
            "step: 160, loss: 0.0005900943651795387\n",
            "step: 170, loss: 0.0006262261886149645\n",
            "step: 180, loss: 0.0004937235498800874\n",
            "step: 190, loss: 0.0009491863893344998\n",
            "step: 200, loss: 0.0004135567578487098\n",
            "step: 210, loss: 0.0024800398387014866\n",
            "step: 220, loss: 0.02492474764585495\n",
            "step: 230, loss: 0.0004470252606552094\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9821029082774049, f1=0.9740698985343857, best_f1=0.9740698985343857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002795990149024874\n",
            "step: 10, loss: 0.00039271486457437277\n",
            "step: 20, loss: 0.0006421508733183146\n",
            "step: 30, loss: 0.003463381202891469\n",
            "step: 40, loss: 0.0010331105440855026\n",
            "step: 50, loss: 0.0006242779199965298\n",
            "step: 60, loss: 0.0002728290273807943\n",
            "step: 70, loss: 0.0013686454622074962\n",
            "step: 80, loss: 0.0001632019120734185\n",
            "step: 90, loss: 0.00022543210070580244\n",
            "step: 100, loss: 0.0005174529505893588\n",
            "step: 110, loss: 0.0004980266094207764\n",
            "step: 120, loss: 0.0004713325761258602\n",
            "step: 130, loss: 0.0005290727131068707\n",
            "step: 140, loss: 0.006490133237093687\n",
            "step: 150, loss: 0.0003347054880578071\n",
            "step: 160, loss: 0.0016416130820289254\n",
            "step: 170, loss: 0.0012405901215970516\n",
            "step: 180, loss: 0.019520243629813194\n",
            "step: 190, loss: 0.0009871205547824502\n",
            "step: 200, loss: 0.00026640217402018607\n",
            "step: 210, loss: 0.0013244390720501542\n",
            "step: 220, loss: 0.0006554904393851757\n",
            "step: 230, loss: 0.0015845817979425192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9798657718120806, f1=0.9751693002257337, best_f1=0.9740698985343857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003735584323294461\n",
            "step: 10, loss: 0.00038998006493784487\n",
            "step: 20, loss: 0.00048559822607785463\n",
            "step: 30, loss: 0.0007600918179377913\n",
            "step: 40, loss: 0.00048725210945121944\n",
            "step: 50, loss: 0.0004121062229387462\n",
            "step: 60, loss: 0.0009643083321861923\n",
            "step: 70, loss: 0.00040500378236174583\n",
            "step: 80, loss: 0.00046541340998373926\n",
            "step: 90, loss: 0.0008937673992477357\n",
            "step: 100, loss: 0.0007659229449927807\n",
            "step: 110, loss: 0.000877357495483011\n",
            "step: 120, loss: 0.00017985753947868943\n",
            "step: 130, loss: 0.0006497909780591726\n",
            "step: 140, loss: 0.020691605284810066\n",
            "step: 150, loss: 0.00022142514353618026\n",
            "step: 160, loss: 0.0005989425117149949\n",
            "step: 170, loss: 0.0006487545906566083\n",
            "step: 180, loss: 0.0006002414156682789\n",
            "step: 190, loss: 0.0004430457775015384\n",
            "step: 200, loss: 0.0005002530524507165\n",
            "step: 210, loss: 0.00039349074359051883\n",
            "step: 220, loss: 0.0004744190664496273\n",
            "step: 230, loss: 0.00022912293206900358\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9787709497206705, f1=0.9740112994350283, best_f1=0.9740698985343857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005207396461628377\n",
            "step: 10, loss: 0.00047161741531454027\n",
            "step: 20, loss: 0.000727832259144634\n",
            "step: 30, loss: 0.0005859676748514175\n",
            "step: 40, loss: 0.00041509445873089135\n",
            "step: 50, loss: 0.0005123650771565735\n",
            "step: 60, loss: 0.004979949910193682\n",
            "step: 70, loss: 0.0004310199583414942\n",
            "step: 80, loss: 0.0008844878757372499\n",
            "step: 90, loss: 0.00036414575879462063\n",
            "step: 100, loss: 0.00028064247453585267\n",
            "step: 110, loss: 0.0005481082480400801\n",
            "step: 120, loss: 0.0071495939046144485\n",
            "step: 130, loss: 0.00035669689532369375\n",
            "step: 140, loss: 0.0007030823617242277\n",
            "step: 150, loss: 0.0005212736432440579\n",
            "step: 160, loss: 0.0006729582091793418\n",
            "step: 170, loss: 0.00020355763263069093\n",
            "step: 180, loss: 0.00057913304772228\n",
            "step: 190, loss: 0.00022348760103341192\n",
            "step: 200, loss: 0.0005450780736282468\n",
            "step: 210, loss: 0.0052553629502654076\n",
            "step: 220, loss: 0.0005520251579582691\n",
            "step: 230, loss: 0.000438844901509583\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9787709497206705, f1=0.9740112994350283, best_f1=0.9740698985343857\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:17, 137.66it/s]\n",
            "load_f1 = 0.978865406006674\n",
            "real_f1 = 0.9777777777777777\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 135.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "KYEeB2m-gw8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dffa426c-f994-4f8d-cbf3-9919a5848ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.6491526365280151\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.42966052889823914\n",
            "step: 20, loss: 0.3214341402053833\n",
            "step: 30, loss: 0.3791128695011139\n",
            "step: 40, loss: 0.391519695520401\n",
            "step: 50, loss: 0.6761741042137146\n",
            "step: 60, loss: 0.3636443018913269\n",
            "step: 70, loss: 0.4337458908557892\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.16339415311813354\n",
            "step: 90, loss: 0.21770863234996796\n",
            "step: 100, loss: 0.3250778615474701\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 110, loss: 0.19720633327960968\n",
            "step: 120, loss: 0.2047078013420105\n",
            "step: 130, loss: 0.14355477690696716\n",
            "step: 140, loss: 0.3585885763168335\n",
            "step: 150, loss: 0.14302562177181244\n",
            "step: 160, loss: 0.1795421838760376\n",
            "step: 170, loss: 0.09944013506174088\n",
            "step: 180, loss: 0.07489364594221115\n",
            "step: 190, loss: 0.10161466151475906\n",
            "step: 200, loss: 0.07330726087093353\n",
            "step: 210, loss: 0.07521487772464752\n",
            "step: 220, loss: 0.177666574716568\n",
            "step: 230, loss: 0.22774143517017365\n",
            "step: 240, loss: 0.09721499681472778\n",
            "step: 250, loss: 0.0381530299782753\n",
            "step: 260, loss: 0.17739206552505493\n",
            "step: 270, loss: 0.3305884599685669\n",
            "step: 280, loss: 0.02893231064081192\n",
            "step: 290, loss: 0.07464081048965454\n",
            "step: 300, loss: 0.13556331396102905\n",
            "step: 310, loss: 0.1924464851617813\n",
            "step: 320, loss: 0.04541882872581482\n",
            "step: 330, loss: 0.0833265408873558\n",
            "step: 340, loss: 0.46184253692626953\n",
            "step: 350, loss: 0.10589896142482758\n",
            "step: 360, loss: 0.14137281477451324\n",
            "step: 370, loss: 0.17565569281578064\n",
            "step: 380, loss: 0.3104109764099121\n",
            "step: 390, loss: 0.05809856206178665\n",
            "step: 400, loss: 0.06762493401765823\n",
            "step: 410, loss: 0.23038724064826965\n",
            "step: 420, loss: 0.04140307754278183\n",
            "step: 430, loss: 0.043083783239126205\n",
            "step: 440, loss: 0.06509575247764587\n",
            "step: 450, loss: 0.02784605883061886\n",
            "step: 460, loss: 0.021357461810112\n",
            "step: 470, loss: 0.026975462213158607\n",
            "step: 480, loss: 0.09120529145002365\n",
            "step: 490, loss: 0.047087159007787704\n",
            "step: 500, loss: 0.03427926450967789\n",
            "step: 510, loss: 0.044501882046461105\n",
            "step: 520, loss: 0.02368675172328949\n",
            "step: 530, loss: 0.05954144150018692\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9374130737134909, f1=0.9385474860335196, best_f1=0.9385474860335196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04833117872476578\n",
            "step: 10, loss: 0.08893131464719772\n",
            "step: 20, loss: 0.0965239554643631\n",
            "step: 30, loss: 0.09201759845018387\n",
            "step: 40, loss: 0.06548429280519485\n",
            "step: 50, loss: 0.06835055351257324\n",
            "step: 60, loss: 0.03111908584833145\n",
            "step: 70, loss: 0.030984481796622276\n",
            "step: 80, loss: 0.025081077590584755\n",
            "step: 90, loss: 0.036526866257190704\n",
            "step: 100, loss: 0.16097721457481384\n",
            "step: 110, loss: 0.005680826958268881\n",
            "step: 120, loss: 0.013409789651632309\n",
            "step: 130, loss: 0.0021936355624347925\n",
            "step: 140, loss: 0.15815596282482147\n",
            "step: 150, loss: 0.03200111538171768\n",
            "step: 160, loss: 0.026012741029262543\n",
            "step: 170, loss: 0.03878254070878029\n",
            "step: 180, loss: 0.05229726433753967\n",
            "step: 190, loss: 0.01722356677055359\n",
            "step: 200, loss: 0.3083826005458832\n",
            "step: 210, loss: 0.013074413873255253\n",
            "step: 220, loss: 0.0037677015643566847\n",
            "step: 230, loss: 0.013552680611610413\n",
            "step: 240, loss: 0.005519213620573282\n",
            "step: 250, loss: 0.015621678903698921\n",
            "step: 260, loss: 0.040193893015384674\n",
            "step: 270, loss: 0.012779662385582924\n",
            "step: 280, loss: 0.15092061460018158\n",
            "step: 290, loss: 0.0424979142844677\n",
            "step: 300, loss: 0.0768866166472435\n",
            "step: 310, loss: 0.022220442071557045\n",
            "step: 320, loss: 0.07091069221496582\n",
            "step: 330, loss: 0.06606373935937881\n",
            "step: 340, loss: 0.04860867187380791\n",
            "step: 350, loss: 0.00299275666475296\n",
            "step: 360, loss: 0.09346690773963928\n",
            "step: 370, loss: 0.01612769067287445\n",
            "step: 380, loss: 0.08748997002840042\n",
            "step: 390, loss: 0.003104636212810874\n",
            "step: 400, loss: 0.11935313791036606\n",
            "step: 410, loss: 0.11990699172019958\n",
            "step: 420, loss: 0.03794903680682182\n",
            "step: 430, loss: 0.165157288312912\n",
            "step: 440, loss: 0.0034976042807102203\n",
            "step: 450, loss: 0.04836495220661163\n",
            "step: 460, loss: 0.052996233105659485\n",
            "step: 470, loss: 0.04631591960787773\n",
            "step: 480, loss: 0.011053401976823807\n",
            "step: 490, loss: 0.07697469741106033\n",
            "step: 500, loss: 0.0060097835958004\n",
            "step: 510, loss: 0.008738423697650433\n",
            "step: 520, loss: 0.15912163257598877\n",
            "step: 530, loss: 0.037082646042108536\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9462068965517241, f1=0.9474169741697417, best_f1=0.9474169741697417\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1250765174627304\n",
            "step: 10, loss: 0.015103642828762531\n",
            "step: 20, loss: 0.04054257646203041\n",
            "step: 30, loss: 0.08030987530946732\n",
            "step: 40, loss: 0.08571026474237442\n",
            "step: 50, loss: 0.002782852854579687\n",
            "step: 60, loss: 0.048279885202646255\n",
            "step: 70, loss: 0.002722123172134161\n",
            "step: 80, loss: 0.09990181028842926\n",
            "step: 90, loss: 0.01770283840596676\n",
            "step: 100, loss: 0.023838605731725693\n",
            "step: 110, loss: 0.028103096410632133\n",
            "step: 120, loss: 0.1749795377254486\n",
            "step: 130, loss: 0.009374367073178291\n",
            "step: 140, loss: 0.03429070860147476\n",
            "step: 150, loss: 0.023538200184702873\n",
            "step: 160, loss: 0.012811439111828804\n",
            "step: 170, loss: 0.012396365404129028\n",
            "step: 180, loss: 0.07055115699768066\n",
            "step: 190, loss: 0.013735630549490452\n",
            "step: 200, loss: 0.044784024357795715\n",
            "step: 210, loss: 0.015353845432400703\n",
            "step: 220, loss: 0.12636099755764008\n",
            "step: 230, loss: 0.06811391562223434\n",
            "step: 240, loss: 0.12665686011314392\n",
            "step: 250, loss: 0.20215845108032227\n",
            "step: 260, loss: 0.13492009043693542\n",
            "step: 270, loss: 0.00379091571085155\n",
            "step: 280, loss: 0.06948499381542206\n",
            "step: 290, loss: 0.013705579563975334\n",
            "step: 300, loss: 0.17852665483951569\n",
            "step: 310, loss: 0.03993232920765877\n",
            "step: 320, loss: 0.010807141661643982\n",
            "step: 330, loss: 0.010357380844652653\n",
            "step: 340, loss: 0.009326335974037647\n",
            "step: 350, loss: 0.072162926197052\n",
            "step: 360, loss: 0.02093052677810192\n",
            "step: 370, loss: 0.0713321790099144\n",
            "step: 380, loss: 0.005146999843418598\n",
            "step: 390, loss: 0.005994261708110571\n",
            "step: 400, loss: 0.07489193975925446\n",
            "step: 410, loss: 0.05083842948079109\n",
            "step: 420, loss: 0.006335200741887093\n",
            "step: 430, loss: 0.011989270336925983\n",
            "step: 440, loss: 0.17367862164974213\n",
            "step: 450, loss: 0.05099098011851311\n",
            "step: 460, loss: 0.11657581478357315\n",
            "step: 470, loss: 0.03079918399453163\n",
            "step: 480, loss: 0.22766394913196564\n",
            "step: 490, loss: 0.06787559390068054\n",
            "step: 500, loss: 0.00808112695813179\n",
            "step: 510, loss: 0.05393407121300697\n",
            "step: 520, loss: 0.0034049360547214746\n",
            "step: 530, loss: 0.008155963383615017\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.948087431693989, f1=0.942035600182565, best_f1=0.942035600182565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017097918316721916\n",
            "step: 10, loss: 0.0057921395637094975\n",
            "step: 20, loss: 0.019388379529118538\n",
            "step: 30, loss: 0.12726058065891266\n",
            "step: 40, loss: 0.02220282517373562\n",
            "step: 50, loss: 0.03957900404930115\n",
            "step: 60, loss: 0.030942648649215698\n",
            "step: 70, loss: 0.22264394164085388\n",
            "step: 80, loss: 0.11005955934524536\n",
            "step: 90, loss: 0.09803146123886108\n",
            "step: 100, loss: 0.002306475769728422\n",
            "step: 110, loss: 0.03265872225165367\n",
            "step: 120, loss: 0.01208868995308876\n",
            "step: 130, loss: 0.08096424490213394\n",
            "step: 140, loss: 0.06405545771121979\n",
            "step: 150, loss: 0.03185221552848816\n",
            "step: 160, loss: 0.030962683260440826\n",
            "step: 170, loss: 0.008163665421307087\n",
            "step: 180, loss: 0.11263397336006165\n",
            "step: 190, loss: 0.04678668454289436\n",
            "step: 200, loss: 0.09676321595907211\n",
            "step: 210, loss: 0.00299422862008214\n",
            "step: 220, loss: 0.04842314496636391\n",
            "step: 230, loss: 0.008686280809342861\n",
            "step: 240, loss: 0.03344310447573662\n",
            "step: 250, loss: 0.006909291725605726\n",
            "step: 260, loss: 0.0010827162768691778\n",
            "step: 270, loss: 0.1785447597503662\n",
            "step: 280, loss: 0.010468407534062862\n",
            "step: 290, loss: 0.0437772199511528\n",
            "step: 300, loss: 0.0069020469672977924\n",
            "step: 310, loss: 0.00690125860273838\n",
            "step: 320, loss: 0.028349444270133972\n",
            "step: 330, loss: 0.06918660551309586\n",
            "step: 340, loss: 0.0010703192092478275\n",
            "step: 350, loss: 0.1317494511604309\n",
            "step: 360, loss: 0.1184052899479866\n",
            "step: 370, loss: 0.0032555253710597754\n",
            "step: 380, loss: 0.002820805413648486\n",
            "step: 390, loss: 0.0016983187524601817\n",
            "step: 400, loss: 0.09812553226947784\n",
            "step: 410, loss: 0.002067424124106765\n",
            "step: 420, loss: 0.06235959380865097\n",
            "step: 430, loss: 0.04636798053979874\n",
            "step: 440, loss: 0.0030799084343016148\n",
            "step: 450, loss: 0.011483849957585335\n",
            "step: 460, loss: 0.009450550191104412\n",
            "step: 470, loss: 0.001526394160464406\n",
            "step: 480, loss: 0.01082915160804987\n",
            "step: 490, loss: 0.0056231156922876835\n",
            "step: 500, loss: 0.03892508149147034\n",
            "step: 510, loss: 0.0049062613397836685\n",
            "step: 520, loss: 0.019762463867664337\n",
            "step: 530, loss: 0.14126752316951752\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9596622889305815, f1=0.948356807511737, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006484378129243851\n",
            "step: 10, loss: 0.04237828403711319\n",
            "step: 20, loss: 0.0012448757188394666\n",
            "step: 30, loss: 0.014078404754400253\n",
            "step: 40, loss: 0.0008059407118707895\n",
            "step: 50, loss: 0.06486999988555908\n",
            "step: 60, loss: 0.02563145011663437\n",
            "step: 70, loss: 0.016178887337446213\n",
            "step: 80, loss: 0.0027675852179527283\n",
            "step: 90, loss: 0.020186936482787132\n",
            "step: 100, loss: 0.01675577647984028\n",
            "step: 110, loss: 0.008182721212506294\n",
            "step: 120, loss: 0.15617358684539795\n",
            "step: 130, loss: 0.021025095134973526\n",
            "step: 140, loss: 0.02759459987282753\n",
            "step: 150, loss: 0.008743987418711185\n",
            "step: 160, loss: 0.0028504091314971447\n",
            "step: 170, loss: 0.011054172180593014\n",
            "step: 180, loss: 0.010235331952571869\n",
            "step: 190, loss: 0.007497337181121111\n",
            "step: 200, loss: 0.0029635014943778515\n",
            "step: 210, loss: 0.0018517090938985348\n",
            "step: 220, loss: 0.0070450324565172195\n",
            "step: 230, loss: 0.0013160653179511428\n",
            "step: 240, loss: 0.006409573368728161\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 250, loss: 0.3246553838253021\n",
            "step: 260, loss: 0.0027585134375840425\n",
            "step: 270, loss: 0.0021660609636455774\n",
            "step: 280, loss: 0.016901927068829536\n",
            "step: 290, loss: 0.09018855541944504\n",
            "step: 300, loss: 0.02966323122382164\n",
            "step: 310, loss: 0.012882005423307419\n",
            "step: 320, loss: 0.004612443037331104\n",
            "step: 330, loss: 0.00046100205508992076\n",
            "step: 340, loss: 0.04698082432150841\n",
            "step: 350, loss: 0.004229849670082331\n",
            "step: 360, loss: 0.0010443764040246606\n",
            "step: 370, loss: 0.0023564640432596207\n",
            "step: 380, loss: 0.0008454180206172168\n",
            "step: 390, loss: 0.0044301762245595455\n",
            "step: 400, loss: 0.005133284255862236\n",
            "step: 410, loss: 0.047880083322525024\n",
            "step: 420, loss: 0.1688830852508545\n",
            "step: 430, loss: 0.004550556186586618\n",
            "step: 440, loss: 0.03607596084475517\n",
            "step: 450, loss: 0.004805159289389849\n",
            "step: 460, loss: 0.10423818975687027\n",
            "step: 470, loss: 0.06346560269594193\n",
            "step: 480, loss: 0.023746749386191368\n",
            "step: 490, loss: 0.018413856625556946\n",
            "step: 500, loss: 0.0007377452566288412\n",
            "step: 510, loss: 0.0037185123655945063\n",
            "step: 520, loss: 0.27414411306381226\n",
            "step: 530, loss: 0.01746106892824173\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9466357308584686, f1=0.9436749769159741, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05815211310982704\n",
            "step: 10, loss: 0.0036940020509064198\n",
            "step: 20, loss: 0.0030863790307193995\n",
            "step: 30, loss: 0.0007204034482128918\n",
            "step: 40, loss: 0.03243004158139229\n",
            "step: 50, loss: 0.0007259981357492507\n",
            "step: 60, loss: 0.0019685719162225723\n",
            "step: 70, loss: 0.0005169707001186907\n",
            "step: 80, loss: 0.0004246525058988482\n",
            "step: 90, loss: 0.0024709301069378853\n",
            "step: 100, loss: 0.0010374098783358932\n",
            "step: 110, loss: 0.0009109490201808512\n",
            "step: 120, loss: 0.022070197388529778\n",
            "step: 130, loss: 0.012900453992187977\n",
            "step: 140, loss: 0.004350211005657911\n",
            "step: 150, loss: 0.028986921533942223\n",
            "step: 160, loss: 0.04474835470318794\n",
            "step: 170, loss: 0.004844271577894688\n",
            "step: 180, loss: 0.010441748425364494\n",
            "step: 190, loss: 0.19190320372581482\n",
            "step: 200, loss: 0.0035069396253675222\n",
            "step: 210, loss: 0.004714600276201963\n",
            "step: 220, loss: 0.00784775149077177\n",
            "step: 230, loss: 0.0011061258846893907\n",
            "step: 240, loss: 0.011653969995677471\n",
            "step: 250, loss: 0.0020998925901949406\n",
            "step: 260, loss: 0.0074006798677146435\n",
            "step: 270, loss: 0.0008902983390726149\n",
            "step: 280, loss: 0.0016694720834493637\n",
            "step: 290, loss: 0.010291142389178276\n",
            "step: 300, loss: 0.02944730781018734\n",
            "step: 310, loss: 0.05608050897717476\n",
            "step: 320, loss: 0.0002582386659923941\n",
            "step: 330, loss: 0.00042752944864332676\n",
            "step: 340, loss: 0.0001282142475247383\n",
            "step: 350, loss: 0.003119983710348606\n",
            "step: 360, loss: 0.0007069393759593368\n",
            "step: 370, loss: 0.023803353309631348\n",
            "step: 380, loss: 0.005015337374061346\n",
            "step: 390, loss: 0.01849723607301712\n",
            "step: 400, loss: 0.022474156692624092\n",
            "step: 410, loss: 0.0008675775025039911\n",
            "step: 420, loss: 0.000890958821401\n",
            "step: 430, loss: 0.041735660284757614\n",
            "step: 440, loss: 0.002032829448580742\n",
            "step: 450, loss: 0.17954561114311218\n",
            "step: 460, loss: 0.004099463578313589\n",
            "step: 470, loss: 0.0027164986822754145\n",
            "step: 480, loss: 0.005386188626289368\n",
            "step: 490, loss: 0.006072087679058313\n",
            "step: 500, loss: 0.014838552102446556\n",
            "step: 510, loss: 0.1508990377187729\n",
            "step: 520, loss: 0.0032240785658359528\n",
            "step: 530, loss: 0.004359085112810135\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9478584729981377, f1=0.9376163873370577, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021644635125994682\n",
            "step: 10, loss: 0.0008458970696665347\n",
            "step: 20, loss: 0.009444255381822586\n",
            "step: 30, loss: 0.003038086462765932\n",
            "step: 40, loss: 0.0057996404357254505\n",
            "step: 50, loss: 0.03916357085108757\n",
            "step: 60, loss: 0.0025465500075370073\n",
            "step: 70, loss: 0.011877084150910378\n",
            "step: 80, loss: 0.0008351026917807758\n",
            "step: 90, loss: 0.0013574989279732108\n",
            "step: 100, loss: 0.0030996461864560843\n",
            "step: 110, loss: 0.0004198820679448545\n",
            "step: 120, loss: 0.002221280476078391\n",
            "step: 130, loss: 0.0003845289465971291\n",
            "step: 140, loss: 0.0008117405814118683\n",
            "step: 150, loss: 0.049897074699401855\n",
            "step: 160, loss: 0.0002940080303233117\n",
            "step: 170, loss: 0.009934872388839722\n",
            "step: 180, loss: 0.13220283389091492\n",
            "step: 190, loss: 0.01328936405479908\n",
            "step: 200, loss: 0.00018060543516185135\n",
            "step: 210, loss: 0.06819842755794525\n",
            "step: 220, loss: 0.0001379907043883577\n",
            "step: 230, loss: 0.0070346649736166\n",
            "step: 240, loss: 0.017064841464161873\n",
            "step: 250, loss: 0.13005836308002472\n",
            "step: 260, loss: 0.026841850951313972\n",
            "step: 270, loss: 0.0025652358308434486\n",
            "step: 280, loss: 0.00563808111473918\n",
            "step: 290, loss: 0.002940954640507698\n",
            "step: 300, loss: 0.0017788023687899113\n",
            "step: 310, loss: 0.0010103201493620872\n",
            "step: 320, loss: 0.0031299942638725042\n",
            "step: 330, loss: 0.0006117494776844978\n",
            "step: 340, loss: 0.0002457398804835975\n",
            "step: 350, loss: 0.0003928454243578017\n",
            "step: 360, loss: 0.0006733086192980409\n",
            "step: 370, loss: 0.0004154439375270158\n",
            "step: 380, loss: 0.011831841431558132\n",
            "step: 390, loss: 0.017211206257343292\n",
            "step: 400, loss: 0.029254641383886337\n",
            "step: 410, loss: 0.0019440613687038422\n",
            "step: 420, loss: 0.09341221302747726\n",
            "step: 430, loss: 0.007998713292181492\n",
            "step: 440, loss: 0.013896690681576729\n",
            "step: 450, loss: 0.012271150015294552\n",
            "step: 460, loss: 0.001511072157882154\n",
            "step: 470, loss: 0.1382066160440445\n",
            "step: 480, loss: 0.0016025914810597897\n",
            "step: 490, loss: 0.010583990253508091\n",
            "step: 500, loss: 0.0008356185280717909\n",
            "step: 510, loss: 0.0013795720878988504\n",
            "step: 520, loss: 0.004150224383920431\n",
            "step: 530, loss: 0.0024057822301983833\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9516279069767443, f1=0.9483960948396094, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001871276879683137\n",
            "step: 10, loss: 0.0024253069423139095\n",
            "step: 20, loss: 0.0016660174587741494\n",
            "step: 30, loss: 0.0008100740960799158\n",
            "step: 40, loss: 0.0003296906652394682\n",
            "step: 50, loss: 0.00025563116651028395\n",
            "step: 60, loss: 0.00043132115388289094\n",
            "step: 70, loss: 0.0022406606003642082\n",
            "step: 80, loss: 0.007075293455272913\n",
            "step: 90, loss: 0.0002534120576456189\n",
            "step: 100, loss: 0.0020379782654345036\n",
            "step: 110, loss: 0.0006572067504748702\n",
            "step: 120, loss: 0.0017595579847693443\n",
            "step: 130, loss: 0.0003076008288189769\n",
            "step: 140, loss: 0.015980619937181473\n",
            "step: 150, loss: 0.09489081799983978\n",
            "step: 160, loss: 0.00024872980429790914\n",
            "step: 170, loss: 0.014491516165435314\n",
            "step: 180, loss: 0.00034264501300640404\n",
            "step: 190, loss: 0.006839381996542215\n",
            "step: 200, loss: 0.0026626414619386196\n",
            "step: 210, loss: 0.2508850693702698\n",
            "step: 220, loss: 0.0033937639091163874\n",
            "step: 230, loss: 0.055090438574552536\n",
            "step: 240, loss: 0.0016477819299325347\n",
            "step: 250, loss: 0.0018970266683027148\n",
            "step: 260, loss: 0.0004539554938673973\n",
            "step: 270, loss: 0.07263660430908203\n",
            "step: 280, loss: 0.0015862053260207176\n",
            "step: 290, loss: 0.0012471158988773823\n",
            "step: 300, loss: 7.215363439172506e-05\n",
            "step: 310, loss: 0.0002991874353028834\n",
            "step: 320, loss: 0.00019750256615225226\n",
            "step: 330, loss: 0.0006047209026291966\n",
            "step: 340, loss: 0.042326439172029495\n",
            "step: 350, loss: 9.352238703286275e-05\n",
            "step: 360, loss: 0.017029568552970886\n",
            "step: 370, loss: 0.03479698672890663\n",
            "step: 380, loss: 6.325679714791477e-05\n",
            "step: 390, loss: 0.0013989099534228444\n",
            "step: 400, loss: 0.0021067315246909857\n",
            "step: 410, loss: 0.014149002730846405\n",
            "step: 420, loss: 0.001018937909975648\n",
            "step: 430, loss: 0.027536818757653236\n",
            "step: 440, loss: 0.0102397371083498\n",
            "step: 450, loss: 0.001375120016746223\n",
            "step: 460, loss: 0.0015729565639048815\n",
            "step: 470, loss: 0.014106407761573792\n",
            "step: 480, loss: 0.0024094569962471724\n",
            "step: 490, loss: 0.08005446940660477\n",
            "step: 500, loss: 0.001553592155687511\n",
            "step: 510, loss: 0.002071579685434699\n",
            "step: 520, loss: 0.0004050632705911994\n",
            "step: 530, loss: 0.0013559508370235562\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9541029207232267, f1=0.9478060046189377, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014590873615816236\n",
            "step: 10, loss: 0.0013663733843713999\n",
            "step: 20, loss: 0.00033509134664200246\n",
            "step: 30, loss: 0.0925876572728157\n",
            "step: 40, loss: 0.0005362362135201693\n",
            "step: 50, loss: 0.0005320553900673985\n",
            "step: 60, loss: 0.0016445884248241782\n",
            "step: 70, loss: 0.05292157828807831\n",
            "step: 80, loss: 0.06996151059865952\n",
            "step: 90, loss: 0.015721336007118225\n",
            "step: 100, loss: 0.019876981154084206\n",
            "step: 110, loss: 0.003441284643486142\n",
            "step: 120, loss: 0.0007430510013364255\n",
            "step: 130, loss: 0.0003606991667766124\n",
            "step: 140, loss: 0.0003971426631323993\n",
            "step: 150, loss: 0.00042121735168620944\n",
            "step: 160, loss: 0.13109421730041504\n",
            "step: 170, loss: 0.022091900929808617\n",
            "step: 180, loss: 0.005753659177571535\n",
            "step: 190, loss: 0.00023140120902098715\n",
            "step: 200, loss: 0.00040039524901658297\n",
            "step: 210, loss: 0.00022884267673362046\n",
            "step: 220, loss: 0.0011472837068140507\n",
            "step: 230, loss: 0.00865130964666605\n",
            "step: 240, loss: 0.00024852954084053636\n",
            "step: 250, loss: 0.021723993122577667\n",
            "step: 260, loss: 0.00020285767095629126\n",
            "step: 270, loss: 0.004636791534721851\n",
            "step: 280, loss: 0.001996305538341403\n",
            "step: 290, loss: 0.00012760108802467585\n",
            "step: 300, loss: 0.0017894773045554757\n",
            "step: 310, loss: 0.20444045960903168\n",
            "step: 320, loss: 0.00996061135083437\n",
            "step: 330, loss: 0.00046684587141498923\n",
            "step: 340, loss: 0.0003875379334203899\n",
            "step: 350, loss: 0.004593688528984785\n",
            "step: 360, loss: 0.0001120672604884021\n",
            "step: 370, loss: 0.00010349402873544022\n",
            "step: 380, loss: 0.000788002391345799\n",
            "step: 390, loss: 0.00033233032445423305\n",
            "step: 400, loss: 0.0013024525251239538\n",
            "step: 410, loss: 0.0010223948629572988\n",
            "step: 420, loss: 0.00011796918988693506\n",
            "step: 430, loss: 0.00034753093495965004\n",
            "step: 440, loss: 6.567218224518001e-05\n",
            "step: 450, loss: 0.013313453644514084\n",
            "step: 460, loss: 7.924372766865417e-05\n",
            "step: 470, loss: 5.939418406342156e-05\n",
            "step: 480, loss: 0.00012202688958495855\n",
            "step: 490, loss: 0.046264518052339554\n",
            "step: 500, loss: 0.00012499201693572104\n",
            "step: 510, loss: 7.699980778852478e-05\n",
            "step: 520, loss: 0.02785784751176834\n",
            "step: 530, loss: 0.050598837435245514\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9487771112136594, f1=0.9446730681298583, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005154603626579046\n",
            "step: 10, loss: 0.002204491524025798\n",
            "step: 20, loss: 0.003984004724770784\n",
            "step: 30, loss: 0.002624194836243987\n",
            "step: 40, loss: 0.00027886719908565283\n",
            "step: 50, loss: 0.00019851946854032576\n",
            "step: 60, loss: 0.003185539273545146\n",
            "step: 70, loss: 7.155588536988944e-05\n",
            "step: 80, loss: 5.4848056606715545e-05\n",
            "step: 90, loss: 0.0023072687909007072\n",
            "step: 100, loss: 0.006977993994951248\n",
            "step: 110, loss: 0.014193912036716938\n",
            "step: 120, loss: 3.931088212993927e-05\n",
            "step: 130, loss: 0.0015901810256764293\n",
            "step: 140, loss: 0.0005083174328319728\n",
            "step: 150, loss: 0.0018454118398949504\n",
            "step: 160, loss: 0.004543599672615528\n",
            "step: 170, loss: 0.00020561114070005715\n",
            "step: 180, loss: 0.00019909856200683862\n",
            "step: 190, loss: 0.00011478488158900291\n",
            "step: 200, loss: 0.005447934381663799\n",
            "step: 210, loss: 0.016171418130397797\n",
            "step: 220, loss: 0.0014931376790627837\n",
            "step: 230, loss: 0.0008402678067795932\n",
            "step: 240, loss: 0.008943093940615654\n",
            "step: 250, loss: 0.0005150712677277625\n",
            "step: 260, loss: 0.001655848347581923\n",
            "step: 270, loss: 0.008428161963820457\n",
            "step: 280, loss: 0.0071356333792209625\n",
            "step: 290, loss: 0.0007028339896351099\n",
            "step: 300, loss: 0.004636804573237896\n",
            "step: 310, loss: 0.004956393502652645\n",
            "step: 320, loss: 0.0034769924823194742\n",
            "step: 330, loss: 0.017812877893447876\n",
            "step: 340, loss: 0.056476593017578125\n",
            "step: 350, loss: 0.0016676264349371195\n",
            "step: 360, loss: 8.68847273522988e-05\n",
            "step: 370, loss: 0.002061012201011181\n",
            "step: 380, loss: 0.001401590183377266\n",
            "step: 390, loss: 7.164628186728805e-05\n",
            "step: 400, loss: 0.00017973626381717622\n",
            "step: 410, loss: 0.0003341316187288612\n",
            "step: 420, loss: 0.015398350544273853\n",
            "step: 430, loss: 0.0004567840078379959\n",
            "step: 440, loss: 0.0012672626180574298\n",
            "step: 450, loss: 0.00016489824338350445\n",
            "step: 460, loss: 0.0006228192360140383\n",
            "step: 470, loss: 0.0010450065601617098\n",
            "step: 480, loss: 0.00029489968437701464\n",
            "step: 490, loss: 0.0009639033814892173\n",
            "step: 500, loss: 0.0001360874157398939\n",
            "step: 510, loss: 6.601821951335296e-05\n",
            "step: 520, loss: 0.00022017641458660364\n",
            "step: 530, loss: 0.0008193730027414858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.949456778460085, f1=0.9446284902981543, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016712395881768316\n",
            "step: 10, loss: 0.001108388532884419\n",
            "step: 20, loss: 6.169991684146225e-05\n",
            "step: 30, loss: 0.003336873836815357\n",
            "step: 40, loss: 7.890420965850353e-05\n",
            "step: 50, loss: 4.8286980018019676e-05\n",
            "step: 60, loss: 2.72929228231078e-05\n",
            "step: 70, loss: 0.00014772408758290112\n",
            "step: 80, loss: 0.0006570580299012363\n",
            "step: 90, loss: 0.000689181499183178\n",
            "step: 100, loss: 0.00035016078618355095\n",
            "step: 110, loss: 6.298408698057756e-05\n",
            "step: 120, loss: 0.0007636989466845989\n",
            "step: 130, loss: 8.739429904380813e-05\n",
            "step: 140, loss: 5.567731204791926e-05\n",
            "step: 150, loss: 4.3596417526714504e-05\n",
            "step: 160, loss: 0.0004071332514286041\n",
            "step: 170, loss: 0.0018879021517932415\n",
            "step: 180, loss: 0.00021247727272566408\n",
            "step: 190, loss: 0.0003635639150161296\n",
            "step: 200, loss: 7.716139953117818e-05\n",
            "step: 210, loss: 8.95224220585078e-05\n",
            "step: 220, loss: 0.059797532856464386\n",
            "step: 230, loss: 9.809842595132068e-05\n",
            "step: 240, loss: 0.0022227393928915262\n",
            "step: 250, loss: 0.0006535633001476526\n",
            "step: 260, loss: 0.00426107831299305\n",
            "step: 270, loss: 0.002698234049603343\n",
            "step: 280, loss: 0.0005045172874815762\n",
            "step: 290, loss: 0.000450472958618775\n",
            "step: 300, loss: 0.00043504475615918636\n",
            "step: 310, loss: 0.2195265144109726\n",
            "step: 320, loss: 0.0010880022309720516\n",
            "step: 330, loss: 0.0002550424251239747\n",
            "step: 340, loss: 0.0011817245976999402\n",
            "step: 350, loss: 0.000675079645588994\n",
            "step: 360, loss: 0.0005276550655253232\n",
            "step: 370, loss: 0.0015056036645546556\n",
            "step: 380, loss: 0.001693460624665022\n",
            "step: 390, loss: 0.0012694016331806779\n",
            "step: 400, loss: 0.0007918644696474075\n",
            "step: 410, loss: 0.0011540402192622423\n",
            "step: 420, loss: 0.001110742217861116\n",
            "step: 430, loss: 0.0027958801947534084\n",
            "step: 440, loss: 0.00031588852289132774\n",
            "step: 450, loss: 0.001616118592210114\n",
            "step: 460, loss: 0.0006350466283038259\n",
            "step: 470, loss: 0.0009607558022253215\n",
            "step: 480, loss: 0.0017280633328482509\n",
            "step: 490, loss: 0.00020071477047167718\n",
            "step: 500, loss: 0.0017837831983342767\n",
            "step: 510, loss: 0.006115948315709829\n",
            "step: 520, loss: 0.001378362183459103\n",
            "step: 530, loss: 0.013242006301879883\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9517625231910947, f1=0.9425393883225208, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024227635003626347\n",
            "step: 10, loss: 0.005571101792156696\n",
            "step: 20, loss: 0.001037565409205854\n",
            "step: 30, loss: 7.208088936749846e-05\n",
            "step: 40, loss: 0.0013393587432801723\n",
            "step: 50, loss: 8.206696656998247e-05\n",
            "step: 60, loss: 0.0001334300177404657\n",
            "step: 70, loss: 0.0012220091884955764\n",
            "step: 80, loss: 0.00012434207019396126\n",
            "step: 90, loss: 0.00024938557180576026\n",
            "step: 100, loss: 0.24834595620632172\n",
            "step: 110, loss: 0.00015885330503806472\n",
            "step: 120, loss: 0.002963718492537737\n",
            "step: 130, loss: 0.0017172226216644049\n",
            "step: 140, loss: 0.0015235078753903508\n",
            "step: 150, loss: 0.00018893145897891372\n",
            "step: 160, loss: 0.00321813952177763\n",
            "step: 170, loss: 0.00012864005111623555\n",
            "step: 180, loss: 0.0001250109780812636\n",
            "step: 190, loss: 0.0037718256935477257\n",
            "step: 200, loss: 0.0022034042049199343\n",
            "step: 210, loss: 0.00013363553443923593\n",
            "step: 220, loss: 0.00012392857752274722\n",
            "step: 230, loss: 9.175492596114054e-05\n",
            "step: 240, loss: 0.03035084716975689\n",
            "step: 250, loss: 9.31762988329865e-05\n",
            "step: 260, loss: 0.0007306287297978997\n",
            "step: 270, loss: 0.0009789887117221951\n",
            "step: 280, loss: 0.0003379075787961483\n",
            "step: 290, loss: 0.0005028376472182572\n",
            "step: 300, loss: 0.0038800423499196768\n",
            "step: 310, loss: 0.0001849018008215353\n",
            "step: 320, loss: 0.007379262242466211\n",
            "step: 330, loss: 0.0018461920553818345\n",
            "step: 340, loss: 0.002155248075723648\n",
            "step: 350, loss: 0.000538715859875083\n",
            "step: 360, loss: 0.0004671070200856775\n",
            "step: 370, loss: 6.134924478828907e-05\n",
            "step: 380, loss: 0.002244328847154975\n",
            "step: 390, loss: 0.0003985915973316878\n",
            "step: 400, loss: 7.652290514670312e-05\n",
            "step: 410, loss: 0.0032637265976518393\n",
            "step: 420, loss: 0.00907284114509821\n",
            "step: 430, loss: 0.0004450421838555485\n",
            "step: 440, loss: 0.0012329414021223783\n",
            "step: 450, loss: 0.0002187084755860269\n",
            "step: 460, loss: 0.00010212411871179938\n",
            "step: 470, loss: 0.0015876634279266\n",
            "step: 480, loss: 0.0001737951533868909\n",
            "step: 490, loss: 8.915551734389737e-05\n",
            "step: 500, loss: 5.059749310021289e-05\n",
            "step: 510, loss: 0.0050760312005877495\n",
            "step: 520, loss: 0.0034069251269102097\n",
            "step: 530, loss: 0.004061132203787565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9539748953974895, f1=0.9434137291280148, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.830186273669824e-05\n",
            "step: 10, loss: 9.411084465682507e-05\n",
            "step: 20, loss: 0.0004892387660220265\n",
            "step: 30, loss: 6.0255901189520955e-05\n",
            "step: 40, loss: 0.006460313219577074\n",
            "step: 50, loss: 0.006922323722392321\n",
            "step: 60, loss: 9.573811257723719e-05\n",
            "step: 70, loss: 6.839606794528663e-05\n",
            "step: 80, loss: 0.00043973367428407073\n",
            "step: 90, loss: 0.0027384802233427763\n",
            "step: 100, loss: 6.0726564697688445e-05\n",
            "step: 110, loss: 4.2267209209967405e-05\n",
            "step: 120, loss: 8.759919728618115e-05\n",
            "step: 130, loss: 4.0928793168859556e-05\n",
            "step: 140, loss: 7.679215195821598e-05\n",
            "step: 150, loss: 0.0016473227879032493\n",
            "step: 160, loss: 0.0002094705996569246\n",
            "step: 170, loss: 0.0008506203885190189\n",
            "step: 180, loss: 0.0002066524320980534\n",
            "step: 190, loss: 0.00010913913138210773\n",
            "step: 200, loss: 8.770901331445202e-05\n",
            "step: 210, loss: 5.7128785556415096e-05\n",
            "step: 220, loss: 5.355707980925217e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 230, loss: 0.0011349869892001152\n",
            "step: 240, loss: 0.0008824800024740398\n",
            "step: 250, loss: 0.0011625366751104593\n",
            "step: 260, loss: 0.000490404199808836\n",
            "step: 270, loss: 5.6228451285278425e-05\n",
            "step: 280, loss: 5.114030864206143e-05\n",
            "step: 290, loss: 7.762199675198644e-05\n",
            "step: 300, loss: 5.413110557128675e-05\n",
            "step: 310, loss: 6.257439235923812e-05\n",
            "step: 320, loss: 6.170599954202771e-05\n",
            "step: 330, loss: 6.205507816048339e-05\n",
            "step: 340, loss: 0.003641868242993951\n",
            "step: 350, loss: 4.0332164644496515e-05\n",
            "step: 360, loss: 0.1254616379737854\n",
            "step: 370, loss: 8.685739885549992e-05\n",
            "step: 380, loss: 0.0005734653677791357\n",
            "step: 390, loss: 0.00022973031445872039\n",
            "step: 400, loss: 0.0014278801390901208\n",
            "step: 410, loss: 0.0003534159332048148\n",
            "step: 420, loss: 7.525015826104209e-05\n",
            "step: 430, loss: 0.0002881059190258384\n",
            "step: 440, loss: 8.998671546578407e-05\n",
            "step: 450, loss: 0.00013205318828113377\n",
            "step: 460, loss: 0.0037152059376239777\n",
            "step: 470, loss: 0.001433345372788608\n",
            "step: 480, loss: 4.1608971514506266e-05\n",
            "step: 490, loss: 6.262358510866761e-05\n",
            "step: 500, loss: 4.408536187838763e-05\n",
            "step: 510, loss: 7.556454511359334e-05\n",
            "step: 520, loss: 5.748072362621315e-05\n",
            "step: 530, loss: 5.992351725581102e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9506641366223909, f1=0.9435215946843853, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009015500545501709\n",
            "step: 10, loss: 0.0004867248353548348\n",
            "step: 20, loss: 6.699713412672281e-05\n",
            "step: 30, loss: 7.522809755755588e-05\n",
            "step: 40, loss: 0.0005618127179332078\n",
            "step: 50, loss: 0.00012290265294723213\n",
            "step: 60, loss: 6.720302917528898e-05\n",
            "step: 70, loss: 8.55637263157405e-05\n",
            "step: 80, loss: 8.747982792556286e-05\n",
            "step: 90, loss: 4.136948336963542e-05\n",
            "step: 100, loss: 5.8451019867789e-05\n",
            "step: 110, loss: 5.781775325885974e-05\n",
            "step: 120, loss: 6.678348290733993e-05\n",
            "step: 130, loss: 0.00030932543450035155\n",
            "step: 140, loss: 8.712247654329985e-05\n",
            "step: 150, loss: 0.0013287357287481427\n",
            "step: 160, loss: 8.78383289091289e-05\n",
            "step: 170, loss: 6.238237983779982e-05\n",
            "step: 180, loss: 6.957594450796023e-05\n",
            "step: 190, loss: 4.691056165029295e-05\n",
            "step: 200, loss: 8.076983067439869e-05\n",
            "step: 210, loss: 9.730157034937292e-05\n",
            "step: 220, loss: 3.9540744182886556e-05\n",
            "step: 230, loss: 4.8293837608071044e-05\n",
            "step: 240, loss: 0.08499671518802643\n",
            "step: 250, loss: 0.0010021600173786283\n",
            "step: 260, loss: 0.0002411987807136029\n",
            "step: 270, loss: 0.00033341324888169765\n",
            "step: 280, loss: 6.534165004268289e-05\n",
            "step: 290, loss: 5.082254210719839e-05\n",
            "step: 300, loss: 0.00011373471352271736\n",
            "step: 310, loss: 4.7437468310818076e-05\n",
            "step: 320, loss: 0.00013565525296144187\n",
            "step: 330, loss: 0.0021012118086218834\n",
            "step: 340, loss: 0.0010242452844977379\n",
            "step: 350, loss: 6.358598329825327e-05\n",
            "step: 360, loss: 0.0013820755993947387\n",
            "step: 370, loss: 0.0018293465254828334\n",
            "step: 380, loss: 0.0011868159053847194\n",
            "step: 390, loss: 0.00023364427033811808\n",
            "step: 400, loss: 0.0038938079960644245\n",
            "step: 410, loss: 4.388467641547322e-05\n",
            "step: 420, loss: 6.220286741154268e-05\n",
            "step: 430, loss: 0.0033587513025850058\n",
            "step: 440, loss: 0.0013240452390164137\n",
            "step: 450, loss: 0.00024875413510017097\n",
            "step: 460, loss: 0.0027842307463288307\n",
            "step: 470, loss: 9.31407994357869e-05\n",
            "step: 480, loss: 0.00011733771680155769\n",
            "step: 490, loss: 8.211256499635056e-05\n",
            "step: 500, loss: 0.0021408561151474714\n",
            "step: 510, loss: 0.0007657846435904503\n",
            "step: 520, loss: 0.0016840153839439154\n",
            "step: 530, loss: 0.0037918591406196356\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.954060324825986, f1=0.9499536607970344, best_f1=0.948356807511737\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001638360321521759\n",
            "step: 10, loss: 0.00218755379319191\n",
            "step: 20, loss: 0.002839829074218869\n",
            "step: 30, loss: 0.009150831960141659\n",
            "step: 40, loss: 8.263331255875528e-05\n",
            "step: 50, loss: 0.0009573168936185539\n",
            "step: 60, loss: 0.0005823491956107318\n",
            "step: 70, loss: 6.69145883875899e-05\n",
            "step: 80, loss: 5.749686897615902e-05\n",
            "step: 90, loss: 0.0001323867036262527\n",
            "step: 100, loss: 2.6245535991620272e-05\n",
            "step: 110, loss: 0.0012305309064686298\n",
            "step: 120, loss: 5.064615106675774e-05\n",
            "step: 130, loss: 0.0003453497192822397\n",
            "step: 140, loss: 4.9063914048019797e-05\n",
            "step: 150, loss: 9.199138730764389e-05\n",
            "step: 160, loss: 7.292097143363208e-05\n",
            "step: 170, loss: 3.6994784750277176e-05\n",
            "step: 180, loss: 7.710505451541394e-05\n",
            "step: 190, loss: 0.0007931714644655585\n",
            "step: 200, loss: 0.0011426301207393408\n",
            "step: 210, loss: 0.00040785191231407225\n",
            "step: 220, loss: 8.074868674157187e-05\n",
            "step: 230, loss: 8.675752906128764e-05\n",
            "step: 240, loss: 5.08676930621732e-05\n",
            "step: 250, loss: 8.481126133119687e-05\n",
            "step: 260, loss: 3.929094600607641e-05\n",
            "step: 270, loss: 0.00796664971858263\n",
            "step: 280, loss: 4.413661008584313e-05\n",
            "step: 290, loss: 0.00018618570175021887\n",
            "step: 300, loss: 6.949021189939231e-05\n",
            "step: 310, loss: 0.054995160549879074\n",
            "step: 320, loss: 6.246293196454644e-05\n",
            "step: 330, loss: 7.908639963716269e-05\n",
            "step: 340, loss: 0.00011789743439294398\n",
            "step: 350, loss: 0.004937917459756136\n",
            "step: 360, loss: 0.00018572768021840602\n",
            "step: 370, loss: 0.00010256384848617017\n",
            "step: 380, loss: 6.646917609032243e-05\n",
            "step: 390, loss: 6.588001997442916e-05\n",
            "step: 400, loss: 0.0004179595271125436\n",
            "step: 410, loss: 6.8118424678687e-05\n",
            "step: 420, loss: 4.4363780034473166e-05\n",
            "step: 430, loss: 3.409734199522063e-05\n",
            "step: 440, loss: 8.076781523413956e-05\n",
            "step: 450, loss: 0.0052860495634377\n",
            "step: 460, loss: 0.0033103099558502436\n",
            "step: 470, loss: 7.977543282322586e-05\n",
            "step: 480, loss: 0.002016331534832716\n",
            "step: 490, loss: 0.0004226517630741\n",
            "step: 500, loss: 0.0008727000094950199\n",
            "step: 510, loss: 7.306714542210102e-05\n",
            "step: 520, loss: 0.00016918468463700265\n",
            "step: 530, loss: 4.436639937921427e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9530887134231305, f1=0.9500462534690102, best_f1=0.948356807511737\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:34, 166.19it/s]\n",
            "load_f1 = 0.9575361642557162\n",
            "real_f1 = 0.9589169000933706\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 140.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DITTO"
      ],
      "metadata": {
        "id": "pnXzXaaYhstq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO STRUCTURED"
      ],
      "metadata": {
        "id": "r23AxFPnhstr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07a8c180-ec13-41c0-fb3d-143c2e9ec390"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4620782136917114\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.2947368421052632, f1=0.27184466019417475, best_f1=0.27184466019417475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4355957508087158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.2947368421052632, f1=0.2692307692307693, best_f1=0.27184466019417475\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3897997736930847\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.48, f1=0.35294117647058826, best_f1=0.35294117647058826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28581491112709045\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5217391304347825, f1=0.46153846153846156, best_f1=0.46153846153846156\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24815836548805237\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8275862068965518, f1=0.6666666666666665, best_f1=0.6666666666666665\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15010076761245728\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8275862068965518, f1=0.7222222222222223, best_f1=0.6666666666666665\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.27455177903175354\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.896551724137931, f1=0.7333333333333334, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16902495920658112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8235294117647058, f1=0.717948717948718, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1791723519563675\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.888888888888889, f1=0.7200000000000001, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16954712569713593\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.888888888888889, f1=0.7692307692307692, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0475720539689064\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8666666666666666, f1=0.9032258064516129, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016384422779083252\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8666666666666666, f1=0.8484848484848484, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012074840255081654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8666666666666666, f1=0.8750000000000001, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006889475509524345\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8666666666666666, f1=0.8750000000000001, best_f1=0.7333333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012365891598165035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8666666666666666, f1=0.8750000000000001, best_f1=0.7333333333333334\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 85253.89it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7857142857142857\n",
            "real_f1 = 0.8387096774193549\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 142.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "6Bang43Lhsts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d16571f-def5-4644-8096-163616596c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 415kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 6.04MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 2.96MB/s]\n",
            "Downloading: 100% 501M/501M [00:07<00:00, 63.0MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5571184754371643\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4547971785068512\n",
            "step: 20, loss: 0.5089476108551025\n",
            "step: 30, loss: 0.326252281665802\n",
            "step: 40, loss: 0.3525920808315277\n",
            "step: 50, loss: 0.43020012974739075\n",
            "step: 60, loss: 0.13975277543067932\n",
            "step: 70, loss: 0.09474581480026245\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.210772305727005\n",
            "step: 90, loss: 0.14980663359165192\n",
            "step: 100, loss: 0.0712539404630661\n",
            "step: 110, loss: 0.026107678189873695\n",
            "step: 120, loss: 0.06513062119483948\n",
            "step: 130, loss: 0.014936202205717564\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 140, loss: 0.12716346979141235\n",
            "step: 150, loss: 0.09087912738323212\n",
            "step: 160, loss: 0.01703525334596634\n",
            "step: 170, loss: 0.035207897424697876\n",
            "step: 180, loss: 0.04009120166301727\n",
            "step: 190, loss: 0.026902396231889725\n",
            "step: 200, loss: 0.17708715796470642\n",
            "step: 210, loss: 0.07163768261671066\n",
            "step: 220, loss: 0.029884839430451393\n",
            "step: 230, loss: 0.1227019727230072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9596510359869138, f1=0.9551912568306011, best_f1=0.9551912568306011\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0056521412916481495\n",
            "step: 10, loss: 0.06425651162862778\n",
            "step: 20, loss: 0.01375807449221611\n",
            "step: 30, loss: 0.02596341073513031\n",
            "step: 40, loss: 0.00403363024815917\n",
            "step: 50, loss: 0.0017544366419315338\n",
            "step: 60, loss: 0.005841127596795559\n",
            "step: 70, loss: 0.026557806879281998\n",
            "step: 80, loss: 0.003974668215960264\n",
            "step: 90, loss: 0.0046739825047552586\n",
            "step: 100, loss: 0.004343666601926088\n",
            "step: 110, loss: 0.0038396401796489954\n",
            "step: 120, loss: 0.00966495368629694\n",
            "step: 130, loss: 0.010035661049187183\n",
            "step: 140, loss: 0.026336681097745895\n",
            "step: 150, loss: 0.16441752016544342\n",
            "step: 160, loss: 0.010978887788951397\n",
            "step: 170, loss: 0.027699556201696396\n",
            "step: 180, loss: 0.007507207337766886\n",
            "step: 190, loss: 0.09999266266822815\n",
            "step: 200, loss: 0.007357250899076462\n",
            "step: 210, loss: 0.0065422882325947285\n",
            "step: 220, loss: 0.0009187002433463931\n",
            "step: 230, loss: 0.0008606495684944093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9809203142536477, f1=0.9752808988764046, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006977270822972059\n",
            "step: 10, loss: 0.008546289056539536\n",
            "step: 20, loss: 0.03132907673716545\n",
            "step: 30, loss: 0.0009501964086666703\n",
            "step: 40, loss: 0.006407429929822683\n",
            "step: 50, loss: 0.07900890707969666\n",
            "step: 60, loss: 0.002140816766768694\n",
            "step: 70, loss: 0.028745412826538086\n",
            "step: 80, loss: 0.001043857540935278\n",
            "step: 90, loss: 0.019408343359827995\n",
            "step: 100, loss: 0.009219052270054817\n",
            "step: 110, loss: 0.0015479808207601309\n",
            "step: 120, loss: 0.0008105872548185289\n",
            "step: 130, loss: 0.0014952468918636441\n",
            "step: 140, loss: 0.00150591554120183\n",
            "step: 150, loss: 0.01892905868589878\n",
            "step: 160, loss: 0.0020774570293724537\n",
            "step: 170, loss: 0.0011529568582773209\n",
            "step: 180, loss: 0.006165739148855209\n",
            "step: 190, loss: 0.040015485137701035\n",
            "step: 200, loss: 0.015825865790247917\n",
            "step: 210, loss: 0.0015911075752228498\n",
            "step: 220, loss: 0.004231204744428396\n",
            "step: 230, loss: 0.008059952408075333\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9741282339707535, f1=0.9751693002257337, best_f1=0.9752808988764046\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013377897441387177\n",
            "step: 10, loss: 0.03867235779762268\n",
            "step: 20, loss: 0.009113507345318794\n",
            "step: 30, loss: 0.0026857987977564335\n",
            "step: 40, loss: 0.07505045086145401\n",
            "step: 50, loss: 0.02889716625213623\n",
            "step: 60, loss: 0.0021421120036393404\n",
            "step: 70, loss: 0.0057172649540007114\n",
            "step: 80, loss: 0.0006819369737058878\n",
            "step: 90, loss: 0.05572260171175003\n",
            "step: 100, loss: 0.005016495008021593\n",
            "step: 110, loss: 0.0004709097556769848\n",
            "step: 120, loss: 0.0011975881643593311\n",
            "step: 130, loss: 0.018140651285648346\n",
            "step: 140, loss: 0.00038166515878401697\n",
            "step: 150, loss: 0.0004175025096628815\n",
            "step: 160, loss: 0.000444899225840345\n",
            "step: 170, loss: 0.05735538899898529\n",
            "step: 180, loss: 0.13349150121212006\n",
            "step: 190, loss: 0.0033524902537465096\n",
            "step: 200, loss: 0.038366757333278656\n",
            "step: 210, loss: 0.0018654598388820887\n",
            "step: 220, loss: 0.0008091090712696314\n",
            "step: 230, loss: 0.00241085235029459\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9842696629213483, f1=0.9853107344632768, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001152229611761868\n",
            "step: 10, loss: 0.02755187824368477\n",
            "step: 20, loss: 0.021301133558154106\n",
            "step: 30, loss: 0.0007139646331779659\n",
            "step: 40, loss: 0.0009433482773602009\n",
            "step: 50, loss: 0.0024546703789383173\n",
            "step: 60, loss: 0.0023304447531700134\n",
            "step: 70, loss: 0.0006627184920944273\n",
            "step: 80, loss: 0.00918598286807537\n",
            "step: 90, loss: 0.06554576009511948\n",
            "step: 100, loss: 0.0006398437544703484\n",
            "step: 110, loss: 0.00048038209206424654\n",
            "step: 120, loss: 0.003939863760024309\n",
            "step: 130, loss: 0.00041748667717911303\n",
            "step: 140, loss: 0.0014255708083510399\n",
            "step: 150, loss: 0.02731780894100666\n",
            "step: 160, loss: 0.001101775444112718\n",
            "step: 170, loss: 0.003941123373806477\n",
            "step: 180, loss: 0.0008150412468239665\n",
            "step: 190, loss: 0.11710339784622192\n",
            "step: 200, loss: 0.02210455946624279\n",
            "step: 210, loss: 0.025139786303043365\n",
            "step: 220, loss: 0.0015924365725368261\n",
            "step: 230, loss: 0.003547766013070941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9854423292273236, f1=0.9841986455981941, best_f1=0.9841986455981941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007830924587324262\n",
            "step: 10, loss: 0.0017798448679968715\n",
            "step: 20, loss: 0.00141806248575449\n",
            "step: 30, loss: 0.005091627594083548\n",
            "step: 40, loss: 0.000475464592454955\n",
            "step: 50, loss: 0.0005190116353332996\n",
            "step: 60, loss: 0.04627177119255066\n",
            "step: 70, loss: 0.0028734535444527864\n",
            "step: 80, loss: 0.020281950011849403\n",
            "step: 90, loss: 0.0184311643242836\n",
            "step: 100, loss: 0.0006286168936640024\n",
            "step: 110, loss: 0.039056241512298584\n",
            "step: 120, loss: 0.00020884719560854137\n",
            "step: 130, loss: 0.010018006898462772\n",
            "step: 140, loss: 0.00024799490347504616\n",
            "step: 150, loss: 0.007183757144957781\n",
            "step: 160, loss: 0.017647387459874153\n",
            "step: 170, loss: 0.0003112497797701508\n",
            "step: 180, loss: 0.0008674376294948161\n",
            "step: 190, loss: 0.002748744795098901\n",
            "step: 200, loss: 0.056238219141960144\n",
            "step: 210, loss: 0.02440202794969082\n",
            "step: 220, loss: 0.006509333848953247\n",
            "step: 230, loss: 0.0014263774501159787\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9854096520763187, f1=0.9852774631936579, best_f1=0.9841986455981941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00281129009090364\n",
            "step: 10, loss: 0.00244984426535666\n",
            "step: 20, loss: 0.0038927439600229263\n",
            "step: 30, loss: 0.0018283601384609938\n",
            "step: 40, loss: 0.003142197849228978\n",
            "step: 50, loss: 0.0006086511421017349\n",
            "step: 60, loss: 0.00021295329497661442\n",
            "step: 70, loss: 0.00017652598035056144\n",
            "step: 80, loss: 0.0008298372849822044\n",
            "step: 90, loss: 0.07933753728866577\n",
            "step: 100, loss: 0.0009421510621905327\n",
            "step: 110, loss: 0.0005270606488920748\n",
            "step: 120, loss: 0.002016794867813587\n",
            "step: 130, loss: 0.0031714087817817926\n",
            "step: 140, loss: 0.0006583920912817121\n",
            "step: 150, loss: 0.013137124478816986\n",
            "step: 160, loss: 0.0008367023547179997\n",
            "step: 170, loss: 0.020539700984954834\n",
            "step: 180, loss: 0.0007127450662665069\n",
            "step: 190, loss: 0.0003582545614335686\n",
            "step: 200, loss: 0.01257602870464325\n",
            "step: 210, loss: 0.08922228217124939\n",
            "step: 220, loss: 0.0006515348213724792\n",
            "step: 230, loss: 0.007610222790390253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9887640449438202, f1=0.9864559819413092, best_f1=0.9864559819413092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005448139738291502\n",
            "step: 10, loss: 0.003050890751183033\n",
            "step: 20, loss: 0.000932756345719099\n",
            "step: 30, loss: 0.0005268182139843702\n",
            "step: 40, loss: 0.028185244649648666\n",
            "step: 50, loss: 0.0008469106396660209\n",
            "step: 60, loss: 0.0007340650190599263\n",
            "step: 70, loss: 0.0002119774726452306\n",
            "step: 80, loss: 0.03455161303281784\n",
            "step: 90, loss: 0.0005734125734306872\n",
            "step: 100, loss: 0.0006106326472945511\n",
            "step: 110, loss: 0.0002901660627685487\n",
            "step: 120, loss: 0.0004644187865778804\n",
            "step: 130, loss: 0.0036196173168718815\n",
            "step: 140, loss: 0.0007525988039560616\n",
            "step: 150, loss: 0.04890598729252815\n",
            "step: 160, loss: 0.0051281931810081005\n",
            "step: 170, loss: 0.06325550377368927\n",
            "step: 180, loss: 0.0004644868313334882\n",
            "step: 190, loss: 0.006105493288487196\n",
            "step: 200, loss: 0.0037512679118663073\n",
            "step: 210, loss: 0.0034380434080958366\n",
            "step: 220, loss: 0.0004930155701003969\n",
            "step: 230, loss: 0.0003075927961617708\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.987736900780379, f1=0.9810055865921787, best_f1=0.9864559819413092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005357021582312882\n",
            "step: 10, loss: 0.0005307900719344616\n",
            "step: 20, loss: 0.0011159003479406238\n",
            "step: 30, loss: 0.0006866385228931904\n",
            "step: 40, loss: 0.002860149135813117\n",
            "step: 50, loss: 0.0006506055360659957\n",
            "step: 60, loss: 0.00016089485143311322\n",
            "step: 70, loss: 0.025853270664811134\n",
            "step: 80, loss: 0.00040461920434609056\n",
            "step: 90, loss: 0.04620451480150223\n",
            "step: 100, loss: 0.000157383838086389\n",
            "step: 110, loss: 0.0004456256574485451\n",
            "step: 120, loss: 0.07638388872146606\n",
            "step: 130, loss: 0.0005516470409929752\n",
            "step: 140, loss: 0.0037901420146226883\n",
            "step: 150, loss: 0.0013709368649870157\n",
            "step: 160, loss: 0.0005019669188186526\n",
            "step: 170, loss: 0.0009973908308893442\n",
            "step: 180, loss: 0.0001909861748572439\n",
            "step: 190, loss: 9.821398271014914e-05\n",
            "step: 200, loss: 0.027429156005382538\n",
            "step: 210, loss: 0.032339222729206085\n",
            "step: 220, loss: 0.00023952455376274884\n",
            "step: 230, loss: 0.0031267309095710516\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9864559819413092, f1=0.9819413092550789, best_f1=0.9864559819413092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00029541627736762166\n",
            "step: 10, loss: 0.0007145173149183393\n",
            "step: 20, loss: 0.0006885498878546059\n",
            "step: 30, loss: 0.0001537794159958139\n",
            "step: 40, loss: 0.004032955504953861\n",
            "step: 50, loss: 4.9405462050344795e-05\n",
            "step: 60, loss: 5.742630310123786e-05\n",
            "step: 70, loss: 0.053021389991045\n",
            "step: 80, loss: 4.3584295781329274e-05\n",
            "step: 90, loss: 6.388096517184749e-05\n",
            "step: 100, loss: 5.7911289331968874e-05\n",
            "step: 110, loss: 0.00534376036375761\n",
            "step: 120, loss: 7.403617928503081e-05\n",
            "step: 130, loss: 0.00014194240793585777\n",
            "step: 140, loss: 7.078640919644386e-05\n",
            "step: 150, loss: 0.009145648218691349\n",
            "step: 160, loss: 3.477298014331609e-05\n",
            "step: 170, loss: 6.167539686430246e-05\n",
            "step: 180, loss: 0.00011614546383498237\n",
            "step: 190, loss: 0.00010163422848563641\n",
            "step: 200, loss: 8.032277401071042e-05\n",
            "step: 210, loss: 0.00021341275714803487\n",
            "step: 220, loss: 7.228289177874103e-05\n",
            "step: 230, loss: 0.0005872318870387971\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9887640449438202, f1=0.9831271091113611, best_f1=0.9864559819413092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00036772829480469227\n",
            "step: 10, loss: 8.074907964328304e-05\n",
            "step: 20, loss: 5.8474917750572786e-05\n",
            "step: 30, loss: 6.683314859401435e-05\n",
            "step: 40, loss: 2.7856398446601816e-05\n",
            "step: 50, loss: 0.003158334642648697\n",
            "step: 60, loss: 0.009408269077539444\n",
            "step: 70, loss: 0.0057839443907141685\n",
            "step: 80, loss: 0.0003454993129707873\n",
            "step: 90, loss: 0.06906718015670776\n",
            "step: 100, loss: 0.0007054173038341105\n",
            "step: 110, loss: 0.0006716093048453331\n",
            "step: 120, loss: 0.00015349585737567395\n",
            "step: 130, loss: 0.00019219121895730495\n",
            "step: 140, loss: 0.013123851269483566\n",
            "step: 150, loss: 0.00021430719061754644\n",
            "step: 160, loss: 0.06969203054904938\n",
            "step: 170, loss: 0.001298008719459176\n",
            "step: 180, loss: 0.00021639444457832724\n",
            "step: 190, loss: 0.0001253537047887221\n",
            "step: 200, loss: 0.004833343904465437\n",
            "step: 210, loss: 7.379087765002623e-05\n",
            "step: 220, loss: 0.0011218705913051963\n",
            "step: 230, loss: 0.04832775145769119\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9853768278965129, f1=0.9808773903262092, best_f1=0.9864559819413092\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011874970368808135\n",
            "step: 10, loss: 0.00010407741501694545\n",
            "step: 20, loss: 0.008841698057949543\n",
            "step: 30, loss: 0.020194925367832184\n",
            "step: 40, loss: 0.0002884138375520706\n",
            "step: 50, loss: 0.0013841385953128338\n",
            "step: 60, loss: 0.013640346936881542\n",
            "step: 70, loss: 0.00017810142890084535\n",
            "step: 80, loss: 4.5995668187970296e-05\n",
            "step: 90, loss: 0.00235801562666893\n",
            "step: 100, loss: 5.3374082199297845e-05\n",
            "step: 110, loss: 4.1403323848498985e-05\n",
            "step: 120, loss: 0.0002692873531486839\n",
            "step: 130, loss: 5.291322304401547e-05\n",
            "step: 140, loss: 8.005223207874224e-05\n",
            "step: 150, loss: 0.0007001247722655535\n",
            "step: 160, loss: 0.0001250748900929466\n",
            "step: 170, loss: 0.0028962434735149145\n",
            "step: 180, loss: 5.3398849559016526e-05\n",
            "step: 190, loss: 0.006834044586867094\n",
            "step: 200, loss: 4.517148408922367e-05\n",
            "step: 210, loss: 0.00796093512326479\n",
            "step: 220, loss: 0.005784039851278067\n",
            "step: 230, loss: 7.33316846890375e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9910313901345291, f1=0.9810479375696767, best_f1=0.9810479375696767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011173069477081299\n",
            "step: 10, loss: 0.0009749981691129506\n",
            "step: 20, loss: 0.00011271137918811291\n",
            "step: 30, loss: 0.0030809538438916206\n",
            "step: 40, loss: 6.244029646040872e-05\n",
            "step: 50, loss: 0.0005644949851557612\n",
            "step: 60, loss: 0.0005551881622523069\n",
            "step: 70, loss: 0.03337322548031807\n",
            "step: 80, loss: 0.00020813824085053056\n",
            "step: 90, loss: 0.0001163445194833912\n",
            "step: 100, loss: 0.0001668213080847636\n",
            "step: 110, loss: 0.000717436196282506\n",
            "step: 120, loss: 0.00014867396384943277\n",
            "step: 130, loss: 0.00011377024202374741\n",
            "step: 140, loss: 0.00017528163152746856\n",
            "step: 150, loss: 8.733115828363225e-05\n",
            "step: 160, loss: 0.009903962723910809\n",
            "step: 170, loss: 6.905738700879738e-05\n",
            "step: 180, loss: 0.03992258012294769\n",
            "step: 190, loss: 0.00014366628602147102\n",
            "step: 200, loss: 0.00011291939881630242\n",
            "step: 210, loss: 0.00024747051065787673\n",
            "step: 220, loss: 0.00020220738952048123\n",
            "step: 230, loss: 7.538011413998902e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.990990990990991, f1=0.9819413092550789, best_f1=0.9810479375696767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.576883293339051e-05\n",
            "step: 10, loss: 7.296170952031389e-05\n",
            "step: 20, loss: 8.534798689652234e-05\n",
            "step: 30, loss: 0.0031314275693148375\n",
            "step: 40, loss: 0.0005225134082138538\n",
            "step: 50, loss: 3.583440411603078e-05\n",
            "step: 60, loss: 0.00011536631791386753\n",
            "step: 70, loss: 9.564743959344923e-05\n",
            "step: 80, loss: 5.559671626542695e-05\n",
            "step: 90, loss: 4.845027069677599e-05\n",
            "step: 100, loss: 0.0005150371580384672\n",
            "step: 110, loss: 5.647432044497691e-05\n",
            "step: 120, loss: 2.138990930689033e-05\n",
            "step: 130, loss: 7.976108463481069e-05\n",
            "step: 140, loss: 3.802963692578487e-05\n",
            "step: 150, loss: 7.879883196437731e-05\n",
            "step: 160, loss: 0.0006694091134704649\n",
            "step: 170, loss: 0.0001571247266838327\n",
            "step: 180, loss: 3.9478061808040366e-05\n",
            "step: 190, loss: 0.001240517827682197\n",
            "step: 200, loss: 5.166221671970561e-05\n",
            "step: 210, loss: 6.222595402505249e-05\n",
            "step: 220, loss: 7.668867328902707e-05\n",
            "step: 230, loss: 0.0004705112660303712\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9910112359550561, f1=0.9830890642615557, best_f1=0.9810479375696767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0318833589553833\n",
            "step: 10, loss: 4.216047454974614e-05\n",
            "step: 20, loss: 0.000341554987244308\n",
            "step: 30, loss: 7.411393744405359e-05\n",
            "step: 40, loss: 0.0005230465903878212\n",
            "step: 50, loss: 3.810758062172681e-05\n",
            "step: 60, loss: 0.02587631531059742\n",
            "step: 70, loss: 0.00010884339280892164\n",
            "step: 80, loss: 3.990623008576222e-05\n",
            "step: 90, loss: 8.324460941366851e-05\n",
            "step: 100, loss: 0.00012394173245411366\n",
            "step: 110, loss: 0.00012033598613925278\n",
            "step: 120, loss: 0.0219047162681818\n",
            "step: 130, loss: 0.00014558089605998248\n",
            "step: 140, loss: 0.004634020384401083\n",
            "step: 150, loss: 8.878044172888622e-05\n",
            "step: 160, loss: 0.017741771414875984\n",
            "step: 170, loss: 2.1863086658413522e-05\n",
            "step: 180, loss: 0.00023138443066272885\n",
            "step: 190, loss: 6.982870399951935e-05\n",
            "step: 200, loss: 0.00030455130035988986\n",
            "step: 210, loss: 0.06005102023482323\n",
            "step: 220, loss: 4.727043415186927e-05\n",
            "step: 230, loss: 0.0001414291764376685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9910112359550561, f1=0.9842342342342343, best_f1=0.9810479375696767\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 205.71it/s]\n",
            "load_f1 = 0.9887892376681614\n",
            "real_f1 = 0.9898989898989898\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 185.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "CrVM9KP9hstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba04a95-3fd3-4781-a35a-1eb286d83c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6003347039222717\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.44009873270988464\n",
            "step: 20, loss: 0.23196345567703247\n",
            "step: 30, loss: 0.384991854429245\n",
            "step: 40, loss: 0.3761900067329407\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.49571672081947327\n",
            "step: 60, loss: 0.33929967880249023\n",
            "step: 70, loss: 0.13385532796382904\n",
            "step: 80, loss: 0.19803635776042938\n",
            "step: 90, loss: 0.1756998896598816\n",
            "step: 100, loss: 0.1591089516878128\n",
            "step: 110, loss: 0.1710689216852188\n",
            "step: 120, loss: 0.12851016223430634\n",
            "step: 130, loss: 0.09186941385269165\n",
            "step: 140, loss: 0.2103874236345291\n",
            "step: 150, loss: 0.11782488226890564\n",
            "step: 160, loss: 0.1561291366815567\n",
            "step: 170, loss: 0.017030376940965652\n",
            "step: 180, loss: 0.06858668476343155\n",
            "step: 190, loss: 0.0901590958237648\n",
            "step: 200, loss: 0.01589100994169712\n",
            "step: 210, loss: 0.02339087426662445\n",
            "step: 220, loss: 0.07939070463180542\n",
            "step: 230, loss: 0.1869005709886551\n",
            "step: 240, loss: 0.06868265569210052\n",
            "step: 250, loss: 0.017085928469896317\n",
            "step: 260, loss: 0.317659854888916\n",
            "step: 270, loss: 0.22827698290348053\n",
            "step: 280, loss: 0.08762544393539429\n",
            "step: 290, loss: 0.03568250313401222\n",
            "step: 300, loss: 0.08086016029119492\n",
            "step: 310, loss: 0.13949653506278992\n",
            "step: 320, loss: 0.07840052247047424\n",
            "step: 330, loss: 0.047867294400930405\n",
            "step: 340, loss: 0.4678080976009369\n",
            "step: 350, loss: 0.17529097199440002\n",
            "step: 360, loss: 0.012781822122633457\n",
            "step: 370, loss: 0.01924186199903488\n",
            "step: 380, loss: 0.060081347823143005\n",
            "step: 390, loss: 0.029125714674592018\n",
            "step: 400, loss: 0.054593656212091446\n",
            "step: 410, loss: 0.26033249497413635\n",
            "step: 420, loss: 0.03550586476922035\n",
            "step: 430, loss: 0.05540226399898529\n",
            "step: 440, loss: 0.030287278816103935\n",
            "step: 450, loss: 0.02072971500456333\n",
            "step: 460, loss: 0.01386328600347042\n",
            "step: 470, loss: 0.017258398234844208\n",
            "step: 480, loss: 0.1495843529701233\n",
            "step: 490, loss: 0.14651800692081451\n",
            "step: 500, loss: 0.07022316753864288\n",
            "step: 510, loss: 0.13961927592754364\n",
            "step: 520, loss: 0.32840636372566223\n",
            "step: 530, loss: 0.017873723059892654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9345622119815669, f1=0.9332105020727776, best_f1=0.9332105020727776\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09339568018913269\n",
            "step: 10, loss: 0.08985365927219391\n",
            "step: 20, loss: 0.04790868982672691\n",
            "step: 30, loss: 0.050902679562568665\n",
            "step: 40, loss: 0.05334793031215668\n",
            "step: 50, loss: 0.022748520597815514\n",
            "step: 60, loss: 0.08535590767860413\n",
            "step: 70, loss: 0.07728558033704758\n",
            "step: 80, loss: 0.018342992290854454\n",
            "step: 90, loss: 0.006353222299367189\n",
            "step: 100, loss: 0.10473329573869705\n",
            "step: 110, loss: 0.011073512025177479\n",
            "step: 120, loss: 0.14829443395137787\n",
            "step: 130, loss: 0.013387330807745457\n",
            "step: 140, loss: 0.11902576684951782\n",
            "step: 150, loss: 0.006839197129011154\n",
            "step: 160, loss: 0.04018162190914154\n",
            "step: 170, loss: 0.11308873444795609\n",
            "step: 180, loss: 0.031195281073451042\n",
            "step: 190, loss: 0.03316260501742363\n",
            "step: 200, loss: 0.17854733765125275\n",
            "step: 210, loss: 0.024588599801063538\n",
            "step: 220, loss: 0.0033879803959280252\n",
            "step: 230, loss: 0.012725789099931717\n",
            "step: 240, loss: 0.023082418367266655\n",
            "step: 250, loss: 0.08190261572599411\n",
            "step: 260, loss: 0.05221843346953392\n",
            "step: 270, loss: 0.025310475379228592\n",
            "step: 280, loss: 0.14571379125118256\n",
            "step: 290, loss: 0.07652604579925537\n",
            "step: 300, loss: 0.03967166319489479\n",
            "step: 310, loss: 0.1805645078420639\n",
            "step: 320, loss: 0.05879371240735054\n",
            "step: 330, loss: 0.11620617657899857\n",
            "step: 340, loss: 0.03335035592317581\n",
            "step: 350, loss: 0.004536710679531097\n",
            "step: 360, loss: 0.052114903926849365\n",
            "step: 370, loss: 0.028320306912064552\n",
            "step: 380, loss: 0.14524634182453156\n",
            "step: 390, loss: 0.006464356556534767\n",
            "step: 400, loss: 0.02606823481619358\n",
            "step: 410, loss: 0.025448691099882126\n",
            "step: 420, loss: 0.059724658727645874\n",
            "step: 430, loss: 0.1422087699174881\n",
            "step: 440, loss: 0.03391382470726967\n",
            "step: 450, loss: 0.15872539579868317\n",
            "step: 460, loss: 0.05175276845693588\n",
            "step: 470, loss: 0.051638782024383545\n",
            "step: 480, loss: 0.02114223875105381\n",
            "step: 490, loss: 0.04144136607646942\n",
            "step: 500, loss: 0.009319709613919258\n",
            "step: 510, loss: 0.014262969605624676\n",
            "step: 520, loss: 0.3283609449863434\n",
            "step: 530, loss: 0.016314508393406868\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9477093937991671, f1=0.9443929564411491, best_f1=0.9443929564411491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1502699851989746\n",
            "step: 10, loss: 0.017738858237862587\n",
            "step: 20, loss: 0.011924118734896183\n",
            "step: 30, loss: 0.1790846586227417\n",
            "step: 40, loss: 0.019994819536805153\n",
            "step: 50, loss: 0.00654454855248332\n",
            "step: 60, loss: 0.06804290413856506\n",
            "step: 70, loss: 0.023174427449703217\n",
            "step: 80, loss: 0.04333172366023064\n",
            "step: 90, loss: 0.01902671530842781\n",
            "step: 100, loss: 0.00784558430314064\n",
            "step: 110, loss: 0.026581896468997\n",
            "step: 120, loss: 0.1662011295557022\n",
            "step: 130, loss: 0.037127505987882614\n",
            "step: 140, loss: 0.026834925636649132\n",
            "step: 150, loss: 0.04891154542565346\n",
            "step: 160, loss: 0.012770306318998337\n",
            "step: 170, loss: 0.009581348858773708\n",
            "step: 180, loss: 0.02234017476439476\n",
            "step: 190, loss: 0.019641349092125893\n",
            "step: 200, loss: 0.1506042182445526\n",
            "step: 210, loss: 0.042372968047857285\n",
            "step: 220, loss: 0.19054532051086426\n",
            "step: 230, loss: 0.03574439138174057\n",
            "step: 240, loss: 0.02593330293893814\n",
            "step: 250, loss: 0.08641500025987625\n",
            "step: 260, loss: 0.10760756582021713\n",
            "step: 270, loss: 0.008356552571058273\n",
            "step: 280, loss: 0.00280699972063303\n",
            "step: 290, loss: 0.002501150593161583\n",
            "step: 300, loss: 0.07237143814563751\n",
            "step: 310, loss: 0.05719583109021187\n",
            "step: 320, loss: 0.011796497739851475\n",
            "step: 330, loss: 0.014285448007285595\n",
            "step: 340, loss: 0.01252404972910881\n",
            "step: 350, loss: 0.12123265117406845\n",
            "step: 360, loss: 0.02628142759203911\n",
            "step: 370, loss: 0.02554069645702839\n",
            "step: 380, loss: 0.048527587205171585\n",
            "step: 390, loss: 0.009687505662441254\n",
            "step: 400, loss: 0.08854657411575317\n",
            "step: 410, loss: 0.02260543406009674\n",
            "step: 420, loss: 0.06470783054828644\n",
            "step: 430, loss: 0.03288492187857628\n",
            "step: 440, loss: 0.22123867273330688\n",
            "step: 450, loss: 0.0855429619550705\n",
            "step: 460, loss: 0.138068288564682\n",
            "step: 470, loss: 0.02882196754217148\n",
            "step: 480, loss: 0.09050162136554718\n",
            "step: 490, loss: 0.020007794722914696\n",
            "step: 500, loss: 0.009399175643920898\n",
            "step: 510, loss: 0.022662591189146042\n",
            "step: 520, loss: 0.013032921589910984\n",
            "step: 530, loss: 0.003272583708167076\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9512867647058824, f1=0.9472718936267768, best_f1=0.9472718936267768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024933168664574623\n",
            "step: 10, loss: 0.003599008545279503\n",
            "step: 20, loss: 0.1870446652173996\n",
            "step: 30, loss: 0.05323387309908867\n",
            "step: 40, loss: 0.014209180139005184\n",
            "step: 50, loss: 0.06401511281728745\n",
            "step: 60, loss: 0.012809846550226212\n",
            "step: 70, loss: 0.008193755522370338\n",
            "step: 80, loss: 0.0028319652192294598\n",
            "step: 90, loss: 0.026979610323905945\n",
            "step: 100, loss: 0.0014948549214750528\n",
            "step: 110, loss: 0.007815110497176647\n",
            "step: 120, loss: 0.025906410068273544\n",
            "step: 130, loss: 0.007865617983043194\n",
            "step: 140, loss: 0.01345906313508749\n",
            "step: 150, loss: 0.006615910213440657\n",
            "step: 160, loss: 0.02133788913488388\n",
            "step: 170, loss: 0.017859235405921936\n",
            "step: 180, loss: 0.1260865181684494\n",
            "step: 190, loss: 0.0490606315433979\n",
            "step: 200, loss: 0.04750797525048256\n",
            "step: 210, loss: 0.002893191995099187\n",
            "step: 220, loss: 0.005175345577299595\n",
            "step: 230, loss: 0.005152999889105558\n",
            "step: 240, loss: 0.0011057500960305333\n",
            "step: 250, loss: 0.1455824375152588\n",
            "step: 260, loss: 0.0017446039710193872\n",
            "step: 270, loss: 0.04534219205379486\n",
            "step: 280, loss: 0.0064215390011668205\n",
            "step: 290, loss: 0.02484455332159996\n",
            "step: 300, loss: 0.018901994451880455\n",
            "step: 310, loss: 0.0006705619161948562\n",
            "step: 320, loss: 0.04115325212478638\n",
            "step: 330, loss: 0.06565925478935242\n",
            "step: 340, loss: 0.053488388657569885\n",
            "step: 350, loss: 0.08232199400663376\n",
            "step: 360, loss: 0.034572288393974304\n",
            "step: 370, loss: 0.010575509630143642\n",
            "step: 380, loss: 0.03074842318892479\n",
            "step: 390, loss: 0.0016857597511261702\n",
            "step: 400, loss: 0.012884721159934998\n",
            "step: 410, loss: 0.01683911681175232\n",
            "step: 420, loss: 0.018877187743782997\n",
            "step: 430, loss: 0.009327530860900879\n",
            "step: 440, loss: 0.022603826597332954\n",
            "step: 450, loss: 0.012707713060081005\n",
            "step: 460, loss: 0.003829309018328786\n",
            "step: 470, loss: 0.0021246757823973894\n",
            "step: 480, loss: 0.011172437109053135\n",
            "step: 490, loss: 0.005537834018468857\n",
            "step: 500, loss: 0.0988599956035614\n",
            "step: 510, loss: 0.08144834637641907\n",
            "step: 520, loss: 0.04277750477194786\n",
            "step: 530, loss: 0.035409145057201385\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9550561797752809, f1=0.9489939167056621, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009825410088524222\n",
            "step: 10, loss: 0.008835206739604473\n",
            "step: 20, loss: 0.004140134435147047\n",
            "step: 30, loss: 0.006376477424055338\n",
            "step: 40, loss: 0.0016157242935150862\n",
            "step: 50, loss: 0.11415305733680725\n",
            "step: 60, loss: 0.05678332597017288\n",
            "step: 70, loss: 0.02341138757765293\n",
            "step: 80, loss: 0.049766600131988525\n",
            "step: 90, loss: 0.06104409694671631\n",
            "step: 100, loss: 0.10378540307283401\n",
            "step: 110, loss: 0.028786929324269295\n",
            "step: 120, loss: 0.17806454002857208\n",
            "step: 130, loss: 0.007034025155007839\n",
            "step: 140, loss: 0.017431480810046196\n",
            "step: 150, loss: 0.010458534583449364\n",
            "step: 160, loss: 0.029914673417806625\n",
            "step: 170, loss: 0.07279708981513977\n",
            "step: 180, loss: 0.008226446807384491\n",
            "step: 190, loss: 0.007597512099891901\n",
            "step: 200, loss: 0.0015167332021519542\n",
            "step: 210, loss: 0.0006514484994113445\n",
            "step: 220, loss: 0.022814916446805\n",
            "step: 230, loss: 0.01435638964176178\n",
            "step: 240, loss: 0.008904571644961834\n",
            "step: 250, loss: 0.09026303142309189\n",
            "step: 260, loss: 0.0030059083364903927\n",
            "step: 270, loss: 0.18246133625507355\n",
            "step: 280, loss: 0.011988168582320213\n",
            "step: 290, loss: 0.017170852050185204\n",
            "step: 300, loss: 0.2600690722465515\n",
            "step: 310, loss: 0.020958106964826584\n",
            "step: 320, loss: 0.01832025870680809\n",
            "step: 330, loss: 0.0017155511304736137\n",
            "step: 340, loss: 0.01306107360869646\n",
            "step: 350, loss: 0.0006590025732293725\n",
            "step: 360, loss: 0.00025066095986403525\n",
            "step: 370, loss: 0.0008974300581030548\n",
            "step: 380, loss: 0.004505994729697704\n",
            "step: 390, loss: 0.0009615116869099438\n",
            "step: 400, loss: 0.003477443242445588\n",
            "step: 410, loss: 0.06651009619235992\n",
            "step: 420, loss: 0.20022737979888916\n",
            "step: 430, loss: 0.04645255208015442\n",
            "step: 440, loss: 0.004102885257452726\n",
            "step: 450, loss: 0.0032368518877774477\n",
            "step: 460, loss: 0.01298394612967968\n",
            "step: 470, loss: 0.020112480968236923\n",
            "step: 480, loss: 0.24357587099075317\n",
            "step: 490, loss: 0.04045308381319046\n",
            "step: 500, loss: 0.11168044805526733\n",
            "step: 510, loss: 0.01930098608136177\n",
            "step: 520, loss: 0.09329552948474884\n",
            "step: 530, loss: 0.058120571076869965\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.954119850187266, f1=0.9438202247191012, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.194025456905365\n",
            "step: 10, loss: 0.0034091342240571976\n",
            "step: 20, loss: 0.038797520101070404\n",
            "step: 30, loss: 0.0036455329973250628\n",
            "step: 40, loss: 0.05067499727010727\n",
            "step: 50, loss: 0.03654770180583\n",
            "step: 60, loss: 0.012171749025583267\n",
            "step: 70, loss: 0.00343883503228426\n",
            "step: 80, loss: 0.0012412999058142304\n",
            "step: 90, loss: 0.13372543454170227\n",
            "step: 100, loss: 0.0027709933929145336\n",
            "step: 110, loss: 0.004271623212844133\n",
            "step: 120, loss: 0.03710530325770378\n",
            "step: 130, loss: 0.004188838880509138\n",
            "step: 140, loss: 0.0035843243822455406\n",
            "step: 150, loss: 0.0012972477125003934\n",
            "step: 160, loss: 0.040992122143507004\n",
            "step: 170, loss: 0.003241756232455373\n",
            "step: 180, loss: 0.002939121797680855\n",
            "step: 190, loss: 0.216959148645401\n",
            "step: 200, loss: 0.01330883800983429\n",
            "step: 210, loss: 0.00319383735768497\n",
            "step: 220, loss: 0.048217952251434326\n",
            "step: 230, loss: 0.0017975657247006893\n",
            "step: 240, loss: 0.0008656203863210976\n",
            "step: 250, loss: 0.0380004420876503\n",
            "step: 260, loss: 0.004827044904232025\n",
            "step: 270, loss: 0.0011224234476685524\n",
            "step: 280, loss: 0.0023138057440519333\n",
            "step: 290, loss: 0.0003689885197672993\n",
            "step: 300, loss: 0.00393203878775239\n",
            "step: 310, loss: 0.1466132402420044\n",
            "step: 320, loss: 0.0004244843148626387\n",
            "step: 330, loss: 0.0007498508202843368\n",
            "step: 340, loss: 0.003965418785810471\n",
            "step: 350, loss: 0.01350912731140852\n",
            "step: 360, loss: 0.013709161430597305\n",
            "step: 370, loss: 0.0281665176153183\n",
            "step: 380, loss: 0.001811079098843038\n",
            "step: 390, loss: 0.001737948041409254\n",
            "step: 400, loss: 0.19862952828407288\n",
            "step: 410, loss: 0.01821267046034336\n",
            "step: 420, loss: 0.002744824392721057\n",
            "step: 430, loss: 0.000750939769204706\n",
            "step: 440, loss: 0.006440326105803251\n",
            "step: 450, loss: 0.1871042251586914\n",
            "step: 460, loss: 0.0027055132668465376\n",
            "step: 470, loss: 0.001858277479186654\n",
            "step: 480, loss: 0.021288124844431877\n",
            "step: 490, loss: 0.02960534766316414\n",
            "step: 500, loss: 0.0004895589081570506\n",
            "step: 510, loss: 0.13847461342811584\n",
            "step: 520, loss: 0.002240309491753578\n",
            "step: 530, loss: 0.0002810674486681819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9547363509099394, f1=0.9445738239403819, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03808574378490448\n",
            "step: 10, loss: 0.00080119539052248\n",
            "step: 20, loss: 0.007596948184072971\n",
            "step: 30, loss: 0.01093664113432169\n",
            "step: 40, loss: 0.002070200862362981\n",
            "step: 50, loss: 0.00367534882389009\n",
            "step: 60, loss: 0.03271147236227989\n",
            "step: 70, loss: 0.028530068695545197\n",
            "step: 80, loss: 0.0007495615282095969\n",
            "step: 90, loss: 0.0003495600540190935\n",
            "step: 100, loss: 0.003196598030626774\n",
            "step: 110, loss: 0.00021321122767403722\n",
            "step: 120, loss: 0.0005431136232800782\n",
            "step: 130, loss: 0.0016594022745266557\n",
            "step: 140, loss: 0.004144384991377592\n",
            "step: 150, loss: 0.003632543608546257\n",
            "step: 160, loss: 0.0006853851373307407\n",
            "step: 170, loss: 0.007780815474689007\n",
            "step: 180, loss: 0.08222349733114243\n",
            "step: 190, loss: 0.12271318584680557\n",
            "step: 200, loss: 0.00617055082693696\n",
            "step: 210, loss: 0.07207228988409042\n",
            "step: 220, loss: 0.05654924735426903\n",
            "step: 230, loss: 0.00018314090266358107\n",
            "step: 240, loss: 0.018548427149653435\n",
            "step: 250, loss: 0.008769973181188107\n",
            "step: 260, loss: 0.11453146487474442\n",
            "step: 270, loss: 0.0019984813407063484\n",
            "step: 280, loss: 0.021220451220870018\n",
            "step: 290, loss: 0.0006445567705668509\n",
            "step: 300, loss: 0.0005015717470087111\n",
            "step: 310, loss: 0.005575982388108969\n",
            "step: 320, loss: 0.09913478046655655\n",
            "step: 330, loss: 0.0016032805433496833\n",
            "step: 340, loss: 0.004118679091334343\n",
            "step: 350, loss: 0.008732292801141739\n",
            "step: 360, loss: 0.030300159007310867\n",
            "step: 370, loss: 0.05266512930393219\n",
            "step: 380, loss: 0.08975031971931458\n",
            "step: 390, loss: 0.050630874931812286\n",
            "step: 400, loss: 0.06287669390439987\n",
            "step: 410, loss: 0.021077826619148254\n",
            "step: 420, loss: 0.005557623226195574\n",
            "step: 430, loss: 0.00039597766590304673\n",
            "step: 440, loss: 0.0008218815783038735\n",
            "step: 450, loss: 0.010897113010287285\n",
            "step: 460, loss: 0.003852268448099494\n",
            "step: 470, loss: 0.12088098376989365\n",
            "step: 480, loss: 0.009906991384923458\n",
            "step: 490, loss: 0.0011144025484099984\n",
            "step: 500, loss: 0.0031504963990300894\n",
            "step: 510, loss: 0.000253954523941502\n",
            "step: 520, loss: 0.0010976827470585704\n",
            "step: 530, loss: 0.0010283497394993901\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9500699953336444, f1=0.947565543071161, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021611833944916725\n",
            "step: 10, loss: 0.006324792746454477\n",
            "step: 20, loss: 0.0012615332379937172\n",
            "step: 30, loss: 0.0017947499873116612\n",
            "step: 40, loss: 0.00027490584761835635\n",
            "step: 50, loss: 0.00015811367484275252\n",
            "step: 60, loss: 0.0003361851559020579\n",
            "step: 70, loss: 0.0022623410914093256\n",
            "step: 80, loss: 0.009011251851916313\n",
            "step: 90, loss: 0.00042912494973279536\n",
            "step: 100, loss: 0.012136452831327915\n",
            "step: 110, loss: 0.0005013309419155121\n",
            "step: 120, loss: 0.0026858914643526077\n",
            "step: 130, loss: 0.007491614669561386\n",
            "step: 140, loss: 7.574685150757432e-05\n",
            "step: 150, loss: 0.0003228116547688842\n",
            "step: 160, loss: 6.144013605080545e-05\n",
            "step: 170, loss: 0.02194204553961754\n",
            "step: 180, loss: 0.0002581071457825601\n",
            "step: 190, loss: 0.0007780360756441951\n",
            "step: 200, loss: 0.003943123389035463\n",
            "step: 210, loss: 0.05922279134392738\n",
            "step: 220, loss: 0.0018362945411354303\n",
            "step: 230, loss: 0.0487089566886425\n",
            "step: 240, loss: 0.009493347257375717\n",
            "step: 250, loss: 0.009239018894731998\n",
            "step: 260, loss: 0.0020818666089326143\n",
            "step: 270, loss: 0.008590423502027988\n",
            "step: 280, loss: 0.0006049475632607937\n",
            "step: 290, loss: 0.0010489514097571373\n",
            "step: 300, loss: 2.8501488486654125e-05\n",
            "step: 310, loss: 0.00143647741060704\n",
            "step: 320, loss: 0.001484779641032219\n",
            "step: 330, loss: 0.0002389859000686556\n",
            "step: 340, loss: 0.17660710215568542\n",
            "step: 350, loss: 0.0008778587216511369\n",
            "step: 360, loss: 0.031835976988077164\n",
            "step: 370, loss: 0.019013304263353348\n",
            "step: 380, loss: 0.002757543232291937\n",
            "step: 390, loss: 0.006647492293268442\n",
            "step: 400, loss: 0.0021095722913742065\n",
            "step: 410, loss: 0.0002932960633188486\n",
            "step: 420, loss: 7.23047778592445e-05\n",
            "step: 430, loss: 0.1055125892162323\n",
            "step: 440, loss: 0.03378206491470337\n",
            "step: 450, loss: 0.0016220908146351576\n",
            "step: 460, loss: 0.006502749398350716\n",
            "step: 470, loss: 0.08273053169250488\n",
            "step: 480, loss: 0.018124613910913467\n",
            "step: 490, loss: 0.0390329509973526\n",
            "step: 500, loss: 0.00024654995650053024\n",
            "step: 510, loss: 0.07213616371154785\n",
            "step: 520, loss: 7.020636985544115e-05\n",
            "step: 530, loss: 0.0020632778760045767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9504483246814535, f1=0.9382948657560056, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010506818071007729\n",
            "step: 10, loss: 0.017617179080843925\n",
            "step: 20, loss: 6.439274875447154e-05\n",
            "step: 30, loss: 0.10992991924285889\n",
            "step: 40, loss: 0.0001306618651142344\n",
            "step: 50, loss: 0.0029243130702525377\n",
            "step: 60, loss: 0.0006657204357907176\n",
            "step: 70, loss: 0.011630827561020851\n",
            "step: 80, loss: 0.14406223595142365\n",
            "step: 90, loss: 0.08816912770271301\n",
            "step: 100, loss: 0.02699378691613674\n",
            "step: 110, loss: 0.015396914444863796\n",
            "step: 120, loss: 0.004446462728083134\n",
            "step: 130, loss: 0.0021289587020874023\n",
            "step: 140, loss: 0.005326205864548683\n",
            "step: 150, loss: 0.017006540670990944\n",
            "step: 160, loss: 0.00010174342605751008\n",
            "step: 170, loss: 0.030870074406266212\n",
            "step: 180, loss: 0.005274270195513964\n",
            "step: 190, loss: 0.0868438109755516\n",
            "step: 200, loss: 0.0003013278474099934\n",
            "step: 210, loss: 0.007664036937057972\n",
            "step: 220, loss: 0.001111867604777217\n",
            "step: 230, loss: 8.694881398696452e-05\n",
            "step: 240, loss: 0.0008613081299699843\n",
            "step: 250, loss: 0.003942043054848909\n",
            "step: 260, loss: 0.008931051008403301\n",
            "step: 270, loss: 0.02446548268198967\n",
            "step: 280, loss: 0.046392448246479034\n",
            "step: 290, loss: 0.014838985167443752\n",
            "step: 300, loss: 0.0014905165880918503\n",
            "step: 310, loss: 7.814223499735817e-05\n",
            "step: 320, loss: 0.0004508514248300344\n",
            "step: 330, loss: 0.0004189646861050278\n",
            "step: 340, loss: 0.004906953778117895\n",
            "step: 350, loss: 0.09036457538604736\n",
            "step: 360, loss: 0.016468243673443794\n",
            "step: 370, loss: 0.00020395117462612689\n",
            "step: 380, loss: 0.00037066690856590867\n",
            "step: 390, loss: 0.0005245094071142375\n",
            "step: 400, loss: 0.02067730389535427\n",
            "step: 410, loss: 0.0055520725436508656\n",
            "step: 420, loss: 0.0019243924180045724\n",
            "step: 430, loss: 0.0058988844975829124\n",
            "step: 440, loss: 0.00034722709096968174\n",
            "step: 450, loss: 0.01746545359492302\n",
            "step: 460, loss: 0.0005827692802995443\n",
            "step: 470, loss: 0.003870014799758792\n",
            "step: 480, loss: 0.0008064809953793883\n",
            "step: 490, loss: 0.006963293068110943\n",
            "step: 500, loss: 0.0287218876183033\n",
            "step: 510, loss: 0.006326075177639723\n",
            "step: 520, loss: 0.005857483949512243\n",
            "step: 530, loss: 0.04311005398631096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9545245751033532, f1=0.9495875343721356, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016946359537541866\n",
            "step: 10, loss: 0.0017057559452950954\n",
            "step: 20, loss: 0.0014024346601217985\n",
            "step: 30, loss: 0.017032641917467117\n",
            "step: 40, loss: 0.0009731698082759976\n",
            "step: 50, loss: 0.001159351086243987\n",
            "step: 60, loss: 0.003887488739565015\n",
            "step: 70, loss: 0.08671070635318756\n",
            "step: 80, loss: 0.0019330764189362526\n",
            "step: 90, loss: 0.0009724590345285833\n",
            "step: 100, loss: 0.01807895116508007\n",
            "step: 110, loss: 0.004669359419494867\n",
            "step: 120, loss: 0.0013821267057210207\n",
            "step: 130, loss: 0.0013684633886441588\n",
            "step: 140, loss: 0.00014163237938191742\n",
            "step: 150, loss: 0.001723324996419251\n",
            "step: 160, loss: 0.041350629180669785\n",
            "step: 170, loss: 8.237065776484087e-05\n",
            "step: 180, loss: 0.0020701305475085974\n",
            "step: 190, loss: 2.7281324946670793e-05\n",
            "step: 200, loss: 0.0001997715880861506\n",
            "step: 210, loss: 0.06110086292028427\n",
            "step: 220, loss: 0.00021797452063765377\n",
            "step: 230, loss: 5.16592881467659e-05\n",
            "step: 240, loss: 0.0010353625984862447\n",
            "step: 250, loss: 0.00036983343306928873\n",
            "step: 260, loss: 0.009486641734838486\n",
            "step: 270, loss: 0.0005825150292366743\n",
            "step: 280, loss: 0.0175316222012043\n",
            "step: 290, loss: 0.0015834912192076445\n",
            "step: 300, loss: 0.0024628289975225925\n",
            "step: 310, loss: 0.046130191534757614\n",
            "step: 320, loss: 0.0011638612486422062\n",
            "step: 330, loss: 0.05777787044644356\n",
            "step: 340, loss: 0.00021939298312645406\n",
            "step: 350, loss: 0.0009972852421924472\n",
            "step: 360, loss: 0.0011054225033149123\n",
            "step: 370, loss: 0.0029933154582977295\n",
            "step: 380, loss: 0.002164258621633053\n",
            "step: 390, loss: 9.572020644554868e-05\n",
            "step: 400, loss: 5.7388460845686495e-05\n",
            "step: 410, loss: 0.0027038827538490295\n",
            "step: 420, loss: 1.7083555576391518e-05\n",
            "step: 430, loss: 9.64282444329001e-05\n",
            "step: 440, loss: 1.2390187293931376e-05\n",
            "step: 450, loss: 0.0018221024656668305\n",
            "step: 460, loss: 0.0025295261293649673\n",
            "step: 470, loss: 0.0029523870907723904\n",
            "step: 480, loss: 0.022185038775205612\n",
            "step: 490, loss: 0.0003402139700483531\n",
            "step: 500, loss: 0.003710906021296978\n",
            "step: 510, loss: 4.3384701712056994e-05\n",
            "step: 520, loss: 8.107975008897483e-05\n",
            "step: 530, loss: 0.0045730313286185265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9542124542124543, f1=0.9499770536943551, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.89962484507123e-06\n",
            "step: 10, loss: 0.0009185949456878006\n",
            "step: 20, loss: 0.00045994031825102866\n",
            "step: 30, loss: 2.2126985641079955e-05\n",
            "step: 40, loss: 0.00022572866873815656\n",
            "step: 50, loss: 3.9451191696571186e-05\n",
            "step: 60, loss: 0.0035459038335829973\n",
            "step: 70, loss: 2.3590157070429996e-05\n",
            "step: 80, loss: 0.00010210357140749693\n",
            "step: 90, loss: 0.0011212137760594487\n",
            "step: 100, loss: 0.0007791469106450677\n",
            "step: 110, loss: 0.0004346510686445981\n",
            "step: 120, loss: 0.001710981596261263\n",
            "step: 130, loss: 0.0001262253208551556\n",
            "step: 140, loss: 0.0004989180597476661\n",
            "step: 150, loss: 0.0011105198645964265\n",
            "step: 160, loss: 0.0006978526362217963\n",
            "step: 170, loss: 0.0007705076714046299\n",
            "step: 180, loss: 0.00022345500474330038\n",
            "step: 190, loss: 0.02360757812857628\n",
            "step: 200, loss: 0.0001460422354284674\n",
            "step: 210, loss: 0.00042558921268209815\n",
            "step: 220, loss: 0.02124403975903988\n",
            "step: 230, loss: 0.00010902770736720413\n",
            "step: 240, loss: 0.002124754711985588\n",
            "step: 250, loss: 0.00018175186414737254\n",
            "step: 260, loss: 0.0021407685708254576\n",
            "step: 270, loss: 0.0012380947591736913\n",
            "step: 280, loss: 0.003300590207800269\n",
            "step: 290, loss: 0.00011981392162851989\n",
            "step: 300, loss: 0.04781793802976608\n",
            "step: 310, loss: 0.020101264119148254\n",
            "step: 320, loss: 0.001518232747912407\n",
            "step: 330, loss: 5.674490239471197e-05\n",
            "step: 340, loss: 0.041359879076480865\n",
            "step: 350, loss: 9.421911090612411e-05\n",
            "step: 360, loss: 0.0005362338852137327\n",
            "step: 370, loss: 0.002391166752204299\n",
            "step: 380, loss: 0.0014423525426536798\n",
            "step: 390, loss: 0.0017169601051136851\n",
            "step: 400, loss: 0.00020402386144269258\n",
            "step: 410, loss: 0.0007027257815934718\n",
            "step: 420, loss: 0.014688081108033657\n",
            "step: 430, loss: 0.0006602771463803947\n",
            "step: 440, loss: 0.00016147756832651794\n",
            "step: 450, loss: 0.0014648475917056203\n",
            "step: 460, loss: 0.0014535801019519567\n",
            "step: 470, loss: 0.0005931239575147629\n",
            "step: 480, loss: 0.0004914425662718713\n",
            "step: 490, loss: 0.0052736313082277775\n",
            "step: 500, loss: 0.029410790652036667\n",
            "step: 510, loss: 0.07951465249061584\n",
            "step: 520, loss: 0.0003328395541757345\n",
            "step: 530, loss: 0.02865997701883316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9471750114836931, f1=0.945337620578778, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0035441387444734573\n",
            "step: 10, loss: 0.0007485010428354144\n",
            "step: 20, loss: 0.012447117827832699\n",
            "step: 30, loss: 4.144976264797151e-05\n",
            "step: 40, loss: 0.002370883710682392\n",
            "step: 50, loss: 0.004878743551671505\n",
            "step: 60, loss: 5.6368826335528865e-05\n",
            "step: 70, loss: 0.35287362337112427\n",
            "step: 80, loss: 0.0018378666136413813\n",
            "step: 90, loss: 0.005428516771644354\n",
            "step: 100, loss: 0.009761886671185493\n",
            "step: 110, loss: 0.04436719790101051\n",
            "step: 120, loss: 0.0018705320544540882\n",
            "step: 130, loss: 0.0008050631731748581\n",
            "step: 140, loss: 0.0020498023368418217\n",
            "step: 150, loss: 0.0007764116744510829\n",
            "step: 160, loss: 0.07289843261241913\n",
            "step: 170, loss: 0.00012582255294546485\n",
            "step: 180, loss: 4.232391802361235e-05\n",
            "step: 190, loss: 0.001502840663306415\n",
            "step: 200, loss: 0.0011968521866947412\n",
            "step: 210, loss: 0.00026346519007347524\n",
            "step: 220, loss: 0.0002020439424086362\n",
            "step: 230, loss: 6.241998926270753e-05\n",
            "step: 240, loss: 0.008303877897560596\n",
            "step: 250, loss: 1.012522534438176e-05\n",
            "step: 260, loss: 0.0002673354174476117\n",
            "step: 270, loss: 0.016556378453969955\n",
            "step: 280, loss: 4.43543067376595e-05\n",
            "step: 290, loss: 0.0012287698918953538\n",
            "step: 300, loss: 0.019862012937664986\n",
            "step: 310, loss: 0.003337386529892683\n",
            "step: 320, loss: 0.01456140261143446\n",
            "step: 330, loss: 0.00235663796775043\n",
            "step: 340, loss: 5.6447821407346055e-05\n",
            "step: 350, loss: 2.058668360405136e-05\n",
            "step: 360, loss: 0.0070128352381289005\n",
            "step: 370, loss: 0.007248914800584316\n",
            "step: 380, loss: 7.509417628170922e-05\n",
            "step: 390, loss: 0.016287218779325485\n",
            "step: 400, loss: 0.00028374575776979327\n",
            "step: 410, loss: 0.013818183913826942\n",
            "step: 420, loss: 0.01296201441437006\n",
            "step: 430, loss: 0.0016987919807434082\n",
            "step: 440, loss: 0.0025158762000501156\n",
            "step: 450, loss: 0.0032338302116841078\n",
            "step: 460, loss: 0.0010550383012741804\n",
            "step: 470, loss: 0.0014266886282712221\n",
            "step: 480, loss: 0.003040294162929058\n",
            "step: 490, loss: 0.0057455734349787235\n",
            "step: 500, loss: 0.00011799191997852176\n",
            "step: 510, loss: 0.012557480484247208\n",
            "step: 520, loss: 0.0007902121869847178\n",
            "step: 530, loss: 4.5733067963737994e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9471264367816092, f1=0.9472234970169803, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002700581680983305\n",
            "step: 10, loss: 0.0011831398587673903\n",
            "step: 20, loss: 0.0002510722551960498\n",
            "step: 30, loss: 2.996525654452853e-05\n",
            "step: 40, loss: 0.03941088914871216\n",
            "step: 50, loss: 0.009593834169209003\n",
            "step: 60, loss: 0.0001283851743210107\n",
            "step: 70, loss: 0.0011954625369980931\n",
            "step: 80, loss: 0.0001837392192101106\n",
            "step: 90, loss: 3.167688919347711e-05\n",
            "step: 100, loss: 0.0002895394281949848\n",
            "step: 110, loss: 0.0001571847387822345\n",
            "step: 120, loss: 4.25617654400412e-05\n",
            "step: 130, loss: 2.1020654457970522e-05\n",
            "step: 140, loss: 0.00025662241387180984\n",
            "step: 150, loss: 0.0020651202648878098\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 160, loss: 0.0005236220313236117\n",
            "step: 170, loss: 0.05606778711080551\n",
            "step: 180, loss: 3.767140515265055e-05\n",
            "step: 190, loss: 0.000208091878448613\n",
            "step: 200, loss: 0.07158249616622925\n",
            "step: 210, loss: 9.935160960594658e-06\n",
            "step: 220, loss: 0.02701457403600216\n",
            "step: 230, loss: 0.0014022699324414134\n",
            "step: 240, loss: 0.007648753002285957\n",
            "step: 250, loss: 0.0005752848810516298\n",
            "step: 260, loss: 6.612471770495176e-05\n",
            "step: 270, loss: 0.0004239870759192854\n",
            "step: 280, loss: 0.0003844038874376565\n",
            "step: 290, loss: 0.0005524393054656684\n",
            "step: 300, loss: 0.001491791452281177\n",
            "step: 310, loss: 0.00025440595345571637\n",
            "step: 320, loss: 7.688505138503388e-05\n",
            "step: 330, loss: 0.00019777730631176382\n",
            "step: 340, loss: 0.0028473425190895796\n",
            "step: 350, loss: 0.00027869694167748094\n",
            "step: 360, loss: 0.11966507881879807\n",
            "step: 370, loss: 0.002777791116386652\n",
            "step: 380, loss: 0.00018952890241052955\n",
            "step: 390, loss: 9.758664236869663e-05\n",
            "step: 400, loss: 0.0003400110872462392\n",
            "step: 410, loss: 7.943552191136405e-05\n",
            "step: 420, loss: 0.00017188338097184896\n",
            "step: 430, loss: 9.511393727734685e-05\n",
            "step: 440, loss: 9.24606774788117e-06\n",
            "step: 450, loss: 0.018942395225167274\n",
            "step: 460, loss: 0.00035707856295630336\n",
            "step: 470, loss: 0.002946528373286128\n",
            "step: 480, loss: 4.813598934561014e-05\n",
            "step: 490, loss: 0.0006054331315681338\n",
            "step: 500, loss: 0.00442213099449873\n",
            "step: 510, loss: 9.400249109603465e-05\n",
            "step: 520, loss: 7.710880163358524e-05\n",
            "step: 530, loss: 1.3824195775669068e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9475620975160993, f1=0.9418764302059497, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006866732146590948\n",
            "step: 10, loss: 0.003470565425232053\n",
            "step: 20, loss: 6.022413072059862e-05\n",
            "step: 30, loss: 0.0020772006828337908\n",
            "step: 40, loss: 0.0002707380335777998\n",
            "step: 50, loss: 0.021110380068421364\n",
            "step: 60, loss: 2.1209996702964418e-05\n",
            "step: 70, loss: 7.451279816450551e-05\n",
            "step: 80, loss: 9.653118468122557e-05\n",
            "step: 90, loss: 1.180129675049102e-05\n",
            "step: 100, loss: 5.3943604143569246e-05\n",
            "step: 110, loss: 0.0015095015987753868\n",
            "step: 120, loss: 6.2994386098580435e-06\n",
            "step: 130, loss: 0.00013630879402626306\n",
            "step: 140, loss: 0.0009778719395399094\n",
            "step: 150, loss: 0.0008207603823393583\n",
            "step: 160, loss: 0.0005173300742171705\n",
            "step: 170, loss: 0.004442967474460602\n",
            "step: 180, loss: 0.00017640912847127765\n",
            "step: 190, loss: 0.0002251030964544043\n",
            "step: 200, loss: 6.615442543989047e-05\n",
            "step: 210, loss: 0.002037404803559184\n",
            "step: 220, loss: 3.786753222811967e-05\n",
            "step: 230, loss: 0.0034977111499756575\n",
            "step: 240, loss: 0.0012094553094357252\n",
            "step: 250, loss: 0.004208695143461227\n",
            "step: 260, loss: 7.816607103450224e-05\n",
            "step: 270, loss: 0.000686628045514226\n",
            "step: 280, loss: 5.005057028029114e-05\n",
            "step: 290, loss: 1.1183155947946943e-05\n",
            "step: 300, loss: 1.0937181286863051e-05\n",
            "step: 310, loss: 2.0454539480851963e-05\n",
            "step: 320, loss: 0.00048262806376442313\n",
            "step: 330, loss: 0.0006260026129893959\n",
            "step: 340, loss: 0.00022857898147776723\n",
            "step: 350, loss: 0.0008247795049101114\n",
            "step: 360, loss: 4.775681009050459e-05\n",
            "step: 370, loss: 0.0014453394105657935\n",
            "step: 380, loss: 0.06975223124027252\n",
            "step: 390, loss: 0.00025145182735286653\n",
            "step: 400, loss: 0.0015411016065627337\n",
            "step: 410, loss: 3.907893187715672e-05\n",
            "step: 420, loss: 0.02056911215186119\n",
            "step: 430, loss: 0.00046847155317664146\n",
            "step: 440, loss: 0.003745677648112178\n",
            "step: 450, loss: 2.287486677232664e-05\n",
            "step: 460, loss: 0.025318648666143417\n",
            "step: 470, loss: 1.368543780699838e-05\n",
            "step: 480, loss: 0.00015277357306331396\n",
            "step: 490, loss: 6.562491034856066e-05\n",
            "step: 500, loss: 0.0012889702338725328\n",
            "step: 510, loss: 6.419308192562312e-05\n",
            "step: 520, loss: 0.0008068914175964892\n",
            "step: 530, loss: 0.0019609297160059214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9522058823529412, f1=0.9431714023831348, best_f1=0.9489939167056621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00185682054143399\n",
            "step: 10, loss: 0.005022736266255379\n",
            "step: 20, loss: 0.0016193331684917212\n",
            "step: 30, loss: 0.01649380847811699\n",
            "step: 40, loss: 1.80962397280382e-05\n",
            "step: 50, loss: 0.0006589906406588852\n",
            "step: 60, loss: 0.0016209259629249573\n",
            "step: 70, loss: 1.6360516383429058e-05\n",
            "step: 80, loss: 0.003048927756026387\n",
            "step: 90, loss: 0.00017082194972317666\n",
            "step: 100, loss: 7.835992437321693e-05\n",
            "step: 110, loss: 0.003931701183319092\n",
            "step: 120, loss: 8.78464343259111e-05\n",
            "step: 130, loss: 0.0025289750192314386\n",
            "step: 140, loss: 0.0011027700966224074\n",
            "step: 150, loss: 6.0444963310146704e-05\n",
            "step: 160, loss: 0.0009118210291489959\n",
            "step: 170, loss: 1.4788932276132982e-05\n",
            "step: 180, loss: 2.6644520403351635e-05\n",
            "step: 190, loss: 0.003125068498775363\n",
            "step: 200, loss: 0.0016613946063444018\n",
            "step: 210, loss: 0.0014104750007390976\n",
            "step: 220, loss: 1.767134563124273e-05\n",
            "step: 230, loss: 0.001040104078128934\n",
            "step: 240, loss: 2.0569188563968055e-05\n",
            "step: 250, loss: 7.399357855319977e-05\n",
            "step: 260, loss: 1.3816598766425159e-05\n",
            "step: 270, loss: 5.7274468417745084e-05\n",
            "step: 280, loss: 2.1899757484789006e-05\n",
            "step: 290, loss: 0.0004913643933832645\n",
            "step: 300, loss: 4.198173337499611e-05\n",
            "step: 310, loss: 0.0453202985227108\n",
            "step: 320, loss: 4.199319664621726e-05\n",
            "step: 330, loss: 1.7228348951903172e-05\n",
            "step: 340, loss: 1.968674587260466e-05\n",
            "step: 350, loss: 0.0018546809442341328\n",
            "step: 360, loss: 0.0005922937416471541\n",
            "step: 370, loss: 1.4613352504966315e-05\n",
            "step: 380, loss: 0.0009397954563610256\n",
            "step: 390, loss: 0.003909976687282324\n",
            "step: 400, loss: 0.005777440033853054\n",
            "step: 410, loss: 7.805398490745574e-05\n",
            "step: 420, loss: 0.0022979455534368753\n",
            "step: 430, loss: 0.00040797522524371743\n",
            "step: 440, loss: 0.003947754390537739\n",
            "step: 450, loss: 0.004116402473300695\n",
            "step: 460, loss: 0.017666101455688477\n",
            "step: 470, loss: 3.478955113678239e-05\n",
            "step: 480, loss: 0.0034676562063395977\n",
            "step: 490, loss: 0.00022699471446685493\n",
            "step: 500, loss: 0.0024785404093563557\n",
            "step: 510, loss: 3.542509148246609e-05\n",
            "step: 520, loss: 1.636038905417081e-05\n",
            "step: 530, loss: 1.3753485291090328e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9513314967860421, f1=0.945537757437071, best_f1=0.9489939167056621\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:23, 241.54it/s]\n",
            "load_f1 = 0.9508804448563485\n",
            "real_f1 = 0.9497464269248502\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 192.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "nyGyaWAphstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5306fa85-2c8a-4cae-f49d-b58d7726a970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5204543471336365\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.44445592164993286\n",
            "step: 20, loss: 0.48205116391181946\n",
            "step: 30, loss: 0.346344530582428\n",
            "step: 40, loss: 0.32370299100875854\n",
            "step: 50, loss: 0.5296871662139893\n",
            "step: 60, loss: 0.40523603558540344\n",
            "step: 70, loss: 0.31843239068984985\n",
            "step: 80, loss: 0.3878343999385834\n",
            "step: 90, loss: 0.25449255108833313\n",
            "step: 100, loss: 0.2612176835536957\n",
            "step: 110, loss: 0.2862390875816345\n",
            "step: 120, loss: 0.3676747977733612\n",
            "step: 130, loss: 0.23270460963249207\n",
            "step: 140, loss: 0.49607983231544495\n",
            "step: 150, loss: 0.30957165360450745\n",
            "step: 160, loss: 0.346572607755661\n",
            "step: 170, loss: 0.19839709997177124\n",
            "step: 180, loss: 0.271323025226593\n",
            "step: 190, loss: 0.48896607756614685\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 200, loss: 0.32567888498306274\n",
            "step: 210, loss: 0.34461069107055664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5504587155963303, f1=0.5571142284569138, best_f1=0.5571142284569138\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25362294912338257\n",
            "step: 10, loss: 0.0943836122751236\n",
            "step: 20, loss: 0.31294354796409607\n",
            "step: 30, loss: 0.3268393874168396\n",
            "step: 40, loss: 0.4135677218437195\n",
            "step: 50, loss: 0.21058088541030884\n",
            "step: 60, loss: 0.3648391366004944\n",
            "step: 70, loss: 0.19315780699253082\n",
            "step: 80, loss: 0.19536042213439941\n",
            "step: 90, loss: 0.14596740901470184\n",
            "step: 100, loss: 0.4408653676509857\n",
            "step: 110, loss: 0.45024287700653076\n",
            "step: 120, loss: 0.159770205616951\n",
            "step: 130, loss: 0.21259279549121857\n",
            "step: 140, loss: 0.1993192434310913\n",
            "step: 150, loss: 0.2745380997657776\n",
            "step: 160, loss: 0.05913499742746353\n",
            "step: 170, loss: 0.25612786412239075\n",
            "step: 180, loss: 0.26973822712898254\n",
            "step: 190, loss: 0.2836220860481262\n",
            "step: 200, loss: 0.10587811470031738\n",
            "step: 210, loss: 0.19501711428165436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6432989690721649, f1=0.6030368763557483, best_f1=0.6030368763557483\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0924774631857872\n",
            "step: 10, loss: 0.07212181389331818\n",
            "step: 20, loss: 0.1407814770936966\n",
            "step: 30, loss: 0.06399515271186829\n",
            "step: 40, loss: 0.3169785141944885\n",
            "step: 50, loss: 0.18412628769874573\n",
            "step: 60, loss: 0.26227933168411255\n",
            "step: 70, loss: 0.18099839985370636\n",
            "step: 80, loss: 0.15664520859718323\n",
            "step: 90, loss: 0.09374655783176422\n",
            "step: 100, loss: 0.15528488159179688\n",
            "step: 110, loss: 0.1418510377407074\n",
            "step: 120, loss: 0.17401796579360962\n",
            "step: 130, loss: 0.13241161406040192\n",
            "step: 140, loss: 0.24967026710510254\n",
            "step: 150, loss: 0.1830185502767563\n",
            "step: 160, loss: 0.10727623850107193\n",
            "step: 170, loss: 0.17548535764217377\n",
            "step: 180, loss: 0.11424857378005981\n",
            "step: 190, loss: 0.06496866792440414\n",
            "step: 200, loss: 0.14333534240722656\n",
            "step: 210, loss: 0.21308235824108124\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6814516129032259, f1=0.6736401673640167, best_f1=0.6736401673640167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09836465120315552\n",
            "step: 10, loss: 0.13565264642238617\n",
            "step: 20, loss: 0.0714925080537796\n",
            "step: 30, loss: 0.07131947576999664\n",
            "step: 40, loss: 0.04665521904826164\n",
            "step: 50, loss: 0.14319421350955963\n",
            "step: 60, loss: 0.26179441809654236\n",
            "step: 70, loss: 0.0833132341504097\n",
            "step: 80, loss: 0.0829327255487442\n",
            "step: 90, loss: 0.06923015415668488\n",
            "step: 100, loss: 0.13550354540348053\n",
            "step: 110, loss: 0.23483745753765106\n",
            "step: 120, loss: 0.19380097091197968\n",
            "step: 130, loss: 0.33453577756881714\n",
            "step: 140, loss: 0.13131281733512878\n",
            "step: 150, loss: 0.09713460505008698\n",
            "step: 160, loss: 0.20997831225395203\n",
            "step: 170, loss: 0.09199447184801102\n",
            "step: 180, loss: 0.03308021277189255\n",
            "step: 190, loss: 0.1660076081752777\n",
            "step: 200, loss: 0.03141496330499649\n",
            "step: 210, loss: 0.4191853702068329\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6802218114602587, f1=0.6730038022813688, best_f1=0.6736401673640167\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1431507021188736\n",
            "step: 10, loss: 0.058938659727573395\n",
            "step: 20, loss: 0.12665033340454102\n",
            "step: 30, loss: 0.03537560626864433\n",
            "step: 40, loss: 0.0766279399394989\n",
            "step: 50, loss: 0.16420520842075348\n",
            "step: 60, loss: 0.11880286782979965\n",
            "step: 70, loss: 0.1627875417470932\n",
            "step: 80, loss: 0.0922975093126297\n",
            "step: 90, loss: 0.12003066390752792\n",
            "step: 100, loss: 0.02341023087501526\n",
            "step: 110, loss: 0.06789064407348633\n",
            "step: 120, loss: 0.14829108119010925\n",
            "step: 130, loss: 0.06113245710730553\n",
            "step: 140, loss: 0.11391206085681915\n",
            "step: 150, loss: 0.10930989682674408\n",
            "step: 160, loss: 0.03712089732289314\n",
            "step: 170, loss: 0.1775893270969391\n",
            "step: 180, loss: 0.12807190418243408\n",
            "step: 190, loss: 0.18507450819015503\n",
            "step: 200, loss: 0.06378979980945587\n",
            "step: 210, loss: 0.0908391922712326\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7171314741035857, f1=0.6880000000000001, best_f1=0.6880000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010551825165748596\n",
            "step: 10, loss: 0.06120726838707924\n",
            "step: 20, loss: 0.04073294997215271\n",
            "step: 30, loss: 0.09850561618804932\n",
            "step: 40, loss: 0.04573439806699753\n",
            "step: 50, loss: 0.07860098034143448\n",
            "step: 60, loss: 0.13739009201526642\n",
            "step: 70, loss: 0.10862398892641068\n",
            "step: 80, loss: 0.14141155779361725\n",
            "step: 90, loss: 0.039288148283958435\n",
            "step: 100, loss: 0.17580841481685638\n",
            "step: 110, loss: 0.07208168506622314\n",
            "step: 120, loss: 0.02343217469751835\n",
            "step: 130, loss: 0.07656055688858032\n",
            "step: 140, loss: 0.07869163900613785\n",
            "step: 150, loss: 0.05555390194058418\n",
            "step: 160, loss: 0.10045108944177628\n",
            "step: 170, loss: 0.14116324484348297\n",
            "step: 180, loss: 0.10796378552913666\n",
            "step: 190, loss: 0.03591613844037056\n",
            "step: 200, loss: 0.05397653952240944\n",
            "step: 210, loss: 0.08426148444414139\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6945454545454546, f1=0.700374531835206, best_f1=0.6880000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11613983660936356\n",
            "step: 10, loss: 0.012826166115701199\n",
            "step: 20, loss: 0.10999773442745209\n",
            "step: 30, loss: 0.085934579372406\n",
            "step: 40, loss: 0.10494457185268402\n",
            "step: 50, loss: 0.11497004330158234\n",
            "step: 60, loss: 0.04273567721247673\n",
            "step: 70, loss: 0.011110448278486729\n",
            "step: 80, loss: 0.10911287367343903\n",
            "step: 90, loss: 0.23498210310935974\n",
            "step: 100, loss: 0.03658336028456688\n",
            "step: 110, loss: 0.20453180372714996\n",
            "step: 120, loss: 0.12468200922012329\n",
            "step: 130, loss: 0.12000086903572083\n",
            "step: 140, loss: 0.06918969005346298\n",
            "step: 150, loss: 0.042039643973112106\n",
            "step: 160, loss: 0.2838110625743866\n",
            "step: 170, loss: 0.13915051519870758\n",
            "step: 180, loss: 0.023063089698553085\n",
            "step: 190, loss: 0.05585508048534393\n",
            "step: 200, loss: 0.044649310410022736\n",
            "step: 210, loss: 0.20069970190525055\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7065420560747664, f1=0.7047619047619047, best_f1=0.6880000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08134378492832184\n",
            "step: 10, loss: 0.06561563163995743\n",
            "step: 20, loss: 0.08877004683017731\n",
            "step: 30, loss: 0.02099023386836052\n",
            "step: 40, loss: 0.019045019522309303\n",
            "step: 50, loss: 0.02572118304669857\n",
            "step: 60, loss: 0.01041332446038723\n",
            "step: 70, loss: 0.07301641255617142\n",
            "step: 80, loss: 0.05404755845665932\n",
            "step: 90, loss: 0.04567752406001091\n",
            "step: 100, loss: 0.10681506246328354\n",
            "step: 110, loss: 0.011736146174371243\n",
            "step: 120, loss: 0.19489572942256927\n",
            "step: 130, loss: 0.01481566857546568\n",
            "step: 140, loss: 0.09489526599645615\n",
            "step: 150, loss: 0.1715685874223709\n",
            "step: 160, loss: 0.22427624464035034\n",
            "step: 170, loss: 0.1718294471502304\n",
            "step: 180, loss: 0.025392621755599976\n",
            "step: 190, loss: 0.01404124777764082\n",
            "step: 200, loss: 0.07400860637426376\n",
            "step: 210, loss: 0.1550140529870987\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.70703125, f1=0.7103174603174603, best_f1=0.6880000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01996748521924019\n",
            "step: 10, loss: 0.1704399585723877\n",
            "step: 20, loss: 0.030652571469545364\n",
            "step: 30, loss: 0.016635362058877945\n",
            "step: 40, loss: 0.14725515246391296\n",
            "step: 50, loss: 0.021704431623220444\n",
            "step: 60, loss: 0.01981111615896225\n",
            "step: 70, loss: 0.11935162544250488\n",
            "step: 80, loss: 0.008596144616603851\n",
            "step: 90, loss: 0.003138526575639844\n",
            "step: 100, loss: 0.04981504753232002\n",
            "step: 110, loss: 0.10894042253494263\n",
            "step: 120, loss: 0.1648443192243576\n",
            "step: 130, loss: 0.04733644053339958\n",
            "step: 140, loss: 0.03595918416976929\n",
            "step: 150, loss: 0.023212825879454613\n",
            "step: 160, loss: 0.14858393371105194\n",
            "step: 170, loss: 0.08493472635746002\n",
            "step: 180, loss: 0.04680859297513962\n",
            "step: 190, loss: 0.02061157301068306\n",
            "step: 200, loss: 0.03568913787603378\n",
            "step: 210, loss: 0.028752649202942848\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6878504672897197, f1=0.6996197718631179, best_f1=0.6880000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005333304405212402\n",
            "step: 10, loss: 0.009074311703443527\n",
            "step: 20, loss: 0.0366930291056633\n",
            "step: 30, loss: 0.005898197181522846\n",
            "step: 40, loss: 0.09324635565280914\n",
            "step: 50, loss: 0.05002511665225029\n",
            "step: 60, loss: 0.010677514597773552\n",
            "step: 70, loss: 0.06868229806423187\n",
            "step: 80, loss: 0.01634952612221241\n",
            "step: 90, loss: 0.05809323862195015\n",
            "step: 100, loss: 0.06252268701791763\n",
            "step: 110, loss: 0.08567079156637192\n",
            "step: 120, loss: 0.05921221151947975\n",
            "step: 130, loss: 0.02497466281056404\n",
            "step: 140, loss: 0.048718880861997604\n",
            "step: 150, loss: 0.013442067429423332\n",
            "step: 160, loss: 0.025134166702628136\n",
            "step: 170, loss: 0.010556574910879135\n",
            "step: 180, loss: 0.01775442063808441\n",
            "step: 190, loss: 0.02277514711022377\n",
            "step: 200, loss: 0.05829544737935066\n",
            "step: 210, loss: 0.04992671310901642\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7056672760511882, f1=0.7026022304832713, best_f1=0.6880000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0591551847755909\n",
            "step: 10, loss: 0.03742740675806999\n",
            "step: 20, loss: 0.08465297520160675\n",
            "step: 30, loss: 0.04610735923051834\n",
            "step: 40, loss: 0.05442475900053978\n",
            "step: 50, loss: 0.024905338883399963\n",
            "step: 60, loss: 0.02981467731297016\n",
            "step: 70, loss: 0.02348119206726551\n",
            "step: 80, loss: 0.018833590671420097\n",
            "step: 90, loss: 0.08011326193809509\n",
            "step: 100, loss: 0.02350062131881714\n",
            "step: 110, loss: 0.039860133081674576\n",
            "step: 120, loss: 0.05309680849313736\n",
            "step: 130, loss: 0.010692841373383999\n",
            "step: 140, loss: 0.06742221862077713\n",
            "step: 150, loss: 0.05523126572370529\n",
            "step: 160, loss: 0.005682777147740126\n",
            "step: 170, loss: 0.05679449066519737\n",
            "step: 180, loss: 0.013057312928140163\n",
            "step: 190, loss: 0.04346107318997383\n",
            "step: 200, loss: 0.01512033399194479\n",
            "step: 210, loss: 0.03875814378261566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.7111984282907662, f1=0.7004048582995952, best_f1=0.6880000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0026921352837234735\n",
            "step: 10, loss: 0.0016754630487412214\n",
            "step: 20, loss: 0.16084198653697968\n",
            "step: 30, loss: 0.0033356046769768\n",
            "step: 40, loss: 0.003332403488457203\n",
            "step: 50, loss: 0.009287690743803978\n",
            "step: 60, loss: 0.004273788072168827\n",
            "step: 70, loss: 0.03149279206991196\n",
            "step: 80, loss: 0.12696312367916107\n",
            "step: 90, loss: 0.17355765402317047\n",
            "step: 100, loss: 0.0025914390571415424\n",
            "step: 110, loss: 0.005097212269902229\n",
            "step: 120, loss: 0.0019363160245120525\n",
            "step: 130, loss: 0.01866072230041027\n",
            "step: 140, loss: 0.2781675457954407\n",
            "step: 150, loss: 0.0037419020663946867\n",
            "step: 160, loss: 0.1003216877579689\n",
            "step: 170, loss: 0.041295379400253296\n",
            "step: 180, loss: 0.06763063371181488\n",
            "step: 190, loss: 0.13111580908298492\n",
            "step: 200, loss: 0.018630243837833405\n",
            "step: 210, loss: 0.06293804198503494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7047970479704797, f1=0.7085714285714286, best_f1=0.6880000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04315698891878128\n",
            "step: 10, loss: 0.04929037392139435\n",
            "step: 20, loss: 0.010230854153633118\n",
            "step: 30, loss: 0.003603088203817606\n",
            "step: 40, loss: 0.013619872741401196\n",
            "step: 50, loss: 0.07097884267568588\n",
            "step: 60, loss: 0.0037598346825689077\n",
            "step: 70, loss: 0.0025797730777412653\n",
            "step: 80, loss: 0.03346508741378784\n",
            "step: 90, loss: 0.006857460364699364\n",
            "step: 100, loss: 0.020555617287755013\n",
            "step: 110, loss: 0.12637004256248474\n",
            "step: 120, loss: 0.012831811793148518\n",
            "step: 130, loss: 0.002184899291023612\n",
            "step: 140, loss: 0.05193113908171654\n",
            "step: 150, loss: 0.002950756112113595\n",
            "step: 160, loss: 0.06999637931585312\n",
            "step: 170, loss: 0.08585961908102036\n",
            "step: 180, loss: 0.09687408804893494\n",
            "step: 190, loss: 0.02572697028517723\n",
            "step: 200, loss: 0.06374017149209976\n",
            "step: 210, loss: 0.040550801903009415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.7100591715976332, f1=0.7030303030303031, best_f1=0.6880000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009266631677746773\n",
            "step: 10, loss: 0.04212541878223419\n",
            "step: 20, loss: 0.007899587973952293\n",
            "step: 30, loss: 0.0032919312361627817\n",
            "step: 40, loss: 0.15613529086112976\n",
            "step: 50, loss: 0.021454960107803345\n",
            "step: 60, loss: 0.03840342164039612\n",
            "step: 70, loss: 0.03520498424768448\n",
            "step: 80, loss: 0.022338127717375755\n",
            "step: 90, loss: 0.0074179088696837425\n",
            "step: 100, loss: 0.0058292546309530735\n",
            "step: 110, loss: 0.003142670262604952\n",
            "step: 120, loss: 0.006605623289942741\n",
            "step: 130, loss: 0.020570239052176476\n",
            "step: 140, loss: 0.00830624345690012\n",
            "step: 150, loss: 0.13285087049007416\n",
            "step: 160, loss: 0.0045492565259337425\n",
            "step: 170, loss: 0.020417578518390656\n",
            "step: 180, loss: 0.001996460137888789\n",
            "step: 190, loss: 0.050434403121471405\n",
            "step: 200, loss: 0.02524098940193653\n",
            "step: 210, loss: 0.08365632593631744\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.700990099009901, f1=0.6987951807228915, best_f1=0.6880000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011488246731460094\n",
            "step: 10, loss: 0.00917941052466631\n",
            "step: 20, loss: 0.03725787624716759\n",
            "step: 30, loss: 0.059596024453639984\n",
            "step: 40, loss: 0.03672037646174431\n",
            "step: 50, loss: 0.002678225515410304\n",
            "step: 60, loss: 0.008130096830427647\n",
            "step: 70, loss: 0.04242724925279617\n",
            "step: 80, loss: 0.10052026808261871\n",
            "step: 90, loss: 0.04838049039244652\n",
            "step: 100, loss: 0.018780097365379333\n",
            "step: 110, loss: 0.09745516628026962\n",
            "step: 120, loss: 0.04439961910247803\n",
            "step: 130, loss: 0.16997268795967102\n",
            "step: 140, loss: 0.014169322326779366\n",
            "step: 150, loss: 0.07656436413526535\n",
            "step: 160, loss: 0.004000192973762751\n",
            "step: 170, loss: 0.004483371507376432\n",
            "step: 180, loss: 0.009357243776321411\n",
            "step: 190, loss: 0.0034678014926612377\n",
            "step: 200, loss: 0.0022190159652382135\n",
            "step: 210, loss: 0.03912757337093353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.7140186915887851, f1=0.7049808429118773, best_f1=0.6880000000000001\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:05, 448.06it/s]\n",
            "load_f1 = 0.7121771217712177\n",
            "real_f1 = 0.7096774193548387\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 189.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "dL0eWrGYhstu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc806de0-2f57-4191-b581-35d1946af62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.46809953451156616\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.44124162197113037\n",
            "step: 20, loss: 0.273418128490448\n",
            "step: 30, loss: 0.4198245406150818\n",
            "step: 40, loss: 0.2516299784183502\n",
            "step: 50, loss: 0.3079386353492737\n",
            "step: 60, loss: 0.44406652450561523\n",
            "step: 70, loss: 0.45767831802368164\n",
            "step: 80, loss: 0.1470637023448944\n",
            "step: 90, loss: 0.3610319197177887\n",
            "step: 100, loss: 0.5344370603561401\n",
            "step: 110, loss: 0.2369273453950882\n",
            "step: 120, loss: 0.3170967400074005\n",
            "step: 130, loss: 0.2993752658367157\n",
            "step: 140, loss: 0.16079939901828766\n",
            "step: 150, loss: 0.3409379720687866\n",
            "step: 160, loss: 0.24357371032238007\n",
            "step: 170, loss: 0.3721579611301422\n",
            "step: 180, loss: 0.16750314831733704\n",
            "step: 190, loss: 0.1566644012928009\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3929799497127533\n",
            "step: 10, loss: 0.32362979650497437\n",
            "step: 20, loss: 0.6283731460571289\n",
            "step: 30, loss: 0.2544998824596405\n",
            "step: 40, loss: 0.5676438212394714\n",
            "step: 50, loss: 0.31128641963005066\n",
            "step: 60, loss: 0.46546876430511475\n",
            "step: 70, loss: 0.32100218534469604\n",
            "step: 80, loss: 0.15285469591617584\n",
            "step: 90, loss: 0.31726256012916565\n",
            "step: 100, loss: 0.24430140852928162\n",
            "step: 110, loss: 0.37549006938934326\n",
            "step: 120, loss: 0.22873279452323914\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 130, loss: 0.5647369623184204\n",
            "step: 140, loss: 0.3069729506969452\n",
            "step: 150, loss: 0.3113163411617279\n",
            "step: 160, loss: 0.31635582447052\n",
            "step: 170, loss: 0.24171464145183563\n",
            "step: 180, loss: 0.17084750533103943\n",
            "step: 190, loss: 0.24224776029586792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.17255252570406795, f1=0.170993733213966, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3773976266384125\n",
            "step: 10, loss: 0.38875091075897217\n",
            "step: 20, loss: 0.46542078256607056\n",
            "step: 30, loss: 0.32381564378738403\n",
            "step: 40, loss: 0.08750315755605698\n",
            "step: 50, loss: 0.38859158754348755\n",
            "step: 60, loss: 0.16112835705280304\n",
            "step: 70, loss: 0.3784819543361664\n",
            "step: 80, loss: 0.3132477104663849\n",
            "step: 90, loss: 0.36997848749160767\n",
            "step: 100, loss: 0.5579948425292969\n",
            "step: 110, loss: 0.6594672203063965\n",
            "step: 120, loss: 0.3712177574634552\n",
            "step: 130, loss: 0.16503696143627167\n",
            "step: 140, loss: 0.37897545099258423\n",
            "step: 150, loss: 0.3035449981689453\n",
            "step: 160, loss: 0.6295177340507507\n",
            "step: 170, loss: 0.4481334984302521\n",
            "step: 180, loss: 0.3913755714893341\n",
            "step: 190, loss: 0.16774539649486542\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24049603939056396\n",
            "step: 10, loss: 0.24121670424938202\n",
            "step: 20, loss: 0.3114635944366455\n",
            "step: 30, loss: 0.23059910535812378\n",
            "step: 40, loss: 0.5472692847251892\n",
            "step: 50, loss: 0.2522914409637451\n",
            "step: 60, loss: 0.37973201274871826\n",
            "step: 70, loss: 0.3202034831047058\n",
            "step: 80, loss: 0.2333616316318512\n",
            "step: 90, loss: 0.17413434386253357\n",
            "step: 100, loss: 0.32002705335617065\n",
            "step: 110, loss: 0.369820237159729\n",
            "step: 120, loss: 0.2584103047847748\n",
            "step: 130, loss: 0.48201441764831543\n",
            "step: 140, loss: 0.29965949058532715\n",
            "step: 150, loss: 0.23375371098518372\n",
            "step: 160, loss: 0.3042123019695282\n",
            "step: 170, loss: 0.4506773054599762\n",
            "step: 180, loss: 0.37391528487205505\n",
            "step: 190, loss: 0.16411082446575165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3877078592777252\n",
            "step: 10, loss: 0.38815534114837646\n",
            "step: 20, loss: 0.16553238034248352\n",
            "step: 30, loss: 0.11227987706661224\n",
            "step: 40, loss: 0.3122049570083618\n",
            "step: 50, loss: 0.5132584571838379\n",
            "step: 60, loss: 0.24661991000175476\n",
            "step: 70, loss: 0.38795197010040283\n",
            "step: 80, loss: 0.3730844259262085\n",
            "step: 90, loss: 0.31018954515457153\n",
            "step: 100, loss: 0.4553123116493225\n",
            "step: 110, loss: 0.406453937292099\n",
            "step: 120, loss: 0.23014920949935913\n",
            "step: 130, loss: 0.5578641295433044\n",
            "step: 140, loss: 0.3854200839996338\n",
            "step: 150, loss: 0.31442368030548096\n",
            "step: 160, loss: 0.16527828574180603\n",
            "step: 170, loss: 0.38708725571632385\n",
            "step: 180, loss: 0.24737673997879028\n",
            "step: 190, loss: 0.31603190302848816\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31236815452575684\n",
            "step: 10, loss: 0.25195637345314026\n",
            "step: 20, loss: 0.3163641393184662\n",
            "step: 30, loss: 0.47537392377853394\n",
            "step: 40, loss: 0.24648517370224\n",
            "step: 50, loss: 0.31295669078826904\n",
            "step: 60, loss: 0.4460686445236206\n",
            "step: 70, loss: 0.311991423368454\n",
            "step: 80, loss: 0.31610459089279175\n",
            "step: 90, loss: 0.23177161812782288\n",
            "step: 100, loss: 0.49651023745536804\n",
            "step: 110, loss: 0.23813335597515106\n",
            "step: 120, loss: 0.4588446021080017\n",
            "step: 130, loss: 0.5209100246429443\n",
            "step: 140, loss: 0.18974804878234863\n",
            "step: 150, loss: 0.383613646030426\n",
            "step: 160, loss: 0.3817117214202881\n",
            "step: 170, loss: 0.3801420032978058\n",
            "step: 180, loss: 0.17798203229904175\n",
            "step: 190, loss: 0.308363139629364\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3067079186439514\n",
            "step: 10, loss: 0.31778815388679504\n",
            "step: 20, loss: 0.30850768089294434\n",
            "step: 30, loss: 0.18696622550487518\n",
            "step: 40, loss: 0.31940826773643494\n",
            "step: 50, loss: 0.08845797926187515\n",
            "step: 60, loss: 0.15647682547569275\n",
            "step: 70, loss: 0.15838924050331116\n",
            "step: 80, loss: 0.2319052666425705\n",
            "step: 90, loss: 0.2340557724237442\n",
            "step: 100, loss: 0.5710362792015076\n",
            "step: 110, loss: 0.4456811845302582\n",
            "step: 120, loss: 0.38255414366722107\n",
            "step: 130, loss: 0.3157128095626831\n",
            "step: 140, loss: 0.24717092514038086\n",
            "step: 150, loss: 0.3109336197376251\n",
            "step: 160, loss: 0.3765864074230194\n",
            "step: 170, loss: 0.3126183748245239\n",
            "step: 180, loss: 0.2377414107322693\n",
            "step: 190, loss: 0.3144899010658264\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23826007544994354\n",
            "step: 10, loss: 0.3877915143966675\n",
            "step: 20, loss: 0.24048307538032532\n",
            "step: 30, loss: 0.3803219795227051\n",
            "step: 40, loss: 0.16971951723098755\n",
            "step: 50, loss: 0.31196334958076477\n",
            "step: 60, loss: 0.45370253920555115\n",
            "step: 70, loss: 0.24155788123607635\n",
            "step: 80, loss: 0.388068825006485\n",
            "step: 90, loss: 0.24134831130504608\n",
            "step: 100, loss: 0.30892252922058105\n",
            "step: 110, loss: 0.3784017860889435\n",
            "step: 120, loss: 0.39314982295036316\n",
            "step: 130, loss: 0.3185468912124634\n",
            "step: 140, loss: 0.3863335847854614\n",
            "step: 150, loss: 0.3188410997390747\n",
            "step: 160, loss: 0.19720429182052612\n",
            "step: 170, loss: 0.247055321931839\n",
            "step: 180, loss: 0.3072035312652588\n",
            "step: 190, loss: 0.31105902791023254\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2446664273738861\n",
            "step: 10, loss: 0.09769579768180847\n",
            "step: 20, loss: 0.3125000298023224\n",
            "step: 30, loss: 0.18309447169303894\n",
            "step: 40, loss: 0.3139534294605255\n",
            "step: 50, loss: 0.3795013725757599\n",
            "step: 60, loss: 0.378933846950531\n",
            "step: 70, loss: 0.16779421269893646\n",
            "step: 80, loss: 0.3910621702671051\n",
            "step: 90, loss: 0.703658401966095\n",
            "step: 100, loss: 0.3786983788013458\n",
            "step: 110, loss: 0.37393972277641296\n",
            "step: 120, loss: 0.586990475654602\n",
            "step: 130, loss: 0.3106079697608948\n",
            "step: 140, loss: 0.3123110830783844\n",
            "step: 150, loss: 0.2389499545097351\n",
            "step: 160, loss: 0.23198562860488892\n",
            "step: 170, loss: 0.4603809714317322\n",
            "step: 180, loss: 0.30456435680389404\n",
            "step: 190, loss: 0.2424805760383606\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2404877245426178\n",
            "step: 10, loss: 0.17320232093334198\n",
            "step: 20, loss: 0.30556198954582214\n",
            "step: 30, loss: 0.37894442677497864\n",
            "step: 40, loss: 0.30690234899520874\n",
            "step: 50, loss: 0.09904744476079941\n",
            "step: 60, loss: 0.31820398569107056\n",
            "step: 70, loss: 0.30769187211990356\n",
            "step: 80, loss: 0.38161465525627136\n",
            "step: 90, loss: 0.1609327495098114\n",
            "step: 100, loss: 0.23597395420074463\n",
            "step: 110, loss: 0.2345932424068451\n",
            "step: 120, loss: 0.38448283076286316\n",
            "step: 130, loss: 0.5185496211051941\n",
            "step: 140, loss: 0.38595110177993774\n",
            "step: 150, loss: 0.3057057559490204\n",
            "step: 160, loss: 0.253286749124527\n",
            "step: 170, loss: 0.3796997666358948\n",
            "step: 180, loss: 0.23863625526428223\n",
            "step: 190, loss: 0.16176390647888184\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5385518074035645\n",
            "step: 10, loss: 0.09501645714044571\n",
            "step: 20, loss: 0.23448780179023743\n",
            "step: 30, loss: 0.23736734688282013\n",
            "step: 40, loss: 0.08939812332391739\n",
            "step: 50, loss: 0.31512919068336487\n",
            "step: 60, loss: 0.23743148148059845\n",
            "step: 70, loss: 0.6232336759567261\n",
            "step: 80, loss: 0.37509679794311523\n",
            "step: 90, loss: 0.3077530860900879\n",
            "step: 100, loss: 0.17253656685352325\n",
            "step: 110, loss: 0.2455463856458664\n",
            "step: 120, loss: 0.30644652247428894\n",
            "step: 130, loss: 0.5951719284057617\n",
            "step: 140, loss: 0.4648991823196411\n",
            "step: 150, loss: 0.31164437532424927\n",
            "step: 160, loss: 0.3109680116176605\n",
            "step: 170, loss: 0.4427157938480377\n",
            "step: 180, loss: 0.314277321100235\n",
            "step: 190, loss: 0.11206144094467163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.39014461636543274\n",
            "step: 10, loss: 0.3094594478607178\n",
            "step: 20, loss: 0.5326707363128662\n",
            "step: 30, loss: 0.37707364559173584\n",
            "step: 40, loss: 0.30448704957962036\n",
            "step: 50, loss: 0.5205159783363342\n",
            "step: 60, loss: 0.4611339569091797\n",
            "step: 70, loss: 0.38837218284606934\n",
            "step: 80, loss: 0.38481757044792175\n",
            "step: 90, loss: 0.4526844322681427\n",
            "step: 100, loss: 0.3137587904930115\n",
            "step: 110, loss: 0.45501843094825745\n",
            "step: 120, loss: 0.17064246535301208\n",
            "step: 130, loss: 0.23620331287384033\n",
            "step: 140, loss: 0.3741360604763031\n",
            "step: 150, loss: 0.37340325117111206\n",
            "step: 160, loss: 0.24377384781837463\n",
            "step: 170, loss: 0.44862255454063416\n",
            "step: 180, loss: 0.3115696609020233\n",
            "step: 190, loss: 0.1715843230485916\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3063986003398895\n",
            "step: 10, loss: 0.30692270398139954\n",
            "step: 20, loss: 0.3862365782260895\n",
            "step: 30, loss: 0.38536185026168823\n",
            "step: 40, loss: 0.24732144176959991\n",
            "step: 50, loss: 0.2413519024848938\n",
            "step: 60, loss: 0.3173897862434387\n",
            "step: 70, loss: 0.16327396035194397\n",
            "step: 80, loss: 0.3126620352268219\n",
            "step: 90, loss: 0.3066665828227997\n",
            "step: 100, loss: 0.16725915670394897\n",
            "step: 110, loss: 0.5976550579071045\n",
            "step: 120, loss: 0.16718338429927826\n",
            "step: 130, loss: 0.23875540494918823\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 140, loss: 0.30774369835853577\n",
            "step: 150, loss: 0.3760443329811096\n",
            "step: 160, loss: 0.45520535111427307\n",
            "step: 170, loss: 0.10299882292747498\n",
            "step: 180, loss: 0.23936845362186432\n",
            "step: 190, loss: 0.3117547035217285\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31128832697868347\n",
            "step: 10, loss: 0.2425956428050995\n",
            "step: 20, loss: 0.39247727394104004\n",
            "step: 30, loss: 0.10381759703159332\n",
            "step: 40, loss: 0.3122803866863251\n",
            "step: 50, loss: 0.38104215264320374\n",
            "step: 60, loss: 0.3156180679798126\n",
            "step: 70, loss: 0.2400793582201004\n",
            "step: 80, loss: 0.39648333191871643\n",
            "step: 90, loss: 0.165862575173378\n",
            "step: 100, loss: 0.3145937919616699\n",
            "step: 110, loss: 0.45210933685302734\n",
            "step: 120, loss: 0.30896613001823425\n",
            "step: 130, loss: 0.31256037950515747\n",
            "step: 140, loss: 0.17226073145866394\n",
            "step: 150, loss: 0.455392986536026\n",
            "step: 160, loss: 0.3813740909099579\n",
            "step: 170, loss: 0.23452100157737732\n",
            "step: 180, loss: 0.30747637152671814\n",
            "step: 190, loss: 0.5263765454292297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5374159216880798\n",
            "step: 10, loss: 0.24098488688468933\n",
            "step: 20, loss: 0.24115359783172607\n",
            "step: 30, loss: 0.3068667948246002\n",
            "step: 40, loss: 0.654503583908081\n",
            "step: 50, loss: 0.31562161445617676\n",
            "step: 60, loss: 0.17333218455314636\n",
            "step: 70, loss: 0.25012606382369995\n",
            "step: 80, loss: 0.23803509771823883\n",
            "step: 90, loss: 0.31348690390586853\n",
            "step: 100, loss: 0.3791303336620331\n",
            "step: 110, loss: 0.4522913098335266\n",
            "step: 120, loss: 0.235283762216568\n",
            "step: 130, loss: 0.3042800724506378\n",
            "step: 140, loss: 0.4519476890563965\n",
            "step: 150, loss: 0.1689731329679489\n",
            "step: 160, loss: 0.3120405972003937\n",
            "step: 170, loss: 0.7463729381561279\n",
            "step: 180, loss: 0.30890387296676636\n",
            "step: 190, loss: 0.45753175020217896\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.170993733213966\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 238.18it/s]\n",
            "load_f1 = 0.1724137931034483\n",
            "real_f1 = 0.17239839213934793\n",
            "733it [00:00, 3462.58it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 189.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO TEXTUAL"
      ],
      "metadata": {
        "id": "zW6LV4zMhstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "3an30TrShstv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed603d6-9dd0-4a82-b6ac-635fac0507c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4796449542045593\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.49684906005859375\n",
            "step: 20, loss: 0.2734176814556122\n",
            "step: 30, loss: 0.3796963095664978\n",
            "step: 40, loss: 0.5247427225112915\n",
            "step: 50, loss: 0.3416529595851898\n",
            "step: 60, loss: 0.5236034393310547\n",
            "step: 70, loss: 0.3107782006263733\n",
            "step: 80, loss: 0.25894659757614136\n",
            "step: 90, loss: 0.2133503556251526\n",
            "step: 100, loss: 0.18393012881278992\n",
            "step: 110, loss: 0.38099223375320435\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.29460930824279785\n",
            "step: 130, loss: 0.3079601228237152\n",
            "step: 140, loss: 0.38826894760131836\n",
            "step: 150, loss: 0.30827468633651733\n",
            "step: 160, loss: 0.4028743505477905\n",
            "step: 170, loss: 0.3258276879787445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.19615384615384615, f1=0.19509851033157133, best_f1=0.19509851033157133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31310898065567017\n",
            "step: 10, loss: 0.4544471800327301\n",
            "step: 20, loss: 0.3106052577495575\n",
            "step: 30, loss: 0.30191949009895325\n",
            "step: 40, loss: 0.05828477814793587\n",
            "step: 50, loss: 0.46576541662216187\n",
            "step: 60, loss: 0.1726066917181015\n",
            "step: 70, loss: 0.49339327216148376\n",
            "step: 80, loss: 0.22759319841861725\n",
            "step: 90, loss: 0.24887855350971222\n",
            "step: 100, loss: 0.5239108800888062\n",
            "step: 110, loss: 0.2609930634498596\n",
            "step: 120, loss: 0.2569308876991272\n",
            "step: 130, loss: 0.541729211807251\n",
            "step: 140, loss: 0.5253250598907471\n",
            "step: 150, loss: 0.41496938467025757\n",
            "step: 160, loss: 0.4195663332939148\n",
            "step: 170, loss: 0.4122472107410431\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.19509851033157133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6104037761688232\n",
            "step: 10, loss: 0.32911673188209534\n",
            "step: 20, loss: 0.2405547797679901\n",
            "step: 30, loss: 0.25820404291152954\n",
            "step: 40, loss: 0.3714671730995178\n",
            "step: 50, loss: 0.5883455872535706\n",
            "step: 60, loss: 0.3161734342575073\n",
            "step: 70, loss: 0.23052072525024414\n",
            "step: 80, loss: 0.3719688951969147\n",
            "step: 90, loss: 0.5422374606132507\n",
            "step: 100, loss: 0.33540067076683044\n",
            "step: 110, loss: 0.16701389849185944\n",
            "step: 120, loss: 0.5885420441627502\n",
            "step: 130, loss: 0.4839666485786438\n",
            "step: 140, loss: 0.46025019884109497\n",
            "step: 150, loss: 0.19927649199962616\n",
            "step: 160, loss: 0.17334763705730438\n",
            "step: 170, loss: 0.31138256192207336\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.19509851033157133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3754129707813263\n",
            "step: 10, loss: 0.5372826457023621\n",
            "step: 20, loss: 0.2506229877471924\n",
            "step: 30, loss: 0.43891987204551697\n",
            "step: 40, loss: 0.24654383957386017\n",
            "step: 50, loss: 0.3852066099643707\n",
            "step: 60, loss: 0.7068683505058289\n",
            "step: 70, loss: 0.3425576388835907\n",
            "step: 80, loss: 0.4391467869281769\n",
            "step: 90, loss: 0.3069654703140259\n",
            "step: 100, loss: 0.3746148943901062\n",
            "step: 110, loss: 0.4431656301021576\n",
            "step: 120, loss: 0.4898681044578552\n",
            "step: 130, loss: 0.3033302426338196\n",
            "step: 140, loss: 0.2388259321451187\n",
            "step: 150, loss: 0.6722103953361511\n",
            "step: 160, loss: 0.12751434743404388\n",
            "step: 170, loss: 0.24266043305397034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.19509851033157133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37829726934432983\n",
            "step: 10, loss: 0.3253650367259979\n",
            "step: 20, loss: 0.318024218082428\n",
            "step: 30, loss: 0.30747196078300476\n",
            "step: 40, loss: 0.25065314769744873\n",
            "step: 50, loss: 0.23768958449363708\n",
            "step: 60, loss: 0.3132242262363434\n",
            "step: 70, loss: 0.3320189118385315\n",
            "step: 80, loss: 0.07391687482595444\n",
            "step: 90, loss: 0.6273192763328552\n",
            "step: 100, loss: 0.2547493278980255\n",
            "step: 110, loss: 0.25394460558891296\n",
            "step: 120, loss: 0.12581779062747955\n",
            "step: 130, loss: 0.2536751329898834\n",
            "step: 140, loss: 0.17465807497501373\n",
            "step: 150, loss: 0.30880340933799744\n",
            "step: 160, loss: 0.24868103861808777\n",
            "step: 170, loss: 0.3767927885055542\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.19509851033157133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.31045106053352356\n",
            "step: 10, loss: 0.11362604051828384\n",
            "step: 20, loss: 0.37092843651771545\n",
            "step: 30, loss: 0.24456791579723358\n",
            "step: 40, loss: 0.44965818524360657\n",
            "step: 50, loss: 0.2576751708984375\n",
            "step: 60, loss: 0.4094502031803131\n",
            "step: 70, loss: 0.3781326413154602\n",
            "step: 80, loss: 0.19025103747844696\n",
            "step: 90, loss: 0.3218136727809906\n",
            "step: 100, loss: 0.1870298683643341\n",
            "step: 110, loss: 0.4535766839981079\n",
            "step: 120, loss: 0.4419199526309967\n",
            "step: 130, loss: 0.4870277941226959\n",
            "step: 140, loss: 0.25056231021881104\n",
            "step: 150, loss: 0.16209495067596436\n",
            "step: 160, loss: 0.3117968738079071\n",
            "step: 170, loss: 0.23809745907783508\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.19509851033157133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.24171456694602966\n",
            "step: 10, loss: 0.45067647099494934\n",
            "step: 20, loss: 0.45677441358566284\n",
            "step: 30, loss: 0.5190474987030029\n",
            "step: 40, loss: 0.12999822199344635\n",
            "step: 50, loss: 0.4298291802406311\n",
            "step: 60, loss: 0.5359781980514526\n",
            "step: 70, loss: 0.3607417941093445\n",
            "step: 80, loss: 0.1724051982164383\n",
            "step: 90, loss: 0.4469091296195984\n",
            "step: 100, loss: 0.3056996166706085\n",
            "step: 110, loss: 0.3683735430240631\n",
            "step: 120, loss: 0.3569650948047638\n",
            "step: 130, loss: 0.16821564733982086\n",
            "step: 140, loss: 0.09507281333208084\n",
            "step: 150, loss: 0.3203033208847046\n",
            "step: 160, loss: 0.37840932607650757\n",
            "step: 170, loss: 0.31282490491867065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.24764468371467024, f1=0.22811671087533159, best_f1=0.22811671087533159\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4394848644733429\n",
            "step: 10, loss: 0.5089384913444519\n",
            "step: 20, loss: 0.24146701395511627\n",
            "step: 30, loss: 0.30201414227485657\n",
            "step: 40, loss: 0.2322951704263687\n",
            "step: 50, loss: 0.36524486541748047\n",
            "step: 60, loss: 0.35631486773490906\n",
            "step: 70, loss: 0.229348286986351\n",
            "step: 80, loss: 0.3112145662307739\n",
            "step: 90, loss: 0.49558889865875244\n",
            "step: 100, loss: 0.23744793236255646\n",
            "step: 110, loss: 0.5244325399398804\n",
            "step: 120, loss: 0.3239734470844269\n",
            "step: 130, loss: 0.3178585469722748\n",
            "step: 140, loss: 0.4608125388622284\n",
            "step: 150, loss: 0.2722031772136688\n",
            "step: 160, loss: 0.15686921775341034\n",
            "step: 170, loss: 0.45353463292121887\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.284688995215311, f1=0.25806451612903225, best_f1=0.25806451612903225\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2872990071773529\n",
            "step: 10, loss: 0.5502951145172119\n",
            "step: 20, loss: 0.37323930859565735\n",
            "step: 30, loss: 0.14938588440418243\n",
            "step: 40, loss: 0.3815414309501648\n",
            "step: 50, loss: 0.20372554659843445\n",
            "step: 60, loss: 0.2997150421142578\n",
            "step: 70, loss: 0.3606926500797272\n",
            "step: 80, loss: 0.15151533484458923\n",
            "step: 90, loss: 0.4198896884918213\n",
            "step: 100, loss: 0.47646480798721313\n",
            "step: 110, loss: 0.3426738679409027\n",
            "step: 120, loss: 0.41460666060447693\n",
            "step: 130, loss: 0.28031203150749207\n",
            "step: 140, loss: 0.34792497754096985\n",
            "step: 150, loss: 0.3438016474246979\n",
            "step: 160, loss: 0.25117018818855286\n",
            "step: 170, loss: 0.4078541398048401\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.28835489833641403, f1=0.23970037453183518, best_f1=0.23970037453183518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4369175434112549\n",
            "step: 10, loss: 0.2812469005584717\n",
            "step: 20, loss: 0.23564413189888\n",
            "step: 30, loss: 0.6073693037033081\n",
            "step: 40, loss: 0.34798431396484375\n",
            "step: 50, loss: 0.34104984998703003\n",
            "step: 60, loss: 0.42365768551826477\n",
            "step: 70, loss: 0.2539379894733429\n",
            "step: 80, loss: 0.27211496233940125\n",
            "step: 90, loss: 0.2252773493528366\n",
            "step: 100, loss: 0.21758247911930084\n",
            "step: 110, loss: 0.24068138003349304\n",
            "step: 120, loss: 0.26480832695961\n",
            "step: 130, loss: 0.12691615521907806\n",
            "step: 140, loss: 0.3144363462924957\n",
            "step: 150, loss: 0.3029501438140869\n",
            "step: 160, loss: 0.22388306260108948\n",
            "step: 170, loss: 0.39685696363449097\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.3054331864904552, f1=0.25852272727272724, best_f1=0.25852272727272724\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3934549391269684\n",
            "step: 10, loss: 0.2658974528312683\n",
            "step: 20, loss: 0.17762893438339233\n",
            "step: 30, loss: 0.44905853271484375\n",
            "step: 40, loss: 0.20726509392261505\n",
            "step: 50, loss: 0.1519460380077362\n",
            "step: 60, loss: 0.35223308205604553\n",
            "step: 70, loss: 0.26750531792640686\n",
            "step: 80, loss: 0.18342231214046478\n",
            "step: 90, loss: 0.34988051652908325\n",
            "step: 100, loss: 0.4269312918186188\n",
            "step: 110, loss: 0.3181116580963135\n",
            "step: 120, loss: 0.40747809410095215\n",
            "step: 130, loss: 0.3897997736930847\n",
            "step: 140, loss: 0.4917646646499634\n",
            "step: 150, loss: 0.3763106167316437\n",
            "step: 160, loss: 0.23177723586559296\n",
            "step: 170, loss: 0.14548908174037933\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.45177664974619286, f1=0.418848167539267, best_f1=0.418848167539267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.34818580746650696\n",
            "step: 10, loss: 0.252633661031723\n",
            "step: 20, loss: 0.15402118861675262\n",
            "step: 30, loss: 0.2730884253978729\n",
            "step: 40, loss: 0.21346242725849152\n",
            "step: 50, loss: 0.13716651499271393\n",
            "step: 60, loss: 0.2541043758392334\n",
            "step: 70, loss: 0.18314093351364136\n",
            "step: 80, loss: 0.1781444549560547\n",
            "step: 90, loss: 0.16486920416355133\n",
            "step: 100, loss: 0.1605304628610611\n",
            "step: 110, loss: 0.2629065215587616\n",
            "step: 120, loss: 0.16050441563129425\n",
            "step: 130, loss: 0.2518408000469208\n",
            "step: 140, loss: 0.13923193514347076\n",
            "step: 150, loss: 0.10592562705278397\n",
            "step: 160, loss: 0.34765979647636414\n",
            "step: 170, loss: 0.36396968364715576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.7481662591687042, f1=0.7848699763593381, best_f1=0.7848699763593381\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15256454050540924\n",
            "step: 10, loss: 0.0437600277364254\n",
            "step: 20, loss: 0.12264621257781982\n",
            "step: 30, loss: 0.22168055176734924\n",
            "step: 40, loss: 0.24100081622600555\n",
            "step: 50, loss: 0.14644353091716766\n",
            "step: 60, loss: 0.11409671604633331\n",
            "step: 70, loss: 0.4024983048439026\n",
            "step: 80, loss: 0.056299418210983276\n",
            "step: 90, loss: 0.21942003071308136\n",
            "step: 100, loss: 0.07679345458745956\n",
            "step: 110, loss: 0.06079447269439697\n",
            "step: 120, loss: 0.03590155020356178\n",
            "step: 130, loss: 0.09686533361673355\n",
            "step: 140, loss: 0.17862039804458618\n",
            "step: 150, loss: 0.1707839071750641\n",
            "step: 160, loss: 0.2458382546901703\n",
            "step: 170, loss: 0.036441683769226074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.7736842105263158, f1=0.8109452736318408, best_f1=0.8109452736318408\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.13591726124286652\n",
            "step: 10, loss: 0.0622139535844326\n",
            "step: 20, loss: 0.14409968256950378\n",
            "step: 30, loss: 0.14361073076725006\n",
            "step: 40, loss: 0.09267888218164444\n",
            "step: 50, loss: 0.020652739331126213\n",
            "step: 60, loss: 0.12286754697561264\n",
            "step: 70, loss: 0.058382607996463776\n",
            "step: 80, loss: 0.029188813641667366\n",
            "step: 90, loss: 0.0358792208135128\n",
            "step: 100, loss: 0.1942116767168045\n",
            "step: 110, loss: 0.12081395834684372\n",
            "step: 120, loss: 0.07683485746383667\n",
            "step: 130, loss: 0.1364748477935791\n",
            "step: 140, loss: 0.06258697062730789\n",
            "step: 150, loss: 0.07330258190631866\n",
            "step: 160, loss: 0.20412221550941467\n",
            "step: 170, loss: 0.1323481947183609\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.7840375586854459, f1=0.7877461706783371, best_f1=0.7877461706783371\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.026065926998853683\n",
            "step: 10, loss: 0.07821966707706451\n",
            "step: 20, loss: 0.024715131148695946\n",
            "step: 30, loss: 0.08543524146080017\n",
            "step: 40, loss: 0.004769920837134123\n",
            "step: 50, loss: 0.07510899752378464\n",
            "step: 60, loss: 0.010077297687530518\n",
            "step: 70, loss: 0.22654062509536743\n",
            "step: 80, loss: 0.039461567997932434\n",
            "step: 90, loss: 0.14541180431842804\n",
            "step: 100, loss: 0.2209073305130005\n",
            "step: 110, loss: 0.07369574904441833\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 120, loss: 0.026296531781554222\n",
            "step: 130, loss: 0.1896834373474121\n",
            "step: 140, loss: 0.07882322371006012\n",
            "step: 150, loss: 0.012008250690996647\n",
            "step: 160, loss: 0.05901148542761803\n",
            "step: 170, loss: 0.17800599336624146\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.7898734177215191, f1=0.827250608272506, best_f1=0.827250608272506\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 310.13it/s]\n",
            "load_f1 = 0.5346938775510204\n",
            "real_f1 = 0.5175600739371534\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 187.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO DIRTY"
      ],
      "metadata": {
        "id": "VngEb4vfhstw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "QfPaCqR4hstw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f2b1b7d-9a18-4c9b-bfa4-359a6811fa39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5638386011123657\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4523698687553406\n",
            "step: 20, loss: 0.5943716764450073\n",
            "step: 30, loss: 0.32205304503440857\n",
            "step: 40, loss: 0.2811664044857025\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.3941330015659332\n",
            "step: 60, loss: 0.15165716409683228\n",
            "step: 70, loss: 0.149578258395195\n",
            "step: 80, loss: 0.0649687871336937\n",
            "step: 90, loss: 0.09953347593545914\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 100, loss: 0.28895601630210876\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "step: 110, loss: 0.09085365384817123\n",
            "step: 120, loss: 0.079067662358284\n",
            "step: 130, loss: 0.15373997390270233\n",
            "step: 140, loss: 0.019916338846087456\n",
            "step: 150, loss: 0.13686060905456543\n",
            "step: 160, loss: 0.02022949792444706\n",
            "step: 170, loss: 0.10317860543727875\n",
            "step: 180, loss: 0.1300976425409317\n",
            "step: 190, loss: 0.035699598491191864\n",
            "step: 200, loss: 0.025361714884638786\n",
            "step: 210, loss: 0.025572264567017555\n",
            "step: 220, loss: 0.12259423732757568\n",
            "step: 230, loss: 0.005914803594350815\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9642857142857144, f1=0.971687429218573, best_f1=0.971687429218573\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0039229257963597775\n",
            "step: 10, loss: 0.09988950937986374\n",
            "step: 20, loss: 0.014446964487433434\n",
            "step: 30, loss: 0.011932021006941795\n",
            "step: 40, loss: 0.019484175369143486\n",
            "step: 50, loss: 0.009907406754791737\n",
            "step: 60, loss: 0.004133705515414476\n",
            "step: 70, loss: 0.0241899024695158\n",
            "step: 80, loss: 0.017749611288309097\n",
            "step: 90, loss: 0.04406895861029625\n",
            "step: 100, loss: 0.0009312891634181142\n",
            "step: 110, loss: 0.020699385553598404\n",
            "step: 120, loss: 0.012304089963436127\n",
            "step: 130, loss: 0.018607843667268753\n",
            "step: 140, loss: 0.009833796881139278\n",
            "step: 150, loss: 0.0781368538737297\n",
            "step: 160, loss: 0.022741515189409256\n",
            "step: 170, loss: 0.0011244738707318902\n",
            "step: 180, loss: 0.00954620074480772\n",
            "step: 190, loss: 0.007966360077261925\n",
            "step: 200, loss: 0.020138736814260483\n",
            "step: 210, loss: 0.05118654668331146\n",
            "step: 220, loss: 0.0043209632858633995\n",
            "step: 230, loss: 0.006040541455149651\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9753914988814317, f1=0.9727891156462585, best_f1=0.9727891156462585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028138870373368263\n",
            "step: 10, loss: 0.0064546167850494385\n",
            "step: 20, loss: 0.02040894888341427\n",
            "step: 30, loss: 0.007111228536814451\n",
            "step: 40, loss: 0.06290104240179062\n",
            "step: 50, loss: 0.04505313187837601\n",
            "step: 60, loss: 0.020750729367136955\n",
            "step: 70, loss: 0.0072885374538600445\n",
            "step: 80, loss: 0.04730507731437683\n",
            "step: 90, loss: 0.0063994573429226875\n",
            "step: 100, loss: 0.0037176255136728287\n",
            "step: 110, loss: 0.04694876819849014\n",
            "step: 120, loss: 0.0010777291608974338\n",
            "step: 130, loss: 0.051488690078258514\n",
            "step: 140, loss: 0.004397118929773569\n",
            "step: 150, loss: 0.05093053728342056\n",
            "step: 160, loss: 0.0013061942299827933\n",
            "step: 170, loss: 0.0009630421991460025\n",
            "step: 180, loss: 0.0036893479991704226\n",
            "step: 190, loss: 0.004109215456992388\n",
            "step: 200, loss: 0.00524417869746685\n",
            "step: 210, loss: 0.00297450739890337\n",
            "step: 220, loss: 0.18423093855381012\n",
            "step: 230, loss: 0.018982985988259315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9688195991091313, f1=0.968609865470852, best_f1=0.9727891156462585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028713420033454895\n",
            "step: 10, loss: 0.008731655776500702\n",
            "step: 20, loss: 0.010256528854370117\n",
            "step: 30, loss: 0.00821844395250082\n",
            "step: 40, loss: 0.15295520424842834\n",
            "step: 50, loss: 0.02324007824063301\n",
            "step: 60, loss: 0.04432080313563347\n",
            "step: 70, loss: 0.009164699353277683\n",
            "step: 80, loss: 0.10067487508058548\n",
            "step: 90, loss: 0.05881055071949959\n",
            "step: 100, loss: 0.030152132734656334\n",
            "step: 110, loss: 0.0008258325979113579\n",
            "step: 120, loss: 0.004054793156683445\n",
            "step: 130, loss: 0.008922174572944641\n",
            "step: 140, loss: 0.0061120400205254555\n",
            "step: 150, loss: 0.0003503590414766222\n",
            "step: 160, loss: 0.019353248178958893\n",
            "step: 170, loss: 0.014148274436593056\n",
            "step: 180, loss: 0.005203989800065756\n",
            "step: 190, loss: 0.001507187495008111\n",
            "step: 200, loss: 0.15500666201114655\n",
            "step: 210, loss: 0.004026660695672035\n",
            "step: 220, loss: 0.005589818581938744\n",
            "step: 230, loss: 0.022959917783737183\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9741282339707535, f1=0.9607182940516273, best_f1=0.9727891156462585\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011314123403280973\n",
            "step: 10, loss: 0.0028423615731298923\n",
            "step: 20, loss: 0.12356982380151749\n",
            "step: 30, loss: 0.010613247752189636\n",
            "step: 40, loss: 0.0022879713214933872\n",
            "step: 50, loss: 0.0008944576256908476\n",
            "step: 60, loss: 0.1055261492729187\n",
            "step: 70, loss: 0.004661455750465393\n",
            "step: 80, loss: 0.011240310035645962\n",
            "step: 90, loss: 0.03648453205823898\n",
            "step: 100, loss: 0.004764285869896412\n",
            "step: 110, loss: 0.0007651667692698538\n",
            "step: 120, loss: 0.0011033033952116966\n",
            "step: 130, loss: 0.0004997379146516323\n",
            "step: 140, loss: 0.0017065175343304873\n",
            "step: 150, loss: 0.018099121749401093\n",
            "step: 160, loss: 0.0009027851046994328\n",
            "step: 170, loss: 0.0004492372099775821\n",
            "step: 180, loss: 0.009134571067988873\n",
            "step: 190, loss: 0.02304721251130104\n",
            "step: 200, loss: 0.0010214895009994507\n",
            "step: 210, loss: 0.05766266584396362\n",
            "step: 220, loss: 0.004509145859628916\n",
            "step: 230, loss: 0.09639514982700348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9798206278026906, f1=0.9627959413754228, best_f1=0.9627959413754228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009530344977974892\n",
            "step: 10, loss: 0.023411085829138756\n",
            "step: 20, loss: 0.0018176346784457564\n",
            "step: 30, loss: 0.0018784867133945227\n",
            "step: 40, loss: 0.0001393311977153644\n",
            "step: 50, loss: 0.0008947452297434211\n",
            "step: 60, loss: 0.009506314061582088\n",
            "step: 70, loss: 0.10812008380889893\n",
            "step: 80, loss: 0.0059811570681631565\n",
            "step: 90, loss: 0.003744247369468212\n",
            "step: 100, loss: 0.0022882830817252398\n",
            "step: 110, loss: 0.017013438045978546\n",
            "step: 120, loss: 0.00033668920514173806\n",
            "step: 130, loss: 0.0030260630883276463\n",
            "step: 140, loss: 0.00432936754077673\n",
            "step: 150, loss: 0.0006080649909563363\n",
            "step: 160, loss: 0.02230977825820446\n",
            "step: 170, loss: 0.0015792545164003968\n",
            "step: 180, loss: 0.0020638068672269583\n",
            "step: 190, loss: 0.0020487322472035885\n",
            "step: 200, loss: 0.0014485330320894718\n",
            "step: 210, loss: 0.005101310089230537\n",
            "step: 220, loss: 0.009511835873126984\n",
            "step: 230, loss: 0.0001861050695879385\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9797297297297298, f1=0.967452300785634, best_f1=0.9627959413754228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005539256962947547\n",
            "step: 10, loss: 0.0417775921523571\n",
            "step: 20, loss: 0.00045473669888451695\n",
            "step: 30, loss: 9.707578283268958e-05\n",
            "step: 40, loss: 0.0002876034704968333\n",
            "step: 50, loss: 0.0001886599784484133\n",
            "step: 60, loss: 0.00021992460824549198\n",
            "step: 70, loss: 0.00015043398889247328\n",
            "step: 80, loss: 0.00014683231711387634\n",
            "step: 90, loss: 0.00484359310939908\n",
            "step: 100, loss: 0.0003707570140250027\n",
            "step: 110, loss: 0.00016181854880414903\n",
            "step: 120, loss: 0.0032959552481770515\n",
            "step: 130, loss: 0.11230093240737915\n",
            "step: 140, loss: 0.0001886206737253815\n",
            "step: 150, loss: 0.11338454484939575\n",
            "step: 160, loss: 0.001316013396717608\n",
            "step: 170, loss: 0.016936402767896652\n",
            "step: 180, loss: 0.0014085503062233329\n",
            "step: 190, loss: 0.02640402689576149\n",
            "step: 200, loss: 0.0009014446404762566\n",
            "step: 210, loss: 0.07797650247812271\n",
            "step: 220, loss: 0.004454297479242086\n",
            "step: 230, loss: 0.005560419522225857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.978675645342312, f1=0.961625282167043, best_f1=0.9627959413754228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027478947304189205\n",
            "step: 10, loss: 0.025405049324035645\n",
            "step: 20, loss: 0.0022652577608823776\n",
            "step: 30, loss: 0.0006364356959238648\n",
            "step: 40, loss: 0.008178996853530407\n",
            "step: 50, loss: 0.0009432208025828004\n",
            "step: 60, loss: 0.0007345497142523527\n",
            "step: 70, loss: 0.00030832039192318916\n",
            "step: 80, loss: 0.0029381413478404284\n",
            "step: 90, loss: 0.00043679174268618226\n",
            "step: 100, loss: 0.0010112974559888244\n",
            "step: 110, loss: 0.013435759581625462\n",
            "step: 120, loss: 0.0026455316692590714\n",
            "step: 130, loss: 0.005952590610831976\n",
            "step: 140, loss: 0.013653111644089222\n",
            "step: 150, loss: 0.14773088693618774\n",
            "step: 160, loss: 0.001068608369678259\n",
            "step: 170, loss: 0.05101301521062851\n",
            "step: 180, loss: 8.492010965710506e-05\n",
            "step: 190, loss: 0.0016383351758122444\n",
            "step: 200, loss: 0.00809935387223959\n",
            "step: 210, loss: 0.017304331064224243\n",
            "step: 220, loss: 0.0010782623430714011\n",
            "step: 230, loss: 0.0005355398752726614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.980963045912654, f1=0.9730941704035874, best_f1=0.9730941704035874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018853724468499422\n",
            "step: 10, loss: 0.00039622950134798884\n",
            "step: 20, loss: 0.0005628018407151103\n",
            "step: 30, loss: 0.00017807212134357542\n",
            "step: 40, loss: 0.00019596857600845397\n",
            "step: 50, loss: 0.00016670936020091176\n",
            "step: 60, loss: 0.0004030083364341408\n",
            "step: 70, loss: 0.008992655202746391\n",
            "step: 80, loss: 8.434526534983888e-05\n",
            "step: 90, loss: 0.045768603682518005\n",
            "step: 100, loss: 0.0006603896617889404\n",
            "step: 110, loss: 0.00036098677082918584\n",
            "step: 120, loss: 0.037099916487932205\n",
            "step: 130, loss: 0.0021421508863568306\n",
            "step: 140, loss: 0.0006951014511287212\n",
            "step: 150, loss: 0.0002476138179190457\n",
            "step: 160, loss: 0.03692522644996643\n",
            "step: 170, loss: 0.00011076157534262165\n",
            "step: 180, loss: 0.007242790423333645\n",
            "step: 190, loss: 5.947998579358682e-05\n",
            "step: 200, loss: 0.0006915254634805024\n",
            "step: 210, loss: 0.030305184423923492\n",
            "step: 220, loss: 5.817749843117781e-05\n",
            "step: 230, loss: 0.00537197245284915\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9832402234636871, f1=0.9711111111111111, best_f1=0.9711111111111111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006491499952971935\n",
            "step: 10, loss: 0.00025756063405424356\n",
            "step: 20, loss: 0.0522838793694973\n",
            "step: 30, loss: 0.0008537247776985168\n",
            "step: 40, loss: 0.00041944000986404717\n",
            "step: 50, loss: 0.0006461867596954107\n",
            "step: 60, loss: 0.0002844557457137853\n",
            "step: 70, loss: 0.008806722238659859\n",
            "step: 80, loss: 0.0007206479203887284\n",
            "step: 90, loss: 0.0005016544018872082\n",
            "step: 100, loss: 0.0003554314316716045\n",
            "step: 110, loss: 0.006514484528452158\n",
            "step: 120, loss: 9.52857153606601e-05\n",
            "step: 130, loss: 6.782682612538338e-05\n",
            "step: 140, loss: 0.0015306829009205103\n",
            "step: 150, loss: 6.709124863846228e-05\n",
            "step: 160, loss: 3.2445612305309623e-05\n",
            "step: 170, loss: 4.733840614790097e-05\n",
            "step: 180, loss: 0.00028384773759171367\n",
            "step: 190, loss: 5.6981090892804787e-05\n",
            "step: 200, loss: 0.00010336384002584964\n",
            "step: 210, loss: 0.0003417646512389183\n",
            "step: 220, loss: 0.010023909620940685\n",
            "step: 230, loss: 6.28947964287363e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9832402234636871, f1=0.9740698985343857, best_f1=0.9711111111111111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.249508780660108e-05\n",
            "step: 10, loss: 0.00010904873488470912\n",
            "step: 20, loss: 0.00898087490350008\n",
            "step: 30, loss: 0.00022130351862870157\n",
            "step: 40, loss: 2.76892078545643e-05\n",
            "step: 50, loss: 0.005271488334983587\n",
            "step: 60, loss: 4.195979636278935e-05\n",
            "step: 70, loss: 9.791512275114655e-05\n",
            "step: 80, loss: 0.008579403162002563\n",
            "step: 90, loss: 0.06750571727752686\n",
            "step: 100, loss: 0.00012364033318590373\n",
            "step: 110, loss: 0.0033069003839045763\n",
            "step: 120, loss: 6.190973363118246e-05\n",
            "step: 130, loss: 6.756982475053519e-05\n",
            "step: 140, loss: 0.0004895308520644903\n",
            "step: 150, loss: 0.00013706745812669396\n",
            "step: 160, loss: 0.049632396548986435\n",
            "step: 170, loss: 0.00032001116778701544\n",
            "step: 180, loss: 4.3803378503071144e-05\n",
            "step: 190, loss: 6.034086618456058e-05\n",
            "step: 200, loss: 0.0007086555124260485\n",
            "step: 210, loss: 3.945243588532321e-05\n",
            "step: 220, loss: 0.00014863081742078066\n",
            "step: 230, loss: 0.00045997879351489246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.980963045912654, f1=0.972972972972973, best_f1=0.9711111111111111\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004106390289962292\n",
            "step: 10, loss: 3.035246118088253e-05\n",
            "step: 20, loss: 0.008016817271709442\n",
            "step: 30, loss: 0.02484768256545067\n",
            "step: 40, loss: 4.683802035287954e-05\n",
            "step: 50, loss: 0.000713006651494652\n",
            "step: 60, loss: 0.002023642882704735\n",
            "step: 70, loss: 8.290139521704987e-05\n",
            "step: 80, loss: 2.3047727154335007e-05\n",
            "step: 90, loss: 3.48784560628701e-05\n",
            "step: 100, loss: 2.6366747988504358e-05\n",
            "step: 110, loss: 3.774517972487956e-05\n",
            "step: 120, loss: 0.0009120141621679068\n",
            "step: 130, loss: 0.00026200167485512793\n",
            "step: 140, loss: 0.00091278221225366\n",
            "step: 150, loss: 5.008328298572451e-05\n",
            "step: 160, loss: 0.008341684006154537\n",
            "step: 170, loss: 0.02050573006272316\n",
            "step: 180, loss: 0.0006398113328032196\n",
            "step: 190, loss: 0.0003219414793420583\n",
            "step: 200, loss: 5.785351822851226e-05\n",
            "step: 210, loss: 0.03307061269879341\n",
            "step: 220, loss: 0.009577676653862\n",
            "step: 230, loss: 7.754421676509082e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9842342342342343, f1=0.9688195991091313, best_f1=0.9688195991091313\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024278820492327213\n",
            "step: 10, loss: 0.005436259787529707\n",
            "step: 20, loss: 3.597027171053924e-05\n",
            "step: 30, loss: 2.620275699882768e-05\n",
            "step: 40, loss: 4.684666055254638e-05\n",
            "step: 50, loss: 0.002899142215028405\n",
            "step: 60, loss: 3.0207671443349682e-05\n",
            "step: 70, loss: 0.00043037926661781967\n",
            "step: 80, loss: 0.00010376393038313836\n",
            "step: 90, loss: 2.873599987651687e-05\n",
            "step: 100, loss: 0.00025326863396912813\n",
            "step: 110, loss: 6.925783236511052e-05\n",
            "step: 120, loss: 0.00011301122140139341\n",
            "step: 130, loss: 7.767554052406922e-05\n",
            "step: 140, loss: 0.0013271105708554387\n",
            "step: 150, loss: 0.0010204671416431665\n",
            "step: 160, loss: 0.000518325250595808\n",
            "step: 170, loss: 3.368299439898692e-05\n",
            "step: 180, loss: 0.012961437925696373\n",
            "step: 190, loss: 3.916263085557148e-05\n",
            "step: 200, loss: 0.00016182553372345865\n",
            "step: 210, loss: 9.863745071925223e-05\n",
            "step: 220, loss: 4.089974027010612e-05\n",
            "step: 230, loss: 5.872430119779892e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9853768278965129, f1=0.972972972972973, best_f1=0.972972972972973\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.230074308812618e-05\n",
            "step: 10, loss: 7.240369450300932e-05\n",
            "step: 20, loss: 0.0022431141696870327\n",
            "step: 30, loss: 3.948892481275834e-05\n",
            "step: 40, loss: 3.410716453799978e-05\n",
            "step: 50, loss: 3.87026448152028e-05\n",
            "step: 60, loss: 2.599440995254554e-05\n",
            "step: 70, loss: 5.015286660636775e-05\n",
            "step: 80, loss: 0.0003334767825435847\n",
            "step: 90, loss: 0.0001334834232693538\n",
            "step: 100, loss: 0.00028011976974084973\n",
            "step: 110, loss: 0.00029639541753567755\n",
            "step: 120, loss: 1.80822353286203e-05\n",
            "step: 130, loss: 0.00018017903494182974\n",
            "step: 140, loss: 4.316839840612374e-05\n",
            "step: 150, loss: 1.719940519251395e-05\n",
            "step: 160, loss: 3.152465433231555e-05\n",
            "step: 170, loss: 0.00015280974912457168\n",
            "step: 180, loss: 2.781596231216099e-05\n",
            "step: 190, loss: 0.00010085992107633501\n",
            "step: 200, loss: 2.5782117518247105e-05\n",
            "step: 210, loss: 4.8727462854003534e-05\n",
            "step: 220, loss: 0.0001125984126701951\n",
            "step: 230, loss: 2.3993798095034435e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9831649831649831, f1=0.9741282339707535, best_f1=0.972972972972973\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00273455074056983\n",
            "step: 10, loss: 2.7905443857889622e-05\n",
            "step: 20, loss: 5.230960960034281e-05\n",
            "step: 30, loss: 3.3030719350790605e-05\n",
            "step: 40, loss: 0.0001433771540177986\n",
            "step: 50, loss: 2.2418296794057824e-05\n",
            "step: 60, loss: 0.008326292037963867\n",
            "step: 70, loss: 0.00011185206676600501\n",
            "step: 80, loss: 5.285022780299187e-05\n",
            "step: 90, loss: 2.6031440938822925e-05\n",
            "step: 100, loss: 2.1684512830688618e-05\n",
            "step: 110, loss: 0.00015729412552900612\n",
            "step: 120, loss: 0.000380361860152334\n",
            "step: 130, loss: 2.5279186957050115e-05\n",
            "step: 140, loss: 0.028418561443686485\n",
            "step: 150, loss: 0.016904566437005997\n",
            "step: 160, loss: 0.00017276551807299256\n",
            "step: 170, loss: 1.5716814232291654e-05\n",
            "step: 180, loss: 2.757391303020995e-05\n",
            "step: 190, loss: 0.00861378014087677\n",
            "step: 200, loss: 0.0008563771261833608\n",
            "step: 210, loss: 0.00010452191054355353\n",
            "step: 220, loss: 0.04774617403745651\n",
            "step: 230, loss: 2.6922029064735398e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.984304932735426, f1=0.9709821428571428, best_f1=0.972972972972973\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 199.34it/s]\n",
            "load_f1 = 0.9831649831649831\n",
            "real_f1 = 0.9809203142536477\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 182.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "QY0y_yZuhstx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd45a4c-1faa-4a12-c6fb-e4957c67e22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 652kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 23.4MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 21.1MB/s]\n",
            "Downloading: 100% 501M/501M [00:06<00:00, 73.1MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.7174947261810303\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4449824094772339\n",
            "step: 20, loss: 0.2804468870162964\n",
            "step: 30, loss: 0.33779892325401306\n",
            "step: 40, loss: 0.39265310764312744\n",
            "step: 50, loss: 0.7488257884979248\n",
            "step: 60, loss: 0.37830686569213867\n",
            "step: 70, loss: 0.46227577328681946\n",
            "step: 80, loss: 0.49813312292099\n",
            "step: 90, loss: 0.33703479170799255\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.3160206079483032\n",
            "step: 110, loss: 0.11733493953943253\n",
            "step: 120, loss: 0.29762789607048035\n",
            "step: 130, loss: 0.11998412013053894\n",
            "step: 140, loss: 0.2610205411911011\n",
            "step: 150, loss: 0.02174721658229828\n",
            "step: 160, loss: 0.08615370839834213\n",
            "step: 170, loss: 0.11679836362600327\n",
            "step: 180, loss: 0.14203999936580658\n",
            "step: 190, loss: 0.14383889734745026\n",
            "step: 200, loss: 0.041643209755420685\n",
            "step: 210, loss: 0.08021432161331177\n",
            "step: 220, loss: 0.03884726017713547\n",
            "step: 230, loss: 0.26130762696266174\n",
            "step: 240, loss: 0.04301196709275246\n",
            "step: 250, loss: 0.08685964345932007\n",
            "step: 260, loss: 0.15705157816410065\n",
            "step: 270, loss: 0.3417229950428009\n",
            "step: 280, loss: 0.088765949010849\n",
            "step: 290, loss: 0.15115827322006226\n",
            "step: 300, loss: 0.05309642478823662\n",
            "step: 310, loss: 0.08456456661224365\n",
            "step: 320, loss: 0.09385097026824951\n",
            "step: 330, loss: 0.09659477323293686\n",
            "step: 340, loss: 0.4262617826461792\n",
            "step: 350, loss: 0.0829811617732048\n",
            "step: 360, loss: 0.05526850372552872\n",
            "step: 370, loss: 0.13068179786205292\n",
            "step: 380, loss: 0.21021413803100586\n",
            "step: 390, loss: 0.04910207539796829\n",
            "step: 400, loss: 0.05459132790565491\n",
            "step: 410, loss: 0.26743385195732117\n",
            "step: 420, loss: 0.02785203419625759\n",
            "step: 430, loss: 0.02743733860552311\n",
            "step: 440, loss: 0.07127755880355835\n",
            "step: 450, loss: 0.12427426129579544\n",
            "step: 460, loss: 0.05253364518284798\n",
            "step: 470, loss: 0.04015463590621948\n",
            "step: 480, loss: 0.09609539061784744\n",
            "step: 490, loss: 0.1974400281906128\n",
            "step: 500, loss: 0.07620927691459656\n",
            "step: 510, loss: 0.033242084085941315\n",
            "step: 520, loss: 0.15634603798389435\n",
            "step: 530, loss: 0.027915067970752716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9463955637707948, f1=0.937847866419295, best_f1=0.937847866419295\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06662846356630325\n",
            "step: 10, loss: 0.07039418071508408\n",
            "step: 20, loss: 0.06630813330411911\n",
            "step: 30, loss: 0.04361686483025551\n",
            "step: 40, loss: 0.17032495141029358\n",
            "step: 50, loss: 0.04512980207800865\n",
            "step: 60, loss: 0.09459268301725388\n",
            "step: 70, loss: 0.0332869291305542\n",
            "step: 80, loss: 0.01953289285302162\n",
            "step: 90, loss: 0.002329480368643999\n",
            "step: 100, loss: 0.1599183827638626\n",
            "step: 110, loss: 0.017012789845466614\n",
            "step: 120, loss: 0.028519073501229286\n",
            "step: 130, loss: 0.004071661736816168\n",
            "step: 140, loss: 0.05506123602390289\n",
            "step: 150, loss: 0.029061252251267433\n",
            "step: 160, loss: 0.039982669055461884\n",
            "step: 170, loss: 0.04918700456619263\n",
            "step: 180, loss: 0.018934864550828934\n",
            "step: 190, loss: 0.03591708466410637\n",
            "step: 200, loss: 0.3211357891559601\n",
            "step: 210, loss: 0.050431061536073685\n",
            "step: 220, loss: 0.0014417769853025675\n",
            "step: 230, loss: 0.1962776929140091\n",
            "step: 240, loss: 0.03674856945872307\n",
            "step: 250, loss: 0.03748684003949165\n",
            "step: 260, loss: 0.10181724280118942\n",
            "step: 270, loss: 0.06666616350412369\n",
            "step: 280, loss: 0.13120439648628235\n",
            "step: 290, loss: 0.03103785403072834\n",
            "step: 300, loss: 0.035520218312740326\n",
            "step: 310, loss: 0.030648738145828247\n",
            "step: 320, loss: 0.09407962113618851\n",
            "step: 330, loss: 0.03157272934913635\n",
            "step: 340, loss: 0.040109895169734955\n",
            "step: 350, loss: 0.005899844691157341\n",
            "step: 360, loss: 0.1340167075395584\n",
            "step: 370, loss: 0.027176126837730408\n",
            "step: 380, loss: 0.11970342695713043\n",
            "step: 390, loss: 0.007393470499664545\n",
            "step: 400, loss: 0.14441163837909698\n",
            "step: 410, loss: 0.039082154631614685\n",
            "step: 420, loss: 0.017247891053557396\n",
            "step: 430, loss: 0.18102186918258667\n",
            "step: 440, loss: 0.005844161845743656\n",
            "step: 450, loss: 0.05956905707716942\n",
            "step: 460, loss: 0.10849976539611816\n",
            "step: 470, loss: 0.07201803475618362\n",
            "step: 480, loss: 0.026008404791355133\n",
            "step: 490, loss: 0.05573197081685066\n",
            "step: 500, loss: 0.0032697892747819424\n",
            "step: 510, loss: 0.028895987197756767\n",
            "step: 520, loss: 0.285699725151062\n",
            "step: 530, loss: 0.050535861402750015\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9530386740331491, f1=0.9466911764705882, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12813852727413177\n",
            "step: 10, loss: 0.046609409153461456\n",
            "step: 20, loss: 0.03346817567944527\n",
            "step: 30, loss: 0.1259748637676239\n",
            "step: 40, loss: 0.07825610041618347\n",
            "step: 50, loss: 0.012851813808083534\n",
            "step: 60, loss: 0.018292104825377464\n",
            "step: 70, loss: 0.01583663374185562\n",
            "step: 80, loss: 0.16596300899982452\n",
            "step: 90, loss: 0.03825230151414871\n",
            "step: 100, loss: 0.022170396521687508\n",
            "step: 110, loss: 0.0026635031681507826\n",
            "step: 120, loss: 0.2127196341753006\n",
            "step: 130, loss: 0.04547547921538353\n",
            "step: 140, loss: 0.022261686623096466\n",
            "step: 150, loss: 0.016338098794221878\n",
            "step: 160, loss: 0.01996716856956482\n",
            "step: 170, loss: 0.005240068770945072\n",
            "step: 180, loss: 0.04181993007659912\n",
            "step: 190, loss: 0.010163750499486923\n",
            "step: 200, loss: 0.018644705414772034\n",
            "step: 210, loss: 0.02632778324186802\n",
            "step: 220, loss: 0.147396519780159\n",
            "step: 230, loss: 0.07786824554204941\n",
            "step: 240, loss: 0.2338661104440689\n",
            "step: 250, loss: 0.03526771068572998\n",
            "step: 260, loss: 0.09810104966163635\n",
            "step: 270, loss: 0.08926811069250107\n",
            "step: 280, loss: 0.0014028125442564487\n",
            "step: 290, loss: 0.006189560983330011\n",
            "step: 300, loss: 0.16725625097751617\n",
            "step: 310, loss: 0.09161274135112762\n",
            "step: 320, loss: 0.029308242723345757\n",
            "step: 330, loss: 0.007692490238696337\n",
            "step: 340, loss: 0.004999042022973299\n",
            "step: 350, loss: 0.13737209141254425\n",
            "step: 360, loss: 0.00897139124572277\n",
            "step: 370, loss: 0.04205271601676941\n",
            "step: 380, loss: 0.05979572981595993\n",
            "step: 390, loss: 0.01792207546532154\n",
            "step: 400, loss: 0.16703073680400848\n",
            "step: 410, loss: 0.009511658921837807\n",
            "step: 420, loss: 0.01033011544495821\n",
            "step: 430, loss: 0.056664079427719116\n",
            "step: 440, loss: 0.21319200098514557\n",
            "step: 450, loss: 0.08325961977243423\n",
            "step: 460, loss: 0.07365136593580246\n",
            "step: 470, loss: 0.019779277965426445\n",
            "step: 480, loss: 0.022973740473389626\n",
            "step: 490, loss: 0.005354771856218576\n",
            "step: 500, loss: 0.042092420160770416\n",
            "step: 510, loss: 0.08498226851224899\n",
            "step: 520, loss: 0.006399867124855518\n",
            "step: 530, loss: 0.01848386786878109\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9453526389537599, f1=0.943378568086102, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04011860489845276\n",
            "step: 10, loss: 0.002041485393419862\n",
            "step: 20, loss: 0.05360068008303642\n",
            "step: 30, loss: 0.11259707808494568\n",
            "step: 40, loss: 0.015807606279850006\n",
            "step: 50, loss: 0.04508981481194496\n",
            "step: 60, loss: 0.015544977970421314\n",
            "step: 70, loss: 0.0839652568101883\n",
            "step: 80, loss: 0.08991718292236328\n",
            "step: 90, loss: 0.07088813185691833\n",
            "step: 100, loss: 0.0016314475797116756\n",
            "step: 110, loss: 0.15775562822818756\n",
            "step: 120, loss: 0.009909240528941154\n",
            "step: 130, loss: 0.04289039969444275\n",
            "step: 140, loss: 0.1992596536874771\n",
            "step: 150, loss: 0.004642983432859182\n",
            "step: 160, loss: 0.0061562261544167995\n",
            "step: 170, loss: 0.08651769161224365\n",
            "step: 180, loss: 0.12309186160564423\n",
            "step: 190, loss: 0.034514833241701126\n",
            "step: 200, loss: 0.07174952328205109\n",
            "step: 210, loss: 0.0012983577325940132\n",
            "step: 220, loss: 0.02181163989007473\n",
            "step: 230, loss: 0.006348251830786467\n",
            "step: 240, loss: 0.016908785328269005\n",
            "step: 250, loss: 0.02271391823887825\n",
            "step: 260, loss: 0.0010114518227055669\n",
            "step: 270, loss: 0.026851089671254158\n",
            "step: 280, loss: 0.0067167095839977264\n",
            "step: 290, loss: 0.03223968669772148\n",
            "step: 300, loss: 0.030136262997984886\n",
            "step: 310, loss: 0.0034949134569615126\n",
            "step: 320, loss: 0.11204579472541809\n",
            "step: 330, loss: 0.004900939296931028\n",
            "step: 340, loss: 0.006868863478302956\n",
            "step: 350, loss: 0.21606290340423584\n",
            "step: 360, loss: 0.06334969401359558\n",
            "step: 370, loss: 0.0018583323108032346\n",
            "step: 380, loss: 0.012407374568283558\n",
            "step: 390, loss: 0.0017460761591792107\n",
            "step: 400, loss: 0.002987045096233487\n",
            "step: 410, loss: 0.0008664357010275126\n",
            "step: 420, loss: 0.010006014257669449\n",
            "step: 430, loss: 0.010375839658081532\n",
            "step: 440, loss: 0.005892714951187372\n",
            "step: 450, loss: 0.008212458342313766\n",
            "step: 460, loss: 0.007980819791555405\n",
            "step: 470, loss: 0.003163154935464263\n",
            "step: 480, loss: 0.01946609653532505\n",
            "step: 490, loss: 0.03913724049925804\n",
            "step: 500, loss: 0.13862906396389008\n",
            "step: 510, loss: 0.06682979315519333\n",
            "step: 520, loss: 0.04484154284000397\n",
            "step: 530, loss: 0.1344504952430725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9489461358313818, f1=0.9499298081422555, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007190250908024609\n",
            "step: 10, loss: 0.018922820687294006\n",
            "step: 20, loss: 0.0020006978884339333\n",
            "step: 30, loss: 0.1526477187871933\n",
            "step: 40, loss: 0.037585075944662094\n",
            "step: 50, loss: 0.11880020052194595\n",
            "step: 60, loss: 0.10828684270381927\n",
            "step: 70, loss: 0.054724179208278656\n",
            "step: 80, loss: 0.034895338118076324\n",
            "step: 90, loss: 0.22583788633346558\n",
            "step: 100, loss: 0.07996881753206253\n",
            "step: 110, loss: 0.009307480417191982\n",
            "step: 120, loss: 0.04238782823085785\n",
            "step: 130, loss: 0.0012286584824323654\n",
            "step: 140, loss: 0.011189102195203304\n",
            "step: 150, loss: 0.016593551263213158\n",
            "step: 160, loss: 0.008548008278012276\n",
            "step: 170, loss: 0.2084297090768814\n",
            "step: 180, loss: 0.03172443434596062\n",
            "step: 190, loss: 0.018631847575306892\n",
            "step: 200, loss: 0.0039008022285997868\n",
            "step: 210, loss: 0.0009866405744105577\n",
            "step: 220, loss: 0.01254820916801691\n",
            "step: 230, loss: 0.005275403149425983\n",
            "step: 240, loss: 0.03856964409351349\n",
            "step: 250, loss: 0.04817376285791397\n",
            "step: 260, loss: 0.0005386040429584682\n",
            "step: 270, loss: 0.0059822434559464455\n",
            "step: 280, loss: 0.008454675786197186\n",
            "step: 290, loss: 0.01816342957317829\n",
            "step: 300, loss: 0.058318160474300385\n",
            "step: 310, loss: 0.017739461734890938\n",
            "step: 320, loss: 0.10525574535131454\n",
            "step: 330, loss: 0.033260613679885864\n",
            "step: 340, loss: 0.055447060614824295\n",
            "step: 350, loss: 0.0020815986208617687\n",
            "step: 360, loss: 0.0003278263611719012\n",
            "step: 370, loss: 0.00035716165439225733\n",
            "step: 380, loss: 0.001334059634245932\n",
            "step: 390, loss: 0.005252726376056671\n",
            "step: 400, loss: 0.061683688312768936\n",
            "step: 410, loss: 0.035892609506845474\n",
            "step: 420, loss: 0.22942900657653809\n",
            "step: 430, loss: 0.005958253052085638\n",
            "step: 440, loss: 0.012170503847301006\n",
            "step: 450, loss: 0.010884671472012997\n",
            "step: 460, loss: 0.0062895407900214195\n",
            "step: 470, loss: 0.00525313476100564\n",
            "step: 480, loss: 0.17536644637584686\n",
            "step: 490, loss: 0.057328108698129654\n",
            "step: 500, loss: 0.021828768774867058\n",
            "step: 510, loss: 0.0030565145425498486\n",
            "step: 520, loss: 0.038099199533462524\n",
            "step: 530, loss: 0.03965415805578232\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9469732519943688, f1=0.9407894736842105, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05330629274249077\n",
            "step: 10, loss: 0.030378155410289764\n",
            "step: 20, loss: 0.041273217648267746\n",
            "step: 30, loss: 0.00042645656503736973\n",
            "step: 40, loss: 0.0005027953302487731\n",
            "step: 50, loss: 0.00032599776750430465\n",
            "step: 60, loss: 0.00033295282628387213\n",
            "step: 70, loss: 0.005844191648066044\n",
            "step: 80, loss: 0.0004958448698744178\n",
            "step: 90, loss: 0.004777100868523121\n",
            "step: 100, loss: 0.004113789647817612\n",
            "step: 110, loss: 0.003060555551201105\n",
            "step: 120, loss: 0.016396306455135345\n",
            "step: 130, loss: 0.00638915179297328\n",
            "step: 140, loss: 0.0005977555993013084\n",
            "step: 150, loss: 0.010017034597694874\n",
            "step: 160, loss: 0.07962377369403839\n",
            "step: 170, loss: 0.013527113944292068\n",
            "step: 180, loss: 0.004077235236763954\n",
            "step: 190, loss: 0.06669740378856659\n",
            "step: 200, loss: 0.03799892216920853\n",
            "step: 210, loss: 0.004325710237026215\n",
            "step: 220, loss: 0.01608295738697052\n",
            "step: 230, loss: 0.0004092398739885539\n",
            "step: 240, loss: 0.0028741161804646254\n",
            "step: 250, loss: 0.005686810705810785\n",
            "step: 260, loss: 0.0004855713923461735\n",
            "step: 270, loss: 0.010415448807179928\n",
            "step: 280, loss: 0.0006141734193079174\n",
            "step: 290, loss: 0.00362754357047379\n",
            "step: 300, loss: 0.008471321314573288\n",
            "step: 310, loss: 0.01367497630417347\n",
            "step: 320, loss: 0.00016594171756878495\n",
            "step: 330, loss: 0.00010327380732633173\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 340, loss: 8.866116695571691e-05\n",
            "step: 350, loss: 0.00061180250486359\n",
            "step: 360, loss: 0.19712930917739868\n",
            "step: 370, loss: 0.02826680988073349\n",
            "step: 380, loss: 0.0015072865644469857\n",
            "step: 390, loss: 0.005570352077484131\n",
            "step: 400, loss: 0.009214511141180992\n",
            "step: 410, loss: 0.00013269441842567176\n",
            "step: 420, loss: 0.010693617165088654\n",
            "step: 430, loss: 0.010272260755300522\n",
            "step: 440, loss: 0.005360281560570002\n",
            "step: 450, loss: 0.059187933802604675\n",
            "step: 460, loss: 0.002333469223231077\n",
            "step: 470, loss: 0.00829261727631092\n",
            "step: 480, loss: 0.00918799638748169\n",
            "step: 490, loss: 0.003358503570780158\n",
            "step: 500, loss: 0.0005837127100676298\n",
            "step: 510, loss: 0.03197629004716873\n",
            "step: 520, loss: 0.0062810699455440044\n",
            "step: 530, loss: 0.0016558916540816426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9507139567019807, f1=0.9402573529411765, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00045150076039135456\n",
            "step: 10, loss: 0.03393935412168503\n",
            "step: 20, loss: 0.04406444728374481\n",
            "step: 30, loss: 0.013944344595074654\n",
            "step: 40, loss: 0.0033781102392822504\n",
            "step: 50, loss: 0.0019875671714544296\n",
            "step: 60, loss: 0.005492204800248146\n",
            "step: 70, loss: 0.016843602061271667\n",
            "step: 80, loss: 0.0043492745608091354\n",
            "step: 90, loss: 0.0014118452090770006\n",
            "step: 100, loss: 0.0019547822885215282\n",
            "step: 110, loss: 0.0061266422271728516\n",
            "step: 120, loss: 0.01712726429104805\n",
            "step: 130, loss: 0.0009857920231297612\n",
            "step: 140, loss: 0.0014878385700285435\n",
            "step: 150, loss: 0.002235780470073223\n",
            "step: 160, loss: 0.0009849787456914783\n",
            "step: 170, loss: 0.0012705123517662287\n",
            "step: 180, loss: 0.07180970907211304\n",
            "step: 190, loss: 0.09813997149467468\n",
            "step: 200, loss: 0.0033159207087010145\n",
            "step: 210, loss: 0.020772816613316536\n",
            "step: 220, loss: 0.099791519343853\n",
            "step: 230, loss: 0.0028903826605528593\n",
            "step: 240, loss: 0.034649286419153214\n",
            "step: 250, loss: 0.005629264749586582\n",
            "step: 260, loss: 0.01530811470001936\n",
            "step: 270, loss: 0.06011362001299858\n",
            "step: 280, loss: 0.004870598204433918\n",
            "step: 290, loss: 0.06864164024591446\n",
            "step: 300, loss: 0.0006425863248296082\n",
            "step: 310, loss: 0.0008062561391852796\n",
            "step: 320, loss: 0.06890639662742615\n",
            "step: 330, loss: 0.0013836658326908946\n",
            "step: 340, loss: 0.033621061593294144\n",
            "step: 350, loss: 0.00034151884028688073\n",
            "step: 360, loss: 0.11599880456924438\n",
            "step: 370, loss: 0.004040147643536329\n",
            "step: 380, loss: 0.0025514266453683376\n",
            "step: 390, loss: 0.00034352776128798723\n",
            "step: 400, loss: 0.010062674060463905\n",
            "step: 410, loss: 0.013205216266214848\n",
            "step: 420, loss: 0.016519447788596153\n",
            "step: 430, loss: 0.005305269733071327\n",
            "step: 440, loss: 0.005281772464513779\n",
            "step: 450, loss: 0.003979567904025316\n",
            "step: 460, loss: 0.0035376420710235834\n",
            "step: 470, loss: 0.11435125768184662\n",
            "step: 480, loss: 0.004244440235197544\n",
            "step: 490, loss: 0.0060084727592766285\n",
            "step: 500, loss: 0.024456029757857323\n",
            "step: 510, loss: 0.0008231118554249406\n",
            "step: 520, loss: 0.004938660655170679\n",
            "step: 530, loss: 0.0016087170224636793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9485396383866481, f1=0.9393382352941178, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005445178248919547\n",
            "step: 10, loss: 0.002619988052174449\n",
            "step: 20, loss: 0.014470331370830536\n",
            "step: 30, loss: 0.0012640110217034817\n",
            "step: 40, loss: 9.818060061661527e-05\n",
            "step: 50, loss: 0.002098489785566926\n",
            "step: 60, loss: 0.0005942665738984942\n",
            "step: 70, loss: 0.0024077098350971937\n",
            "step: 80, loss: 0.0002765694516710937\n",
            "step: 90, loss: 0.03614798188209534\n",
            "step: 100, loss: 0.0038958291988819838\n",
            "step: 110, loss: 0.0015810129698365927\n",
            "step: 120, loss: 0.0009156573796644807\n",
            "step: 130, loss: 0.000954392715357244\n",
            "step: 140, loss: 0.0055544390343129635\n",
            "step: 150, loss: 0.001793433679267764\n",
            "step: 160, loss: 0.0003251214511692524\n",
            "step: 170, loss: 0.010165512561798096\n",
            "step: 180, loss: 0.0022287415340542793\n",
            "step: 190, loss: 0.005152914673089981\n",
            "step: 200, loss: 0.008344259113073349\n",
            "step: 210, loss: 0.05768483504652977\n",
            "step: 220, loss: 0.0004128810251131654\n",
            "step: 230, loss: 0.044495925307273865\n",
            "step: 240, loss: 0.019637450575828552\n",
            "step: 250, loss: 0.0017042807303369045\n",
            "step: 260, loss: 0.004975108429789543\n",
            "step: 270, loss: 0.004361879080533981\n",
            "step: 280, loss: 0.0023248582147061825\n",
            "step: 290, loss: 0.001160992425866425\n",
            "step: 300, loss: 0.0010251641506329179\n",
            "step: 310, loss: 0.0072908117435872555\n",
            "step: 320, loss: 0.001258320757187903\n",
            "step: 330, loss: 0.0002674695278983563\n",
            "step: 340, loss: 0.04990715533494949\n",
            "step: 350, loss: 0.000789240060839802\n",
            "step: 360, loss: 0.014516561292111874\n",
            "step: 370, loss: 0.06472840160131454\n",
            "step: 380, loss: 0.000351218186551705\n",
            "step: 390, loss: 0.004386264365166426\n",
            "step: 400, loss: 0.006188230589032173\n",
            "step: 410, loss: 0.028171146288514137\n",
            "step: 420, loss: 0.007374287117272615\n",
            "step: 430, loss: 0.012724155560135841\n",
            "step: 440, loss: 0.04562061280012131\n",
            "step: 450, loss: 0.0004720105498563498\n",
            "step: 460, loss: 0.00014055131759960204\n",
            "step: 470, loss: 0.04300640523433685\n",
            "step: 480, loss: 0.0009804927976801991\n",
            "step: 490, loss: 0.0053429072722792625\n",
            "step: 500, loss: 0.00011205917689949274\n",
            "step: 510, loss: 0.004075071308761835\n",
            "step: 520, loss: 0.00011444846313679591\n",
            "step: 530, loss: 0.02897736057639122\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.951545916012921, f1=0.945287356321839, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.56019978527911e-05\n",
            "step: 10, loss: 0.032777540385723114\n",
            "step: 20, loss: 0.000942347280215472\n",
            "step: 30, loss: 0.0349113792181015\n",
            "step: 40, loss: 0.0004920365754514933\n",
            "step: 50, loss: 0.00017006247071549296\n",
            "step: 60, loss: 0.02223755605518818\n",
            "step: 70, loss: 0.00932091660797596\n",
            "step: 80, loss: 0.044637732207775116\n",
            "step: 90, loss: 0.1038971096277237\n",
            "step: 100, loss: 0.0018363728886470199\n",
            "step: 110, loss: 0.02522057481110096\n",
            "step: 120, loss: 0.008444871753454208\n",
            "step: 130, loss: 0.0013934081653133035\n",
            "step: 140, loss: 0.0008028101292438805\n",
            "step: 150, loss: 0.007025551050901413\n",
            "step: 160, loss: 0.0003541082551237196\n",
            "step: 170, loss: 0.004576093517243862\n",
            "step: 180, loss: 0.00024405476870015264\n",
            "step: 190, loss: 3.249506698921323e-05\n",
            "step: 200, loss: 4.671474380302243e-05\n",
            "step: 210, loss: 0.0007867863751016557\n",
            "step: 220, loss: 0.0022931343410164118\n",
            "step: 230, loss: 0.001071558566763997\n",
            "step: 240, loss: 0.0003177865582983941\n",
            "step: 250, loss: 0.003669347381219268\n",
            "step: 260, loss: 0.00010079316416522488\n",
            "step: 270, loss: 0.0006053723627701402\n",
            "step: 280, loss: 0.08691337704658508\n",
            "step: 290, loss: 0.002073430921882391\n",
            "step: 300, loss: 0.038025543093681335\n",
            "step: 310, loss: 0.14822065830230713\n",
            "step: 320, loss: 0.0016014097491279244\n",
            "step: 330, loss: 0.0004766864294651896\n",
            "step: 340, loss: 0.06560280174016953\n",
            "step: 350, loss: 0.01692194491624832\n",
            "step: 360, loss: 0.0019013938726857305\n",
            "step: 370, loss: 0.012039422057569027\n",
            "step: 380, loss: 0.009115018881857395\n",
            "step: 390, loss: 0.000710161286406219\n",
            "step: 400, loss: 0.004758063703775406\n",
            "step: 410, loss: 0.003815303323790431\n",
            "step: 420, loss: 0.00014928368909750134\n",
            "step: 430, loss: 0.03176030516624451\n",
            "step: 440, loss: 7.322348392335698e-05\n",
            "step: 450, loss: 0.007098865229636431\n",
            "step: 460, loss: 0.026609573513269424\n",
            "step: 470, loss: 5.928441169089638e-05\n",
            "step: 480, loss: 4.0433424146613106e-05\n",
            "step: 490, loss: 0.02519090846180916\n",
            "step: 500, loss: 0.0022861596662551165\n",
            "step: 510, loss: 0.0017599723068997264\n",
            "step: 520, loss: 0.009394537657499313\n",
            "step: 530, loss: 0.15243938565254211\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9407407407407408, f1=0.9394495412844037, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012511645909398794\n",
            "step: 10, loss: 0.0018410427728667855\n",
            "step: 20, loss: 0.0009698260109871626\n",
            "step: 30, loss: 0.012869482859969139\n",
            "step: 40, loss: 0.0033758245408535004\n",
            "step: 50, loss: 0.003966032061725855\n",
            "step: 60, loss: 0.013925706967711449\n",
            "step: 70, loss: 0.00025475784786976874\n",
            "step: 80, loss: 0.005073506850749254\n",
            "step: 90, loss: 0.0070862979628145695\n",
            "step: 100, loss: 0.0008982808794826269\n",
            "step: 110, loss: 0.0026571720372885466\n",
            "step: 120, loss: 0.08144856989383698\n",
            "step: 130, loss: 0.0002650766109582037\n",
            "step: 140, loss: 0.0020946357399225235\n",
            "step: 150, loss: 0.0077498252503573895\n",
            "step: 160, loss: 0.023664314299821854\n",
            "step: 170, loss: 0.0034636908676475286\n",
            "step: 180, loss: 0.0014163508312776685\n",
            "step: 190, loss: 0.0002013984340010211\n",
            "step: 200, loss: 0.004886111710220575\n",
            "step: 210, loss: 0.00941146444529295\n",
            "step: 220, loss: 0.002678218763321638\n",
            "step: 230, loss: 0.06943763047456741\n",
            "step: 240, loss: 0.0016322557348757982\n",
            "step: 250, loss: 0.0005355554167181253\n",
            "step: 260, loss: 0.004128619562834501\n",
            "step: 270, loss: 0.00013858708553016186\n",
            "step: 280, loss: 0.0006304687121883035\n",
            "step: 290, loss: 0.0009705129195936024\n",
            "step: 300, loss: 0.002703799633309245\n",
            "step: 310, loss: 0.0239659883081913\n",
            "step: 320, loss: 0.055129650980234146\n",
            "step: 330, loss: 0.004746661055833101\n",
            "step: 340, loss: 0.0018136565340682864\n",
            "step: 350, loss: 0.0008561498834751546\n",
            "step: 360, loss: 0.00013869788381271064\n",
            "step: 370, loss: 0.0025098081678152084\n",
            "step: 380, loss: 0.03033541329205036\n",
            "step: 390, loss: 0.0002535638923291117\n",
            "step: 400, loss: 0.00036528115742839873\n",
            "step: 410, loss: 0.0012832414358854294\n",
            "step: 420, loss: 0.0006113097770139575\n",
            "step: 430, loss: 0.0018583715427666903\n",
            "step: 440, loss: 0.0001872796128736809\n",
            "step: 450, loss: 0.0010270837228745222\n",
            "step: 460, loss: 0.015527365729212761\n",
            "step: 470, loss: 0.0043870932422578335\n",
            "step: 480, loss: 0.0008395116892643273\n",
            "step: 490, loss: 0.020415084436535835\n",
            "step: 500, loss: 0.0003713170881383121\n",
            "step: 510, loss: 0.0002644560590852052\n",
            "step: 520, loss: 0.0027984946500509977\n",
            "step: 530, loss: 0.008947985246777534\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9468331021729081, f1=0.9406896551724139, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.129491859814152e-05\n",
            "step: 10, loss: 0.0028599353972822428\n",
            "step: 20, loss: 0.00025183873367495835\n",
            "step: 30, loss: 0.0005771557916887105\n",
            "step: 40, loss: 4.517481283983216e-05\n",
            "step: 50, loss: 5.984805466141552e-05\n",
            "step: 60, loss: 0.0003651083679869771\n",
            "step: 70, loss: 0.0012997087324038148\n",
            "step: 80, loss: 0.001203400082886219\n",
            "step: 90, loss: 0.0007135743508115411\n",
            "step: 100, loss: 0.006839582230895758\n",
            "step: 110, loss: 0.0007944925455376506\n",
            "step: 120, loss: 0.002192085376009345\n",
            "step: 130, loss: 0.0031044832430779934\n",
            "step: 140, loss: 0.0015595054719597101\n",
            "step: 150, loss: 0.0031502740457654\n",
            "step: 160, loss: 0.001208788831718266\n",
            "step: 170, loss: 0.003875638358294964\n",
            "step: 180, loss: 0.0008071581833064556\n",
            "step: 190, loss: 0.0005055352812632918\n",
            "step: 200, loss: 0.0007827008957974613\n",
            "step: 210, loss: 0.0005269284010864794\n",
            "step: 220, loss: 0.004200655035674572\n",
            "step: 230, loss: 0.0010597821092233062\n",
            "step: 240, loss: 0.0014860149240121245\n",
            "step: 250, loss: 0.03323015198111534\n",
            "step: 260, loss: 0.029082899913191795\n",
            "step: 270, loss: 0.003539895173162222\n",
            "step: 280, loss: 0.005252073053270578\n",
            "step: 290, loss: 0.0038017428014427423\n",
            "step: 300, loss: 0.013089069165289402\n",
            "step: 310, loss: 0.0015913064125925303\n",
            "step: 320, loss: 7.126209675334394e-05\n",
            "step: 330, loss: 3.1399944418808445e-05\n",
            "step: 340, loss: 0.012929671443998814\n",
            "step: 350, loss: 0.00041444774251431227\n",
            "step: 360, loss: 0.0031718506943434477\n",
            "step: 370, loss: 0.0015635319286957383\n",
            "step: 380, loss: 0.0016826495993882418\n",
            "step: 390, loss: 0.0028361016884446144\n",
            "step: 400, loss: 4.984647239325568e-05\n",
            "step: 410, loss: 0.0006381837883964181\n",
            "step: 420, loss: 0.0029353906866163015\n",
            "step: 430, loss: 0.002645349595695734\n",
            "step: 440, loss: 0.00014463224215433002\n",
            "step: 450, loss: 0.001953497529029846\n",
            "step: 460, loss: 9.527475049253553e-05\n",
            "step: 470, loss: 0.0006134920404292643\n",
            "step: 480, loss: 0.0002464063873048872\n",
            "step: 490, loss: 0.005606933031231165\n",
            "step: 500, loss: 0.0018546974752098322\n",
            "step: 510, loss: 0.0012365153525024652\n",
            "step: 520, loss: 0.00027225996018387377\n",
            "step: 530, loss: 0.02801484428346157\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9425287356321841, f1=0.9399358092618065, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004622664302587509\n",
            "step: 10, loss: 0.001639554975554347\n",
            "step: 20, loss: 0.0019140229560434818\n",
            "step: 30, loss: 2.3469030566047877e-05\n",
            "step: 40, loss: 0.0006422038422897458\n",
            "step: 50, loss: 0.0001390898396493867\n",
            "step: 60, loss: 0.00015852091019041836\n",
            "step: 70, loss: 0.17298537492752075\n",
            "step: 80, loss: 8.820345101412386e-05\n",
            "step: 90, loss: 0.001853913301602006\n",
            "step: 100, loss: 0.32258638739585876\n",
            "step: 110, loss: 0.004405909217894077\n",
            "step: 120, loss: 0.0034233778715133667\n",
            "step: 130, loss: 0.004768860060721636\n",
            "step: 140, loss: 0.00014075699436943978\n",
            "step: 150, loss: 0.0004067036497872323\n",
            "step: 160, loss: 0.0021316860802471638\n",
            "step: 170, loss: 0.0006086903158575296\n",
            "step: 180, loss: 0.0002109039924107492\n",
            "step: 190, loss: 0.0009252864983864129\n",
            "step: 200, loss: 0.0018693895544856787\n",
            "step: 210, loss: 0.0005143804592080414\n",
            "step: 220, loss: 0.0026840665377676487\n",
            "step: 230, loss: 0.0010376524878665805\n",
            "step: 240, loss: 0.0013700282434001565\n",
            "step: 250, loss: 6.100461905589327e-05\n",
            "step: 260, loss: 0.0005405879346653819\n",
            "step: 270, loss: 9.711955499369651e-05\n",
            "step: 280, loss: 0.0033475186210125685\n",
            "step: 290, loss: 0.0033407495357096195\n",
            "step: 300, loss: 0.011944049037992954\n",
            "step: 310, loss: 0.0008299993351101875\n",
            "step: 320, loss: 0.006149237975478172\n",
            "step: 330, loss: 0.03876996785402298\n",
            "step: 340, loss: 0.00043917750008404255\n",
            "step: 350, loss: 0.0004165095160715282\n",
            "step: 360, loss: 0.0006244293763302267\n",
            "step: 370, loss: 0.001216345583088696\n",
            "step: 380, loss: 0.00019991632143501192\n",
            "step: 390, loss: 0.0006290408200584352\n",
            "step: 400, loss: 6.528422818519175e-05\n",
            "step: 410, loss: 0.000559281324967742\n",
            "step: 420, loss: 0.007952719926834106\n",
            "step: 430, loss: 0.0062842415645718575\n",
            "step: 440, loss: 0.000666198437102139\n",
            "step: 450, loss: 0.00407251063734293\n",
            "step: 460, loss: 4.4428001274354756e-05\n",
            "step: 470, loss: 0.0010464175138622522\n",
            "step: 480, loss: 0.0007324417820200324\n",
            "step: 490, loss: 0.00015076829004101455\n",
            "step: 500, loss: 7.872544665588066e-05\n",
            "step: 510, loss: 0.057788971811532974\n",
            "step: 520, loss: 0.001688378513790667\n",
            "step: 530, loss: 0.001179031445644796\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9487648673376029, f1=0.9457858769931662, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001420232729287818\n",
            "step: 10, loss: 0.0015574427088722587\n",
            "step: 20, loss: 0.00010855622531380504\n",
            "step: 30, loss: 3.3884207368828356e-05\n",
            "step: 40, loss: 0.004556823056191206\n",
            "step: 50, loss: 0.002252815291285515\n",
            "step: 60, loss: 2.724626028793864e-05\n",
            "step: 70, loss: 0.0012604743242263794\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.0007655859808437526\n",
            "step: 90, loss: 0.0001958157808985561\n",
            "step: 100, loss: 0.0030006933957338333\n",
            "step: 110, loss: 0.0002522230497561395\n",
            "step: 120, loss: 0.0010587701108306646\n",
            "step: 130, loss: 0.00011037817603209987\n",
            "step: 140, loss: 0.011892483569681644\n",
            "step: 150, loss: 0.001276044175028801\n",
            "step: 160, loss: 0.0004103960527572781\n",
            "step: 170, loss: 0.0019748739432543516\n",
            "step: 180, loss: 0.0005825518164783716\n",
            "step: 190, loss: 0.00041007468826137483\n",
            "step: 200, loss: 2.73608930001501e-05\n",
            "step: 210, loss: 0.0018666342366486788\n",
            "step: 220, loss: 0.000115916998765897\n",
            "step: 230, loss: 0.0024783117696642876\n",
            "step: 240, loss: 0.0008311852579936385\n",
            "step: 250, loss: 5.0627342716325074e-05\n",
            "step: 260, loss: 0.061407916247844696\n",
            "step: 270, loss: 3.3221320336451754e-05\n",
            "step: 280, loss: 0.0010374155826866627\n",
            "step: 290, loss: 3.375595770194195e-05\n",
            "step: 300, loss: 2.009396666835528e-05\n",
            "step: 310, loss: 7.305815233848989e-05\n",
            "step: 320, loss: 0.00015797509695403278\n",
            "step: 330, loss: 0.005856194067746401\n",
            "step: 340, loss: 0.010663222521543503\n",
            "step: 350, loss: 0.0018575915601104498\n",
            "step: 360, loss: 0.09124357998371124\n",
            "step: 370, loss: 0.0006091097020544112\n",
            "step: 380, loss: 2.5796634872676805e-05\n",
            "step: 390, loss: 7.887057290645316e-05\n",
            "step: 400, loss: 7.246114546433091e-05\n",
            "step: 410, loss: 8.168644126271829e-05\n",
            "step: 420, loss: 0.000508182100020349\n",
            "step: 430, loss: 0.0001971853052964434\n",
            "step: 440, loss: 0.0001259334821952507\n",
            "step: 450, loss: 0.020665045827627182\n",
            "step: 460, loss: 0.0014968763571232557\n",
            "step: 470, loss: 0.004719669930636883\n",
            "step: 480, loss: 1.7862335880636238e-05\n",
            "step: 490, loss: 8.608513599028811e-05\n",
            "step: 500, loss: 2.1807187295053154e-05\n",
            "step: 510, loss: 0.0001703365269349888\n",
            "step: 520, loss: 2.552485602791421e-05\n",
            "step: 530, loss: 3.0660812626592815e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9485396383866481, f1=0.9424758398527383, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004342400352470577\n",
            "step: 10, loss: 0.0007217947859317064\n",
            "step: 20, loss: 2.2775071556679904e-05\n",
            "step: 30, loss: 0.03048732504248619\n",
            "step: 40, loss: 0.00035212640068493783\n",
            "step: 50, loss: 0.0004678008845075965\n",
            "step: 60, loss: 0.0008293272694572806\n",
            "step: 70, loss: 0.00022184144472703338\n",
            "step: 80, loss: 0.0005145965842530131\n",
            "step: 90, loss: 2.812348975567147e-05\n",
            "step: 100, loss: 2.2060628907638602e-05\n",
            "step: 110, loss: 0.00016255021910183132\n",
            "step: 120, loss: 5.381739538279362e-05\n",
            "step: 130, loss: 0.0003800346457865089\n",
            "step: 140, loss: 0.0023837625049054623\n",
            "step: 150, loss: 0.0034875082783401012\n",
            "step: 160, loss: 0.001247349544428289\n",
            "step: 170, loss: 0.00016988349671009928\n",
            "step: 180, loss: 0.00015561329200863838\n",
            "step: 190, loss: 0.0020571162458509207\n",
            "step: 200, loss: 0.00012843866716139019\n",
            "step: 210, loss: 0.01575685851275921\n",
            "step: 220, loss: 1.6472922652610578e-05\n",
            "step: 230, loss: 0.000333186995703727\n",
            "step: 240, loss: 0.0020844340324401855\n",
            "step: 250, loss: 0.001919322065077722\n",
            "step: 260, loss: 0.00313810957595706\n",
            "step: 270, loss: 0.0025389075744897127\n",
            "step: 280, loss: 5.7821998780127615e-05\n",
            "step: 290, loss: 7.75705193518661e-05\n",
            "step: 300, loss: 2.0611647414625622e-05\n",
            "step: 310, loss: 3.3670828997856006e-05\n",
            "step: 320, loss: 5.32373487658333e-05\n",
            "step: 330, loss: 0.0016231213230639696\n",
            "step: 340, loss: 0.0006051517557352781\n",
            "step: 350, loss: 0.00013521935034077615\n",
            "step: 360, loss: 4.921403888147324e-05\n",
            "step: 370, loss: 0.0003841146535705775\n",
            "step: 380, loss: 0.0002571730292402208\n",
            "step: 390, loss: 0.00433031702414155\n",
            "step: 400, loss: 0.0024363910779356956\n",
            "step: 410, loss: 1.8857113900594413e-05\n",
            "step: 420, loss: 0.0003463909379206598\n",
            "step: 430, loss: 0.23893171548843384\n",
            "step: 440, loss: 0.009395492263138294\n",
            "step: 450, loss: 2.864609814423602e-05\n",
            "step: 460, loss: 0.023486211895942688\n",
            "step: 470, loss: 0.00017045668209902942\n",
            "step: 480, loss: 0.00042917224345728755\n",
            "step: 490, loss: 0.0004770659434143454\n",
            "step: 500, loss: 0.002746665384620428\n",
            "step: 510, loss: 0.01524451095610857\n",
            "step: 520, loss: 0.0005102226859889925\n",
            "step: 530, loss: 0.0017612582305446267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9452181987000929, f1=0.94547134935305, best_f1=0.9466911764705882\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00016812178364489228\n",
            "step: 10, loss: 0.0014646017225459218\n",
            "step: 20, loss: 0.0011518137762323022\n",
            "step: 30, loss: 0.004499153699725866\n",
            "step: 40, loss: 0.0001411809353157878\n",
            "step: 50, loss: 0.0034377831034362316\n",
            "step: 60, loss: 0.010872084647417068\n",
            "step: 70, loss: 0.00012266798876225948\n",
            "step: 80, loss: 0.00013895878510084003\n",
            "step: 90, loss: 0.0003151529817841947\n",
            "step: 100, loss: 0.00018544877821113914\n",
            "step: 110, loss: 0.0029468899592757225\n",
            "step: 120, loss: 0.000158568233018741\n",
            "step: 130, loss: 0.00909784622490406\n",
            "step: 140, loss: 6.663425301667303e-05\n",
            "step: 150, loss: 0.00015085334598552436\n",
            "step: 160, loss: 0.00011335467570461333\n",
            "step: 170, loss: 0.0005695211584679782\n",
            "step: 180, loss: 0.0025327452458441257\n",
            "step: 190, loss: 0.00289501273073256\n",
            "step: 200, loss: 0.00341470493003726\n",
            "step: 210, loss: 0.00017569582269061357\n",
            "step: 220, loss: 0.0006023910245858133\n",
            "step: 230, loss: 0.003134890226647258\n",
            "step: 240, loss: 0.025503678247332573\n",
            "step: 250, loss: 0.00014024948177393526\n",
            "step: 260, loss: 0.0004433748545125127\n",
            "step: 270, loss: 0.000502136186696589\n",
            "step: 280, loss: 4.640388215193525e-05\n",
            "step: 290, loss: 6.0776390455430374e-05\n",
            "step: 300, loss: 0.0003783343418035656\n",
            "step: 310, loss: 0.00952970515936613\n",
            "step: 320, loss: 7.018547330517322e-05\n",
            "step: 330, loss: 6.520382885355502e-05\n",
            "step: 340, loss: 0.001289075706154108\n",
            "step: 350, loss: 0.004455302841961384\n",
            "step: 360, loss: 0.0026052219327539206\n",
            "step: 370, loss: 0.003912735264748335\n",
            "step: 380, loss: 0.00042789377039298415\n",
            "step: 390, loss: 0.0637037381529808\n",
            "step: 400, loss: 0.0027492432855069637\n",
            "step: 410, loss: 0.000858452869579196\n",
            "step: 420, loss: 0.00011667457147268578\n",
            "step: 430, loss: 5.352950029191561e-05\n",
            "step: 440, loss: 0.0003363971773069352\n",
            "step: 450, loss: 0.008881657384335995\n",
            "step: 460, loss: 0.005197683349251747\n",
            "step: 470, loss: 3.284795820945874e-05\n",
            "step: 480, loss: 0.004953771363943815\n",
            "step: 490, loss: 0.0019998704083263874\n",
            "step: 500, loss: 0.002305199857801199\n",
            "step: 510, loss: 0.0005047296290285885\n",
            "step: 520, loss: 8.7915534095373e-05\n",
            "step: 530, loss: 0.00029713514959439635\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9472222222222223, f1=0.9455719557195572, best_f1=0.9466911764705882\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:21, 266.55it/s]\n",
            "load_f1 = 0.9474654377880185\n",
            "real_f1 = 0.9483870967741935\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 208.60it/s]\n"
          ]
        }
      ]
    }
  ]
}