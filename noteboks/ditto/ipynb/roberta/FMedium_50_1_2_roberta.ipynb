{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FMedium_50_1_2_roberta.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_34HObszwKdY",
        "iX3OOr8bwRyk",
        "uAyGpUo9ifJM",
        "pw03GW7dmkqy",
        "jeDvm9a1dIlo",
        "NJ3ExOzkeDVk",
        "bRxHd3j2eEH8",
        "h62Yut_pgNFQ",
        "10svv34hgw7-",
        "pnXzXaaYhstq"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34HObszwKdY"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSgSb9vUtCyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96a4948-a5d2-4f93-8948-48b7ee90e3a4"
      },
      "source": [
        "!git clone https://github.com/megagonlabs/ditto\n",
        "%cd ditto\n",
        "!pip install -r requirements.txt\n",
        "!git clone https://github.com/rit-git/Snippext_public\n",
        "%cd Snippext_public\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ditto'...\n",
            "remote: Enumerating objects: 291, done.\u001b[K\n",
            "remote: Total 291 (delta 0), reused 0 (delta 0), pack-reused 291\u001b[K\n",
            "Receiving objects: 100% (291/291), 26.87 MiB | 17.03 MiB/s, done.\n",
            "Resolving deltas: 100% (142/142), done.\n",
            "/content/ditto\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 55.0 MB/s \n",
            "\u001b[?25hCollecting regex==2019.12.20\n",
            "  Downloading regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "\u001b[K     |████████████████████████████████| 689 kB 50.7 MB/s \n",
            "\u001b[?25hCollecting scipy==1.3.2\n",
            "  Downloading scipy-1.3.2-cp37-cp37m-manylinux1_x86_64.whl (25.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.2 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Downloading sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting spacy==3.1\n",
            "  Downloading spacy-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 57.1 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.9.0+cu111\u001b[0m\n",
            "\u001b[?25hCloning into 'Snippext_public'...\n",
            "remote: Enumerating objects: 413, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 413 (delta 15), reused 14 (delta 14), pack-reused 391\u001b[K\n",
            "Receiving objects: 100% (413/413), 21.06 MiB | 13.05 MiB/s, done.\n",
            "Resolving deltas: 100% (196/196), done.\n",
            "/content/ditto/Snippext_public\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gensim==3.8.1\n",
            "  Using cached gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "Collecting numpy==1.19.2\n",
            "  Using cached numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "Collecting regex==2019.12.20\n",
            "  Using cached regex-2019.12.20-cp37-cp37m-manylinux2010_x86_64.whl (689 kB)\n",
            "Collecting spacy==2.2.3\n",
            "  Downloading spacy-2.2.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 25.7 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.85\n",
            "  Using cached sentencepiece-0.1.85-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.0)\n",
            "Collecting tensorboardX==2.0\n",
            "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\n",
            "\u001b[K     |████████████████████████████████| 195 kB 59.5 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 7.2 kB/s \n",
            "\u001b[?25hCollecting tqdm==4.41.0\n",
            "  Downloading tqdm-4.41.0-py2.py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 68.4 MB/s \n",
            "\u001b[?25hCollecting jsonlines==1.2.0\n",
            "  Downloading jsonlines-1.2.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting nltk==3.4.5\n",
            "  Downloading nltk-3.4.5.zip (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 52.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (3.0.6)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "  Downloading thinc-7.3.1-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 55.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (1.0.7)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (0.10.1)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.3->-r requirements.txt (line 4)) (57.4.0)\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "  Downloading blis-0.4.1-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 45.4 MB/s \n",
            "\u001b[?25hCollecting srsly<1.1.0,>=0.1.0\n",
            "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 70.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==2.0->-r requirements.txt (line 8)) (3.17.3)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 54.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r requirements.txt (line 11)) (3.7.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 67.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 4)) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.1.0->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r requirements.txt (line 11)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r requirements.txt (line 6)) (3.1.0)\n",
            "Building wheels for collected packages: nltk, sacremoses\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.5-py3-none-any.whl size=1449921 sha256=f2c11164224cfa860006ef23e2c60e666c31d419924394cb9e174368fdf2fc76\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/8b/7f/473521e0c731c6566d631b281f323842bbda9bd819eb9a3ead\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=089d8524b094f355cd9e78c432c8b25a0406a2a32c6eb6555ef85cd714dd5ddd\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built nltk sacremoses\n",
            "Installing collected packages: numpy, tqdm, srsly, regex, plac, blis, tokenizers, thinc, sentencepiece, sacremoses, catalogue, transformers, torch, tensorboardX, spacy, nltk, jsonlines, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.4\n",
            "    Uninstalling srsly-2.4.4:\n",
            "      Successfully uninstalled srsly-2.4.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.8\n",
            "    Uninstalling blis-0.7.8:\n",
            "      Successfully uninstalled blis-0.7.8\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.0\n",
            "    Uninstalling thinc-8.1.0:\n",
            "      Successfully uninstalled thinc-8.1.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.8\n",
            "    Uninstalling catalogue-2.0.8:\n",
            "      Successfully uninstalled catalogue-2.0.8\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.0+cu113\n",
            "    Uninstalling torch-1.12.0+cu113:\n",
            "      Successfully uninstalled torch-1.12.0+cu113\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.1\n",
            "    Uninstalling spacy-3.4.1:\n",
            "      Successfully uninstalled spacy-3.4.1\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.4.0 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "en-core-web-sm 3.4.0 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.2.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.4.1 catalogue-1.0.0 gensim-3.8.1 jsonlines-1.2.0 nltk-3.4.5 numpy-1.19.2 plac-1.1.3 regex-2019.12.20 sacremoses-0.0.53 sentencepiece-0.1.85 spacy-2.2.3 srsly-1.0.5 tensorboardX-2.0 thinc-7.3.1 tokenizers-0.8.1rc2 torch-1.4.0 tqdm-4.41.0 transformers-3.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ditto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3OOr8bwRyk"
      },
      "source": [
        "## Install fp16 optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_hfO6D_uLby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5986c5-31db-4c5e-f347-f1464482bff5"
      },
      "source": [
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir ./\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10085, done.\u001b[K\n",
            "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 10085 (delta 98), reused 131 (delta 59), pack-reused 9884\u001b[K\n",
            "Receiving objects: 100% (10085/10085), 14.95 MiB | 10.70 MiB/s, done.\n",
            "Resolving deltas: 100% (6902/6902), done.\n",
            "/content/ditto/apex\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-htj_0b2_\n",
            "Created temporary directory: /tmp/pip-req-tracker-hbn967t5\n",
            "Initialized build tracking at /tmp/pip-req-tracker-hbn967t5\n",
            "Created build tracker: /tmp/pip-req-tracker-hbn967t5\n",
            "Entered build tracker: /tmp/pip-req-tracker-hbn967t5\n",
            "Created temporary directory: /tmp/pip-install-2lfurfqi\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ditto/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-zz2vzxaf\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/ditto/apex to build tracker '/tmp/pip-req-tracker-hbn967t5'\n",
            "    Running setup.py (path:/tmp/pip-req-build-zz2vzxaf/setup.py) egg_info for package from file:///content/ditto/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-r_zif3q7\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-r_zif3q7/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-r_zif3q7/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-r_zif3q7/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-r_zif3q7/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-r_zif3q7/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-r_zif3q7/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-zz2vzxaf has version 0.1, which satisfies requirement apex==0.1 from file:///content/ditto/apex\n",
            "  Removed apex==0.1 from file:///content/ditto/apex from build tracker '/tmp/pip-req-tracker-hbn967t5'\n",
            "Created temporary directory: /tmp/pip-unpack-3dtur6oy\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-jdb1celf\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-jdb1celf\n",
            "  Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-zz2vzxaf/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-zz2vzxaf/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-jdb1celf\n",
            "\n",
            "\n",
            "  torch.__version__  = 1.4.0\n",
            "\n",
            "\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  copying apex/_autocast_utils.py -> build/lib/apex\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/__init__.py -> build/lib/apex/fused_dense\n",
            "  copying apex/fused_dense/fused_dense.py -> build/lib/apex/fused_dense\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adagrad.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/transformer\n",
            "  copying apex/transformer/microbatches.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/parallel_state.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/utils.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/__init__.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/enums.py -> build/lib/apex/transformer\n",
            "  copying apex/transformer/log_util.py -> build/lib/apex/transformer\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/__init__.py -> build/lib/apex/transformer/amp\n",
            "  copying apex/transformer/amp/grad_scaler.py -> build/lib/apex/transformer/amp\n",
            "  creating build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/__init__.py -> build/lib/apex/transformer/_data\n",
            "  copying apex/transformer/_data/_batchsampler.py -> build/lib/apex/transformer/_data\n",
            "  creating build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/__init__.py -> build/lib/apex/transformer/functional\n",
            "  copying apex/transformer/functional/fused_softmax.py -> build/lib/apex/transformer/functional\n",
            "  creating build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/utils.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/__init__.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  copying apex/transformer/pipeline_parallel/_timers.py -> build/lib/apex/transformer/pipeline_parallel\n",
            "  creating build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/layer_norm.py -> build/lib/apex/transformer/layers\n",
            "  copying apex/transformer/layers/__init__.py -> build/lib/apex/transformer/layers\n",
            "  creating build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/memory.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/utils.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/__init__.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/mappings.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/data.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/layers.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  copying apex/transformer/tensor_parallel/random.py -> build/lib/apex/transformer/tensor_parallel\n",
            "  creating build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_gpt.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/__init__.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_bert.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/distributed_test_base.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/arguments.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/commons.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/global_vars.py -> build/lib/apex/transformer/testing\n",
            "  copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib/apex/transformer/testing\n",
            "  creating build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib/apex/transformer/pipeline_parallel/schedules\n",
            "  creating build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/__init__.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/bottleneck.py -> build/lib/apex/contrib/bottleneck\n",
            "  copying apex/contrib/bottleneck/test.py -> build/lib/apex/contrib/bottleneck\n",
            "  creating build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/__init__.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib/apex/contrib/conv_bias_relu\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/__init__.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib/apex/contrib/index_mul_2d\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/__init__.py -> build/lib/apex/contrib/fmha\n",
            "  copying apex/contrib/fmha/fmha.py -> build/lib/apex/contrib/fmha\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  creating build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/permutation_lib.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/__init__.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/asp.py -> build/lib/apex/contrib/sparsity\n",
            "  copying apex/contrib/sparsity/sparse_masklib.py -> build/lib/apex/contrib/sparsity\n",
            "  creating build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/layer_norm.py -> build/lib/apex/contrib/layer_norm\n",
            "  copying apex/contrib/layer_norm/__init__.py -> build/lib/apex/contrib/layer_norm\n",
            "  creating build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/__init__.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib/apex/contrib/peer_memory\n",
            "  copying apex/contrib/peer_memory/peer_memory.py -> build/lib/apex/contrib/peer_memory\n",
            "  creating build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/__init__.py -> build/lib/apex/contrib/transducer\n",
            "  copying apex/contrib/transducer/transducer.py -> build/lib/apex/contrib/transducer\n",
            "  creating build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/__init__.py -> build/lib/apex/contrib/clip_grad\n",
            "  copying apex/contrib/clip_grad/clip_grad.py -> build/lib/apex/contrib/clip_grad\n",
            "  creating build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/__init__.py -> build/lib/apex/contrib/focal_loss\n",
            "  copying apex/contrib/focal_loss/focal_loss.py -> build/lib/apex/contrib/focal_loss\n",
            "  creating build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib/apex/contrib/sparsity/permutation_search_kernels\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  copying build/lib/apex/fused_dense/fused_dense.py -> build/bdist.linux-x86_64/wheel/apex/fused_dense\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_mixed_precision_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/amp/grad_scaler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/amp\n",
            "  copying build/lib/apex/transformer/microbatches.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/_data/_batchsampler.py -> build/bdist.linux-x86_64/wheel/apex/transformer/_data\n",
            "  copying build/lib/apex/transformer/parallel_state.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  copying build/lib/apex/transformer/functional/fused_softmax.py -> build/bdist.linux-x86_64/wheel/apex/transformer/functional\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/p2p_communication.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/schedules/common.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel/schedules\n",
            "  copying build/lib/apex/transformer/pipeline_parallel/_timers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/pipeline_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  copying build/lib/apex/transformer/layers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/layers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/memory.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/utils.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/mappings.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/data.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/cross_entropy.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/layers.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  copying build/lib/apex/transformer/tensor_parallel/random.py -> build/bdist.linux-x86_64/wheel/apex/transformer/tensor_parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_gpt.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/__init__.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_bert.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/distributed_test_base.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/arguments.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/commons.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/global_vars.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/testing/standalone_transformer_lm.py -> build/bdist.linux-x86_64/wheel/apex/transformer/testing\n",
            "  copying build/lib/apex/transformer/enums.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  copying build/lib/apex/transformer/log_util.py -> build/bdist.linux-x86_64/wheel/apex/transformer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck_module_test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/halo_exchangers.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/bottleneck.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  copying build/lib/apex/contrib/bottleneck/test.py -> build/bdist.linux-x86_64/wheel/apex/contrib/bottleneck\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/bdist.linux-x86_64/wheel/apex/contrib/conv_bias_relu\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  copying build/lib/apex/contrib/index_mul_2d/index_mul_2d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/index_mul_2d\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  copying build/lib/apex/contrib/fmha/fmha.py -> build/bdist.linux-x86_64/wheel/apex/contrib/fmha\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_lib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  copying build/lib/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity/permutation_search_kernels\n",
            "  copying build/lib/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/wheel/apex/contrib/sparsity\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  copying build/lib/apex/contrib/layer_norm/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/layer_norm\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  copying build/lib/apex/contrib/peer_memory/peer_memory.py -> build/bdist.linux-x86_64/wheel/apex/contrib/peer_memory\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  copying build/lib/apex/contrib/transducer/transducer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/transducer\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  copying build/lib/apex/contrib/clip_grad/clip_grad.py -> build/bdist.linux-x86_64/wheel/apex/contrib/clip_grad\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/contrib/focal_loss/focal_loss.py -> build/bdist.linux-x86_64/wheel/apex/contrib/focal_loss\n",
            "  copying build/lib/apex/_autocast_utils.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.7.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-jdb1celf/apex-0.1-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/_autocast_utils.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/__init__.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck.py'\n",
            "  adding 'apex/contrib/bottleneck/bottleneck_module_test.py'\n",
            "  adding 'apex/contrib/bottleneck/halo_exchangers.py'\n",
            "  adding 'apex/contrib/bottleneck/test.py'\n",
            "  adding 'apex/contrib/clip_grad/__init__.py'\n",
            "  adding 'apex/contrib/clip_grad/clip_grad.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/__init__.py'\n",
            "  adding 'apex/contrib/conv_bias_relu/conv_bias_relu.py'\n",
            "  adding 'apex/contrib/fmha/__init__.py'\n",
            "  adding 'apex/contrib/fmha/fmha.py'\n",
            "  adding 'apex/contrib/focal_loss/__init__.py'\n",
            "  adding 'apex/contrib/focal_loss/focal_loss.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/index_mul_2d/__init__.py'\n",
            "  adding 'apex/contrib/index_mul_2d/index_mul_2d.py'\n",
            "  adding 'apex/contrib/layer_norm/__init__.py'\n",
            "  adding 'apex/contrib/layer_norm/layer_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/mask_softmax_dropout_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/distributed_fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/peer_memory/__init__.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchange_module_tests.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_halo_exchanger_1d.py'\n",
            "  adding 'apex/contrib/peer_memory/peer_memory.py'\n",
            "  adding 'apex/contrib/sparsity/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/asp.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_lib.py'\n",
            "  adding 'apex/contrib/sparsity/sparse_masklib.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/__init__.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py'\n",
            "  adding 'apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py'\n",
            "  adding 'apex/contrib/transducer/__init__.py'\n",
            "  adding 'apex/contrib/transducer/transducer.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/fused_dense/__init__.py'\n",
            "  adding 'apex/fused_dense/fused_dense.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adagrad.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_mixed_precision_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/transformer/__init__.py'\n",
            "  adding 'apex/transformer/enums.py'\n",
            "  adding 'apex/transformer/log_util.py'\n",
            "  adding 'apex/transformer/microbatches.py'\n",
            "  adding 'apex/transformer/parallel_state.py'\n",
            "  adding 'apex/transformer/utils.py'\n",
            "  adding 'apex/transformer/_data/__init__.py'\n",
            "  adding 'apex/transformer/_data/_batchsampler.py'\n",
            "  adding 'apex/transformer/amp/__init__.py'\n",
            "  adding 'apex/transformer/amp/grad_scaler.py'\n",
            "  adding 'apex/transformer/functional/__init__.py'\n",
            "  adding 'apex/transformer/functional/fused_softmax.py'\n",
            "  adding 'apex/transformer/layers/__init__.py'\n",
            "  adding 'apex/transformer/layers/layer_norm.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/_timers.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/p2p_communication.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/utils.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/__init__.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/common.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py'\n",
            "  adding 'apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py'\n",
            "  adding 'apex/transformer/tensor_parallel/__init__.py'\n",
            "  adding 'apex/transformer/tensor_parallel/cross_entropy.py'\n",
            "  adding 'apex/transformer/tensor_parallel/data.py'\n",
            "  adding 'apex/transformer/tensor_parallel/layers.py'\n",
            "  adding 'apex/transformer/tensor_parallel/mappings.py'\n",
            "  adding 'apex/transformer/tensor_parallel/memory.py'\n",
            "  adding 'apex/transformer/tensor_parallel/random.py'\n",
            "  adding 'apex/transformer/tensor_parallel/utils.py'\n",
            "  adding 'apex/transformer/testing/__init__.py'\n",
            "  adding 'apex/transformer/testing/arguments.py'\n",
            "  adding 'apex/transformer/testing/commons.py'\n",
            "  adding 'apex/transformer/testing/distributed_test_base.py'\n",
            "  adding 'apex/transformer/testing/global_vars.py'\n",
            "  adding 'apex/transformer/testing/standalone_bert.py'\n",
            "  adding 'apex/transformer/testing/standalone_gpt.py'\n",
            "  adding 'apex/transformer/testing/standalone_transformer_lm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-py3-none-any.whl size=298114 sha256=1cca4568e89ef21aa253d92b38c6a4ded58a057aeb7a7a19f75ec5d9daa4a3d8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-htj_0b2_/wheels/6b/22/a2/1665526ee3c3061243260fd522525eca31398e04bfa5ad7e1f\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.7/dist-packages\n",
            "  sysconfig: /usr/lib/python3.7/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.7/apex\n",
            "  sysconfig: /usr/include/python3.7m/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.7/dist-packages\n",
            "sysconfig: /usr/lib/python3.7/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.7/UNKNOWN\n",
            "sysconfig: /usr/include/python3.7m/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-hbn967t5'\n",
            "/content/ditto\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKSZwBG_uyzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277e4392-320a-4693-84b7-24f42156766a"
      },
      "source": [
        "# some issue with colab\n",
        "!pip install --upgrade \"urllib3==1.25.4\" awscli"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urllib3==1.25.4\n",
            "  Downloading urllib3-1.25.4-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 32.2 MB/s \n",
            "\u001b[?25hCollecting awscli\n",
            "  Downloading awscli-1.25.47-py3-none-any.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 48.7 MB/s \n",
            "\u001b[?25hCollecting colorama<0.4.5,>=0.2.5\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting rsa<4.8,>=3.1.2\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting docutils<0.17,>=0.10\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 67.8 MB/s \n",
            "\u001b[?25hCollecting botocore==1.27.47\n",
            "  Downloading botocore-1.27.47-py3-none-any.whl (9.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.0 MB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.5,>=3.10 in /usr/local/lib/python3.7/dist-packages (from awscli) (3.13)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore==1.27.47->awscli) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.27.47->awscli) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.17.1\n",
            "    Uninstalling docutils-0.17.1:\n",
            "      Successfully uninstalled docutils-0.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.41.0 which is incompatible.\n",
            "fastai 2.7.7 requires torch<1.13,>=1.7, but you have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed awscli-1.25.47 botocore-1.27.47 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.6.0 urllib3-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1"
      ],
      "metadata": {
        "id": "KR84V9pFRkw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a692186-dfde-4045-c71a-041d86448c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.2)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.4.0\n",
            "    Uninstalling torch-1.4.0:\n",
            "      Successfully uninstalled torch-1.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.13.0 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.12.0+cu113 requires torch==1.12.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## importa config.json"
      ],
      "metadata": {
        "id": "uAyGpUo9ifJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pauloh48/IC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xawOMn6icU7",
        "outputId": "d85fa540-9e38-43be-dfec-edc1ed72193c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'IC'...\n",
            "remote: Enumerating objects: 982, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 982 (delta 16), reused 7 (delta 0), pack-reused 930\u001b[K\n",
            "Receiving objects: 100% (982/982), 251.89 MiB | 17.35 MiB/s, done.\n",
            "Resolving deltas: 100% (600/600), done.\n",
            "Checking out files: 100% (1273/1273), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## remove e move"
      ],
      "metadata": {
        "id": "pw03GW7dmkqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "FQVym9vwmx-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6522093e-dc44-4f10-cd5b-a7900360c132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "apex\t      ditto.jpg    LICENSE     requirements.txt        run_all_wdc.py\n",
            "blocking      ditto_light  matcher.py  results_ditto\t       Snippext_public\n",
            "configs.json  IC\t   output      run_all_er_magellan.py  train_ditto.py\n",
            "data\t      input\t   README.md   run_all_vary_size.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm configs.json\n",
        "!mv -n /content/ditto/IC/datasesErros/FMedium_50_1_2/configs.json /content/ditto/"
      ],
      "metadata": {
        "id": "bVI2JBvFmm2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DA"
      ],
      "metadata": {
        "id": "bm4nohJxf9bD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA STRUCTURED"
      ],
      "metadata": {
        "id": "jeDvm9a1dIlo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76DEFGNcGW0"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qxLFPNvcGgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090e683e-8a9a-4152-ee91-0fbfe9776a74"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 505kB/s]\n",
            "Downloading: 100% 899k/899k [00:00<00:00, 2.39MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.80MB/s]\n",
            "Downloading: 100% 501M/501M [00:10<00:00, 47.1MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4246775209903717\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.2692307692307693, f1=0.2666666666666667, best_f1=0.2666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.47920364141464233\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.288659793814433, f1=0.2692307692307693, best_f1=0.2692307692307693\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.41634470224380493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.35443037974683544, f1=0.28571428571428575, best_f1=0.28571428571428575\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.26893535256385803\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.4905660377358491, f1=0.3870967741935483, best_f1=0.3870967741935483\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3324672281742096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.5, f1=0.4067796610169491, best_f1=0.4067796610169491\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2114650309085846\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.5090909090909091, f1=0.4210526315789474, best_f1=0.4210526315789474\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.40469202399253845\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.6111111111111112, f1=0.5128205128205129, best_f1=0.5128205128205129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.418935626745224\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7857142857142857, f1=0.588235294117647, best_f1=0.588235294117647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.22870619595050812\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.75, f1=0.6923076923076924, best_f1=0.588235294117647\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3659387230873108\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.8333333333333333, f1=0.7142857142857143, best_f1=0.7142857142857143\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15962573885917664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.8461538461538461, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08004716038703918\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8461538461538461, f1=0.7142857142857143, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06320689618587494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8333333333333333, f1=0.7142857142857143, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012955262325704098\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8333333333333333, f1=0.7142857142857143, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022751232609152794\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8333333333333333, f1=0.7142857142857143, best_f1=0.6666666666666666\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 110472.26it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.8666666666666666\n",
            "real_f1 = 0.8387096774193549\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 193.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "FjO-q4GLeCE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "WZ7mparQevgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b82696-6f63-48b7-f315-2a94d995a346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5749607086181641\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.46850907802581787\n",
            "step: 20, loss: 0.4881693124771118\n",
            "step: 30, loss: 0.33247241377830505\n",
            "step: 40, loss: 0.3667101562023163\n",
            "step: 50, loss: 0.6017252802848816\n",
            "step: 60, loss: 0.4771202504634857\n",
            "step: 70, loss: 0.3134121894836426\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 0.30139631032943726\n",
            "step: 90, loss: 0.1567903310060501\n",
            "step: 100, loss: 0.1754891276359558\n",
            "step: 110, loss: 0.10042215883731842\n",
            "step: 120, loss: 0.019791826605796814\n",
            "step: 130, loss: 0.016893509775400162\n",
            "step: 140, loss: 0.016371598467230797\n",
            "step: 150, loss: 0.2144383192062378\n",
            "step: 160, loss: 0.029869118705391884\n",
            "step: 170, loss: 0.0361224040389061\n",
            "step: 180, loss: 0.00593141233548522\n",
            "step: 190, loss: 0.026628723368048668\n",
            "step: 200, loss: 0.046193677932024\n",
            "step: 210, loss: 0.00944427214562893\n",
            "step: 220, loss: 0.10953821986913681\n",
            "step: 230, loss: 0.0028549060225486755\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9474860335195531, f1=0.9503386004514672, best_f1=0.9503386004514672\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.039359938353300095\n",
            "step: 10, loss: 0.15921860933303833\n",
            "step: 20, loss: 0.03479389473795891\n",
            "step: 30, loss: 0.009623296558856964\n",
            "step: 40, loss: 0.015882203355431557\n",
            "step: 50, loss: 0.006108604837208986\n",
            "step: 60, loss: 0.002012949436903\n",
            "step: 70, loss: 0.010435818694531918\n",
            "step: 80, loss: 0.016093332320451736\n",
            "step: 90, loss: 0.01686667464673519\n",
            "step: 100, loss: 0.004625271074473858\n",
            "step: 110, loss: 0.0018149052048102021\n",
            "step: 120, loss: 0.07217506319284439\n",
            "step: 130, loss: 0.00717606907710433\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 140, loss: 0.002941774670034647\n",
            "step: 150, loss: 0.06971067190170288\n",
            "step: 160, loss: 0.0019276937237009406\n",
            "step: 170, loss: 0.008184879086911678\n",
            "step: 180, loss: 0.0014377129264175892\n",
            "step: 190, loss: 0.010182122699916363\n",
            "step: 200, loss: 0.011403673328459263\n",
            "step: 210, loss: 0.02603006176650524\n",
            "step: 220, loss: 0.0022311736829578876\n",
            "step: 230, loss: 0.0010596424108371139\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9865771812080537, f1=0.9841986455981941, best_f1=0.9841986455981941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011491893790662289\n",
            "step: 10, loss: 0.0018524796469137073\n",
            "step: 20, loss: 0.02068229392170906\n",
            "step: 30, loss: 0.00045445625437423587\n",
            "step: 40, loss: 0.0008231078390963376\n",
            "step: 50, loss: 0.125575989484787\n",
            "step: 60, loss: 0.017027901485562325\n",
            "step: 70, loss: 0.001315944828093052\n",
            "step: 80, loss: 0.001280092285014689\n",
            "step: 90, loss: 0.0013528355630114675\n",
            "step: 100, loss: 0.0012068775249645114\n",
            "step: 110, loss: 0.006118736229836941\n",
            "step: 120, loss: 0.0005890278844162822\n",
            "step: 130, loss: 0.005621536634862423\n",
            "step: 140, loss: 0.0233791321516037\n",
            "step: 150, loss: 0.0050691841170191765\n",
            "step: 160, loss: 0.0011806858237832785\n",
            "step: 170, loss: 0.0005101613351143897\n",
            "step: 180, loss: 0.0013866238296031952\n",
            "step: 190, loss: 0.11672782152891159\n",
            "step: 200, loss: 0.034093957394361496\n",
            "step: 210, loss: 0.0021342942491173744\n",
            "step: 220, loss: 0.0037592395674437284\n",
            "step: 230, loss: 0.005575776100158691\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9841269841269841, f1=0.9794988610478361, best_f1=0.9841986455981941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002391889924183488\n",
            "step: 10, loss: 0.0018033395754173398\n",
            "step: 20, loss: 0.0009095299756154418\n",
            "step: 30, loss: 0.0004366959328763187\n",
            "step: 40, loss: 0.0473574623465538\n",
            "step: 50, loss: 0.0010431077098473907\n",
            "step: 60, loss: 0.002116978634148836\n",
            "step: 70, loss: 0.0008783339872024953\n",
            "step: 80, loss: 0.0003721842367667705\n",
            "step: 90, loss: 0.000929115922190249\n",
            "step: 100, loss: 0.0005511687486432493\n",
            "step: 110, loss: 0.00040994753362610936\n",
            "step: 120, loss: 0.0007666167221032083\n",
            "step: 130, loss: 0.013717906549572945\n",
            "step: 140, loss: 0.001048875623382628\n",
            "step: 150, loss: 0.0005482014385052025\n",
            "step: 160, loss: 0.005349561106413603\n",
            "step: 170, loss: 0.0013123378157615662\n",
            "step: 180, loss: 0.12456031888723373\n",
            "step: 190, loss: 0.001657150685787201\n",
            "step: 200, loss: 0.0013862771447747946\n",
            "step: 210, loss: 0.00038301452877931297\n",
            "step: 220, loss: 0.000313088356051594\n",
            "step: 230, loss: 0.0012603256618604064\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9888392857142857, f1=0.9854096520763187, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004122177779208869\n",
            "step: 10, loss: 0.0010608364827930927\n",
            "step: 20, loss: 0.0005736601306125522\n",
            "step: 30, loss: 0.0005463029956445098\n",
            "step: 40, loss: 0.0016830320237204432\n",
            "step: 50, loss: 0.0011525126174092293\n",
            "step: 60, loss: 0.012857920490205288\n",
            "step: 70, loss: 0.0008982711588032544\n",
            "step: 80, loss: 0.09284624457359314\n",
            "step: 90, loss: 0.0416363887488842\n",
            "step: 100, loss: 0.0005968048353679478\n",
            "step: 110, loss: 0.0047010802663862705\n",
            "step: 120, loss: 0.0012454735115170479\n",
            "step: 130, loss: 0.021749893203377724\n",
            "step: 140, loss: 0.00023877414059825242\n",
            "step: 150, loss: 0.07245459407567978\n",
            "step: 160, loss: 0.05145087465643883\n",
            "step: 170, loss: 0.006063238251954317\n",
            "step: 180, loss: 0.05455849692225456\n",
            "step: 190, loss: 0.15559250116348267\n",
            "step: 200, loss: 0.009829890914261341\n",
            "step: 210, loss: 0.022847985848784447\n",
            "step: 220, loss: 0.0013664179714396596\n",
            "step: 230, loss: 0.005443754605948925\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9820627802690582, f1=0.9819413092550789, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014138576807454228\n",
            "step: 10, loss: 0.01813112571835518\n",
            "step: 20, loss: 0.0018935460830107331\n",
            "step: 30, loss: 0.0009518121369183064\n",
            "step: 40, loss: 0.0004841394838877022\n",
            "step: 50, loss: 0.00045530337956734\n",
            "step: 60, loss: 0.008144647814333439\n",
            "step: 70, loss: 0.0011564072920009494\n",
            "step: 80, loss: 0.004247407428920269\n",
            "step: 90, loss: 0.045359767973423004\n",
            "step: 100, loss: 0.0006385858869180083\n",
            "step: 110, loss: 0.012242245487868786\n",
            "step: 120, loss: 0.0005540656275115907\n",
            "step: 130, loss: 0.000584003864787519\n",
            "step: 140, loss: 0.00032273941906169057\n",
            "step: 150, loss: 0.012855594977736473\n",
            "step: 160, loss: 0.04994453489780426\n",
            "step: 170, loss: 0.00035542139085009694\n",
            "step: 180, loss: 0.00012574611173477024\n",
            "step: 190, loss: 0.00021636734891217202\n",
            "step: 200, loss: 0.006695536896586418\n",
            "step: 210, loss: 0.000449707149527967\n",
            "step: 220, loss: 0.01944686844944954\n",
            "step: 230, loss: 0.0018084825715050101\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.987736900780379, f1=0.9810055865921787, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002641086233779788\n",
            "step: 10, loss: 0.0013663964346051216\n",
            "step: 20, loss: 0.0006558587774634361\n",
            "step: 30, loss: 0.0003140756452921778\n",
            "step: 40, loss: 0.006934059783816338\n",
            "step: 50, loss: 0.0008339554769918323\n",
            "step: 60, loss: 0.0006352144409902394\n",
            "step: 70, loss: 0.001360758556984365\n",
            "step: 80, loss: 0.0005287437234073877\n",
            "step: 90, loss: 0.0009046524064615369\n",
            "step: 100, loss: 0.0012762820115312934\n",
            "step: 110, loss: 0.0008001212845556438\n",
            "step: 120, loss: 0.002484228229150176\n",
            "step: 130, loss: 0.003765497822314501\n",
            "step: 140, loss: 0.0007617758237756789\n",
            "step: 150, loss: 0.011167776770889759\n",
            "step: 160, loss: 0.00039085137541405857\n",
            "step: 170, loss: 0.00383256608620286\n",
            "step: 180, loss: 0.0008226247155107558\n",
            "step: 190, loss: 0.0003382998111192137\n",
            "step: 200, loss: 0.0031323556322604418\n",
            "step: 210, loss: 0.018907401710748672\n",
            "step: 220, loss: 0.0035078763030469418\n",
            "step: 230, loss: 0.0010915284510701895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9843749999999999, f1=0.9709821428571428, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010825308272615075\n",
            "step: 10, loss: 0.004788877908140421\n",
            "step: 20, loss: 0.0030097232665866613\n",
            "step: 30, loss: 0.004795734770596027\n",
            "step: 40, loss: 0.002004623180255294\n",
            "step: 50, loss: 0.07714107632637024\n",
            "step: 60, loss: 0.0034655805211514235\n",
            "step: 70, loss: 0.0005395955522544682\n",
            "step: 80, loss: 0.007911363616585732\n",
            "step: 90, loss: 0.0017501743277534842\n",
            "step: 100, loss: 0.0009460980072617531\n",
            "step: 110, loss: 0.002895552897825837\n",
            "step: 120, loss: 0.000487611599965021\n",
            "step: 130, loss: 0.0009787912713363767\n",
            "step: 140, loss: 0.0004401872574817389\n",
            "step: 150, loss: 0.06553880125284195\n",
            "step: 160, loss: 0.019189540296792984\n",
            "step: 170, loss: 0.043698374181985855\n",
            "step: 180, loss: 0.0005674316780641675\n",
            "step: 190, loss: 0.02004486881196499\n",
            "step: 200, loss: 0.10437820851802826\n",
            "step: 210, loss: 0.00466080941259861\n",
            "step: 220, loss: 0.0014410995645448565\n",
            "step: 230, loss: 0.0011073511559516191\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9888392857142857, f1=0.9842696629213483, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003127569332718849\n",
            "step: 10, loss: 0.0018240626668557525\n",
            "step: 20, loss: 0.004555943887680769\n",
            "step: 30, loss: 0.0018184434156864882\n",
            "step: 40, loss: 0.001262512058019638\n",
            "step: 50, loss: 0.0022873857524245977\n",
            "step: 60, loss: 0.0009763341513462365\n",
            "step: 70, loss: 0.03994136303663254\n",
            "step: 80, loss: 0.0008102519786916673\n",
            "step: 90, loss: 0.04124089702963829\n",
            "step: 100, loss: 0.000563468667678535\n",
            "step: 110, loss: 0.004982004873454571\n",
            "step: 120, loss: 0.012612714432179928\n",
            "step: 130, loss: 0.0008648114162497222\n",
            "step: 140, loss: 0.0008576103718951344\n",
            "step: 150, loss: 0.000941912061534822\n",
            "step: 160, loss: 0.11776172369718552\n",
            "step: 170, loss: 0.0010207444429397583\n",
            "step: 180, loss: 0.019943414255976677\n",
            "step: 190, loss: 0.00043674366315826774\n",
            "step: 200, loss: 0.00043662727694027126\n",
            "step: 210, loss: 0.018589498475193977\n",
            "step: 220, loss: 0.000755196379031986\n",
            "step: 230, loss: 0.0002565741306170821\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9898989898989898, f1=0.9854096520763187, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011646943166851997\n",
            "step: 10, loss: 0.0008354673627763987\n",
            "step: 20, loss: 0.0007002146448940039\n",
            "step: 30, loss: 0.0003369509940966964\n",
            "step: 40, loss: 0.002132667461410165\n",
            "step: 50, loss: 0.00032884610118344426\n",
            "step: 60, loss: 0.0011027005966752768\n",
            "step: 70, loss: 0.0029693145770579576\n",
            "step: 80, loss: 0.0007858749013394117\n",
            "step: 90, loss: 0.0005348837585188448\n",
            "step: 100, loss: 0.0004555688938125968\n",
            "step: 110, loss: 0.0006405137246474624\n",
            "step: 120, loss: 0.0002933098585344851\n",
            "step: 130, loss: 0.0006403704173862934\n",
            "step: 140, loss: 0.00045555937686003745\n",
            "step: 150, loss: 0.0017310632392764091\n",
            "step: 160, loss: 0.000244354538153857\n",
            "step: 170, loss: 0.00044579882523976266\n",
            "step: 180, loss: 0.0016552135348320007\n",
            "step: 190, loss: 0.0007036784663796425\n",
            "step: 200, loss: 0.0012967863585799932\n",
            "step: 210, loss: 0.002943645231425762\n",
            "step: 220, loss: 0.0008318835170939565\n",
            "step: 230, loss: 0.000266330927843228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9899665551839464, f1=0.9766925638179801, best_f1=0.9766925638179801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00018158349848818034\n",
            "step: 10, loss: 0.0005067270831204951\n",
            "step: 20, loss: 0.0004088840796612203\n",
            "step: 30, loss: 0.0003647050471045077\n",
            "step: 40, loss: 0.015296838246285915\n",
            "step: 50, loss: 0.0001514881441835314\n",
            "step: 60, loss: 0.003173881908878684\n",
            "step: 70, loss: 0.0002850172168109566\n",
            "step: 80, loss: 0.0008256282308138907\n",
            "step: 90, loss: 0.2078460454940796\n",
            "step: 100, loss: 0.000865673297084868\n",
            "step: 110, loss: 0.0005826579872518778\n",
            "step: 120, loss: 0.0007244347361847758\n",
            "step: 130, loss: 0.0008579139830544591\n",
            "step: 140, loss: 0.00677891168743372\n",
            "step: 150, loss: 0.0008423206163570285\n",
            "step: 160, loss: 0.0012624820228666067\n",
            "step: 170, loss: 0.0009016256663016975\n",
            "step: 180, loss: 0.0011807222617790103\n",
            "step: 190, loss: 0.00080782052827999\n",
            "step: 200, loss: 0.025475259870290756\n",
            "step: 210, loss: 0.0010011357953771949\n",
            "step: 220, loss: 0.0015283881220966578\n",
            "step: 230, loss: 0.0016879182076081634\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9888641425389755, f1=0.9777777777777777, best_f1=0.9766925638179801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010439383331686258\n",
            "step: 10, loss: 0.0008032218320295215\n",
            "step: 20, loss: 0.009155446663498878\n",
            "step: 30, loss: 0.03918091952800751\n",
            "step: 40, loss: 0.0014421433443203568\n",
            "step: 50, loss: 0.007322038058191538\n",
            "step: 60, loss: 0.0011096384841948748\n",
            "step: 70, loss: 0.0010311234509572387\n",
            "step: 80, loss: 0.0005366962868720293\n",
            "step: 90, loss: 0.0028757101390510798\n",
            "step: 100, loss: 0.0006357422098517418\n",
            "step: 110, loss: 0.00040390630601905286\n",
            "step: 120, loss: 0.0006690098671242595\n",
            "step: 130, loss: 0.0007641787524335086\n",
            "step: 140, loss: 0.0018382417038083076\n",
            "step: 150, loss: 0.0018224369268864393\n",
            "step: 160, loss: 0.002098604803904891\n",
            "step: 170, loss: 0.0017038461519405246\n",
            "step: 180, loss: 0.0010176405776292086\n",
            "step: 190, loss: 0.0030626626685261726\n",
            "step: 200, loss: 0.0009553136769682169\n",
            "step: 210, loss: 0.000653458759188652\n",
            "step: 220, loss: 0.00499627785757184\n",
            "step: 230, loss: 0.0009705123375169933\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9888392857142857, f1=0.975609756097561, best_f1=0.9766925638179801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009064821642823517\n",
            "step: 10, loss: 0.0014831981388852\n",
            "step: 20, loss: 0.0012262002564966679\n",
            "step: 30, loss: 0.0022044831421226263\n",
            "step: 40, loss: 0.0011590187205001712\n",
            "step: 50, loss: 0.004461321979761124\n",
            "step: 60, loss: 0.0007047895342111588\n",
            "step: 70, loss: 0.0006067253998480737\n",
            "step: 80, loss: 0.0012450410285964608\n",
            "step: 90, loss: 0.0006319336243905127\n",
            "step: 100, loss: 0.002722573932260275\n",
            "step: 110, loss: 0.00103367876727134\n",
            "step: 120, loss: 0.0010633219499140978\n",
            "step: 130, loss: 0.0009093428961932659\n",
            "step: 140, loss: 0.009741341695189476\n",
            "step: 150, loss: 0.0005354938912205398\n",
            "step: 160, loss: 0.02215294912457466\n",
            "step: 170, loss: 0.0012819922994822264\n",
            "step: 180, loss: 0.038765184581279755\n",
            "step: 190, loss: 0.0009244114626199007\n",
            "step: 200, loss: 0.0002377719501964748\n",
            "step: 210, loss: 0.008508148603141308\n",
            "step: 220, loss: 0.0008930545882321894\n",
            "step: 230, loss: 0.0008512748754583299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9899441340782122, f1=0.984304932735426, best_f1=0.9766925638179801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006150684785097837\n",
            "step: 10, loss: 0.0005661786417476833\n",
            "step: 20, loss: 0.0008034311467781663\n",
            "step: 30, loss: 0.0012053161626681685\n",
            "step: 40, loss: 0.0009978715097531676\n",
            "step: 50, loss: 0.0004850525874644518\n",
            "step: 60, loss: 0.0007443802896887064\n",
            "step: 70, loss: 0.0006105101201683283\n",
            "step: 80, loss: 0.0005215138662606478\n",
            "step: 90, loss: 0.002038378268480301\n",
            "step: 100, loss: 0.0009986805962398648\n",
            "step: 110, loss: 0.0012081742752343416\n",
            "step: 120, loss: 0.00024060223950073123\n",
            "step: 130, loss: 0.0016002599149942398\n",
            "step: 140, loss: 0.0005862759426236153\n",
            "step: 150, loss: 0.0002534749801270664\n",
            "step: 160, loss: 0.0009425480966456234\n",
            "step: 170, loss: 0.0005900856922380626\n",
            "step: 180, loss: 0.0005571925430558622\n",
            "step: 190, loss: 0.0004797503352165222\n",
            "step: 200, loss: 0.04705464839935303\n",
            "step: 210, loss: 0.0005548528279177845\n",
            "step: 220, loss: 0.0008348909905180335\n",
            "step: 230, loss: 0.0003847593907266855\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.9910714285714286, f1=0.9810479375696767, best_f1=0.9810479375696767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003657794091850519\n",
            "step: 10, loss: 0.0008247914374805987\n",
            "step: 20, loss: 0.0014788296539336443\n",
            "step: 30, loss: 0.0007185635040514171\n",
            "step: 40, loss: 0.00039466694579459727\n",
            "step: 50, loss: 0.000491255079396069\n",
            "step: 60, loss: 0.04100343957543373\n",
            "step: 70, loss: 0.0008057819213718176\n",
            "step: 80, loss: 0.0004293776291888207\n",
            "step: 90, loss: 0.0004183083074167371\n",
            "step: 100, loss: 0.00039798414218239486\n",
            "step: 110, loss: 0.0006196595495566726\n",
            "step: 120, loss: 0.029335565865039825\n",
            "step: 130, loss: 0.0009146287338808179\n",
            "step: 140, loss: 0.010814890265464783\n",
            "step: 150, loss: 0.0008352791192010045\n",
            "step: 160, loss: 0.007682515308260918\n",
            "step: 170, loss: 0.0003435051185078919\n",
            "step: 180, loss: 0.0009881331352517009\n",
            "step: 190, loss: 0.005649772938340902\n",
            "step: 200, loss: 0.0011936441296711564\n",
            "step: 210, loss: 0.14563481509685516\n",
            "step: 220, loss: 0.0006773225031793118\n",
            "step: 230, loss: 0.0007147907163016498\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9910714285714286, f1=0.9832026875699889, best_f1=0.9810479375696767\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:12, 197.42it/s]\n",
            "load_f1 = 0.9910514541387023\n",
            "real_f1 = 0.987709497206704\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 181.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "r_G0OicNeCnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "EkIRgx40ezP8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d1df2c-a0da-4579-9f4b-8b80d6828335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6195750832557678\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.38829389214515686\n",
            "step: 20, loss: 0.3180695176124573\n",
            "step: 30, loss: 0.2854365408420563\n",
            "step: 40, loss: 0.23946513235569\n",
            "step: 50, loss: 0.26806050539016724\n",
            "step: 60, loss: 0.2350151240825653\n",
            "step: 70, loss: 0.2107764631509781\n",
            "step: 80, loss: 0.11545302718877792\n",
            "step: 90, loss: 0.1774236559867859\n",
            "step: 100, loss: 0.1574803590774536\n",
            "step: 110, loss: 0.11532028019428253\n",
            "step: 120, loss: 0.0959954783320427\n",
            "step: 130, loss: 0.18213242292404175\n",
            "step: 140, loss: 0.29258987307548523\n",
            "step: 150, loss: 0.10148898512125015\n",
            "step: 160, loss: 0.13887456059455872\n",
            "step: 170, loss: 0.02861957997083664\n",
            "step: 180, loss: 0.10345789045095444\n",
            "step: 190, loss: 0.06777550280094147\n",
            "step: 200, loss: 0.04701218381524086\n",
            "step: 210, loss: 0.05442056804895401\n",
            "step: 220, loss: 0.0840514749288559\n",
            "step: 230, loss: 0.35393282771110535\n",
            "step: 240, loss: 0.07250714302062988\n",
            "step: 250, loss: 0.05303320661187172\n",
            "step: 260, loss: 0.4171780049800873\n",
            "step: 270, loss: 0.17428551614284515\n",
            "step: 280, loss: 0.041784901171922684\n",
            "step: 290, loss: 0.0592411644756794\n",
            "step: 300, loss: 0.10072270780801773\n",
            "step: 310, loss: 0.10440503805875778\n",
            "step: 320, loss: 0.08218853175640106\n",
            "step: 330, loss: 0.08539004623889923\n",
            "step: 340, loss: 0.3183068335056305\n",
            "step: 350, loss: 0.09827771037817001\n",
            "step: 360, loss: 0.05853777006268501\n",
            "step: 370, loss: 0.05657683312892914\n",
            "step: 380, loss: 0.13621315360069275\n",
            "step: 390, loss: 0.010775083675980568\n",
            "step: 400, loss: 0.013357091695070267\n",
            "step: 410, loss: 0.26296260952949524\n",
            "step: 420, loss: 0.006587633863091469\n",
            "step: 430, loss: 0.08699800819158554\n",
            "step: 440, loss: 0.09320119023323059\n",
            "step: 450, loss: 0.01375657320022583\n",
            "step: 460, loss: 0.03007686324417591\n",
            "step: 470, loss: 0.04058968275785446\n",
            "step: 480, loss: 0.13403400778770447\n",
            "step: 490, loss: 0.1138634979724884\n",
            "step: 500, loss: 0.03143114969134331\n",
            "step: 510, loss: 0.044840727001428604\n",
            "step: 520, loss: 0.10868675261735916\n",
            "step: 530, loss: 0.04951345548033714\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9397701149425287, f1=0.9375, best_f1=0.9375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04418318718671799\n",
            "step: 10, loss: 0.01077682338654995\n",
            "step: 20, loss: 0.033372845500707626\n",
            "step: 30, loss: 0.04482161998748779\n",
            "step: 40, loss: 0.059944819658994675\n",
            "step: 50, loss: 0.060861047357320786\n",
            "step: 60, loss: 0.0239575132727623\n",
            "step: 70, loss: 0.015069874003529549\n",
            "step: 80, loss: 0.0025747367180883884\n",
            "step: 90, loss: 0.02231748215854168\n",
            "step: 100, loss: 0.09061697125434875\n",
            "step: 110, loss: 0.00821329653263092\n",
            "step: 120, loss: 0.17619206011295319\n",
            "step: 130, loss: 0.0026532390620559454\n",
            "step: 140, loss: 0.05626025050878525\n",
            "step: 150, loss: 0.0045733824372291565\n",
            "step: 160, loss: 0.024099450558423996\n",
            "step: 170, loss: 0.03793139383196831\n",
            "step: 180, loss: 0.02706124633550644\n",
            "step: 190, loss: 0.029204223304986954\n",
            "step: 200, loss: 0.19023315608501434\n",
            "step: 210, loss: 0.012335681356489658\n",
            "step: 220, loss: 0.0034230404999107122\n",
            "step: 230, loss: 0.009067919105291367\n",
            "step: 240, loss: 0.023455237969756126\n",
            "step: 250, loss: 0.022619834169745445\n",
            "step: 260, loss: 0.058058496564626694\n",
            "step: 270, loss: 0.016743319109082222\n",
            "step: 280, loss: 0.0655268132686615\n",
            "step: 290, loss: 0.01788077875971794\n",
            "step: 300, loss: 0.025173643603920937\n",
            "step: 310, loss: 0.015940073877573013\n",
            "step: 320, loss: 0.055894412100315094\n",
            "step: 330, loss: 0.07008296996355057\n",
            "step: 340, loss: 0.14381136000156403\n",
            "step: 350, loss: 0.0054294653236866\n",
            "step: 360, loss: 0.11554902791976929\n",
            "step: 370, loss: 0.01243690587580204\n",
            "step: 380, loss: 0.1466854363679886\n",
            "step: 390, loss: 0.002363581210374832\n",
            "step: 400, loss: 0.03257618844509125\n",
            "step: 410, loss: 0.015931615605950356\n",
            "step: 420, loss: 0.0065810359083116055\n",
            "step: 430, loss: 0.0863334983587265\n",
            "step: 440, loss: 0.014184548519551754\n",
            "step: 450, loss: 0.04397467523813248\n",
            "step: 460, loss: 0.023547643795609474\n",
            "step: 470, loss: 0.1465912163257599\n",
            "step: 480, loss: 0.02190978452563286\n",
            "step: 490, loss: 0.04171859100461006\n",
            "step: 500, loss: 0.007143421098589897\n",
            "step: 510, loss: 0.03536178916692734\n",
            "step: 520, loss: 0.4011705219745636\n",
            "step: 530, loss: 0.11365282535552979\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.948339483394834, f1=0.9465930018416207, best_f1=0.9465930018416207\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09861103445291519\n",
            "step: 10, loss: 0.045668892562389374\n",
            "step: 20, loss: 0.003755461424589157\n",
            "step: 30, loss: 0.017586147412657738\n",
            "step: 40, loss: 0.07451201975345612\n",
            "step: 50, loss: 0.13184668123722076\n",
            "step: 60, loss: 0.026927752420306206\n",
            "step: 70, loss: 0.007242362480610609\n",
            "step: 80, loss: 0.0022750271018594503\n",
            "step: 90, loss: 0.0021483516320586205\n",
            "step: 100, loss: 0.14340654015541077\n",
            "step: 110, loss: 0.03573586791753769\n",
            "step: 120, loss: 0.07263118028640747\n",
            "step: 130, loss: 0.03601580858230591\n",
            "step: 140, loss: 0.02191873826086521\n",
            "step: 150, loss: 0.013111701235175133\n",
            "step: 160, loss: 0.023603232577443123\n",
            "step: 170, loss: 0.005901306867599487\n",
            "step: 180, loss: 0.0066317967139184475\n",
            "step: 190, loss: 0.0027615604922175407\n",
            "step: 200, loss: 0.005343063268810511\n",
            "step: 210, loss: 0.012098330073058605\n",
            "step: 220, loss: 0.1714666783809662\n",
            "step: 230, loss: 0.027462000027298927\n",
            "step: 240, loss: 0.07877866923809052\n",
            "step: 250, loss: 0.12283655256032944\n",
            "step: 260, loss: 0.03712173178792\n",
            "step: 270, loss: 0.003841045079752803\n",
            "step: 280, loss: 0.01660451479256153\n",
            "step: 290, loss: 0.003598500043153763\n",
            "step: 300, loss: 0.10588007420301437\n",
            "step: 310, loss: 0.01833556778728962\n",
            "step: 320, loss: 0.056773193180561066\n",
            "step: 330, loss: 0.06820520013570786\n",
            "step: 340, loss: 0.0019566104747354984\n",
            "step: 350, loss: 0.0635165274143219\n",
            "step: 360, loss: 0.005522614344954491\n",
            "step: 370, loss: 0.009383146651089191\n",
            "step: 380, loss: 0.026748230680823326\n",
            "step: 390, loss: 0.004793671425431967\n",
            "step: 400, loss: 0.053563814610242844\n",
            "step: 410, loss: 0.026450442150235176\n",
            "step: 420, loss: 0.00498968455940485\n",
            "step: 430, loss: 0.02735324203968048\n",
            "step: 440, loss: 0.14162589609622955\n",
            "step: 450, loss: 0.05119911953806877\n",
            "step: 460, loss: 0.06776376068592072\n",
            "step: 470, loss: 0.008375919423997402\n",
            "step: 480, loss: 0.046370942145586014\n",
            "step: 490, loss: 0.016290392726659775\n",
            "step: 500, loss: 0.015726299956440926\n",
            "step: 510, loss: 0.03823477402329445\n",
            "step: 520, loss: 0.024492798373103142\n",
            "step: 530, loss: 0.021350255236029625\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.936636493568366, f1=0.924618320610687, best_f1=0.9465930018416207\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013868561945855618\n",
            "step: 10, loss: 0.006749461404979229\n",
            "step: 20, loss: 0.023767784237861633\n",
            "step: 30, loss: 0.006939297541975975\n",
            "step: 40, loss: 0.01313658058643341\n",
            "step: 50, loss: 0.06783848255872726\n",
            "step: 60, loss: 0.0048400601372122765\n",
            "step: 70, loss: 0.004274267703294754\n",
            "step: 80, loss: 0.0010107702109962702\n",
            "step: 90, loss: 0.010934142395853996\n",
            "step: 100, loss: 0.0007852931739762425\n",
            "step: 110, loss: 0.0020359796471893787\n",
            "step: 120, loss: 0.0014818159397691488\n",
            "step: 130, loss: 0.013647424057126045\n",
            "step: 140, loss: 0.01086380984634161\n",
            "step: 150, loss: 0.011842883192002773\n",
            "step: 160, loss: 0.0020314918365329504\n",
            "step: 170, loss: 0.02550419047474861\n",
            "step: 180, loss: 0.06183188036084175\n",
            "step: 190, loss: 0.0752398744225502\n",
            "step: 200, loss: 0.036042820662260056\n",
            "step: 210, loss: 0.0006472073728218675\n",
            "step: 220, loss: 0.016508957371115685\n",
            "step: 230, loss: 0.004695782903581858\n",
            "step: 240, loss: 0.0044116792269051075\n",
            "step: 250, loss: 0.08387573063373566\n",
            "step: 260, loss: 0.0017916675424203277\n",
            "step: 270, loss: 0.012840809300541878\n",
            "step: 280, loss: 0.0015744552947580814\n",
            "step: 290, loss: 0.01141241192817688\n",
            "step: 300, loss: 0.001432837569154799\n",
            "step: 310, loss: 0.04906873777508736\n",
            "step: 320, loss: 0.04061371460556984\n",
            "step: 330, loss: 0.02430754527449608\n",
            "step: 340, loss: 0.002304907189682126\n",
            "step: 350, loss: 0.08500571548938751\n",
            "step: 360, loss: 0.009120822884142399\n",
            "step: 370, loss: 0.006842937786132097\n",
            "step: 380, loss: 0.0014214471448212862\n",
            "step: 390, loss: 0.002748647006228566\n",
            "step: 400, loss: 0.010783931240439415\n",
            "step: 410, loss: 0.004605782683938742\n",
            "step: 420, loss: 0.02916596829891205\n",
            "step: 430, loss: 0.03008342906832695\n",
            "step: 440, loss: 0.026318863034248352\n",
            "step: 450, loss: 0.012912783771753311\n",
            "step: 460, loss: 0.020026493817567825\n",
            "step: 470, loss: 0.001209118403494358\n",
            "step: 480, loss: 0.009413805790245533\n",
            "step: 490, loss: 0.001186465029604733\n",
            "step: 500, loss: 0.04135559871792793\n",
            "step: 510, loss: 0.12716490030288696\n",
            "step: 520, loss: 0.12741152942180634\n",
            "step: 530, loss: 0.11983725428581238\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9495327102803738, f1=0.9496738117427772, best_f1=0.9496738117427772\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004828718490898609\n",
            "step: 10, loss: 0.010370386764407158\n",
            "step: 20, loss: 0.0016652728663757443\n",
            "step: 30, loss: 0.0037784716114401817\n",
            "step: 40, loss: 0.0004705647297669202\n",
            "step: 50, loss: 0.028088200837373734\n",
            "step: 60, loss: 0.03197770565748215\n",
            "step: 70, loss: 0.07989158481359482\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 80, loss: 0.0002706082595977932\n",
            "step: 90, loss: 0.008069940842688084\n",
            "step: 100, loss: 0.1347769796848297\n",
            "step: 110, loss: 0.0017226701602339745\n",
            "step: 120, loss: 0.06257607787847519\n",
            "step: 130, loss: 0.009422523900866508\n",
            "step: 140, loss: 0.007926706224679947\n",
            "step: 150, loss: 0.0035203187726438046\n",
            "step: 160, loss: 0.017584064975380898\n",
            "step: 170, loss: 0.029857521876692772\n",
            "step: 180, loss: 0.004885590635240078\n",
            "step: 190, loss: 0.002478365320712328\n",
            "step: 200, loss: 0.0035888468846678734\n",
            "step: 210, loss: 0.0026602260768413544\n",
            "step: 220, loss: 0.009524616412818432\n",
            "step: 230, loss: 0.004727265797555447\n",
            "step: 240, loss: 0.035636939108371735\n",
            "step: 250, loss: 0.06381992995738983\n",
            "step: 260, loss: 0.0008637248538434505\n",
            "step: 270, loss: 0.014435887336730957\n",
            "step: 280, loss: 0.002846041927114129\n",
            "step: 290, loss: 0.03222327679395676\n",
            "step: 300, loss: 0.003798709250986576\n",
            "step: 310, loss: 0.02155149169266224\n",
            "step: 320, loss: 0.0022985690739005804\n",
            "step: 330, loss: 0.005142589099705219\n",
            "step: 340, loss: 0.00022823632752988487\n",
            "step: 350, loss: 0.00017041164392139763\n",
            "step: 360, loss: 0.0009351901244372129\n",
            "step: 370, loss: 0.0006851719808764756\n",
            "step: 380, loss: 0.000494864652864635\n",
            "step: 390, loss: 0.16114583611488342\n",
            "step: 400, loss: 0.00240715267136693\n",
            "step: 410, loss: 0.08420846611261368\n",
            "step: 420, loss: 0.16627348959445953\n",
            "step: 430, loss: 0.0649028941988945\n",
            "step: 440, loss: 0.004660909995436668\n",
            "step: 450, loss: 0.005890071392059326\n",
            "step: 460, loss: 0.0010647211456671357\n",
            "step: 470, loss: 0.0047214641235768795\n",
            "step: 480, loss: 0.015430866740643978\n",
            "step: 490, loss: 0.00807873997837305\n",
            "step: 500, loss: 0.001701834611594677\n",
            "step: 510, loss: 0.0019665330182760954\n",
            "step: 520, loss: 0.035503000020980835\n",
            "step: 530, loss: 0.0022471253760159016\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.947022972339428, f1=0.9471221338324755, best_f1=0.9496738117427772\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.042691558599472046\n",
            "step: 10, loss: 0.00021706051484216005\n",
            "step: 20, loss: 0.0001952366583282128\n",
            "step: 30, loss: 0.002838411368429661\n",
            "step: 40, loss: 0.027339160442352295\n",
            "step: 50, loss: 0.00023792219872120768\n",
            "step: 60, loss: 0.00046535764704458416\n",
            "step: 70, loss: 0.00012037142005283386\n",
            "step: 80, loss: 0.00018496070697437972\n",
            "step: 90, loss: 0.006512483116239309\n",
            "step: 100, loss: 0.07109612226486206\n",
            "step: 110, loss: 0.009273515082895756\n",
            "step: 120, loss: 0.0966082215309143\n",
            "step: 130, loss: 0.01470737624913454\n",
            "step: 140, loss: 0.0015284842811524868\n",
            "step: 150, loss: 0.0025299149565398693\n",
            "step: 160, loss: 0.009758105501532555\n",
            "step: 170, loss: 0.00034765651798807085\n",
            "step: 180, loss: 0.0018886367324739695\n",
            "step: 190, loss: 0.10327029973268509\n",
            "step: 200, loss: 0.0020483750849962234\n",
            "step: 210, loss: 0.01196528784930706\n",
            "step: 220, loss: 0.0008376715704798698\n",
            "step: 230, loss: 0.007083740551024675\n",
            "step: 240, loss: 0.0037496881559491158\n",
            "step: 250, loss: 0.055335719138383865\n",
            "step: 260, loss: 0.00043686703429557383\n",
            "step: 270, loss: 0.0017618633573874831\n",
            "step: 280, loss: 0.008035135455429554\n",
            "step: 290, loss: 7.739800639683381e-05\n",
            "step: 300, loss: 0.0018306886777281761\n",
            "step: 310, loss: 0.011305023916065693\n",
            "step: 320, loss: 9.427618351764977e-05\n",
            "step: 330, loss: 0.0006701161619275808\n",
            "step: 340, loss: 0.00045807810965925455\n",
            "step: 350, loss: 0.002167181111872196\n",
            "step: 360, loss: 0.5373721718788147\n",
            "step: 370, loss: 0.05726729333400726\n",
            "step: 380, loss: 0.003843617858365178\n",
            "step: 390, loss: 0.011269516311585903\n",
            "step: 400, loss: 0.001624679658561945\n",
            "step: 410, loss: 0.0014548507751896977\n",
            "step: 420, loss: 0.0027017726097255945\n",
            "step: 430, loss: 0.00154875498265028\n",
            "step: 440, loss: 0.000678412732668221\n",
            "step: 450, loss: 0.2177598476409912\n",
            "step: 460, loss: 0.001033721142448485\n",
            "step: 470, loss: 0.002017016988247633\n",
            "step: 480, loss: 0.0035404085647314787\n",
            "step: 490, loss: 0.0027604857459664345\n",
            "step: 500, loss: 0.07179655134677887\n",
            "step: 510, loss: 0.13966286182403564\n",
            "step: 520, loss: 0.002893150318413973\n",
            "step: 530, loss: 0.0018584574572741985\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9499072356215214, f1=0.9480519480519481, best_f1=0.9480519480519481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008115533739328384\n",
            "step: 10, loss: 0.0012356216320767999\n",
            "step: 20, loss: 0.0018830305198207498\n",
            "step: 30, loss: 0.0013937752228230238\n",
            "step: 40, loss: 0.000522955204360187\n",
            "step: 50, loss: 0.0036985771730542183\n",
            "step: 60, loss: 0.02837488241493702\n",
            "step: 70, loss: 0.0010746610350906849\n",
            "step: 80, loss: 0.00035958102671429515\n",
            "step: 90, loss: 0.0001564528065500781\n",
            "step: 100, loss: 0.0032387583050876856\n",
            "step: 110, loss: 0.0008433608454652131\n",
            "step: 120, loss: 0.00037574514863081276\n",
            "step: 130, loss: 0.00015679514035582542\n",
            "step: 140, loss: 0.0031298529356718063\n",
            "step: 150, loss: 0.008625196292996407\n",
            "step: 160, loss: 0.0005136434338055551\n",
            "step: 170, loss: 0.03756881132721901\n",
            "step: 180, loss: 0.0024655216839164495\n",
            "step: 190, loss: 0.010038180276751518\n",
            "step: 200, loss: 0.0001875757152447477\n",
            "step: 210, loss: 0.0004905639798380435\n",
            "step: 220, loss: 0.0011440524831414223\n",
            "step: 230, loss: 0.0007233688957057893\n",
            "step: 240, loss: 0.011309699155390263\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 250, loss: 0.0016697775572538376\n",
            "step: 260, loss: 0.0013453905703499913\n",
            "step: 270, loss: 0.00017885844863485545\n",
            "step: 280, loss: 0.00416909996420145\n",
            "step: 290, loss: 0.0024572948459535837\n",
            "step: 300, loss: 0.00033752553281374276\n",
            "step: 310, loss: 0.0006765780854038894\n",
            "step: 320, loss: 0.0011396161280572414\n",
            "step: 330, loss: 0.0003331278567202389\n",
            "step: 340, loss: 0.00037073384737595916\n",
            "step: 350, loss: 0.0022324789315462112\n",
            "step: 360, loss: 0.0037210409063845873\n",
            "step: 370, loss: 0.008884641341865063\n",
            "step: 380, loss: 0.011585624888539314\n",
            "step: 390, loss: 0.00374084385111928\n",
            "step: 400, loss: 0.009017900563776493\n",
            "step: 410, loss: 0.0018541074823588133\n",
            "step: 420, loss: 0.04876028373837471\n",
            "step: 430, loss: 0.0011817277409136295\n",
            "step: 440, loss: 0.021017378196120262\n",
            "step: 450, loss: 0.00045298662735149264\n",
            "step: 460, loss: 0.00019035635341424495\n",
            "step: 470, loss: 0.11863642185926437\n",
            "step: 480, loss: 0.007507176138460636\n",
            "step: 490, loss: 0.004768256098031998\n",
            "step: 500, loss: 0.0009924032492563128\n",
            "step: 510, loss: 0.001269471482373774\n",
            "step: 520, loss: 0.004771859385073185\n",
            "step: 530, loss: 0.0005451340693980455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9460966542750929, f1=0.9406307977736549, best_f1=0.9480519480519481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017049560556188226\n",
            "step: 10, loss: 0.0019402399193495512\n",
            "step: 20, loss: 0.01670052297413349\n",
            "step: 30, loss: 0.0007115743937902153\n",
            "step: 40, loss: 7.1160146035254e-05\n",
            "step: 50, loss: 0.00039753393502905965\n",
            "step: 60, loss: 0.003645066637545824\n",
            "step: 70, loss: 0.004084846470504999\n",
            "step: 80, loss: 0.008193192072212696\n",
            "step: 90, loss: 0.0019775358960032463\n",
            "step: 100, loss: 0.00016937602777034044\n",
            "step: 110, loss: 9.322859114035964e-05\n",
            "step: 120, loss: 0.003645936492830515\n",
            "step: 130, loss: 0.0005409144796431065\n",
            "step: 140, loss: 9.166409290628508e-05\n",
            "step: 150, loss: 0.0003088700177613646\n",
            "step: 160, loss: 8.749312837608159e-05\n",
            "step: 170, loss: 0.1792164146900177\n",
            "step: 180, loss: 0.0006916804704815149\n",
            "step: 190, loss: 0.003214142983779311\n",
            "step: 200, loss: 0.0036865875590592623\n",
            "step: 210, loss: 0.0754641517996788\n",
            "step: 220, loss: 0.00041799352038651705\n",
            "step: 230, loss: 0.02392565831542015\n",
            "step: 240, loss: 0.013042602688074112\n",
            "step: 250, loss: 0.02852877415716648\n",
            "step: 260, loss: 0.00019911346316803247\n",
            "step: 270, loss: 0.0049168081022799015\n",
            "step: 280, loss: 0.0656610056757927\n",
            "step: 290, loss: 0.0005546347238123417\n",
            "step: 300, loss: 8.142797014443204e-05\n",
            "step: 310, loss: 0.008461005054414272\n",
            "step: 320, loss: 0.0008265807409770787\n",
            "step: 330, loss: 0.0011074991198256612\n",
            "step: 340, loss: 0.023165859282016754\n",
            "step: 350, loss: 5.6719542044447735e-05\n",
            "step: 360, loss: 0.011618077754974365\n",
            "step: 370, loss: 0.0644153580069542\n",
            "step: 380, loss: 0.0001834808790590614\n",
            "step: 390, loss: 0.000881669286172837\n",
            "step: 400, loss: 0.002305038506165147\n",
            "step: 410, loss: 0.042109180241823196\n",
            "step: 420, loss: 0.0017960109980776906\n",
            "step: 430, loss: 0.0009667086997069418\n",
            "step: 440, loss: 0.0008792859152890742\n",
            "step: 450, loss: 0.000246581737883389\n",
            "step: 460, loss: 0.0012438218109309673\n",
            "step: 470, loss: 0.034215714782476425\n",
            "step: 480, loss: 0.001903533935546875\n",
            "step: 490, loss: 0.0016529569402337074\n",
            "step: 500, loss: 0.00011618631106102839\n",
            "step: 510, loss: 0.00026236107805743814\n",
            "step: 520, loss: 7.866132364142686e-05\n",
            "step: 530, loss: 2.894861245295033e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9459840300610615, f1=0.9437883797827114, best_f1=0.9480519480519481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.203970017144457e-05\n",
            "step: 10, loss: 0.0011167289922013879\n",
            "step: 20, loss: 0.00011762724898289889\n",
            "step: 30, loss: 0.06953306496143341\n",
            "step: 40, loss: 4.253941006027162e-05\n",
            "step: 50, loss: 0.0004283515445422381\n",
            "step: 60, loss: 0.00030567546491511166\n",
            "step: 70, loss: 0.00805695727467537\n",
            "step: 80, loss: 0.0015020680148154497\n",
            "step: 90, loss: 0.07082222402095795\n",
            "step: 100, loss: 8.075460937106982e-05\n",
            "step: 110, loss: 0.0022442550398409367\n",
            "step: 120, loss: 6.10781426075846e-05\n",
            "step: 130, loss: 9.047582716448233e-05\n",
            "step: 140, loss: 6.916038546478376e-05\n",
            "step: 150, loss: 0.03527715429663658\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 160, loss: 0.03421415761113167\n",
            "step: 170, loss: 0.014774585142731667\n",
            "step: 180, loss: 0.04330837354063988\n",
            "step: 190, loss: 0.0072081321850419044\n",
            "step: 200, loss: 0.003312890650704503\n",
            "step: 210, loss: 0.0038483846001327038\n",
            "step: 220, loss: 0.00044459704076871276\n",
            "step: 230, loss: 0.0001911245344672352\n",
            "step: 240, loss: 0.0033181544858962297\n",
            "step: 250, loss: 0.004341404419392347\n",
            "step: 260, loss: 4.316939157433808e-05\n",
            "step: 270, loss: 0.0005373925087042153\n",
            "step: 280, loss: 0.007333206478506327\n",
            "step: 290, loss: 3.149231997667812e-05\n",
            "step: 300, loss: 0.000931727874558419\n",
            "step: 310, loss: 0.002766784280538559\n",
            "step: 320, loss: 0.000328754133079201\n",
            "step: 330, loss: 0.00011229618394281715\n",
            "step: 340, loss: 8.180601435014978e-05\n",
            "step: 350, loss: 0.0014451071619987488\n",
            "step: 360, loss: 0.07142665982246399\n",
            "step: 370, loss: 5.7183613535016775e-05\n",
            "step: 380, loss: 0.17247669398784637\n",
            "step: 390, loss: 1.5090850865817629e-05\n",
            "step: 400, loss: 0.3672911822795868\n",
            "step: 410, loss: 0.036210644990205765\n",
            "step: 420, loss: 0.0009024759056046605\n",
            "step: 430, loss: 0.003074999898672104\n",
            "step: 440, loss: 8.341103239217773e-05\n",
            "step: 450, loss: 0.019735299050807953\n",
            "step: 460, loss: 0.0004289963690098375\n",
            "step: 470, loss: 0.0006663970416411757\n",
            "step: 480, loss: 0.0002876341750379652\n",
            "step: 490, loss: 0.0004618423117790371\n",
            "step: 500, loss: 0.004309126641601324\n",
            "step: 510, loss: 0.00042629503877833486\n",
            "step: 520, loss: 0.0013471016427502036\n",
            "step: 530, loss: 0.01011730171740055\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9494199535962878, f1=0.9478060046189377, best_f1=0.9480519480519481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0020805061794817448\n",
            "step: 10, loss: 0.0025735311210155487\n",
            "step: 20, loss: 0.0001216834643855691\n",
            "step: 30, loss: 0.0011287289671599865\n",
            "step: 40, loss: 6.453658716054633e-05\n",
            "step: 50, loss: 0.00025596169871278107\n",
            "step: 60, loss: 0.0001468222471885383\n",
            "step: 70, loss: 8.762624929659069e-05\n",
            "step: 80, loss: 7.633084169356152e-05\n",
            "step: 90, loss: 9.245140245184302e-05\n",
            "step: 100, loss: 0.0019040191546082497\n",
            "step: 110, loss: 0.01927047222852707\n",
            "step: 120, loss: 5.860928649781272e-05\n",
            "step: 130, loss: 0.00013597190263681114\n",
            "step: 140, loss: 0.00044964029802940786\n",
            "step: 150, loss: 0.0009914110414683819\n",
            "step: 160, loss: 0.05610959976911545\n",
            "step: 170, loss: 0.00020316033624112606\n",
            "step: 180, loss: 0.0006139242323115468\n",
            "step: 190, loss: 0.000896351644769311\n",
            "step: 200, loss: 0.004673956427723169\n",
            "step: 210, loss: 0.009700175374746323\n",
            "step: 220, loss: 0.002895104233175516\n",
            "step: 230, loss: 0.00011702445044647902\n",
            "step: 240, loss: 0.00010437016317155212\n",
            "step: 250, loss: 0.000989567837677896\n",
            "step: 260, loss: 0.0016534858150407672\n",
            "step: 270, loss: 0.00015256059123203158\n",
            "step: 280, loss: 0.02145696058869362\n",
            "step: 290, loss: 5.1375507609918714e-05\n",
            "step: 300, loss: 5.3763902542414144e-05\n",
            "step: 310, loss: 0.015637055039405823\n",
            "step: 320, loss: 0.0020660965237766504\n",
            "step: 330, loss: 0.0018502267776057124\n",
            "step: 340, loss: 4.772676766151562e-05\n",
            "step: 350, loss: 0.0004614792997017503\n",
            "step: 360, loss: 3.0982442694948986e-05\n",
            "step: 370, loss: 4.0517159504815936e-05\n",
            "step: 380, loss: 0.0007957048364914954\n",
            "step: 390, loss: 8.14536542748101e-05\n",
            "step: 400, loss: 0.00021855374507140368\n",
            "step: 410, loss: 0.002016671933233738\n",
            "step: 420, loss: 7.369017839664593e-05\n",
            "step: 430, loss: 0.005399645771831274\n",
            "step: 440, loss: 8.229049853980541e-05\n",
            "step: 450, loss: 0.0001965826959349215\n",
            "step: 460, loss: 0.0002570189826656133\n",
            "step: 470, loss: 0.003639376489445567\n",
            "step: 480, loss: 7.243195432238281e-05\n",
            "step: 490, loss: 0.0001455118035664782\n",
            "step: 500, loss: 7.640875992365181e-05\n",
            "step: 510, loss: 2.9116097721271217e-05\n",
            "step: 520, loss: 0.0012259387876838446\n",
            "step: 530, loss: 0.0011454758932814002\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9491525423728813, f1=0.9464285714285713, best_f1=0.9480519480519481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.871389369829558e-05\n",
            "step: 10, loss: 0.012474087066948414\n",
            "step: 20, loss: 2.5603323592804372e-05\n",
            "step: 30, loss: 0.000256739353062585\n",
            "step: 40, loss: 0.00012815964873880148\n",
            "step: 50, loss: 8.380485087400302e-05\n",
            "step: 60, loss: 4.299860665923916e-05\n",
            "step: 70, loss: 3.135479710181244e-05\n",
            "step: 80, loss: 0.015768280252814293\n",
            "step: 90, loss: 0.005002176854759455\n",
            "step: 100, loss: 0.0005108140758238733\n",
            "step: 110, loss: 8.942740532802418e-05\n",
            "step: 120, loss: 0.0010891688289120793\n",
            "step: 130, loss: 0.08856499195098877\n",
            "step: 140, loss: 0.00010632543126121163\n",
            "step: 150, loss: 0.00015606968372594565\n",
            "step: 160, loss: 0.0031008555088192225\n",
            "step: 170, loss: 0.00018309388542547822\n",
            "step: 180, loss: 2.4437407773802988e-05\n",
            "step: 190, loss: 0.0004334982659202069\n",
            "step: 200, loss: 9.320990648120642e-05\n",
            "step: 210, loss: 0.00013577446225099266\n",
            "step: 220, loss: 0.0015400918200612068\n",
            "step: 230, loss: 4.454745430848561e-05\n",
            "step: 240, loss: 0.0008536375244148076\n",
            "step: 250, loss: 4.7773810365470126e-05\n",
            "step: 260, loss: 0.05147653818130493\n",
            "step: 270, loss: 0.0025477283634245396\n",
            "step: 280, loss: 0.00011880040256073698\n",
            "step: 290, loss: 0.0002920240513049066\n",
            "step: 300, loss: 4.054770033690147e-05\n",
            "step: 310, loss: 9.114912245422602e-05\n",
            "step: 320, loss: 0.0014982311986386776\n",
            "step: 330, loss: 2.2660142349195667e-05\n",
            "step: 340, loss: 0.00022493538563139737\n",
            "step: 350, loss: 0.00040841830195859075\n",
            "step: 360, loss: 0.0003702152753248811\n",
            "step: 370, loss: 0.004468976520001888\n",
            "step: 380, loss: 0.0012400811538100243\n",
            "step: 390, loss: 0.002504673320800066\n",
            "step: 400, loss: 4.7610643377993256e-05\n",
            "step: 410, loss: 0.001843355712480843\n",
            "step: 420, loss: 0.00010015438601840287\n",
            "step: 430, loss: 0.005465260706841946\n",
            "step: 440, loss: 6.827187462477013e-05\n",
            "step: 450, loss: 0.0003563502978067845\n",
            "step: 460, loss: 0.00010335518163628876\n",
            "step: 470, loss: 0.0007149773300625384\n",
            "step: 480, loss: 7.969318539835513e-05\n",
            "step: 490, loss: 4.9009966460289434e-05\n",
            "step: 500, loss: 0.001661685761064291\n",
            "step: 510, loss: 5.5004456953611225e-05\n",
            "step: 520, loss: 2.694442810025066e-05\n",
            "step: 530, loss: 0.008940287865698338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9494855004677268, f1=0.9424426766495088, best_f1=0.9480519480519481\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002505919197574258\n",
            "step: 10, loss: 0.030191997066140175\n",
            "step: 20, loss: 0.0003431527875363827\n",
            "step: 30, loss: 5.55777114641387e-05\n",
            "step: 40, loss: 4.478835398913361e-05\n",
            "step: 50, loss: 2.1241286958684213e-05\n",
            "step: 60, loss: 0.012205475009977818\n",
            "step: 70, loss: 2.9179071134421974e-05\n",
            "step: 80, loss: 8.680135215399787e-05\n",
            "step: 90, loss: 2.7488098567118868e-05\n",
            "step: 100, loss: 0.0938107818365097\n",
            "step: 110, loss: 2.063025203824509e-05\n",
            "step: 120, loss: 0.0001239758712472394\n",
            "step: 130, loss: 0.00013503266382031143\n",
            "step: 140, loss: 0.00013255955127533525\n",
            "step: 150, loss: 2.217979090346489e-05\n",
            "step: 160, loss: 5.538838013308123e-05\n",
            "step: 170, loss: 4.173240449745208e-05\n",
            "step: 180, loss: 2.272010715387296e-05\n",
            "step: 190, loss: 0.00022781257575843483\n",
            "step: 200, loss: 0.0020701370667666197\n",
            "step: 210, loss: 0.0003390217025298625\n",
            "step: 220, loss: 2.3986538508324884e-05\n",
            "step: 230, loss: 5.4424828704213724e-05\n",
            "step: 240, loss: 0.0006975647993385792\n",
            "step: 250, loss: 0.004260603338479996\n",
            "step: 260, loss: 3.0184084607753903e-05\n",
            "step: 270, loss: 0.0026077984366565943\n",
            "step: 280, loss: 9.04009721125476e-05\n",
            "step: 290, loss: 1.928548408614006e-05\n",
            "step: 300, loss: 7.392124098259956e-05\n",
            "step: 310, loss: 3.310101237730123e-05\n",
            "step: 320, loss: 0.005131850019097328\n",
            "step: 330, loss: 0.0029288972727954388\n",
            "step: 340, loss: 0.00015797055675648153\n",
            "step: 350, loss: 8.352693839697167e-05\n",
            "step: 360, loss: 0.0009891230147331953\n",
            "step: 370, loss: 2.673312155820895e-05\n",
            "step: 380, loss: 0.00012619694462046027\n",
            "step: 390, loss: 0.024217167869210243\n",
            "step: 400, loss: 2.69135653070407e-05\n",
            "step: 410, loss: 0.00016848323866724968\n",
            "step: 420, loss: 0.0003728090086951852\n",
            "step: 430, loss: 5.959202826488763e-05\n",
            "step: 440, loss: 0.0011040396057069302\n",
            "step: 450, loss: 0.08750869333744049\n",
            "step: 460, loss: 8.764781523495913e-05\n",
            "step: 470, loss: 0.03974539786577225\n",
            "step: 480, loss: 0.011272784322500229\n",
            "step: 490, loss: 9.27430737647228e-05\n",
            "step: 500, loss: 0.00021320246742106974\n",
            "step: 510, loss: 0.0025757434777915478\n",
            "step: 520, loss: 8.204580808524042e-05\n",
            "step: 530, loss: 4.074530806974508e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9507735583684951, f1=0.941509433962264, best_f1=0.941509433962264\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00045176551793701947\n",
            "step: 10, loss: 2.4984936317196116e-05\n",
            "step: 20, loss: 0.0009765371796675026\n",
            "step: 30, loss: 2.8481839763117023e-05\n",
            "step: 40, loss: 2.2235761207411997e-05\n",
            "step: 50, loss: 0.0014021291863173246\n",
            "step: 60, loss: 0.00015835955855436623\n",
            "step: 70, loss: 5.329161649569869e-05\n",
            "step: 80, loss: 0.0006053528049960732\n",
            "step: 90, loss: 2.2261679987423122e-05\n",
            "step: 100, loss: 2.0216537450323813e-05\n",
            "step: 110, loss: 1.8253493180964142e-05\n",
            "step: 120, loss: 2.1311827367753722e-05\n",
            "step: 130, loss: 1.2412569958542008e-05\n",
            "step: 140, loss: 2.2138934582471848e-05\n",
            "step: 150, loss: 2.413558650005143e-05\n",
            "step: 160, loss: 7.626148726558313e-05\n",
            "step: 170, loss: 0.00022264647122938186\n",
            "step: 180, loss: 2.7930831492994912e-05\n",
            "step: 190, loss: 8.440313104074448e-05\n",
            "step: 200, loss: 1.74191445694305e-05\n",
            "step: 210, loss: 0.006448476575314999\n",
            "step: 220, loss: 1.7668557120487094e-05\n",
            "step: 230, loss: 0.00039178592851385474\n",
            "step: 240, loss: 0.00021143778576515615\n",
            "step: 250, loss: 3.10637078655418e-05\n",
            "step: 260, loss: 9.450033394386992e-05\n",
            "step: 270, loss: 0.00010759750148281455\n",
            "step: 280, loss: 1.556408460601233e-05\n",
            "step: 290, loss: 1.4107513379713055e-05\n",
            "step: 300, loss: 1.5266061382135376e-05\n",
            "step: 310, loss: 4.104997424292378e-05\n",
            "step: 320, loss: 2.2696132873534225e-05\n",
            "step: 330, loss: 3.816586104221642e-05\n",
            "step: 340, loss: 0.00021775650384370238\n",
            "step: 350, loss: 1.1466342584753875e-05\n",
            "step: 360, loss: 0.05713341385126114\n",
            "step: 370, loss: 0.006788663566112518\n",
            "step: 380, loss: 1.6927395336097106e-05\n",
            "step: 390, loss: 0.031123202294111252\n",
            "step: 400, loss: 1.9549965145415626e-05\n",
            "step: 410, loss: 3.589072366594337e-05\n",
            "step: 420, loss: 1.4275125977292191e-05\n",
            "step: 430, loss: 2.1181213014642708e-05\n",
            "step: 440, loss: 1.1242826076340862e-05\n",
            "step: 450, loss: 1.5556555808871053e-05\n",
            "step: 460, loss: 0.00033054148661904037\n",
            "step: 470, loss: 0.00022345002798829228\n",
            "step: 480, loss: 9.83840163826244e-06\n",
            "step: 490, loss: 1.0911277968261857e-05\n",
            "step: 500, loss: 1.0400923201814294e-05\n",
            "step: 510, loss: 1.6115052858367562e-05\n",
            "step: 520, loss: 1.1958065442740917e-05\n",
            "step: 530, loss: 1.263605008716695e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.951366373320982, f1=0.9437993497445425, best_f1=0.9437993497445425\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.164145790738985e-05\n",
            "step: 10, loss: 0.0013020102633163333\n",
            "step: 20, loss: 0.0004918506601825356\n",
            "step: 30, loss: 2.2331874788505957e-05\n",
            "step: 40, loss: 3.431926961638965e-05\n",
            "step: 50, loss: 2.37662679865025e-05\n",
            "step: 60, loss: 1.295269794354681e-05\n",
            "step: 70, loss: 0.022755874320864677\n",
            "step: 80, loss: 9.705981210572645e-05\n",
            "step: 90, loss: 1.559725023980718e-05\n",
            "step: 100, loss: 1.6577154383412562e-05\n",
            "step: 110, loss: 0.000986472237855196\n",
            "step: 120, loss: 1.2799966498278081e-05\n",
            "step: 130, loss: 3.320016912766732e-05\n",
            "step: 140, loss: 2.6105526558239944e-05\n",
            "step: 150, loss: 1.7407926861778833e-05\n",
            "step: 160, loss: 1.7102462152251974e-05\n",
            "step: 170, loss: 3.1880226742941886e-05\n",
            "step: 180, loss: 9.996697917813435e-05\n",
            "step: 190, loss: 1.7210400983458385e-05\n",
            "step: 200, loss: 2.2410078599932604e-05\n",
            "step: 210, loss: 0.0009061736054718494\n",
            "step: 220, loss: 2.0267687432351522e-05\n",
            "step: 230, loss: 1.2524311387096532e-05\n",
            "step: 240, loss: 3.36046505253762e-05\n",
            "step: 250, loss: 0.0003122595662716776\n",
            "step: 260, loss: 4.448561230674386e-05\n",
            "step: 270, loss: 7.572163303848356e-05\n",
            "step: 280, loss: 4.797249857801944e-05\n",
            "step: 290, loss: 1.135830461862497e-05\n",
            "step: 300, loss: 1.3887670320400503e-05\n",
            "step: 310, loss: 0.0005803940584883094\n",
            "step: 320, loss: 1.2766451618517749e-05\n",
            "step: 330, loss: 0.004119178280234337\n",
            "step: 340, loss: 5.5622687796130776e-05\n",
            "step: 350, loss: 0.00013929394481237978\n",
            "step: 360, loss: 1.4111154996498954e-05\n",
            "step: 370, loss: 0.009323655627667904\n",
            "step: 380, loss: 0.0002437518269289285\n",
            "step: 390, loss: 1.7738786482368596e-05\n",
            "step: 400, loss: 0.001496975659392774\n",
            "step: 410, loss: 1.5273297321982682e-05\n",
            "step: 420, loss: 1.2379020517983008e-05\n",
            "step: 430, loss: 0.00017174193635582924\n",
            "step: 440, loss: 1.8350368918618187e-05\n",
            "step: 450, loss: 1.3958480849396437e-05\n",
            "step: 460, loss: 7.530658331234008e-05\n",
            "step: 470, loss: 1.3109153769619297e-05\n",
            "step: 480, loss: 2.9775808798149228e-05\n",
            "step: 490, loss: 1.747114583849907e-05\n",
            "step: 500, loss: 1.5739115042379126e-05\n",
            "step: 510, loss: 2.2033022105460986e-05\n",
            "step: 520, loss: 1.156314647232648e-05\n",
            "step: 530, loss: 1.3287954971019644e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.952513966480447, f1=0.9417792268281323, best_f1=0.9417792268281323\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003997367457486689\n",
            "step: 10, loss: 2.749753366515506e-05\n",
            "step: 20, loss: 0.0008294796571135521\n",
            "step: 30, loss: 9.049909567693248e-05\n",
            "step: 40, loss: 1.3347565982257947e-05\n",
            "step: 50, loss: 0.0004737253475468606\n",
            "step: 60, loss: 1.2598813555086963e-05\n",
            "step: 70, loss: 2.67893192358315e-05\n",
            "step: 80, loss: 1.0970893526973668e-05\n",
            "step: 90, loss: 0.0008743273792788386\n",
            "step: 100, loss: 8.981619430414867e-06\n",
            "step: 110, loss: 0.001307241735048592\n",
            "step: 120, loss: 2.7263780793873593e-05\n",
            "step: 130, loss: 1.763856562320143e-05\n",
            "step: 140, loss: 9.044945727509912e-06\n",
            "step: 150, loss: 1.907618388941046e-05\n",
            "step: 160, loss: 2.9229770007077605e-05\n",
            "step: 170, loss: 1.474809232604457e-05\n",
            "step: 180, loss: 1.1633988833636977e-05\n",
            "step: 190, loss: 0.0027404637075960636\n",
            "step: 200, loss: 0.00018320477101951838\n",
            "step: 210, loss: 0.00014174042735248804\n",
            "step: 220, loss: 1.2192707799840719e-05\n",
            "step: 230, loss: 0.0004523284442257136\n",
            "step: 240, loss: 1.5638359400327317e-05\n",
            "step: 250, loss: 9.251337178284302e-05\n",
            "step: 260, loss: 9.681955816631671e-06\n",
            "step: 270, loss: 1.430479187547462e-05\n",
            "step: 280, loss: 1.0467980246176012e-05\n",
            "step: 290, loss: 2.1556688807322644e-05\n",
            "step: 300, loss: 9.97998722596094e-06\n",
            "step: 310, loss: 0.042312659323215485\n",
            "step: 320, loss: 1.5403722500195727e-05\n",
            "step: 330, loss: 1.366791912005283e-05\n",
            "step: 340, loss: 1.6905043594306335e-05\n",
            "step: 350, loss: 0.007390133570879698\n",
            "step: 360, loss: 1.526228697912302e-05\n",
            "step: 370, loss: 1.3369906810112298e-05\n",
            "step: 380, loss: 1.4211775123840198e-05\n",
            "step: 390, loss: 1.1626529158093035e-05\n",
            "step: 400, loss: 0.0004982162499800324\n",
            "step: 410, loss: 1.443891324015567e-05\n",
            "step: 420, loss: 0.0005434257909655571\n",
            "step: 430, loss: 8.754375812713988e-06\n",
            "step: 440, loss: 1.3656689588970039e-05\n",
            "step: 450, loss: 8.015103958314285e-05\n",
            "step: 460, loss: 0.006436201743781567\n",
            "step: 470, loss: 1.3071832654532045e-05\n",
            "step: 480, loss: 0.0008459454402327538\n",
            "step: 490, loss: 2.0011306332889944e-05\n",
            "step: 500, loss: 1.0453079084982164e-05\n",
            "step: 510, loss: 1.1037916010536719e-05\n",
            "step: 520, loss: 1.337352478003595e-05\n",
            "step: 530, loss: 9.6670428320067e-06\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.9530450953045096, f1=0.9437470943747095, best_f1=0.9437470943747095\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:24, 237.03it/s]\n",
            "load_f1 = 0.9512761020881672\n",
            "real_f1 = 0.9496738117427772\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 183.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "OdkSbOTQeC3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "x4XWqpo1e0O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4074f8-fcfb-41d5-c34d-f97b2af75536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.48668450117111206\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.438909649848938\n",
            "step: 20, loss: 0.43601876497268677\n",
            "step: 30, loss: 0.3605321943759918\n",
            "step: 40, loss: 0.3367757499217987\n",
            "step: 50, loss: 0.4260593354701996\n",
            "step: 60, loss: 0.443242609500885\n",
            "step: 70, loss: 0.30107778310775757\n",
            "step: 80, loss: 0.3863406181335449\n",
            "step: 90, loss: 0.22510124742984772\n",
            "step: 100, loss: 0.26911023259162903\n",
            "step: 110, loss: 0.279682457447052\n",
            "step: 120, loss: 0.32042914628982544\n",
            "step: 130, loss: 0.18761219084262848\n",
            "step: 140, loss: 0.5689704418182373\n",
            "step: 150, loss: 0.29663020372390747\n",
            "step: 160, loss: 0.3015713691711426\n",
            "step: 170, loss: 0.1050080731511116\n",
            "step: 180, loss: 0.28945523500442505\n",
            "step: 190, loss: 0.6445323824882507\n",
            "step: 200, loss: 0.20184697210788727\n",
            "step: 210, loss: 0.2694958746433258\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5464285714285715, f1=0.5803921568627451, best_f1=0.5803921568627451\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.28153783082962036\n",
            "step: 10, loss: 0.07556692510843277\n",
            "step: 20, loss: 0.37949398159980774\n",
            "step: 30, loss: 0.3090801537036896\n",
            "step: 40, loss: 0.32320934534072876\n",
            "step: 50, loss: 0.11541657149791718\n",
            "step: 60, loss: 0.308791846036911\n",
            "step: 70, loss: 0.222229465842247\n",
            "step: 80, loss: 0.24936895072460175\n",
            "step: 90, loss: 0.20131559669971466\n",
            "step: 100, loss: 0.3350042700767517\n",
            "step: 110, loss: 0.45955345034599304\n",
            "step: 120, loss: 0.13602770864963531\n",
            "step: 130, loss: 0.21266694366931915\n",
            "step: 140, loss: 0.12925861775875092\n",
            "step: 150, loss: 0.26120978593826294\n",
            "step: 160, loss: 0.08182371407747269\n",
            "step: 170, loss: 0.26985424757003784\n",
            "step: 180, loss: 0.1796920746564865\n",
            "step: 190, loss: 0.22460409998893738\n",
            "step: 200, loss: 0.14888590574264526\n",
            "step: 210, loss: 0.13927456736564636\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6045627376425856, f1=0.6356877323420074, best_f1=0.6356877323420074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08643538504838943\n",
            "step: 10, loss: 0.15192952752113342\n",
            "step: 20, loss: 0.24178749322891235\n",
            "step: 30, loss: 0.038194622844457626\n",
            "step: 40, loss: 0.31044021248817444\n",
            "step: 50, loss: 0.29293882846832275\n",
            "step: 60, loss: 0.2435871958732605\n",
            "step: 70, loss: 0.15499304234981537\n",
            "step: 80, loss: 0.14503879845142365\n",
            "step: 90, loss: 0.07094749063253403\n",
            "step: 100, loss: 0.17166930437088013\n",
            "step: 110, loss: 0.1008991003036499\n",
            "step: 120, loss: 0.16244202852249146\n",
            "step: 130, loss: 0.1095937192440033\n",
            "step: 140, loss: 0.11374469101428986\n",
            "step: 150, loss: 0.09566116333007812\n",
            "step: 160, loss: 0.20154789090156555\n",
            "step: 170, loss: 0.202677384018898\n",
            "step: 180, loss: 0.1478595733642578\n",
            "step: 190, loss: 0.06534461677074432\n",
            "step: 200, loss: 0.11632432788610458\n",
            "step: 210, loss: 0.12982161343097687\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.634989200863931, f1=0.6214442013129102, best_f1=0.6214442013129102\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08482685685157776\n",
            "step: 10, loss: 0.088365338742733\n",
            "step: 20, loss: 0.0895482525229454\n",
            "step: 30, loss: 0.060315750539302826\n",
            "step: 40, loss: 0.07297733426094055\n",
            "step: 50, loss: 0.16792583465576172\n",
            "step: 60, loss: 0.21748585999011993\n",
            "step: 70, loss: 0.029178114607930183\n",
            "step: 80, loss: 0.06591637432575226\n",
            "step: 90, loss: 0.11871214956045151\n",
            "step: 100, loss: 0.21012647449970245\n",
            "step: 110, loss: 0.4207897484302521\n",
            "step: 120, loss: 0.22940269112586975\n",
            "step: 130, loss: 0.22966234385967255\n",
            "step: 140, loss: 0.21946780383586884\n",
            "step: 150, loss: 0.1207684874534607\n",
            "step: 160, loss: 0.11267613619565964\n",
            "step: 170, loss: 0.07182630151510239\n",
            "step: 180, loss: 0.03344938904047012\n",
            "step: 190, loss: 0.13416296243667603\n",
            "step: 200, loss: 0.0502881221473217\n",
            "step: 210, loss: 0.3598298728466034\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6326129666011788, f1=0.668041237113402, best_f1=0.6214442013129102\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16849906742572784\n",
            "step: 10, loss: 0.03252726420760155\n",
            "step: 20, loss: 0.14807216823101044\n",
            "step: 30, loss: 0.041149746626615524\n",
            "step: 40, loss: 0.09005340933799744\n",
            "step: 50, loss: 0.1194400042295456\n",
            "step: 60, loss: 0.1587425172328949\n",
            "step: 70, loss: 0.2195877730846405\n",
            "step: 80, loss: 0.08325926959514618\n",
            "step: 90, loss: 0.02925419807434082\n",
            "step: 100, loss: 0.011959430761635303\n",
            "step: 110, loss: 0.09865714609622955\n",
            "step: 120, loss: 0.12793028354644775\n",
            "step: 130, loss: 0.10181865841150284\n",
            "step: 140, loss: 0.3998469412326813\n",
            "step: 150, loss: 0.13092568516731262\n",
            "step: 160, loss: 0.04876243695616722\n",
            "step: 170, loss: 0.02149749919772148\n",
            "step: 180, loss: 0.1908382773399353\n",
            "step: 190, loss: 0.12318955361843109\n",
            "step: 200, loss: 0.10514963418245316\n",
            "step: 210, loss: 0.1291574090719223\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6490566037735849, f1=0.6615087040618955, best_f1=0.6615087040618955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018432648852467537\n",
            "step: 10, loss: 0.03269780054688454\n",
            "step: 20, loss: 0.008607362397015095\n",
            "step: 30, loss: 0.05452512577176094\n",
            "step: 40, loss: 0.03389492258429527\n",
            "step: 50, loss: 0.08506370335817337\n",
            "step: 60, loss: 0.20780810713768005\n",
            "step: 70, loss: 0.14202499389648438\n",
            "step: 80, loss: 0.1195085421204567\n",
            "step: 90, loss: 0.17919111251831055\n",
            "step: 100, loss: 0.1342800110578537\n",
            "step: 110, loss: 0.01891876570880413\n",
            "step: 120, loss: 0.008612936362624168\n",
            "step: 130, loss: 0.06720773875713348\n",
            "step: 140, loss: 0.07732772082090378\n",
            "step: 150, loss: 0.11094838380813599\n",
            "step: 160, loss: 0.06992298364639282\n",
            "step: 170, loss: 0.11312435567378998\n",
            "step: 180, loss: 0.04960786551237106\n",
            "step: 190, loss: 0.026407387107610703\n",
            "step: 200, loss: 0.18976077437400818\n",
            "step: 210, loss: 0.02259535901248455\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6338797814207651, f1=0.6641509433962264, best_f1=0.6615087040618955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10747488588094711\n",
            "step: 10, loss: 0.031191643327474594\n",
            "step: 20, loss: 0.007949508726596832\n",
            "step: 30, loss: 0.04132949560880661\n",
            "step: 40, loss: 0.05393214151263237\n",
            "step: 50, loss: 0.11125777661800385\n",
            "step: 60, loss: 0.05308928340673447\n",
            "step: 70, loss: 0.0053015160374343395\n",
            "step: 80, loss: 0.1729053258895874\n",
            "step: 90, loss: 0.08445370942354202\n",
            "step: 100, loss: 0.05927707627415657\n",
            "step: 110, loss: 0.03955446183681488\n",
            "step: 120, loss: 0.03363833948969841\n",
            "step: 130, loss: 0.024731166660785675\n",
            "step: 140, loss: 0.033305395394563675\n",
            "step: 150, loss: 0.04278366640210152\n",
            "step: 160, loss: 0.16981777548789978\n",
            "step: 170, loss: 0.10761465132236481\n",
            "step: 180, loss: 0.034319065511226654\n",
            "step: 190, loss: 0.057131338864564896\n",
            "step: 200, loss: 0.05010353401303291\n",
            "step: 210, loss: 0.18575835227966309\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6396761133603239, f1=0.6694386694386695, best_f1=0.6615087040618955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09107420593500137\n",
            "step: 10, loss: 0.017921052873134613\n",
            "step: 20, loss: 0.013842111453413963\n",
            "step: 30, loss: 0.030774561688303947\n",
            "step: 40, loss: 0.08469988405704498\n",
            "step: 50, loss: 0.022281933575868607\n",
            "step: 60, loss: 0.11088753491640091\n",
            "step: 70, loss: 0.09630400687456131\n",
            "step: 80, loss: 0.022106921300292015\n",
            "step: 90, loss: 0.009542075917124748\n",
            "step: 100, loss: 0.20108966529369354\n",
            "step: 110, loss: 0.0019705144222825766\n",
            "step: 120, loss: 0.05965230613946915\n",
            "step: 130, loss: 0.07845363765954971\n",
            "step: 140, loss: 0.03661620616912842\n",
            "step: 150, loss: 0.10203049331903458\n",
            "step: 160, loss: 0.13471318781375885\n",
            "step: 170, loss: 0.059851426631212234\n",
            "step: 180, loss: 0.04301206395030022\n",
            "step: 190, loss: 0.10827255994081497\n",
            "step: 200, loss: 0.03160892799496651\n",
            "step: 210, loss: 0.08308248966932297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6435845213849287, f1=0.6640159045725645, best_f1=0.6615087040618955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.051390159875154495\n",
            "step: 10, loss: 0.037951987236738205\n",
            "step: 20, loss: 0.17136316001415253\n",
            "step: 30, loss: 0.005868652369827032\n",
            "step: 40, loss: 0.055541932582855225\n",
            "step: 50, loss: 0.13319142162799835\n",
            "step: 60, loss: 0.15324942767620087\n",
            "step: 70, loss: 0.06353515386581421\n",
            "step: 80, loss: 0.11848428845405579\n",
            "step: 90, loss: 0.01803024299442768\n",
            "step: 100, loss: 0.009777948260307312\n",
            "step: 110, loss: 0.013111673295497894\n",
            "step: 120, loss: 0.19448161125183105\n",
            "step: 130, loss: 0.06195026636123657\n",
            "step: 140, loss: 0.2107706069946289\n",
            "step: 150, loss: 0.016002172604203224\n",
            "step: 160, loss: 0.07352626323699951\n",
            "step: 170, loss: 0.0984923467040062\n",
            "step: 180, loss: 0.02922138385474682\n",
            "step: 190, loss: 0.006003404967486858\n",
            "step: 200, loss: 0.00969408918172121\n",
            "step: 210, loss: 0.08667650073766708\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.6440677966101696, f1=0.6752293577981651, best_f1=0.6615087040618955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0378078855574131\n",
            "step: 10, loss: 0.010951886884868145\n",
            "step: 20, loss: 0.004460430704057217\n",
            "step: 30, loss: 0.0024596452713012695\n",
            "step: 40, loss: 0.05461179465055466\n",
            "step: 50, loss: 0.014455419965088367\n",
            "step: 60, loss: 0.015217820182442665\n",
            "step: 70, loss: 0.01641494408249855\n",
            "step: 80, loss: 0.05355599895119667\n",
            "step: 90, loss: 0.04924474284052849\n",
            "step: 100, loss: 0.04096512496471405\n",
            "step: 110, loss: 0.10200949013233185\n",
            "step: 120, loss: 0.04041188582777977\n",
            "step: 130, loss: 0.010258179157972336\n",
            "step: 140, loss: 0.020111963152885437\n",
            "step: 150, loss: 0.1370549201965332\n",
            "step: 160, loss: 0.11004069447517395\n",
            "step: 170, loss: 0.01823848858475685\n",
            "step: 180, loss: 0.07922553271055222\n",
            "step: 190, loss: 0.015268264338374138\n",
            "step: 200, loss: 0.22104492783546448\n",
            "step: 210, loss: 0.020941076800227165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6457564575645757, f1=0.6799276672694394, best_f1=0.6615087040618955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02994038537144661\n",
            "step: 10, loss: 0.013735486194491386\n",
            "step: 20, loss: 0.006496243178844452\n",
            "step: 30, loss: 0.013215932063758373\n",
            "step: 40, loss: 0.0026937234215438366\n",
            "step: 50, loss: 0.014397809281945229\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 60, loss: 0.1477545201778412\n",
            "step: 70, loss: 0.002812086371704936\n",
            "step: 80, loss: 0.05527050420641899\n",
            "step: 90, loss: 0.13411131501197815\n",
            "step: 100, loss: 0.011351427063345909\n",
            "step: 110, loss: 0.001020799740217626\n",
            "step: 120, loss: 0.0061863139271736145\n",
            "step: 130, loss: 0.006646968889981508\n",
            "step: 140, loss: 0.029056988656520844\n",
            "step: 150, loss: 0.004628725349903107\n",
            "step: 160, loss: 0.0027996604330837727\n",
            "step: 170, loss: 0.0685679242014885\n",
            "step: 180, loss: 0.004564494825899601\n",
            "step: 190, loss: 0.08029980212450027\n",
            "step: 200, loss: 0.004940506536513567\n",
            "step: 210, loss: 0.1685795783996582\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6356275303643724, f1=0.688659793814433, best_f1=0.6615087040618955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014503756538033485\n",
            "step: 10, loss: 0.02380989119410515\n",
            "step: 20, loss: 0.08754090219736099\n",
            "step: 30, loss: 0.04886600002646446\n",
            "step: 40, loss: 0.0027332021854817867\n",
            "step: 50, loss: 0.03656753525137901\n",
            "step: 60, loss: 0.009897866286337376\n",
            "step: 70, loss: 0.0665193721652031\n",
            "step: 80, loss: 0.05473249778151512\n",
            "step: 90, loss: 0.03734053671360016\n",
            "step: 100, loss: 0.0007257495308294892\n",
            "step: 110, loss: 0.010914953425526619\n",
            "step: 120, loss: 0.006229388993233442\n",
            "step: 130, loss: 0.0017266958020627499\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 140, loss: 0.01251225359737873\n",
            "step: 150, loss: 0.00119341432582587\n",
            "step: 160, loss: 0.002700187498703599\n",
            "step: 170, loss: 0.0018081102753058076\n",
            "step: 180, loss: 0.010999424383044243\n",
            "step: 190, loss: 0.004964136052876711\n",
            "step: 200, loss: 0.021333681419491768\n",
            "step: 210, loss: 0.008539094589650631\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6339622641509433, f1=0.6971428571428572, best_f1=0.6615087040618955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00907278060913086\n",
            "step: 10, loss: 0.04291369393467903\n",
            "step: 20, loss: 0.04182393476366997\n",
            "step: 30, loss: 0.0019081997452303767\n",
            "step: 40, loss: 0.00244339881464839\n",
            "step: 50, loss: 0.027217473834753036\n",
            "step: 60, loss: 0.010023158974945545\n",
            "step: 70, loss: 0.09204992651939392\n",
            "step: 80, loss: 0.07200545072555542\n",
            "step: 90, loss: 0.044841136783361435\n",
            "step: 100, loss: 0.0679549127817154\n",
            "step: 110, loss: 0.07548706978559494\n",
            "step: 120, loss: 0.06383275240659714\n",
            "step: 130, loss: 0.05059768259525299\n",
            "step: 140, loss: 0.006027671974152327\n",
            "step: 150, loss: 0.0802813321352005\n",
            "step: 160, loss: 0.06380664557218552\n",
            "step: 170, loss: 0.029357196763157845\n",
            "step: 180, loss: 0.007588121108710766\n",
            "step: 190, loss: 0.023213708773255348\n",
            "step: 200, loss: 0.01434690784662962\n",
            "step: 210, loss: 0.07764070481061935\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6310517529215358, f1=0.6757215619694398, best_f1=0.6615087040618955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005404351744800806\n",
            "step: 10, loss: 0.018300889059901237\n",
            "step: 20, loss: 0.03054545447230339\n",
            "step: 30, loss: 0.06947214901447296\n",
            "step: 40, loss: 0.08781830966472626\n",
            "step: 50, loss: 0.009871447458863258\n",
            "step: 60, loss: 0.022771628573536873\n",
            "step: 70, loss: 0.04109417274594307\n",
            "step: 80, loss: 0.06911454349756241\n",
            "step: 90, loss: 0.017350200563669205\n",
            "step: 100, loss: 0.006593918427824974\n",
            "step: 110, loss: 0.10767147690057755\n",
            "step: 120, loss: 0.002383989281952381\n",
            "step: 130, loss: 0.011523446999490261\n",
            "step: 140, loss: 0.004574749153107405\n",
            "step: 150, loss: 0.049129828810691833\n",
            "step: 160, loss: 0.005410439800471067\n",
            "step: 170, loss: 0.09271451830863953\n",
            "step: 180, loss: 0.0025384880136698484\n",
            "step: 190, loss: 0.031122615560889244\n",
            "step: 200, loss: 0.00741127273067832\n",
            "step: 210, loss: 0.0023616233374923468\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6365348399246704, f1=0.7088122605363985, best_f1=0.6615087040618955\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002663656137883663\n",
            "step: 10, loss: 0.010268422774970531\n",
            "step: 20, loss: 0.0018080779118463397\n",
            "step: 30, loss: 0.016377722844481468\n",
            "step: 40, loss: 0.010451441630721092\n",
            "step: 50, loss: 0.009629807434976101\n",
            "step: 60, loss: 0.0402354821562767\n",
            "step: 70, loss: 0.003966909367591143\n",
            "step: 80, loss: 0.03892144933342934\n",
            "step: 90, loss: 0.002478274516761303\n",
            "step: 100, loss: 0.008441167883574963\n",
            "step: 110, loss: 0.06271214783191681\n",
            "step: 120, loss: 0.0023818297777324915\n",
            "step: 130, loss: 0.005913670174777508\n",
            "step: 140, loss: 0.01466827467083931\n",
            "step: 150, loss: 0.0029394489247351885\n",
            "step: 160, loss: 0.10005604475736618\n",
            "step: 170, loss: 0.011428341269493103\n",
            "step: 180, loss: 0.004604777321219444\n",
            "step: 190, loss: 0.04766465723514557\n",
            "step: 200, loss: 0.006014121230691671\n",
            "step: 210, loss: 0.04559463635087013\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.638532110091743, f1=0.6987060998151571, best_f1=0.6615087040618955\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:05, 403.70it/s]\n",
            "load_f1 = 0.642166344294004\n",
            "real_f1 = 0.6404715127701375\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:23, 187.22it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "ewoOK8t9eDFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HJDquM2Oe05D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73bcbbeb-5cf0-4915-d90d-662800c9cad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4721939265727997\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 10, loss: 0.47613510489463806\n",
            "step: 20, loss: 0.25346338748931885\n",
            "step: 30, loss: 0.41786324977874756\n",
            "step: 40, loss: 0.227517232298851\n",
            "step: 50, loss: 0.32105615735054016\n",
            "step: 60, loss: 0.43835651874542236\n",
            "step: 70, loss: 0.4291508197784424\n",
            "step: 80, loss: 0.1545601338148117\n",
            "step: 90, loss: 0.31386473774909973\n",
            "step: 100, loss: 0.4197383522987366\n",
            "step: 110, loss: 0.24510334432125092\n",
            "step: 120, loss: 0.3216686546802521\n",
            "step: 130, loss: 0.31577616930007935\n",
            "step: 140, loss: 0.1891362965106964\n",
            "step: 150, loss: 0.3352096974849701\n",
            "step: 160, loss: 0.2541508078575134\n",
            "step: 170, loss: 0.3696262836456299\n",
            "step: 180, loss: 0.15600700676441193\n",
            "step: 190, loss: 0.15963852405548096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.38224178552627563\n",
            "step: 10, loss: 0.3239496350288391\n",
            "step: 20, loss: 0.6958616375923157\n",
            "step: 30, loss: 0.25872254371643066\n",
            "step: 40, loss: 0.5276517868041992\n",
            "step: 50, loss: 0.3066784739494324\n",
            "step: 60, loss: 0.44309791922569275\n",
            "step: 70, loss: 0.2899143099784851\n",
            "step: 80, loss: 0.16406014561653137\n",
            "step: 90, loss: 0.3014567196369171\n",
            "step: 100, loss: 0.2517034411430359\n",
            "step: 110, loss: 0.2967916429042816\n",
            "step: 120, loss: 0.20935609936714172\n",
            "step: 130, loss: 0.37728720903396606\n",
            "step: 140, loss: 0.27568215131759644\n",
            "step: 150, loss: 0.299304723739624\n",
            "step: 160, loss: 0.33835312724113464\n",
            "step: 170, loss: 0.1673140674829483\n",
            "step: 180, loss: 0.16231922805309296\n",
            "step: 190, loss: 0.2551964521408081\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.5444126074498566, f1=0.5301837270341208, best_f1=0.5301837270341208\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3474060297012329\n",
            "step: 10, loss: 0.29429635405540466\n",
            "step: 20, loss: 0.24400964379310608\n",
            "step: 30, loss: 0.2088981568813324\n",
            "step: 40, loss: 0.1496153324842453\n",
            "step: 50, loss: 0.27692723274230957\n",
            "step: 60, loss: 0.16164249181747437\n",
            "step: 70, loss: 0.3652121424674988\n",
            "step: 80, loss: 0.21840238571166992\n",
            "step: 90, loss: 0.2762959599494934\n",
            "step: 100, loss: 0.32488206028938293\n",
            "step: 110, loss: 0.5348231792449951\n",
            "step: 120, loss: 0.39158567786216736\n",
            "step: 130, loss: 0.17641697824001312\n",
            "step: 140, loss: 0.17310555279254913\n",
            "step: 150, loss: 0.301565021276474\n",
            "step: 160, loss: 0.37858906388282776\n",
            "step: 170, loss: 0.2862355411052704\n",
            "step: 180, loss: 0.21704131364822388\n",
            "step: 190, loss: 0.1558499038219452\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.6233766233766234, f1=0.5934065934065934, best_f1=0.5934065934065934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09851516038179398\n",
            "step: 10, loss: 0.14321371912956238\n",
            "step: 20, loss: 0.19104906916618347\n",
            "step: 30, loss: 0.054376598447561264\n",
            "step: 40, loss: 0.2794499099254608\n",
            "step: 50, loss: 0.18828585743904114\n",
            "step: 60, loss: 0.21759343147277832\n",
            "step: 70, loss: 0.1758642941713333\n",
            "step: 80, loss: 0.1399543583393097\n",
            "step: 90, loss: 0.10608511418104172\n",
            "step: 100, loss: 0.24378976225852966\n",
            "step: 110, loss: 0.3161776065826416\n",
            "step: 120, loss: 0.051511798053979874\n",
            "step: 130, loss: 0.15808390080928802\n",
            "step: 140, loss: 0.3319988548755646\n",
            "step: 150, loss: 0.06764258444309235\n",
            "step: 160, loss: 0.04853322356939316\n",
            "step: 170, loss: 0.13800500333309174\n",
            "step: 180, loss: 0.22034338116645813\n",
            "step: 190, loss: 0.14195463061332703\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6951871657754011, f1=0.7002652519893899, best_f1=0.7002652519893899\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25658518075942993\n",
            "step: 10, loss: 0.2626730799674988\n",
            "step: 20, loss: 0.044123370200395584\n",
            "step: 30, loss: 0.13416333496570587\n",
            "step: 40, loss: 0.06290885806083679\n",
            "step: 50, loss: 0.08811171352863312\n",
            "step: 60, loss: 0.08299019187688828\n",
            "step: 70, loss: 0.17759570479393005\n",
            "step: 80, loss: 0.10969599336385727\n",
            "step: 90, loss: 0.22210289537906647\n",
            "step: 100, loss: 0.13756507635116577\n",
            "step: 110, loss: 0.286873996257782\n",
            "step: 120, loss: 0.17014329135417938\n",
            "step: 130, loss: 0.4878247082233429\n",
            "step: 140, loss: 0.105364590883255\n",
            "step: 150, loss: 0.23506823182106018\n",
            "step: 160, loss: 0.011187820695340633\n",
            "step: 170, loss: 0.05111226066946983\n",
            "step: 180, loss: 0.06764166057109833\n",
            "step: 190, loss: 0.3330989480018616\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7282608695652173, f1=0.7229551451187335, best_f1=0.7229551451187335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12476036697626114\n",
            "step: 10, loss: 0.20228920876979828\n",
            "step: 20, loss: 0.044153325259685516\n",
            "step: 30, loss: 0.07410579174757004\n",
            "step: 40, loss: 0.08496959507465363\n",
            "step: 50, loss: 0.2366665005683899\n",
            "step: 60, loss: 0.18382571637630463\n",
            "step: 70, loss: 0.1661592423915863\n",
            "step: 80, loss: 0.20454946160316467\n",
            "step: 90, loss: 0.1030678004026413\n",
            "step: 100, loss: 0.036031439900398254\n",
            "step: 110, loss: 0.022962328046560287\n",
            "step: 120, loss: 0.10466384887695312\n",
            "step: 130, loss: 0.2555747628211975\n",
            "step: 140, loss: 0.029761094599962234\n",
            "step: 150, loss: 0.03506653755903244\n",
            "step: 160, loss: 0.21102330088615417\n",
            "step: 170, loss: 0.4158933758735657\n",
            "step: 180, loss: 0.1703430414199829\n",
            "step: 190, loss: 0.09501216560602188\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7377521613832854, f1=0.7740112994350282, best_f1=0.7740112994350282\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08248136937618256\n",
            "step: 10, loss: 0.05382460728287697\n",
            "step: 20, loss: 0.014702262356877327\n",
            "step: 30, loss: 0.10852986574172974\n",
            "step: 40, loss: 0.06541217118501663\n",
            "step: 50, loss: 0.009382733143866062\n",
            "step: 60, loss: 0.09326282143592834\n",
            "step: 70, loss: 0.03203771263360977\n",
            "step: 80, loss: 0.004799552261829376\n",
            "step: 90, loss: 0.05469565838575363\n",
            "step: 100, loss: 0.21189376711845398\n",
            "step: 110, loss: 0.16723360121250153\n",
            "step: 120, loss: 0.027913354337215424\n",
            "step: 130, loss: 0.09611400961875916\n",
            "step: 140, loss: 0.09305035322904587\n",
            "step: 150, loss: 0.08031883090734482\n",
            "step: 160, loss: 0.18216653168201447\n",
            "step: 170, loss: 0.13867859542369843\n",
            "step: 180, loss: 0.04546016454696655\n",
            "step: 190, loss: 0.10095618665218353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7696202531645571, f1=0.7722772277227723, best_f1=0.7722772277227723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0963403582572937\n",
            "step: 10, loss: 0.032376598566770554\n",
            "step: 20, loss: 0.032395150512456894\n",
            "step: 30, loss: 0.07958222925662994\n",
            "step: 40, loss: 0.10859053581953049\n",
            "step: 50, loss: 0.046812545508146286\n",
            "step: 60, loss: 0.04804302006959915\n",
            "step: 70, loss: 0.03141620010137558\n",
            "step: 80, loss: 0.11410502344369888\n",
            "step: 90, loss: 0.10511887818574905\n",
            "step: 100, loss: 0.004725284408777952\n",
            "step: 110, loss: 0.026858963072299957\n",
            "step: 120, loss: 0.00942388828843832\n",
            "step: 130, loss: 0.023621421307325363\n",
            "step: 140, loss: 0.03211945667862892\n",
            "step: 150, loss: 0.07055453956127167\n",
            "step: 160, loss: 0.033163394778966904\n",
            "step: 170, loss: 0.012243675999343395\n",
            "step: 180, loss: 0.10787006467580795\n",
            "step: 190, loss: 0.002648416208103299\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7936507936507936, f1=0.8031496062992125, best_f1=0.8031496062992125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005718203727155924\n",
            "step: 10, loss: 0.011232424527406693\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 20, loss: 0.01966974511742592\n",
            "step: 30, loss: 0.012193798087537289\n",
            "step: 40, loss: 0.23649442195892334\n",
            "step: 50, loss: 0.04783615469932556\n",
            "step: 60, loss: 0.039956409484148026\n",
            "step: 70, loss: 0.002136032097041607\n",
            "step: 80, loss: 0.1355612725019455\n",
            "step: 90, loss: 0.02489280514419079\n",
            "step: 100, loss: 0.04990222677588463\n",
            "step: 110, loss: 0.17796678841114044\n",
            "step: 120, loss: 0.03398673236370087\n",
            "step: 130, loss: 0.03721490874886513\n",
            "step: 140, loss: 0.10135317593812943\n",
            "step: 150, loss: 0.013713710941374302\n",
            "step: 160, loss: 0.011729654856026173\n",
            "step: 170, loss: 0.039847683161497116\n",
            "step: 180, loss: 0.021303653717041016\n",
            "step: 190, loss: 0.000613425683695823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8148148148148148, f1=0.8160000000000001, best_f1=0.8160000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008149472996592522\n",
            "step: 10, loss: 0.0424569770693779\n",
            "step: 20, loss: 0.002357723657041788\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "step: 30, loss: 0.0922163650393486\n",
            "step: 40, loss: 0.0020353212021291256\n",
            "step: 50, loss: 0.044193997979164124\n",
            "step: 60, loss: 0.0006214666645973921\n",
            "step: 70, loss: 0.0006036697886884212\n",
            "step: 80, loss: 0.046434223651885986\n",
            "step: 90, loss: 0.004443855490535498\n",
            "step: 100, loss: 0.011406171135604382\n",
            "step: 110, loss: 0.08570490032434464\n",
            "step: 120, loss: 0.005196318030357361\n",
            "step: 130, loss: 0.005240065045654774\n",
            "step: 140, loss: 0.009558452293276787\n",
            "step: 150, loss: 0.010075921192765236\n",
            "step: 160, loss: 0.01937168277800083\n",
            "step: 170, loss: 0.0007690918864682317\n",
            "step: 180, loss: 0.19611194729804993\n",
            "step: 190, loss: 0.0022682833950966597\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8138297872340426, f1=0.8164383561643836, best_f1=0.8160000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012606888078153133\n",
            "step: 10, loss: 0.0006535182474181056\n",
            "step: 20, loss: 0.13319240510463715\n",
            "step: 30, loss: 0.03486981615424156\n",
            "step: 40, loss: 0.01694658398628235\n",
            "step: 50, loss: 0.009635351598262787\n",
            "step: 60, loss: 0.01657724380493164\n",
            "step: 70, loss: 0.004294670652598143\n",
            "step: 80, loss: 0.09827946126461029\n",
            "step: 90, loss: 0.01951613463461399\n",
            "step: 100, loss: 0.014636226929724216\n",
            "step: 110, loss: 0.21873396635055542\n",
            "step: 120, loss: 0.005158707499504089\n",
            "step: 130, loss: 0.03222658112645149\n",
            "step: 140, loss: 0.0044281380251049995\n",
            "step: 150, loss: 0.0016411395044997334\n",
            "step: 160, loss: 0.09091600030660629\n",
            "step: 170, loss: 0.025587264448404312\n",
            "step: 180, loss: 0.008525043725967407\n",
            "step: 190, loss: 0.0636666864156723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8093994778067886, f1=0.8042895442359249, best_f1=0.8160000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006401387043297291\n",
            "step: 10, loss: 0.0014025520067662\n",
            "step: 20, loss: 0.004948504269123077\n",
            "step: 30, loss: 0.049474943429231644\n",
            "step: 40, loss: 0.0033231317065656185\n",
            "step: 50, loss: 0.045820944011211395\n",
            "step: 60, loss: 0.01239522360265255\n",
            "step: 70, loss: 0.0008924033609218895\n",
            "step: 80, loss: 0.06228470802307129\n",
            "step: 90, loss: 0.04352736845612526\n",
            "step: 100, loss: 0.008031567558646202\n",
            "step: 110, loss: 0.0644315555691719\n",
            "step: 120, loss: 0.04582544043660164\n",
            "step: 130, loss: 0.005049959756433964\n",
            "step: 140, loss: 0.0011250709649175406\n",
            "step: 150, loss: 0.007787968032062054\n",
            "step: 160, loss: 0.0008138103294186294\n",
            "step: 170, loss: 0.005025146529078484\n",
            "step: 180, loss: 0.002560446737334132\n",
            "step: 190, loss: 0.0015454163076356053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8103896103896104, f1=0.8093994778067886, best_f1=0.8160000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0030791601166129112\n",
            "step: 10, loss: 0.0007740490254946053\n",
            "step: 20, loss: 0.0008660836610943079\n",
            "step: 30, loss: 0.03373381122946739\n",
            "step: 40, loss: 0.05572792887687683\n",
            "step: 50, loss: 0.0005784758250229061\n",
            "step: 60, loss: 0.04760413244366646\n",
            "step: 70, loss: 0.03188464418053627\n",
            "step: 80, loss: 0.0016496728640049696\n",
            "step: 90, loss: 0.00989248976111412\n",
            "step: 100, loss: 0.0004634282668121159\n",
            "step: 110, loss: 0.011281769722700119\n",
            "step: 120, loss: 0.002385897096246481\n",
            "step: 130, loss: 0.016290796920657158\n",
            "step: 140, loss: 0.00764966057613492\n",
            "step: 150, loss: 0.003386370139196515\n",
            "step: 160, loss: 0.02572176232933998\n",
            "step: 170, loss: 0.0014000984374433756\n",
            "step: 180, loss: 0.0013485309900715947\n",
            "step: 190, loss: 0.19229605793952942\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8164383561643836, f1=0.8314606741573033, best_f1=0.8314606741573033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01285574585199356\n",
            "step: 10, loss: 0.0061126709915697575\n",
            "step: 20, loss: 0.001953793689608574\n",
            "step: 30, loss: 0.008387657813727856\n",
            "step: 40, loss: 0.0017681436147540808\n",
            "step: 50, loss: 0.01257795374840498\n",
            "step: 60, loss: 0.00463897967711091\n",
            "step: 70, loss: 0.010537656024098396\n",
            "step: 80, loss: 0.001601331285201013\n",
            "step: 90, loss: 0.003696844447404146\n",
            "step: 100, loss: 0.00035799015313386917\n",
            "step: 110, loss: 0.0007079343195073307\n",
            "step: 120, loss: 0.008380675688385963\n",
            "step: 130, loss: 0.0027066529728472233\n",
            "step: 140, loss: 0.0002580313594080508\n",
            "step: 150, loss: 0.005470717791467905\n",
            "step: 160, loss: 0.0008978981641121209\n",
            "step: 170, loss: 0.0005249838577583432\n",
            "step: 180, loss: 0.03112836182117462\n",
            "step: 190, loss: 0.024622172117233276\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8097826086956521, f1=0.8287292817679558, best_f1=0.8314606741573033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011271062307059765\n",
            "step: 10, loss: 0.002715321956202388\n",
            "step: 20, loss: 0.008908742107450962\n",
            "step: 30, loss: 0.024388499557971954\n",
            "step: 40, loss: 0.0013666542945429683\n",
            "step: 50, loss: 0.0011275794822722673\n",
            "step: 60, loss: 0.16013002395629883\n",
            "step: 70, loss: 0.013943035155534744\n",
            "step: 80, loss: 0.0014228124637156725\n",
            "step: 90, loss: 0.006333461031317711\n",
            "step: 100, loss: 0.002611895091831684\n",
            "step: 110, loss: 0.10533315688371658\n",
            "step: 120, loss: 0.0006198053597472608\n",
            "step: 130, loss: 0.0010392325930297375\n",
            "step: 140, loss: 0.0013081220677122474\n",
            "step: 150, loss: 0.00039143391768448055\n",
            "step: 160, loss: 0.004219063092023134\n",
            "step: 170, loss: 0.13883104920387268\n",
            "step: 180, loss: 0.003649107413366437\n",
            "step: 190, loss: 0.10686155408620834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8081841432225065, f1=0.8272251308900523, best_f1=0.8314606741573033\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:09, 217.26it/s]\n",
            "load_f1 = 0.7146282973621103\n",
            "real_f1 = 0.7173396674584323\n",
            "733it [00:00, 3206.87it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 181.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA TEXTUAL"
      ],
      "metadata": {
        "id": "NJ3ExOzkeDVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "R1O9a5RjeDtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "a2WpDwuee1mM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b65e52f0-573d-40fa-fcf0-9de63c8fe9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.49010276794433594\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5194310545921326\n",
            "step: 20, loss: 0.319023996591568\n",
            "step: 30, loss: 0.41302621364593506\n",
            "step: 40, loss: 0.5559542179107666\n",
            "step: 50, loss: 0.32027530670166016\n",
            "step: 60, loss: 0.5609486103057861\n",
            "step: 70, loss: 0.2999364137649536\n",
            "step: 80, loss: 0.2550352215766907\n",
            "step: 90, loss: 0.23540018498897552\n",
            "step: 100, loss: 0.17442084848880768\n",
            "step: 110, loss: 0.42830097675323486\n",
            "step: 120, loss: 0.31859803199768066\n",
            "step: 130, loss: 0.3481338322162628\n",
            "step: 140, loss: 0.3469024896621704\n",
            "step: 150, loss: 0.3355696201324463\n",
            "step: 160, loss: 0.34595656394958496\n",
            "step: 170, loss: 0.2735685706138611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.540650406504065, f1=0.540084388185654, best_f1=0.540084388185654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2300741821527481\n",
            "step: 10, loss: 0.3218590021133423\n",
            "step: 20, loss: 0.22512437403202057\n",
            "step: 30, loss: 0.14948469400405884\n",
            "step: 40, loss: 0.10641095042228699\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 50, loss: 0.1574273556470871\n",
            "step: 60, loss: 0.12474176287651062\n",
            "step: 70, loss: 0.26913151144981384\n",
            "step: 80, loss: 0.14030323922634125\n",
            "step: 90, loss: 0.09151621162891388\n",
            "step: 100, loss: 0.28927990794181824\n",
            "step: 110, loss: 0.1668616086244583\n",
            "step: 120, loss: 0.040234409272670746\n",
            "step: 130, loss: 0.17619848251342773\n",
            "step: 140, loss: 0.18451941013336182\n",
            "step: 150, loss: 0.08058889955282211\n",
            "step: 160, loss: 0.15982602536678314\n",
            "step: 170, loss: 0.06398415565490723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7579462102689486, f1=0.7506053268765133, best_f1=0.7506053268765133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2110593467950821\n",
            "step: 10, loss: 0.0831843838095665\n",
            "step: 20, loss: 0.1884152591228485\n",
            "step: 30, loss: 0.05716912820935249\n",
            "step: 40, loss: 0.06561034917831421\n",
            "step: 50, loss: 0.13929779827594757\n",
            "step: 60, loss: 0.04097763076424599\n",
            "step: 70, loss: 0.14796826243400574\n",
            "step: 80, loss: 0.06686688959598541\n",
            "step: 90, loss: 0.12856528162956238\n",
            "step: 100, loss: 0.08464375138282776\n",
            "step: 110, loss: 0.0700530931353569\n",
            "step: 120, loss: 0.0663740262389183\n",
            "step: 130, loss: 0.08004765957593918\n",
            "step: 140, loss: 0.14042213559150696\n",
            "step: 150, loss: 0.07809337973594666\n",
            "step: 160, loss: 0.03508491441607475\n",
            "step: 170, loss: 0.03867239132523537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8217054263565892, f1=0.8295165394402035, best_f1=0.8295165394402035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030557166785001755\n",
            "step: 10, loss: 0.04859202727675438\n",
            "step: 20, loss: 0.00927011203020811\n",
            "step: 30, loss: 0.15069977939128876\n",
            "step: 40, loss: 0.0862884670495987\n",
            "step: 50, loss: 0.07349174469709396\n",
            "step: 60, loss: 0.5084205865859985\n",
            "step: 70, loss: 0.03915085643529892\n",
            "step: 80, loss: 0.2071535885334015\n",
            "step: 90, loss: 0.20729470252990723\n",
            "step: 100, loss: 0.06130681559443474\n",
            "step: 110, loss: 0.18020124733448029\n",
            "step: 120, loss: 0.18878725171089172\n",
            "step: 130, loss: 0.0709327980875969\n",
            "step: 140, loss: 0.11522123962640762\n",
            "step: 150, loss: 0.284432590007782\n",
            "step: 160, loss: 0.004916605539619923\n",
            "step: 170, loss: 0.10027725994586945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8436018957345971, f1=0.8590909090909091, best_f1=0.8590909090909091\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017453664913773537\n",
            "step: 10, loss: 0.05779789388179779\n",
            "step: 20, loss: 0.03557255119085312\n",
            "step: 30, loss: 0.1445176601409912\n",
            "step: 40, loss: 0.03377673402428627\n",
            "step: 50, loss: 0.05423358827829361\n",
            "step: 60, loss: 0.1432185024023056\n",
            "step: 70, loss: 0.040925126522779465\n",
            "step: 80, loss: 0.002480302704498172\n",
            "step: 90, loss: 0.020396243780851364\n",
            "step: 100, loss: 0.022584950551390648\n",
            "step: 110, loss: 0.05448553338646889\n",
            "step: 120, loss: 0.06779459863901138\n",
            "step: 130, loss: 0.003299910807982087\n",
            "step: 140, loss: 0.038444884121418\n",
            "step: 150, loss: 0.007811662741005421\n",
            "step: 160, loss: 0.005026435945183039\n",
            "step: 170, loss: 0.012619076296687126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8450000000000001, f1=0.883495145631068, best_f1=0.883495145631068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1529625803232193\n",
            "step: 10, loss: 0.013217302970588207\n",
            "step: 20, loss: 0.046746641397476196\n",
            "step: 30, loss: 0.005098366644233465\n",
            "step: 40, loss: 0.012020055204629898\n",
            "step: 50, loss: 0.0015909125795587897\n",
            "step: 60, loss: 0.0068725175224244595\n",
            "step: 70, loss: 0.06683771312236786\n",
            "step: 80, loss: 0.03255106508731842\n",
            "step: 90, loss: 0.0884452611207962\n",
            "step: 100, loss: 0.0037789279595017433\n",
            "step: 110, loss: 0.005167996045202017\n",
            "step: 120, loss: 0.06876006722450256\n",
            "step: 130, loss: 0.08997875452041626\n",
            "step: 140, loss: 0.025086915120482445\n",
            "step: 150, loss: 0.01202399656176567\n",
            "step: 160, loss: 0.10346207022666931\n",
            "step: 170, loss: 0.016099631786346436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8391959798994975, f1=0.8753056234718827, best_f1=0.883495145631068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024241948500275612\n",
            "step: 10, loss: 0.021630655974149704\n",
            "step: 20, loss: 0.007023070007562637\n",
            "step: 30, loss: 0.09008078277111053\n",
            "step: 40, loss: 0.0019780530128628016\n",
            "step: 50, loss: 0.0004225738230161369\n",
            "step: 60, loss: 0.004767242819070816\n",
            "step: 70, loss: 0.01414753869175911\n",
            "step: 80, loss: 0.0908842384815216\n",
            "step: 90, loss: 0.03426520526409149\n",
            "step: 100, loss: 0.0008119124686345458\n",
            "step: 110, loss: 0.026260562241077423\n",
            "step: 120, loss: 0.010915661230683327\n",
            "step: 130, loss: 0.004184776917099953\n",
            "step: 140, loss: 0.011227346025407314\n",
            "step: 150, loss: 0.07998758554458618\n",
            "step: 160, loss: 0.02276971936225891\n",
            "step: 170, loss: 0.019153552129864693\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8442211055276382, f1=0.8571428571428571, best_f1=0.883495145631068\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013702222146093845\n",
            "step: 10, loss: 0.024916771799325943\n",
            "step: 20, loss: 0.03760712593793869\n",
            "step: 30, loss: 0.00019523689115885645\n",
            "step: 40, loss: 0.00301631772890687\n",
            "step: 50, loss: 0.002072283299639821\n",
            "step: 60, loss: 0.0073394873179495335\n",
            "step: 70, loss: 0.018223728984594345\n",
            "step: 80, loss: 0.0006724553531967103\n",
            "step: 90, loss: 0.03911643102765083\n",
            "step: 100, loss: 0.0002155398397007957\n",
            "step: 110, loss: 0.0389416366815567\n",
            "step: 120, loss: 0.015814045444130898\n",
            "step: 130, loss: 0.0068559711799025536\n",
            "step: 140, loss: 0.16510142385959625\n",
            "step: 150, loss: 0.004153816029429436\n",
            "step: 160, loss: 0.0016220244579017162\n",
            "step: 170, loss: 0.04434197023510933\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.8578431372549019, f1=0.8699763593380615, best_f1=0.8699763593380615\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.031761638820171356\n",
            "step: 10, loss: 0.004387764260172844\n",
            "step: 20, loss: 0.0012959972955286503\n",
            "step: 30, loss: 0.012796529568731785\n",
            "step: 40, loss: 0.0018643420189619064\n",
            "step: 50, loss: 0.0001363919145660475\n",
            "step: 60, loss: 0.06515432894229889\n",
            "step: 70, loss: 0.0004413092974573374\n",
            "step: 80, loss: 0.002926016226410866\n",
            "step: 90, loss: 0.008174971677362919\n",
            "step: 100, loss: 0.004400677513331175\n",
            "step: 110, loss: 0.1276959925889969\n",
            "step: 120, loss: 0.0014084827853366733\n",
            "step: 130, loss: 0.0004050096613354981\n",
            "step: 140, loss: 0.01668861322104931\n",
            "step: 150, loss: 0.005358417052775621\n",
            "step: 160, loss: 0.001518524019047618\n",
            "step: 170, loss: 0.00599901657551527\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8615384615384616, f1=0.8731707317073171, best_f1=0.8731707317073171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00436368677765131\n",
            "step: 10, loss: 0.042021751403808594\n",
            "step: 20, loss: 0.05720031261444092\n",
            "step: 30, loss: 0.04112917557358742\n",
            "step: 40, loss: 0.0011877876240760088\n",
            "step: 50, loss: 0.014320841059088707\n",
            "step: 60, loss: 0.0014834951143711805\n",
            "step: 70, loss: 0.05322641879320145\n",
            "step: 80, loss: 0.016169728711247444\n",
            "step: 90, loss: 0.00020494013733696193\n",
            "step: 100, loss: 0.00668115122243762\n",
            "step: 110, loss: 0.011103978380560875\n",
            "step: 120, loss: 0.016562342643737793\n",
            "step: 130, loss: 0.0002951652277261019\n",
            "step: 140, loss: 0.00012412064825184643\n",
            "step: 150, loss: 7.012181595200673e-05\n",
            "step: 160, loss: 0.0015419236151501536\n",
            "step: 170, loss: 0.014142556115984917\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8601036269430052, f1=0.8822055137844612, best_f1=0.8731707317073171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021610796451568604\n",
            "step: 10, loss: 0.0213010311126709\n",
            "step: 20, loss: 0.0002267305535497144\n",
            "step: 30, loss: 0.011113767512142658\n",
            "step: 40, loss: 0.001778175006620586\n",
            "step: 50, loss: 0.004515563137829304\n",
            "step: 60, loss: 0.017810989171266556\n",
            "step: 70, loss: 0.0003743080887943506\n",
            "step: 80, loss: 0.024238115176558495\n",
            "step: 90, loss: 0.0005268657114356756\n",
            "step: 100, loss: 0.004271375015377998\n",
            "step: 110, loss: 0.00016767725173849612\n",
            "step: 120, loss: 0.0004965534899383783\n",
            "step: 130, loss: 0.011350876651704311\n",
            "step: 140, loss: 0.0014501223340630531\n",
            "step: 150, loss: 0.00010917047620750964\n",
            "step: 160, loss: 0.021473353728652\n",
            "step: 170, loss: 0.006817016750574112\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8585607940446649, f1=0.8516746411483254, best_f1=0.8731707317073171\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001456680241972208\n",
            "step: 10, loss: 0.0007332711829803884\n",
            "step: 20, loss: 0.00018157489830628037\n",
            "step: 30, loss: 0.03978349640965462\n",
            "step: 40, loss: 8.98008220246993e-05\n",
            "step: 50, loss: 0.0003724255075212568\n",
            "step: 60, loss: 0.015903787687420845\n",
            "step: 70, loss: 4.6515855501638725e-05\n",
            "step: 80, loss: 5.612139284494333e-05\n",
            "step: 90, loss: 0.000758194422814995\n",
            "step: 100, loss: 0.00011044973507523537\n",
            "step: 110, loss: 0.0008936421945691109\n",
            "step: 120, loss: 0.019272690638899803\n",
            "step: 130, loss: 0.0025709124747663736\n",
            "step: 140, loss: 8.907991286832839e-05\n",
            "step: 150, loss: 0.0601656474173069\n",
            "step: 160, loss: 0.005934497807174921\n",
            "step: 170, loss: 0.0003121339250355959\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.8647214854111406, f1=0.880597014925373, best_f1=0.880597014925373\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00014444574480876327\n",
            "step: 10, loss: 0.0007142752292566001\n",
            "step: 20, loss: 0.0006081031751818955\n",
            "step: 30, loss: 0.04111752286553383\n",
            "step: 40, loss: 0.00022144766990095377\n",
            "step: 50, loss: 0.003409275086596608\n",
            "step: 60, loss: 0.002318992745131254\n",
            "step: 70, loss: 0.024707753211259842\n",
            "step: 80, loss: 0.00024355655477847904\n",
            "step: 90, loss: 0.003095713211223483\n",
            "step: 100, loss: 0.009847949258983135\n",
            "step: 110, loss: 0.0003170525305904448\n",
            "step: 120, loss: 0.010560132563114166\n",
            "step: 130, loss: 0.00014781222853343934\n",
            "step: 140, loss: 0.062399473041296005\n",
            "step: 150, loss: 0.0003042052558157593\n",
            "step: 160, loss: 0.0002893870114348829\n",
            "step: 170, loss: 0.0065491278655827045\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8753180661577608, f1=0.8894348894348895, best_f1=0.8894348894348895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025352926924824715\n",
            "step: 10, loss: 0.004231694620102644\n",
            "step: 20, loss: 0.0002863703412003815\n",
            "step: 30, loss: 0.02894543670117855\n",
            "step: 40, loss: 0.001945263473317027\n",
            "step: 50, loss: 0.0009616558672860265\n",
            "step: 60, loss: 0.009578770026564598\n",
            "step: 70, loss: 0.0033253857400268316\n",
            "step: 80, loss: 0.0006149326218292117\n",
            "step: 90, loss: 0.00031783245503902435\n",
            "step: 100, loss: 0.0007363494951277971\n",
            "step: 110, loss: 0.020664038136601448\n",
            "step: 120, loss: 0.00016300789138767868\n",
            "step: 130, loss: 0.04101790860295296\n",
            "step: 140, loss: 0.03436487540602684\n",
            "step: 150, loss: 0.021306538954377174\n",
            "step: 160, loss: 0.0004397889715619385\n",
            "step: 170, loss: 0.03285538777709007\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8702290076335877, f1=0.8719211822660098, best_f1=0.8894348894348895\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.5824719816446304e-05\n",
            "step: 10, loss: 0.04534517601132393\n",
            "step: 20, loss: 0.0011615960393100977\n",
            "step: 30, loss: 0.0022192385513335466\n",
            "step: 40, loss: 7.108335557859391e-05\n",
            "step: 50, loss: 4.9834146921057254e-05\n",
            "step: 60, loss: 0.0002869473828468472\n",
            "step: 70, loss: 0.07959194481372833\n",
            "step: 80, loss: 0.00010441240738146007\n",
            "step: 90, loss: 0.0194824431091547\n",
            "step: 100, loss: 0.034344382584095\n",
            "step: 110, loss: 0.00019019316823687404\n",
            "step: 120, loss: 7.019969052635133e-05\n",
            "step: 130, loss: 0.00037740039988420904\n",
            "step: 140, loss: 0.0322285033762455\n",
            "step: 150, loss: 0.0006633492303080857\n",
            "step: 160, loss: 8.424588304478675e-05\n",
            "step: 170, loss: 0.0020319472532719374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8623376623376624, f1=0.8706467661691543, best_f1=0.8894348894348895\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:06, 292.80it/s]\n",
            "load_f1 = 0.38048780487804873\n",
            "real_f1 = 0.34666666666666673\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:24, 179.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DA DIRTY"
      ],
      "metadata": {
        "id": "6pmKonkXeD7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "bRxHd3j2eEH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da entry_swap  \\\n",
        "  --da span_shuffle \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "0lnLoRSEe2fE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df24b368-f074-48ce-d365-fd6db56edba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 429kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 663kB/s] \n",
            "Downloading: 100% 456k/456k [00:00<00:00, 503kB/s]\n",
            "Downloading: 100% 501M/501M [00:07<00:00, 65.5MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5590389966964722\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5002449750900269\n",
            "step: 20, loss: 0.46249833703041077\n",
            "step: 30, loss: 0.32963353395462036\n",
            "step: 40, loss: 0.2778749167919159\n",
            "step: 50, loss: 0.32605987787246704\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.13435427844524384\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 70, loss: 0.14397946000099182\n",
            "step: 80, loss: 0.23299908638000488\n",
            "step: 90, loss: 0.18722543120384216\n",
            "step: 100, loss: 0.20928028225898743\n",
            "step: 110, loss: 0.15669581294059753\n",
            "step: 120, loss: 0.1520737260580063\n",
            "step: 130, loss: 0.07708626985549927\n",
            "step: 140, loss: 0.014633585698902607\n",
            "step: 150, loss: 0.22405731678009033\n",
            "step: 160, loss: 0.08602719753980637\n",
            "step: 170, loss: 0.20835788547992706\n",
            "step: 180, loss: 0.20830488204956055\n",
            "step: 190, loss: 0.04173213988542557\n",
            "step: 200, loss: 0.05206559598445892\n",
            "step: 210, loss: 0.07175896316766739\n",
            "step: 220, loss: 0.09271375089883804\n",
            "step: 230, loss: 0.040392547845840454\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9511111111111112, f1=0.9572072072072072, best_f1=0.9572072072072072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008876264095306396\n",
            "step: 10, loss: 0.16282671689987183\n",
            "step: 20, loss: 0.06934989243745804\n",
            "step: 30, loss: 0.046789463609457016\n",
            "step: 40, loss: 0.05833005532622337\n",
            "step: 50, loss: 0.11078061163425446\n",
            "step: 60, loss: 0.0244491808116436\n",
            "step: 70, loss: 0.01987435668706894\n",
            "step: 80, loss: 0.06586655229330063\n",
            "step: 90, loss: 0.016266178339719772\n",
            "step: 100, loss: 0.011983667500317097\n",
            "step: 110, loss: 0.009227056987583637\n",
            "step: 120, loss: 0.009595858864486217\n",
            "step: 130, loss: 0.014045978896319866\n",
            "step: 140, loss: 0.03010598011314869\n",
            "step: 150, loss: 0.10542688518762589\n",
            "step: 160, loss: 0.04740702360868454\n",
            "step: 170, loss: 0.001814771443605423\n",
            "step: 180, loss: 0.009511174634099007\n",
            "step: 190, loss: 0.008443239144980907\n",
            "step: 200, loss: 0.026588426902890205\n",
            "step: 210, loss: 0.01656336709856987\n",
            "step: 220, loss: 0.039098791778087616\n",
            "step: 230, loss: 0.0023090827744454145\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.961883408071749, f1=0.9590909090909091, best_f1=0.9590909090909091\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05080454424023628\n",
            "step: 10, loss: 0.003119959030300379\n",
            "step: 20, loss: 0.0022576828487217426\n",
            "step: 30, loss: 0.009573020972311497\n",
            "step: 40, loss: 0.0633724182844162\n",
            "step: 50, loss: 0.0029667168855667114\n",
            "step: 60, loss: 0.007985192351043224\n",
            "step: 70, loss: 0.1474623829126358\n",
            "step: 80, loss: 0.01662452332675457\n",
            "step: 90, loss: 0.00894701387733221\n",
            "step: 100, loss: 0.0388542041182518\n",
            "step: 110, loss: 0.006835360545665026\n",
            "step: 120, loss: 0.0016621522372588515\n",
            "step: 130, loss: 0.0062571982853114605\n",
            "step: 140, loss: 0.007041057571768761\n",
            "step: 150, loss: 0.10149605572223663\n",
            "step: 160, loss: 0.003862140467390418\n",
            "step: 170, loss: 0.001707739313133061\n",
            "step: 180, loss: 0.02550848014652729\n",
            "step: 190, loss: 0.03275108337402344\n",
            "step: 200, loss: 0.015710093080997467\n",
            "step: 210, loss: 0.0036216392181813717\n",
            "step: 220, loss: 0.13284960389137268\n",
            "step: 230, loss: 0.014392430894076824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9764309764309763, f1=0.9672316384180792, best_f1=0.9672316384180792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.018998317420482635\n",
            "step: 10, loss: 0.0035267246421426535\n",
            "step: 20, loss: 0.0069399019703269005\n",
            "step: 30, loss: 0.0010110011789947748\n",
            "step: 40, loss: 0.10288895666599274\n",
            "step: 50, loss: 0.03252794221043587\n",
            "step: 60, loss: 0.00636560982093215\n",
            "step: 70, loss: 0.07242341339588165\n",
            "step: 80, loss: 0.10070707648992538\n",
            "step: 90, loss: 0.0812988430261612\n",
            "step: 100, loss: 0.00975632481276989\n",
            "step: 110, loss: 0.004891911521553993\n",
            "step: 120, loss: 0.009860473684966564\n",
            "step: 130, loss: 0.043512556701898575\n",
            "step: 140, loss: 0.019150134176015854\n",
            "step: 150, loss: 0.001956071238964796\n",
            "step: 160, loss: 0.012039711698889732\n",
            "step: 170, loss: 0.00418445747345686\n",
            "step: 180, loss: 0.031163634732365608\n",
            "step: 190, loss: 0.002646433189511299\n",
            "step: 200, loss: 0.07090921700000763\n",
            "step: 210, loss: 0.0812133252620697\n",
            "step: 220, loss: 0.0033443348947912455\n",
            "step: 230, loss: 0.003969802055507898\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9743016759776536, f1=0.9706546275395034, best_f1=0.9672316384180792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008764320518821478\n",
            "step: 10, loss: 0.004367002751678228\n",
            "step: 20, loss: 0.045865487307310104\n",
            "step: 30, loss: 0.0035576571244746447\n",
            "step: 40, loss: 0.007287757005542517\n",
            "step: 50, loss: 0.002448836574330926\n",
            "step: 60, loss: 0.0025546010583639145\n",
            "step: 70, loss: 0.0013177931541576982\n",
            "step: 80, loss: 0.014353899285197258\n",
            "step: 90, loss: 0.054778847843408585\n",
            "step: 100, loss: 0.0010106057161465287\n",
            "step: 110, loss: 0.0008980968268588185\n",
            "step: 120, loss: 0.0017204777104780078\n",
            "step: 130, loss: 0.0010007923701778054\n",
            "step: 140, loss: 0.00251578725874424\n",
            "step: 150, loss: 0.008259815163910389\n",
            "step: 160, loss: 0.0007321587763726711\n",
            "step: 170, loss: 0.002930195303633809\n",
            "step: 180, loss: 0.005918445996940136\n",
            "step: 190, loss: 0.12139478325843811\n",
            "step: 200, loss: 0.003912093583494425\n",
            "step: 210, loss: 0.0017600596183910966\n",
            "step: 220, loss: 0.010193523950874805\n",
            "step: 230, loss: 0.006326672155410051\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9761634506242906, f1=0.9597238204833141, best_f1=0.9672316384180792\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0034894910641014576\n",
            "step: 10, loss: 0.0026658466085791588\n",
            "step: 20, loss: 0.0016642019618302584\n",
            "step: 30, loss: 0.0006596704479306936\n",
            "step: 40, loss: 0.00052338105160743\n",
            "step: 50, loss: 0.0006622977089136839\n",
            "step: 60, loss: 0.0009859332349151373\n",
            "step: 70, loss: 0.16875769197940826\n",
            "step: 80, loss: 0.0019027404487133026\n",
            "step: 90, loss: 0.0033460943959653378\n",
            "step: 100, loss: 0.0011739309411495924\n",
            "step: 110, loss: 0.010459682904183865\n",
            "step: 120, loss: 0.0012457799166440964\n",
            "step: 130, loss: 0.0011825573164969683\n",
            "step: 140, loss: 0.003935706801712513\n",
            "step: 150, loss: 0.005316666327416897\n",
            "step: 160, loss: 0.005191100295633078\n",
            "step: 170, loss: 0.006289668381214142\n",
            "step: 180, loss: 0.000427121325628832\n",
            "step: 190, loss: 0.0003306346188765019\n",
            "step: 200, loss: 0.0015607759123668075\n",
            "step: 210, loss: 0.0014192754169926047\n",
            "step: 220, loss: 0.0610097274184227\n",
            "step: 230, loss: 0.0018149679526686668\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9782857142857143, f1=0.9738339021615473, best_f1=0.9738339021615473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001775477547198534\n",
            "step: 10, loss: 0.0007527230773121119\n",
            "step: 20, loss: 0.0012890624348074198\n",
            "step: 30, loss: 0.0009798755636438727\n",
            "step: 40, loss: 0.001203481457196176\n",
            "step: 50, loss: 0.03715460002422333\n",
            "step: 60, loss: 0.0015592710115015507\n",
            "step: 70, loss: 0.010675081983208656\n",
            "step: 80, loss: 0.015021707862615585\n",
            "step: 90, loss: 0.005443382542580366\n",
            "step: 100, loss: 0.003117184154689312\n",
            "step: 110, loss: 0.0007647420279681683\n",
            "step: 120, loss: 0.0005544920568354428\n",
            "step: 130, loss: 0.0030184166971594095\n",
            "step: 140, loss: 0.0008047962328419089\n",
            "step: 150, loss: 0.044492945075035095\n",
            "step: 160, loss: 0.0005738445906899869\n",
            "step: 170, loss: 0.03457023203372955\n",
            "step: 180, loss: 0.0010694321244955063\n",
            "step: 190, loss: 0.0010087746195495129\n",
            "step: 200, loss: 0.0028301419224590063\n",
            "step: 210, loss: 0.0019519024062901735\n",
            "step: 220, loss: 0.0015077515272423625\n",
            "step: 230, loss: 0.0017423059325665236\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.983050847457627, f1=0.9737742303306728, best_f1=0.9737742303306728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024567553773522377\n",
            "step: 10, loss: 0.003734566504135728\n",
            "step: 20, loss: 0.009708719328045845\n",
            "step: 30, loss: 0.0014933435013517737\n",
            "step: 40, loss: 0.0013504510279744864\n",
            "step: 50, loss: 0.0021251640282571316\n",
            "step: 60, loss: 0.0004348848888184875\n",
            "step: 70, loss: 0.00016084290109574795\n",
            "step: 80, loss: 0.02360565960407257\n",
            "step: 90, loss: 0.0005059735849499702\n",
            "step: 100, loss: 0.008084507659077644\n",
            "step: 110, loss: 0.0024726546835154295\n",
            "step: 120, loss: 0.0006037665298208594\n",
            "step: 130, loss: 0.0020876647904515266\n",
            "step: 140, loss: 0.0004540120135061443\n",
            "step: 150, loss: 0.11995416134595871\n",
            "step: 160, loss: 0.0012721734819933772\n",
            "step: 170, loss: 0.02769819088280201\n",
            "step: 180, loss: 0.0007951320731081069\n",
            "step: 190, loss: 0.20086024701595306\n",
            "step: 200, loss: 0.01712074875831604\n",
            "step: 210, loss: 0.00539805693551898\n",
            "step: 220, loss: 0.001158643513917923\n",
            "step: 230, loss: 0.000567821494769305\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9753914988814317, f1=0.9751693002257337, best_f1=0.9737742303306728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001512962393462658\n",
            "step: 10, loss: 0.00835393089801073\n",
            "step: 20, loss: 0.029500683769583702\n",
            "step: 30, loss: 0.001689296099357307\n",
            "step: 40, loss: 0.0006335757207125425\n",
            "step: 50, loss: 0.0017673051916062832\n",
            "step: 60, loss: 0.013249123468995094\n",
            "step: 70, loss: 0.11510413885116577\n",
            "step: 80, loss: 0.0005430823657661676\n",
            "step: 90, loss: 0.01674739643931389\n",
            "step: 100, loss: 0.006926300935447216\n",
            "step: 110, loss: 0.05998816713690758\n",
            "step: 120, loss: 0.014749906957149506\n",
            "step: 130, loss: 0.0009336175280623138\n",
            "step: 140, loss: 0.0011852113530039787\n",
            "step: 150, loss: 0.005729489494115114\n",
            "step: 160, loss: 0.0035600843839347363\n",
            "step: 170, loss: 0.0002837661886587739\n",
            "step: 180, loss: 0.0007952682208269835\n",
            "step: 190, loss: 0.0002298820181749761\n",
            "step: 200, loss: 0.014588548801839352\n",
            "step: 210, loss: 0.18189887702465057\n",
            "step: 220, loss: 0.0012435121461749077\n",
            "step: 230, loss: 0.0002818311913870275\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9785310734463276, f1=0.976054732041049, best_f1=0.9737742303306728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004407633387017995\n",
            "step: 10, loss: 0.0018396996892988682\n",
            "step: 20, loss: 0.0008099167025648057\n",
            "step: 30, loss: 0.00022026384249329567\n",
            "step: 40, loss: 0.0008325439994223416\n",
            "step: 50, loss: 0.00017427460988983512\n",
            "step: 60, loss: 0.00047756280400790274\n",
            "step: 70, loss: 0.016363754868507385\n",
            "step: 80, loss: 0.003825288498774171\n",
            "step: 90, loss: 0.0004755085101351142\n",
            "step: 100, loss: 0.00027340915403328836\n",
            "step: 110, loss: 0.0011966486927121878\n",
            "step: 120, loss: 0.00836675614118576\n",
            "step: 130, loss: 0.0009078617440536618\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 140, loss: 0.001342790899798274\n",
            "step: 150, loss: 0.06483083218336105\n",
            "step: 160, loss: 0.0012474070535972714\n",
            "step: 170, loss: 0.003342608455568552\n",
            "step: 180, loss: 0.02375645563006401\n",
            "step: 190, loss: 0.008231544867157936\n",
            "step: 200, loss: 0.0006797741516493261\n",
            "step: 210, loss: 0.020311735570430756\n",
            "step: 220, loss: 0.0004307323833927512\n",
            "step: 230, loss: 0.0005108733894303441\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9740112994350283, f1=0.9728506787330317, best_f1=0.9737742303306728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005804618122056127\n",
            "step: 10, loss: 0.000537108804564923\n",
            "step: 20, loss: 0.0005562698934227228\n",
            "step: 30, loss: 0.0014927067095413804\n",
            "step: 40, loss: 0.00016358558787032962\n",
            "step: 50, loss: 0.00023354623408522457\n",
            "step: 60, loss: 0.004780137911438942\n",
            "step: 70, loss: 0.00036760116927325726\n",
            "step: 80, loss: 0.0005678044981323183\n",
            "step: 90, loss: 0.14308218657970428\n",
            "step: 100, loss: 0.0004337129939813167\n",
            "step: 110, loss: 0.014832425862550735\n",
            "step: 120, loss: 0.006978536956012249\n",
            "step: 130, loss: 0.00022309744963422418\n",
            "step: 140, loss: 0.006642024032771587\n",
            "step: 150, loss: 0.0006830415804870427\n",
            "step: 160, loss: 0.004134808201342821\n",
            "step: 170, loss: 0.0008274375577457249\n",
            "step: 180, loss: 0.0016250257613137364\n",
            "step: 190, loss: 0.00031529925763607025\n",
            "step: 200, loss: 0.0013804135378450155\n",
            "step: 210, loss: 0.0005855982308275998\n",
            "step: 220, loss: 0.0007696808315813541\n",
            "step: 230, loss: 0.000476013810839504\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9752252252252253, f1=0.9762174405436014, best_f1=0.9737742303306728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004944141255691648\n",
            "step: 10, loss: 0.00019221450202167034\n",
            "step: 20, loss: 0.010824046097695827\n",
            "step: 30, loss: 0.0193479023873806\n",
            "step: 40, loss: 0.0014686400536447763\n",
            "step: 50, loss: 0.0036108682397753\n",
            "step: 60, loss: 0.0004758824943564832\n",
            "step: 70, loss: 0.00015627358516212553\n",
            "step: 80, loss: 8.744386286707595e-05\n",
            "step: 90, loss: 0.000926309556234628\n",
            "step: 100, loss: 0.0002076769305858761\n",
            "step: 110, loss: 0.00019612156029324979\n",
            "step: 120, loss: 0.0009168636752292514\n",
            "step: 130, loss: 0.0002818428329192102\n",
            "step: 140, loss: 0.000571102078538388\n",
            "step: 150, loss: 0.0010690436465665698\n",
            "step: 160, loss: 0.0065476675517857075\n",
            "step: 170, loss: 0.0007189156021922827\n",
            "step: 180, loss: 0.0007681918214075267\n",
            "step: 190, loss: 0.008740263991057873\n",
            "step: 200, loss: 0.0007593158516101539\n",
            "step: 210, loss: 0.0005417648935690522\n",
            "step: 220, loss: 0.011795799247920513\n",
            "step: 230, loss: 0.000789787620306015\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9785794813979707, f1=0.9740112994350283, best_f1=0.9737742303306728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012193990405648947\n",
            "step: 10, loss: 0.002167463768273592\n",
            "step: 20, loss: 0.0007755612023174763\n",
            "step: 30, loss: 0.00011430019367253408\n",
            "step: 40, loss: 0.0012664623791351914\n",
            "step: 50, loss: 0.000929126632399857\n",
            "step: 60, loss: 0.00021305744303390384\n",
            "step: 70, loss: 0.0030134047847241163\n",
            "step: 80, loss: 0.006873562932014465\n",
            "step: 90, loss: 0.000519985449500382\n",
            "step: 100, loss: 0.0006969112437218428\n",
            "step: 110, loss: 0.0008448171429336071\n",
            "step: 120, loss: 0.0005219014710746706\n",
            "step: 130, loss: 0.009569162502884865\n",
            "step: 140, loss: 0.0007654592045582831\n",
            "step: 150, loss: 0.00029317860025912523\n",
            "step: 160, loss: 0.005878959316760302\n",
            "step: 170, loss: 0.00040913833072409034\n",
            "step: 180, loss: 0.02872474119067192\n",
            "step: 190, loss: 0.00017126752936746925\n",
            "step: 200, loss: 5.880032404093072e-05\n",
            "step: 210, loss: 0.00033293021260760725\n",
            "step: 220, loss: 0.00015400198753923178\n",
            "step: 230, loss: 0.0010411542607471347\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9796380090497738, f1=0.9738339021615473, best_f1=0.9737742303306728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00043155785533599555\n",
            "step: 10, loss: 0.0001147369112004526\n",
            "step: 20, loss: 0.0001462110667489469\n",
            "step: 30, loss: 0.0002742572105489671\n",
            "step: 40, loss: 0.00019366387277841568\n",
            "step: 50, loss: 0.00018376664957031608\n",
            "step: 60, loss: 0.00033081145375035703\n",
            "step: 70, loss: 0.0002528868499211967\n",
            "step: 80, loss: 0.00039444788126274943\n",
            "step: 90, loss: 0.0007410874241031706\n",
            "step: 100, loss: 0.0002875987847801298\n",
            "step: 110, loss: 0.000194416381418705\n",
            "step: 120, loss: 3.49578658642713e-05\n",
            "step: 130, loss: 0.000449606915935874\n",
            "step: 140, loss: 0.003178945044055581\n",
            "step: 150, loss: 0.0005364902317523956\n",
            "step: 160, loss: 0.0004739846335723996\n",
            "step: 170, loss: 0.00015884212916716933\n",
            "step: 180, loss: 0.00016487178800161928\n",
            "step: 190, loss: 7.01579701853916e-05\n",
            "step: 200, loss: 0.0027983002364635468\n",
            "step: 210, loss: 0.00010831780673470348\n",
            "step: 220, loss: 0.00017930915055330843\n",
            "step: 230, loss: 0.00031480463803745806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9785310734463276, f1=0.9762174405436014, best_f1=0.9737742303306728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00539482431486249\n",
            "step: 10, loss: 6.916259007994086e-05\n",
            "step: 20, loss: 0.00043736951192840934\n",
            "step: 30, loss: 0.0001726114860503003\n",
            "step: 40, loss: 4.546922355075367e-05\n",
            "step: 50, loss: 6.613929872401059e-05\n",
            "step: 60, loss: 0.05538269504904747\n",
            "step: 70, loss: 0.00028460242901928723\n",
            "step: 80, loss: 0.00016855089052114636\n",
            "step: 90, loss: 0.0001604340213816613\n",
            "step: 100, loss: 5.046693695476279e-05\n",
            "step: 110, loss: 0.000112894871563185\n",
            "step: 120, loss: 0.046775054186582565\n",
            "step: 130, loss: 0.00017188051424454898\n",
            "step: 140, loss: 0.012829378247261047\n",
            "step: 150, loss: 0.00024040078278630972\n",
            "step: 160, loss: 0.0132053904235363\n",
            "step: 170, loss: 3.991255653090775e-05\n",
            "step: 180, loss: 0.00015035834803711623\n",
            "step: 190, loss: 0.0001528222783235833\n",
            "step: 200, loss: 0.0005606774939224124\n",
            "step: 210, loss: 0.0001548241125419736\n",
            "step: 220, loss: 0.0003376179956831038\n",
            "step: 230, loss: 6.519549788208678e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9785310734463276, f1=0.976271186440678, best_f1=0.9737742303306728\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 156.34it/s]\n",
            "load_f1 = 0.9853107344632768\n",
            "real_f1 = 0.9819413092550789\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 144.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "sW78AaaneEUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --da span_shuffle  \\\n",
        "  --da attr_del \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "1Hwn5WkZe3Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030a82bb-3b38-40d4-ab0b-7400a3ddaa99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6697006821632385\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.42482995986938477\n",
            "step: 20, loss: 0.2766985595226288\n",
            "step: 30, loss: 0.3048263192176819\n",
            "step: 40, loss: 0.2817775011062622\n",
            "step: 50, loss: 0.14254218339920044\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.23891796171665192\n",
            "step: 70, loss: 0.2215806096792221\n",
            "step: 80, loss: 0.08493991196155548\n",
            "step: 90, loss: 0.3684939444065094\n",
            "step: 100, loss: 0.20191404223442078\n",
            "step: 110, loss: 0.16636905074119568\n",
            "step: 120, loss: 0.4916318655014038\n",
            "step: 130, loss: 0.18801818788051605\n",
            "step: 140, loss: 0.21646051108837128\n",
            "step: 150, loss: 0.09096415340900421\n",
            "step: 160, loss: 0.24299974739551544\n",
            "step: 170, loss: 0.10577455163002014\n",
            "step: 180, loss: 0.059722695499658585\n",
            "step: 190, loss: 0.2353293001651764\n",
            "step: 200, loss: 0.09523861855268478\n",
            "step: 210, loss: 0.07635896652936935\n",
            "step: 220, loss: 0.08411570638418198\n",
            "step: 230, loss: 0.23592601716518402\n",
            "step: 240, loss: 0.07038753479719162\n",
            "step: 250, loss: 0.08807475119829178\n",
            "step: 260, loss: 0.2028736174106598\n",
            "step: 270, loss: 0.2522951066493988\n",
            "step: 280, loss: 0.029201863333582878\n",
            "step: 290, loss: 0.24671682715415955\n",
            "step: 300, loss: 0.052507344633340836\n",
            "step: 310, loss: 0.0980801060795784\n",
            "step: 320, loss: 0.10320968180894852\n",
            "step: 330, loss: 0.09699789434671402\n",
            "step: 340, loss: 0.3705860674381256\n",
            "step: 350, loss: 0.05930127948522568\n",
            "step: 360, loss: 0.05049298331141472\n",
            "step: 370, loss: 0.09354082494974136\n",
            "step: 380, loss: 0.0968519002199173\n",
            "step: 390, loss: 0.02204907312989235\n",
            "step: 400, loss: 0.057815078645944595\n",
            "step: 410, loss: 0.17021037638187408\n",
            "step: 420, loss: 0.03543499484658241\n",
            "step: 430, loss: 0.012207510881125927\n",
            "step: 440, loss: 0.044864434748888016\n",
            "step: 450, loss: 0.11591032147407532\n",
            "step: 460, loss: 0.08917752653360367\n",
            "step: 470, loss: 0.01968301273882389\n",
            "step: 480, loss: 0.2673661708831787\n",
            "step: 490, loss: 0.07659497112035751\n",
            "step: 500, loss: 0.04518282413482666\n",
            "step: 510, loss: 0.025072526186704636\n",
            "step: 520, loss: 0.2286616414785385\n",
            "step: 530, loss: 0.02770482376217842\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9399358092618065, f1=0.9405850091407678, best_f1=0.9405850091407678\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11850927770137787\n",
            "step: 10, loss: 0.11083777993917465\n",
            "step: 20, loss: 0.1633167862892151\n",
            "step: 30, loss: 0.026139769703149796\n",
            "step: 40, loss: 0.1148395910859108\n",
            "step: 50, loss: 0.03881968930363655\n",
            "step: 60, loss: 0.014027444645762444\n",
            "step: 70, loss: 0.033569734543561935\n",
            "step: 80, loss: 0.07366305589675903\n",
            "step: 90, loss: 0.02659616805613041\n",
            "step: 100, loss: 0.25081318616867065\n",
            "step: 110, loss: 0.016110675409436226\n",
            "step: 120, loss: 0.1624959409236908\n",
            "step: 130, loss: 0.005682327318936586\n",
            "step: 140, loss: 0.03563275560736656\n",
            "step: 150, loss: 0.07505583018064499\n",
            "step: 160, loss: 0.022785713896155357\n",
            "step: 170, loss: 0.0286654494702816\n",
            "step: 180, loss: 0.04123426973819733\n",
            "step: 190, loss: 0.04467933624982834\n",
            "step: 200, loss: 0.0546259842813015\n",
            "step: 210, loss: 0.03839515894651413\n",
            "step: 220, loss: 0.001487046480178833\n",
            "step: 230, loss: 0.08876004070043564\n",
            "step: 240, loss: 0.09556561708450317\n",
            "step: 250, loss: 0.02000131830573082\n",
            "step: 260, loss: 0.04857221990823746\n",
            "step: 270, loss: 0.05757991969585419\n",
            "step: 280, loss: 0.14812280237674713\n",
            "step: 290, loss: 0.027644777670502663\n",
            "step: 300, loss: 0.030512256547808647\n",
            "step: 310, loss: 0.06935875862836838\n",
            "step: 320, loss: 0.04165484011173248\n",
            "step: 330, loss: 0.06223583593964577\n",
            "step: 340, loss: 0.10653398931026459\n",
            "step: 350, loss: 0.005626088008284569\n",
            "step: 360, loss: 0.054528020322322845\n",
            "step: 370, loss: 0.015933044254779816\n",
            "step: 380, loss: 0.26876136660575867\n",
            "step: 390, loss: 0.04130322113633156\n",
            "step: 400, loss: 0.02810104377567768\n",
            "step: 410, loss: 0.01551960501819849\n",
            "step: 420, loss: 0.11681908369064331\n",
            "step: 430, loss: 0.25731751322746277\n",
            "step: 440, loss: 0.004227197263389826\n",
            "step: 450, loss: 0.07989617437124252\n",
            "step: 460, loss: 0.017921220511198044\n",
            "step: 470, loss: 0.048715267330408096\n",
            "step: 480, loss: 0.015830496326088905\n",
            "step: 490, loss: 0.1019844263792038\n",
            "step: 500, loss: 0.0029289121739566326\n",
            "step: 510, loss: 0.13924413919448853\n",
            "step: 520, loss: 0.4036087393760681\n",
            "step: 530, loss: 0.20844189822673798\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9406934306569343, f1=0.9417652411282984, best_f1=0.9417652411282984\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1633431315422058\n",
            "step: 10, loss: 0.0671287253499031\n",
            "step: 20, loss: 0.017226798459887505\n",
            "step: 30, loss: 0.0609397366642952\n",
            "step: 40, loss: 0.0925477147102356\n",
            "step: 50, loss: 0.10476009547710419\n",
            "step: 60, loss: 0.0433499850332737\n",
            "step: 70, loss: 0.01863253302872181\n",
            "step: 80, loss: 0.011910736560821533\n",
            "step: 90, loss: 0.05366196110844612\n",
            "step: 100, loss: 0.1888040453195572\n",
            "step: 110, loss: 0.07603811472654343\n",
            "step: 120, loss: 0.15573018789291382\n",
            "step: 130, loss: 0.06559617072343826\n",
            "step: 140, loss: 0.019029108807444572\n",
            "step: 150, loss: 0.022139113396406174\n",
            "step: 160, loss: 0.010360032320022583\n",
            "step: 170, loss: 0.020111864432692528\n",
            "step: 180, loss: 0.01346915028989315\n",
            "step: 190, loss: 0.013483825139701366\n",
            "step: 200, loss: 0.11957959085702896\n",
            "step: 210, loss: 0.037187572568655014\n",
            "step: 220, loss: 0.08251512795686722\n",
            "step: 230, loss: 0.0856996700167656\n",
            "step: 240, loss: 0.08140837401151657\n",
            "step: 250, loss: 0.09853841364383698\n",
            "step: 260, loss: 0.1761779934167862\n",
            "step: 270, loss: 0.006857358850538731\n",
            "step: 280, loss: 0.005285211838781834\n",
            "step: 290, loss: 0.00586601672694087\n",
            "step: 300, loss: 0.16299326717853546\n",
            "step: 310, loss: 0.16263587772846222\n",
            "step: 320, loss: 0.014376086182892323\n",
            "step: 330, loss: 0.032322924584150314\n",
            "step: 340, loss: 0.006190289743244648\n",
            "step: 350, loss: 0.09204686433076859\n",
            "step: 360, loss: 0.005800099112093449\n",
            "step: 370, loss: 0.0993383526802063\n",
            "step: 380, loss: 0.0018863441655412316\n",
            "step: 390, loss: 0.0024386306758970022\n",
            "step: 400, loss: 0.16195346415042877\n",
            "step: 410, loss: 0.048396412283182144\n",
            "step: 420, loss: 0.013087151572108269\n",
            "step: 430, loss: 0.040630556643009186\n",
            "step: 440, loss: 0.2496170550584793\n",
            "step: 450, loss: 0.009232385084033012\n",
            "step: 460, loss: 0.09675264358520508\n",
            "step: 470, loss: 0.010295290499925613\n",
            "step: 480, loss: 0.26602959632873535\n",
            "step: 490, loss: 0.05206288769841194\n",
            "step: 500, loss: 0.07094442844390869\n",
            "step: 510, loss: 0.0052596693858504295\n",
            "step: 520, loss: 0.010203525424003601\n",
            "step: 530, loss: 0.01845373399555683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9502762430939227, f1=0.9437269372693727, best_f1=0.9437269372693727\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07661855965852737\n",
            "step: 10, loss: 0.0013236721279099584\n",
            "step: 20, loss: 0.07374272495508194\n",
            "step: 30, loss: 0.14805231988430023\n",
            "step: 40, loss: 0.006854254752397537\n",
            "step: 50, loss: 0.07737947255373001\n",
            "step: 60, loss: 0.040915146470069885\n",
            "step: 70, loss: 0.031095199286937714\n",
            "step: 80, loss: 0.1242005005478859\n",
            "step: 90, loss: 0.2078724205493927\n",
            "step: 100, loss: 0.002045457949861884\n",
            "step: 110, loss: 0.16446730494499207\n",
            "step: 120, loss: 0.012053915299475193\n",
            "step: 130, loss: 0.1137235090136528\n",
            "step: 140, loss: 0.03104647807776928\n",
            "step: 150, loss: 0.007266766391694546\n",
            "step: 160, loss: 0.008552387356758118\n",
            "step: 170, loss: 0.03852609917521477\n",
            "step: 180, loss: 0.011150750331580639\n",
            "step: 190, loss: 0.035624805837869644\n",
            "step: 200, loss: 0.1507958620786667\n",
            "step: 210, loss: 0.001832086592912674\n",
            "step: 220, loss: 0.0058513423427939415\n",
            "step: 230, loss: 0.02179870940744877\n",
            "step: 240, loss: 0.011682654730975628\n",
            "step: 250, loss: 0.08417682349681854\n",
            "step: 260, loss: 0.005175299942493439\n",
            "step: 270, loss: 0.09562189131975174\n",
            "step: 280, loss: 0.029022419825196266\n",
            "step: 290, loss: 0.05633404105901718\n",
            "step: 300, loss: 0.002138789277523756\n",
            "step: 310, loss: 0.01118482742458582\n",
            "step: 320, loss: 0.11747857183218002\n",
            "step: 330, loss: 0.04635664448142052\n",
            "step: 340, loss: 0.02195911668241024\n",
            "step: 350, loss: 0.10990853607654572\n",
            "step: 360, loss: 0.02671167068183422\n",
            "step: 370, loss: 0.003426506882533431\n",
            "step: 380, loss: 0.028026657178997993\n",
            "step: 390, loss: 0.0017970781773328781\n",
            "step: 400, loss: 0.06959256529808044\n",
            "step: 410, loss: 0.0012131903786212206\n",
            "step: 420, loss: 0.004684193059802055\n",
            "step: 430, loss: 0.05495302006602287\n",
            "step: 440, loss: 0.00607256731018424\n",
            "step: 450, loss: 0.05285625159740448\n",
            "step: 460, loss: 0.1064915582537651\n",
            "step: 470, loss: 0.011799881234765053\n",
            "step: 480, loss: 0.0918971598148346\n",
            "step: 490, loss: 0.009951089508831501\n",
            "step: 500, loss: 0.02158990502357483\n",
            "step: 510, loss: 0.1505831778049469\n",
            "step: 520, loss: 0.02205829694867134\n",
            "step: 530, loss: 0.13754770159721375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9542056074766355, f1=0.9447565543071161, best_f1=0.9447565543071161\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016945890383794904\n",
            "step: 10, loss: 0.048739682883024216\n",
            "step: 20, loss: 0.041346099227666855\n",
            "step: 30, loss: 0.07946321368217468\n",
            "step: 40, loss: 0.008394918404519558\n",
            "step: 50, loss: 0.027042176574468613\n",
            "step: 60, loss: 0.03857841715216637\n",
            "step: 70, loss: 0.0012305558193475008\n",
            "step: 80, loss: 0.0016548385610803962\n",
            "step: 90, loss: 0.10198512673377991\n",
            "step: 100, loss: 0.03830462321639061\n",
            "step: 110, loss: 0.00464896485209465\n",
            "step: 120, loss: 0.0249002818018198\n",
            "step: 130, loss: 0.0034557320177555084\n",
            "step: 140, loss: 0.008202480152249336\n",
            "step: 150, loss: 0.058819323778152466\n",
            "step: 160, loss: 0.009572068229317665\n",
            "step: 170, loss: 0.011615686118602753\n",
            "step: 180, loss: 0.055878277868032455\n",
            "step: 190, loss: 0.0698762908577919\n",
            "step: 200, loss: 0.007105968426913023\n",
            "step: 210, loss: 0.003605601843446493\n",
            "step: 220, loss: 0.00932051707059145\n",
            "step: 230, loss: 0.0019506351090967655\n",
            "step: 240, loss: 0.010737366043031216\n",
            "step: 250, loss: 0.12761788070201874\n",
            "step: 260, loss: 0.0007994546322152019\n",
            "step: 270, loss: 0.018653040751814842\n",
            "step: 280, loss: 0.013206969015300274\n",
            "step: 290, loss: 0.001188413007184863\n",
            "step: 300, loss: 0.05841061472892761\n",
            "step: 310, loss: 0.032105930149555206\n",
            "step: 320, loss: 0.16082637012004852\n",
            "step: 330, loss: 0.002248752396553755\n",
            "step: 340, loss: 0.007726301904767752\n",
            "step: 350, loss: 0.002841444918885827\n",
            "step: 360, loss: 0.004654078744351864\n",
            "step: 370, loss: 0.0008036816143430769\n",
            "step: 380, loss: 0.0026700496673583984\n",
            "step: 390, loss: 0.021099692210555077\n",
            "step: 400, loss: 0.014050963334739208\n",
            "step: 410, loss: 0.041918765753507614\n",
            "step: 420, loss: 0.2040967494249344\n",
            "step: 430, loss: 0.19978344440460205\n",
            "step: 440, loss: 0.0075347451493144035\n",
            "step: 450, loss: 0.014611993916332722\n",
            "step: 460, loss: 0.016975373029708862\n",
            "step: 470, loss: 0.017224902287125587\n",
            "step: 480, loss: 0.019139951094985008\n",
            "step: 490, loss: 0.05787867680191994\n",
            "step: 500, loss: 0.008362739346921444\n",
            "step: 510, loss: 0.01947741210460663\n",
            "step: 520, loss: 0.125391885638237\n",
            "step: 530, loss: 0.023186059668660164\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.951048951048951, f1=0.9423791821561338, best_f1=0.9447565543071161\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02193440869450569\n",
            "step: 10, loss: 0.0027963221073150635\n",
            "step: 20, loss: 0.013721349649131298\n",
            "step: 30, loss: 0.0008170215296559036\n",
            "step: 40, loss: 0.003211370436474681\n",
            "step: 50, loss: 0.0009545133216306567\n",
            "step: 60, loss: 0.029714742675423622\n",
            "step: 70, loss: 0.00046315661165863276\n",
            "step: 80, loss: 0.004436904098838568\n",
            "step: 90, loss: 0.058803949505090714\n",
            "step: 100, loss: 0.014932101592421532\n",
            "step: 110, loss: 0.0035889367572963238\n",
            "step: 120, loss: 0.07879437506198883\n",
            "step: 130, loss: 0.010268568992614746\n",
            "step: 140, loss: 0.0020059761591255665\n",
            "step: 150, loss: 0.0032238962594419718\n",
            "step: 160, loss: 0.09963731467723846\n",
            "step: 170, loss: 0.04762892797589302\n",
            "step: 180, loss: 0.00339901982806623\n",
            "step: 190, loss: 0.113227479159832\n",
            "step: 200, loss: 0.01951337605714798\n",
            "step: 210, loss: 0.033852044492959976\n",
            "step: 220, loss: 0.014307722449302673\n",
            "step: 230, loss: 0.025862596929073334\n",
            "step: 240, loss: 0.0018338320078328252\n",
            "step: 250, loss: 0.050500333309173584\n",
            "step: 260, loss: 0.0006081422325223684\n",
            "step: 270, loss: 0.02647169679403305\n",
            "step: 280, loss: 0.006856406573206186\n",
            "step: 290, loss: 0.007457926869392395\n",
            "step: 300, loss: 0.0073790960013866425\n",
            "step: 310, loss: 0.025386033579707146\n",
            "step: 320, loss: 0.0002959695120807737\n",
            "step: 330, loss: 0.005118971690535545\n",
            "step: 340, loss: 0.0009110938990488648\n",
            "step: 350, loss: 0.018417201936244965\n",
            "step: 360, loss: 0.004387013614177704\n",
            "step: 370, loss: 0.038774292916059494\n",
            "step: 380, loss: 0.001317375572398305\n",
            "step: 390, loss: 0.007505771238356829\n",
            "step: 400, loss: 0.0021684623789042234\n",
            "step: 410, loss: 0.0005466509610414505\n",
            "step: 420, loss: 0.012031152844429016\n",
            "step: 430, loss: 0.002577077364549041\n",
            "step: 440, loss: 0.023680338636040688\n",
            "step: 450, loss: 0.1549837589263916\n",
            "step: 460, loss: 0.003012373112142086\n",
            "step: 470, loss: 0.0014337071916088462\n",
            "step: 480, loss: 0.005941456649452448\n",
            "step: 490, loss: 0.009398638270795345\n",
            "step: 500, loss: 0.03486188128590584\n",
            "step: 510, loss: 0.2158873975276947\n",
            "step: 520, loss: 0.0014942694688215852\n",
            "step: 530, loss: 0.010053979232907295\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.939679547596607, f1=0.9284020862968232, best_f1=0.9447565543071161\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0041841547936201096\n",
            "step: 10, loss: 0.0009535756544210017\n",
            "step: 20, loss: 0.0025403383187949657\n",
            "step: 30, loss: 0.01772012934088707\n",
            "step: 40, loss: 0.006639059633016586\n",
            "step: 50, loss: 0.03328147530555725\n",
            "step: 60, loss: 0.0036344430409371853\n",
            "step: 70, loss: 0.0011449980083853006\n",
            "step: 80, loss: 0.0040032099932432175\n",
            "step: 90, loss: 0.002095466712489724\n",
            "step: 100, loss: 0.058484502136707306\n",
            "step: 110, loss: 0.00041915549081750214\n",
            "step: 120, loss: 0.001046219258569181\n",
            "step: 130, loss: 0.00020191351359244436\n",
            "step: 140, loss: 0.0009225609828718007\n",
            "step: 150, loss: 0.0018878320697695017\n",
            "step: 160, loss: 0.00023387865803670138\n",
            "step: 170, loss: 0.003782265819609165\n",
            "step: 180, loss: 0.0019589969888329506\n",
            "step: 190, loss: 0.0038875604514032602\n",
            "step: 200, loss: 0.0006782782729715109\n",
            "step: 210, loss: 0.009992657229304314\n",
            "step: 220, loss: 0.000997658004052937\n",
            "step: 230, loss: 0.0007313641835935414\n",
            "step: 240, loss: 0.0344170480966568\n",
            "step: 250, loss: 0.01780339516699314\n",
            "step: 260, loss: 0.03895309939980507\n",
            "step: 270, loss: 0.005891980137676001\n",
            "step: 280, loss: 0.005473291501402855\n",
            "step: 290, loss: 0.00921713188290596\n",
            "step: 300, loss: 0.001480131410062313\n",
            "step: 310, loss: 0.0014971448108553886\n",
            "step: 320, loss: 0.07683499157428741\n",
            "step: 330, loss: 0.09624642878770828\n",
            "step: 340, loss: 0.0021475821267813444\n",
            "step: 350, loss: 0.005516918376088142\n",
            "step: 360, loss: 0.010527366772294044\n",
            "step: 370, loss: 0.003802514635026455\n",
            "step: 380, loss: 0.0520879365503788\n",
            "step: 390, loss: 0.0044400948099792\n",
            "step: 400, loss: 0.1321883499622345\n",
            "step: 410, loss: 0.0018694312311708927\n",
            "step: 420, loss: 0.01182466372847557\n",
            "step: 430, loss: 0.003454296151176095\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 440, loss: 0.10132309794425964\n",
            "step: 450, loss: 0.00428317254409194\n",
            "step: 460, loss: 0.0029346062801778316\n",
            "step: 470, loss: 0.16416846215724945\n",
            "step: 480, loss: 0.007800453808158636\n",
            "step: 490, loss: 0.01707109436392784\n",
            "step: 500, loss: 0.012385190464556217\n",
            "step: 510, loss: 0.0012228611158207059\n",
            "step: 520, loss: 0.003531770082190633\n",
            "step: 530, loss: 0.006098998244851828\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9507434944237918, f1=0.9406819243344232, best_f1=0.9447565543071161\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019206051947548985\n",
            "step: 10, loss: 0.010865258984267712\n",
            "step: 20, loss: 0.005915977992117405\n",
            "step: 30, loss: 0.008720132522284985\n",
            "step: 40, loss: 0.0003611368010751903\n",
            "step: 50, loss: 0.00038205640157684684\n",
            "step: 60, loss: 0.022662583738565445\n",
            "step: 70, loss: 0.004961596801877022\n",
            "step: 80, loss: 0.003559354692697525\n",
            "step: 90, loss: 0.004144890233874321\n",
            "step: 100, loss: 0.0036347778514027596\n",
            "step: 110, loss: 0.0016418533632531762\n",
            "step: 120, loss: 0.006343544460833073\n",
            "step: 130, loss: 0.012154273688793182\n",
            "step: 140, loss: 0.03400922194123268\n",
            "step: 150, loss: 0.002922773826867342\n",
            "step: 160, loss: 0.0014243649784475565\n",
            "step: 170, loss: 0.01806371845304966\n",
            "step: 180, loss: 0.0018728792201727629\n",
            "step: 190, loss: 0.0012761109974235296\n",
            "step: 200, loss: 0.007148296106606722\n",
            "step: 210, loss: 0.12962785363197327\n",
            "step: 220, loss: 0.0077002509497106075\n",
            "step: 230, loss: 0.04222767427563667\n",
            "step: 240, loss: 0.043305184692144394\n",
            "step: 250, loss: 0.0015025126049295068\n",
            "step: 260, loss: 0.0007687482866458595\n",
            "step: 270, loss: 0.0037894505076110363\n",
            "step: 280, loss: 0.0010509940329939127\n",
            "step: 290, loss: 0.0034970089327543974\n",
            "step: 300, loss: 0.003382547292858362\n",
            "step: 310, loss: 0.0014286328805610538\n",
            "step: 320, loss: 0.0006224092794582248\n",
            "step: 330, loss: 0.0007020526099950075\n",
            "step: 340, loss: 0.039972901344299316\n",
            "step: 350, loss: 0.00014727974485140294\n",
            "step: 360, loss: 0.00730685843154788\n",
            "step: 370, loss: 0.00235500605776906\n",
            "step: 380, loss: 0.0006780849071219563\n",
            "step: 390, loss: 0.003659845795482397\n",
            "step: 400, loss: 0.004322448279708624\n",
            "step: 410, loss: 0.0034815631806850433\n",
            "step: 420, loss: 0.001974957063794136\n",
            "step: 430, loss: 0.002990745473653078\n",
            "step: 440, loss: 0.017144203186035156\n",
            "step: 450, loss: 0.0023517298977822065\n",
            "step: 460, loss: 0.019183047115802765\n",
            "step: 470, loss: 0.13373161852359772\n",
            "step: 480, loss: 0.007870492525398731\n",
            "step: 490, loss: 0.03192789480090141\n",
            "step: 500, loss: 0.0013474165461957455\n",
            "step: 510, loss: 0.005898824892938137\n",
            "step: 520, loss: 0.02756776288151741\n",
            "step: 530, loss: 0.021487072110176086\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9552238805970149, f1=0.9477611940298507, best_f1=0.9477611940298507\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00071014987770468\n",
            "step: 10, loss: 0.002311386400833726\n",
            "step: 20, loss: 0.0004911215510219336\n",
            "step: 30, loss: 0.07836867868900299\n",
            "step: 40, loss: 0.012581465765833855\n",
            "step: 50, loss: 0.0038473522290587425\n",
            "step: 60, loss: 0.00506492517888546\n",
            "step: 70, loss: 0.01368271466344595\n",
            "step: 80, loss: 0.0057622650638222694\n",
            "step: 90, loss: 0.02972980961203575\n",
            "step: 100, loss: 0.001308218576014042\n",
            "step: 110, loss: 0.0030736951157450676\n",
            "step: 120, loss: 0.0013617640361189842\n",
            "step: 130, loss: 0.000869410578161478\n",
            "step: 140, loss: 0.0006821901770308614\n",
            "step: 150, loss: 0.04195883125066757\n",
            "step: 160, loss: 0.0004156018840149045\n",
            "step: 170, loss: 0.002980210119858384\n",
            "step: 180, loss: 0.000532334262970835\n",
            "step: 190, loss: 0.000522100308444351\n",
            "step: 200, loss: 0.0005691102705895901\n",
            "step: 210, loss: 0.20318008959293365\n",
            "step: 220, loss: 0.0024973335675895214\n",
            "step: 230, loss: 0.0017257985891774297\n",
            "step: 240, loss: 0.001588917337357998\n",
            "step: 250, loss: 0.01069992408156395\n",
            "step: 260, loss: 0.0005297359311953187\n",
            "step: 270, loss: 0.0009490235825069249\n",
            "step: 280, loss: 0.00048532593064010143\n",
            "step: 290, loss: 0.004723472520709038\n",
            "step: 300, loss: 0.0036212143022567034\n",
            "step: 310, loss: 0.0008829139405861497\n",
            "step: 320, loss: 0.0005699383327737451\n",
            "step: 330, loss: 0.000447666592663154\n",
            "step: 340, loss: 0.0005850886809639633\n",
            "step: 350, loss: 0.06718646734952927\n",
            "step: 360, loss: 0.0016432985430583358\n",
            "step: 370, loss: 0.16487659513950348\n",
            "step: 380, loss: 0.0034343767911195755\n",
            "step: 390, loss: 0.004847930744290352\n",
            "step: 400, loss: 0.060234468430280685\n",
            "step: 410, loss: 0.022829430177807808\n",
            "step: 420, loss: 0.0019362177699804306\n",
            "step: 430, loss: 0.0011209234362468123\n",
            "step: 440, loss: 0.00020693267288152128\n",
            "step: 450, loss: 0.025436710566282272\n",
            "step: 460, loss: 0.00032058756914921105\n",
            "step: 470, loss: 0.0007743436144664884\n",
            "step: 480, loss: 0.0030932778026908636\n",
            "step: 490, loss: 0.0030832639895379543\n",
            "step: 500, loss: 0.016558660194277763\n",
            "step: 510, loss: 0.03168904408812523\n",
            "step: 520, loss: 0.0030171589460223913\n",
            "step: 530, loss: 0.043983619660139084\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9553072625698324, f1=0.9401309635173059, best_f1=0.9401309635173059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009976224973797798\n",
            "step: 10, loss: 0.0011306960368528962\n",
            "step: 20, loss: 0.00015873383381403983\n",
            "step: 30, loss: 0.030487509444355965\n",
            "step: 40, loss: 0.0005366219556890428\n",
            "step: 50, loss: 0.0006892002420499921\n",
            "step: 60, loss: 0.011690470390021801\n",
            "step: 70, loss: 0.00016034946020226926\n",
            "step: 80, loss: 0.0002340311330044642\n",
            "step: 90, loss: 0.0019962750375270844\n",
            "step: 100, loss: 0.0010912524303421378\n",
            "step: 110, loss: 0.007926961407065392\n",
            "step: 120, loss: 0.00013700684939976782\n",
            "step: 130, loss: 0.010461152531206608\n",
            "step: 140, loss: 0.009703652933239937\n",
            "step: 150, loss: 0.0037412059027701616\n",
            "step: 160, loss: 0.026477716863155365\n",
            "step: 170, loss: 0.017896300181746483\n",
            "step: 180, loss: 0.01488988846540451\n",
            "step: 190, loss: 0.0001825814979383722\n",
            "step: 200, loss: 0.0013085838872939348\n",
            "step: 210, loss: 0.006266804412007332\n",
            "step: 220, loss: 0.00046296193613670766\n",
            "step: 230, loss: 0.00643958942964673\n",
            "step: 240, loss: 0.0029146091546863317\n",
            "step: 250, loss: 0.004525250289589167\n",
            "step: 260, loss: 0.006069452501833439\n",
            "step: 270, loss: 0.013669655658304691\n",
            "step: 280, loss: 0.010769391432404518\n",
            "step: 290, loss: 0.0007578355143778026\n",
            "step: 300, loss: 0.01785266026854515\n",
            "step: 310, loss: 0.011683288961648941\n",
            "step: 320, loss: 0.003919489216059446\n",
            "step: 330, loss: 0.006708613596856594\n",
            "step: 340, loss: 0.00012721731036435813\n",
            "step: 350, loss: 0.09346777945756912\n",
            "step: 360, loss: 0.00021781674877274781\n",
            "step: 370, loss: 0.0011054769856855273\n",
            "step: 380, loss: 0.0017648829380050302\n",
            "step: 390, loss: 0.0004778634465765208\n",
            "step: 400, loss: 0.020897461101412773\n",
            "step: 410, loss: 0.00025225180434063077\n",
            "step: 420, loss: 0.00010982620733557269\n",
            "step: 430, loss: 5.926869926042855e-05\n",
            "step: 440, loss: 0.00014386739348992705\n",
            "step: 450, loss: 0.0027737990021705627\n",
            "step: 460, loss: 0.0032577079255133867\n",
            "step: 470, loss: 0.00047128243022598326\n",
            "step: 480, loss: 0.000731186824850738\n",
            "step: 490, loss: 0.0013191611506044865\n",
            "step: 500, loss: 0.02535558119416237\n",
            "step: 510, loss: 0.07476960867643356\n",
            "step: 520, loss: 0.0777757316827774\n",
            "step: 530, loss: 0.011967122554779053\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9521152952115295, f1=0.9395348837209302, best_f1=0.9401309635173059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001000558040686883\n",
            "step: 10, loss: 0.0019097620388492942\n",
            "step: 20, loss: 0.0003651557199191302\n",
            "step: 30, loss: 0.0027731447480618954\n",
            "step: 40, loss: 0.000688264612108469\n",
            "step: 50, loss: 0.0024108279030770063\n",
            "step: 60, loss: 0.001566292834468186\n",
            "step: 70, loss: 0.001073856488801539\n",
            "step: 80, loss: 0.008687414228916168\n",
            "step: 90, loss: 0.010892585851252079\n",
            "step: 100, loss: 0.008663121610879898\n",
            "step: 110, loss: 0.0003427068586461246\n",
            "step: 120, loss: 0.006297205574810505\n",
            "step: 130, loss: 0.0009523631306365132\n",
            "step: 140, loss: 0.00047360730241052806\n",
            "step: 150, loss: 0.010842467658221722\n",
            "step: 160, loss: 0.0004847443487960845\n",
            "step: 170, loss: 0.000852097466122359\n",
            "step: 180, loss: 0.0002154172252630815\n",
            "step: 190, loss: 0.005739058367908001\n",
            "step: 200, loss: 0.0004647981550078839\n",
            "step: 210, loss: 0.0005375635228119791\n",
            "step: 220, loss: 0.03737970069050789\n",
            "step: 230, loss: 0.0016198876546695828\n",
            "step: 240, loss: 0.0011101925047114491\n",
            "step: 250, loss: 0.006006523035466671\n",
            "step: 260, loss: 0.0010844907956197858\n",
            "step: 270, loss: 0.003537493757903576\n",
            "step: 280, loss: 0.0037276397924870253\n",
            "step: 290, loss: 0.012525452300906181\n",
            "step: 300, loss: 7.932820153655484e-05\n",
            "step: 310, loss: 0.08078023046255112\n",
            "step: 320, loss: 0.0005072678904980421\n",
            "step: 330, loss: 8.290849655168131e-05\n",
            "step: 340, loss: 0.002103975508362055\n",
            "step: 350, loss: 0.0008747795945964754\n",
            "step: 360, loss: 0.0019410697277635336\n",
            "step: 370, loss: 0.001390046556480229\n",
            "step: 380, loss: 0.008472390472888947\n",
            "step: 390, loss: 0.0019288567127659917\n",
            "step: 400, loss: 0.0019521935610100627\n",
            "step: 410, loss: 0.0031845879275351763\n",
            "step: 420, loss: 0.0020231720991432667\n",
            "step: 430, loss: 0.002125658793374896\n",
            "step: 440, loss: 0.00016742463049013168\n",
            "step: 450, loss: 0.00189426657743752\n",
            "step: 460, loss: 0.0012698685750365257\n",
            "step: 470, loss: 0.0017041245009750128\n",
            "step: 480, loss: 0.0013725502649322152\n",
            "step: 490, loss: 0.0014767699176445603\n",
            "step: 500, loss: 0.0010484873782843351\n",
            "step: 510, loss: 0.0011398263741284609\n",
            "step: 520, loss: 0.00614657299593091\n",
            "step: 530, loss: 0.007391444873064756\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9513212795549374, f1=0.9428704133766836, best_f1=0.9401309635173059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014422694221138954\n",
            "step: 10, loss: 0.003238589968532324\n",
            "step: 20, loss: 0.0615772120654583\n",
            "step: 30, loss: 0.0016494692536070943\n",
            "step: 40, loss: 0.0027673018630594015\n",
            "step: 50, loss: 0.001230378053151071\n",
            "step: 60, loss: 0.017192212864756584\n",
            "step: 70, loss: 0.02582237310707569\n",
            "step: 80, loss: 0.2843347191810608\n",
            "step: 90, loss: 0.007435736246407032\n",
            "step: 100, loss: 0.007282155100256205\n",
            "step: 110, loss: 0.0020361016504466534\n",
            "step: 120, loss: 0.0001492744340794161\n",
            "step: 130, loss: 0.0007107518031261861\n",
            "step: 140, loss: 0.00016580361989326775\n",
            "step: 150, loss: 0.0009632413275539875\n",
            "step: 160, loss: 0.005768784787505865\n",
            "step: 170, loss: 0.0005120148998685181\n",
            "step: 180, loss: 0.002480539260432124\n",
            "step: 190, loss: 0.00032284590997733176\n",
            "step: 200, loss: 0.0008851474267430604\n",
            "step: 210, loss: 0.0016027861274778843\n",
            "step: 220, loss: 0.0004544316907413304\n",
            "step: 230, loss: 0.002415132476016879\n",
            "step: 240, loss: 0.0036966681946069\n",
            "step: 250, loss: 0.22486895322799683\n",
            "step: 260, loss: 0.006617333739995956\n",
            "step: 270, loss: 0.005790180992335081\n",
            "step: 280, loss: 0.001208052970468998\n",
            "step: 290, loss: 0.002818757202476263\n",
            "step: 300, loss: 0.001864681369625032\n",
            "step: 310, loss: 0.006020802538841963\n",
            "step: 320, loss: 0.007552598137408495\n",
            "step: 330, loss: 0.000976661453023553\n",
            "step: 340, loss: 0.0009066408383660018\n",
            "step: 350, loss: 0.00015589222311973572\n",
            "step: 360, loss: 0.0002512051141820848\n",
            "step: 370, loss: 0.0015695232432335615\n",
            "step: 380, loss: 0.0004541230446193367\n",
            "step: 390, loss: 0.0392950177192688\n",
            "step: 400, loss: 0.0023522183764725924\n",
            "step: 410, loss: 0.0017387184780091047\n",
            "step: 420, loss: 0.00037689178134314716\n",
            "step: 430, loss: 0.0004771612584590912\n",
            "step: 440, loss: 0.0033039022237062454\n",
            "step: 450, loss: 0.09919843077659607\n",
            "step: 460, loss: 0.0004816099535673857\n",
            "step: 470, loss: 0.0014539632247760892\n",
            "step: 480, loss: 0.0004830543475691229\n",
            "step: 490, loss: 0.004059716127812862\n",
            "step: 500, loss: 0.0009145071962848306\n",
            "step: 510, loss: 0.008727718144655228\n",
            "step: 520, loss: 0.0034723978023976088\n",
            "step: 530, loss: 0.002985855797305703\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9519813519813519, f1=0.9386991109031353, best_f1=0.9401309635173059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003120788314845413\n",
            "step: 10, loss: 0.0004238539841026068\n",
            "step: 20, loss: 0.0006967085646465421\n",
            "step: 30, loss: 0.00011697584704961628\n",
            "step: 40, loss: 0.0002654220152180642\n",
            "step: 50, loss: 0.0008990766364149749\n",
            "step: 60, loss: 0.00014387082774192095\n",
            "step: 70, loss: 0.0027672722935676575\n",
            "step: 80, loss: 0.010472938418388367\n",
            "step: 90, loss: 0.0003670849837362766\n",
            "step: 100, loss: 0.000499542395118624\n",
            "step: 110, loss: 0.00040804274613037705\n",
            "step: 120, loss: 0.00011626350897131488\n",
            "step: 130, loss: 0.0006402336293831468\n",
            "step: 140, loss: 0.00016748129564803094\n",
            "step: 150, loss: 0.0013794589322060347\n",
            "step: 160, loss: 0.0005199673469178379\n",
            "step: 170, loss: 0.203932985663414\n",
            "step: 180, loss: 0.0004810614336747676\n",
            "step: 190, loss: 0.0003682238457258791\n",
            "step: 200, loss: 0.00023435910406988114\n",
            "step: 210, loss: 0.0001890283019747585\n",
            "step: 220, loss: 0.0007740436121821404\n",
            "step: 230, loss: 0.0003056245041079819\n",
            "step: 240, loss: 0.01138278841972351\n",
            "step: 250, loss: 0.00014478107914328575\n",
            "step: 260, loss: 0.00021528829529415816\n",
            "step: 270, loss: 0.028225358575582504\n",
            "step: 280, loss: 0.0015932932728901505\n",
            "step: 290, loss: 8.797529153525829e-05\n",
            "step: 300, loss: 0.0003118881140835583\n",
            "step: 310, loss: 0.000873104901984334\n",
            "step: 320, loss: 0.0001690835488261655\n",
            "step: 330, loss: 0.00013560423394665122\n",
            "step: 340, loss: 0.00016694673104211688\n",
            "step: 350, loss: 0.0007292545633390546\n",
            "step: 360, loss: 0.09224891662597656\n",
            "step: 370, loss: 0.0007569835870526731\n",
            "step: 380, loss: 0.0006269195000641048\n",
            "step: 390, loss: 0.0005613019457086921\n",
            "step: 400, loss: 0.0009742798283696175\n",
            "step: 410, loss: 0.00014258523879107088\n",
            "step: 420, loss: 0.00033588253427296877\n",
            "step: 430, loss: 0.027074959129095078\n",
            "step: 440, loss: 6.445019243983552e-05\n",
            "step: 450, loss: 0.0007411550614051521\n",
            "step: 460, loss: 0.006918574217706919\n",
            "step: 470, loss: 0.000506464799400419\n",
            "step: 480, loss: 0.0019125047838315368\n",
            "step: 490, loss: 0.0022309215273708105\n",
            "step: 500, loss: 0.0001435356680303812\n",
            "step: 510, loss: 0.0022635094355791807\n",
            "step: 520, loss: 0.00038543675327673554\n",
            "step: 530, loss: 0.0005996098043397069\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9530516431924883, f1=0.9362907031618688, best_f1=0.9401309635173059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003165921662002802\n",
            "step: 10, loss: 0.00102839688770473\n",
            "step: 20, loss: 0.001819205703213811\n",
            "step: 30, loss: 0.001081134076230228\n",
            "step: 40, loss: 0.0010621313704177737\n",
            "step: 50, loss: 0.017583578824996948\n",
            "step: 60, loss: 0.0008672701078467071\n",
            "step: 70, loss: 0.0006404023733921349\n",
            "step: 80, loss: 0.00016632424376439303\n",
            "step: 90, loss: 0.0002555109967943281\n",
            "step: 100, loss: 0.00043633513269014657\n",
            "step: 110, loss: 0.0003406973264645785\n",
            "step: 120, loss: 4.552491009235382e-05\n",
            "step: 130, loss: 6.163900252431631e-05\n",
            "step: 140, loss: 0.0012639994965866208\n",
            "step: 150, loss: 0.0007797794532962143\n",
            "step: 160, loss: 0.0004686517349909991\n",
            "step: 170, loss: 0.02062404155731201\n",
            "step: 180, loss: 0.0002286303206346929\n",
            "step: 190, loss: 0.001738423714414239\n",
            "step: 200, loss: 7.890831329859793e-05\n",
            "step: 210, loss: 0.00018107589858118445\n",
            "step: 220, loss: 7.630043546669185e-05\n",
            "step: 230, loss: 0.0001952486636582762\n",
            "step: 240, loss: 0.0007169988821260631\n",
            "step: 250, loss: 0.042740095406770706\n",
            "step: 260, loss: 0.0008523592259734869\n",
            "step: 270, loss: 0.0006419589044526219\n",
            "step: 280, loss: 0.00036193677806295455\n",
            "step: 290, loss: 0.0002608936920296401\n",
            "step: 300, loss: 9.585882071405649e-05\n",
            "step: 310, loss: 0.006524840835481882\n",
            "step: 320, loss: 5.8439120039111e-05\n",
            "step: 330, loss: 0.0005194316618144512\n",
            "step: 340, loss: 0.00046790947089903057\n",
            "step: 350, loss: 9.413600491825491e-05\n",
            "step: 360, loss: 0.0007144242408685386\n",
            "step: 370, loss: 0.0009965515928342938\n",
            "step: 380, loss: 0.00078967324225232\n",
            "step: 390, loss: 0.015832198783755302\n",
            "step: 400, loss: 0.0005470269825309515\n",
            "step: 410, loss: 9.367903112433851e-05\n",
            "step: 420, loss: 0.0004584876587614417\n",
            "step: 430, loss: 0.0009494818514212966\n",
            "step: 440, loss: 0.0005222384352236986\n",
            "step: 450, loss: 0.00010116813791682944\n",
            "step: 460, loss: 0.02519332244992256\n",
            "step: 470, loss: 0.0011648592771962285\n",
            "step: 480, loss: 0.0004084585525561124\n",
            "step: 490, loss: 0.0008827819838188589\n",
            "step: 500, loss: 0.0013422733172774315\n",
            "step: 510, loss: 0.04109785705804825\n",
            "step: 520, loss: 0.0004097700584679842\n",
            "step: 530, loss: 0.135600745677948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9508804448563485, f1=0.9422632794457274, best_f1=0.9401309635173059\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 8.40523571241647e-05\n",
            "step: 10, loss: 0.0011047481093555689\n",
            "step: 20, loss: 0.0003067019861191511\n",
            "step: 30, loss: 0.007369996514171362\n",
            "step: 40, loss: 0.0002269558171974495\n",
            "step: 50, loss: 0.0005353505839593709\n",
            "step: 60, loss: 0.0007733466336503625\n",
            "step: 70, loss: 0.00045332417357712984\n",
            "step: 80, loss: 0.00014731069677509367\n",
            "step: 90, loss: 0.0007991233142092824\n",
            "step: 100, loss: 0.0007062916993163526\n",
            "step: 110, loss: 0.0004284815222490579\n",
            "step: 120, loss: 6.37167613604106e-05\n",
            "step: 130, loss: 6.243526877369732e-05\n",
            "step: 140, loss: 3.133173959213309e-05\n",
            "step: 150, loss: 7.890728738857433e-05\n",
            "step: 160, loss: 0.00010227632446913049\n",
            "step: 170, loss: 0.00010123423271579668\n",
            "step: 180, loss: 0.008243363350629807\n",
            "step: 190, loss: 0.0008681448525749147\n",
            "step: 200, loss: 0.0005257417797110975\n",
            "step: 210, loss: 0.000247410120209679\n",
            "step: 220, loss: 2.332654366909992e-05\n",
            "step: 230, loss: 0.07368088513612747\n",
            "step: 240, loss: 3.570380067685619e-05\n",
            "step: 250, loss: 2.200853850808926e-05\n",
            "step: 260, loss: 2.693166970857419e-05\n",
            "step: 270, loss: 1.5191521924862172e-05\n",
            "step: 280, loss: 2.5353041564812884e-05\n",
            "step: 290, loss: 0.003606627229601145\n",
            "step: 300, loss: 0.00026545338914729655\n",
            "step: 310, loss: 0.04885741323232651\n",
            "step: 320, loss: 8.95270422915928e-05\n",
            "step: 330, loss: 2.7248264814261347e-05\n",
            "step: 340, loss: 8.852910104906186e-05\n",
            "step: 350, loss: 0.00028753711376339197\n",
            "step: 360, loss: 0.0006503452896140516\n",
            "step: 370, loss: 0.010990628972649574\n",
            "step: 380, loss: 7.557598291896284e-05\n",
            "step: 390, loss: 0.0009535133140161633\n",
            "step: 400, loss: 0.0004516319604590535\n",
            "step: 410, loss: 4.111522139282897e-05\n",
            "step: 420, loss: 0.0003444109170231968\n",
            "step: 430, loss: 2.715970913413912e-05\n",
            "step: 440, loss: 0.01502978801727295\n",
            "step: 450, loss: 0.01741507090628147\n",
            "step: 460, loss: 0.0057188053615391254\n",
            "step: 470, loss: 3.0436558517976664e-05\n",
            "step: 480, loss: 0.0015186493983492255\n",
            "step: 490, loss: 0.005112168844789267\n",
            "step: 500, loss: 0.015098005533218384\n",
            "step: 510, loss: 0.0001098588909371756\n",
            "step: 520, loss: 2.4473678422509693e-05\n",
            "step: 530, loss: 0.00012455855903681368\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9510945505356311, f1=0.9414498141263941, best_f1=0.9401309635173059\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:33, 170.91it/s]\n",
            "load_f1 = 0.9528257823446986\n",
            "real_f1 = 0.9522024367385192\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 143.90it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE"
      ],
      "metadata": {
        "id": "U6rVRw-HgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE STRUCTURED"
      ],
      "metadata": {
        "id": "3ifpsOJMgNFH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGz87W_6gNFI"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xE7bDM3EgNFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af631d92-e430-4f16-9496-e5a7b050c5fc"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.44288310408592224\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.2916666666666667, f1=0.2692307692307693, best_f1=0.2692307692307693\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.44488978385925293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.3888888888888889, f1=0.34285714285714286, best_f1=0.34285714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.40789881348609924\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.3373493975903615, f1=0.27184466019417475, best_f1=0.34285714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2661260962486267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.32558139534883723, f1=0.27184466019417475, best_f1=0.34285714285714286\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3470004200935364\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.4444444444444445, f1=0.3181818181818182, best_f1=0.3181818181818182\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17063285410404205\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.42622950819672134, f1=0.2978723404255319, best_f1=0.3181818181818182\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4828394651412964\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.52, f1=0.37333333333333335, best_f1=0.37333333333333335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32334354519844055\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.4827586206896552, f1=0.358974358974359, best_f1=0.37333333333333335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3611554503440857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.75, f1=0.48888888888888893, best_f1=0.48888888888888893\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.19394394755363464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6341463414634146, f1=0.4210526315789474, best_f1=0.48888888888888893\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.20908652245998383\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.7857142857142857, f1=0.5116279069767441, best_f1=0.5116279069767441\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11156976222991943\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.8275862068965518, f1=0.5, best_f1=0.5\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07442896068096161\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8571428571428571, f1=0.5116279069767441, best_f1=0.5116279069767441\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02772248536348343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8571428571428571, f1=0.5116279069767441, best_f1=0.5116279069767441\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10652393847703934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8571428571428571, f1=0.5116279069767441, best_f1=0.5116279069767441\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 111831.72it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.8571428571428571\n",
            "real_f1 = 0.8148148148148148\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:26, 164.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "M1GZmC0LgNFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "zyjgIIwdgNFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd0182c-7637-4887-e8b9-acfd7a67d66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 402kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 808kB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 465kB/s] \n",
            "Downloading: 100% 501M/501M [00:08<00:00, 60.7MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5874584913253784\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.45140376687049866\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.5111132860183716\n",
            "step: 30, loss: 0.2837994694709778\n",
            "step: 40, loss: 0.41759932041168213\n",
            "step: 50, loss: 0.543333888053894\n",
            "step: 60, loss: 0.2946127653121948\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 70, loss: 0.2058243602514267\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 80, loss: 0.587806761264801\n",
            "step: 90, loss: 0.28936052322387695\n",
            "step: 100, loss: 0.17613622546195984\n",
            "step: 110, loss: 0.11514599621295929\n",
            "step: 120, loss: 0.015885699540376663\n",
            "step: 130, loss: 0.005764250177890062\n",
            "step: 140, loss: 0.015761306509375572\n",
            "step: 150, loss: 0.06328599900007248\n",
            "step: 160, loss: 0.023276785388588905\n",
            "step: 170, loss: 0.07634744793176651\n",
            "step: 180, loss: 0.11228172481060028\n",
            "step: 190, loss: 0.019163645803928375\n",
            "step: 200, loss: 0.050970807671546936\n",
            "step: 210, loss: 0.012772344052791595\n",
            "step: 220, loss: 0.16693054139614105\n",
            "step: 230, loss: 0.016894962638616562\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9562363238512035, f1=0.9646017699115044, best_f1=0.9646017699115044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006803951226174831\n",
            "step: 10, loss: 0.074605792760849\n",
            "step: 20, loss: 0.022260962054133415\n",
            "step: 30, loss: 0.014167780056595802\n",
            "step: 40, loss: 0.006572495214641094\n",
            "step: 50, loss: 0.0012891343794763088\n",
            "step: 60, loss: 0.005202824715524912\n",
            "step: 70, loss: 0.015608426183462143\n",
            "step: 80, loss: 0.0038929732982069254\n",
            "step: 90, loss: 0.0013165355194360018\n",
            "step: 100, loss: 0.002704617567360401\n",
            "step: 110, loss: 0.061507850885391235\n",
            "step: 120, loss: 0.00114253640640527\n",
            "step: 130, loss: 0.004052772186696529\n",
            "step: 140, loss: 0.0022636314388364553\n",
            "step: 150, loss: 0.13831950724124908\n",
            "step: 160, loss: 0.004431865643709898\n",
            "step: 170, loss: 0.008387415669858456\n",
            "step: 180, loss: 0.0015908795176073909\n",
            "step: 190, loss: 0.03249112516641617\n",
            "step: 200, loss: 0.0010587162105366588\n",
            "step: 210, loss: 0.05850614607334137\n",
            "step: 220, loss: 0.002049278002232313\n",
            "step: 230, loss: 0.002775452798232436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9788182831661093, f1=0.9775784753363228, best_f1=0.9775784753363228\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.059696175158023834\n",
            "step: 10, loss: 0.0023161827120929956\n",
            "step: 20, loss: 0.003911598585546017\n",
            "step: 30, loss: 0.0008613210520707071\n",
            "step: 40, loss: 0.0013093145098537207\n",
            "step: 50, loss: 0.051522932946681976\n",
            "step: 60, loss: 0.05756384879350662\n",
            "step: 70, loss: 0.0013612705515697598\n",
            "step: 80, loss: 0.013513020239770412\n",
            "step: 90, loss: 0.0009028029162436724\n",
            "step: 100, loss: 0.0009078781004063785\n",
            "step: 110, loss: 0.007591970730572939\n",
            "step: 120, loss: 0.00043979694601148367\n",
            "step: 130, loss: 0.020047351717948914\n",
            "step: 140, loss: 0.01637950725853443\n",
            "step: 150, loss: 0.003738304367288947\n",
            "step: 160, loss: 0.01176741998642683\n",
            "step: 170, loss: 0.018737779930233955\n",
            "step: 180, loss: 0.09030510485172272\n",
            "step: 190, loss: 0.034087955951690674\n",
            "step: 200, loss: 0.026625845581293106\n",
            "step: 210, loss: 0.003592089982703328\n",
            "step: 220, loss: 0.0061670634895563126\n",
            "step: 230, loss: 0.019799504429101944\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9820627802690582, f1=0.9786276715410572, best_f1=0.9786276715410572\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00917510874569416\n",
            "step: 10, loss: 0.006820904091000557\n",
            "step: 20, loss: 0.0052161565981805325\n",
            "step: 30, loss: 0.0007594324997626245\n",
            "step: 40, loss: 0.12963329255580902\n",
            "step: 50, loss: 0.041292015463113785\n",
            "step: 60, loss: 0.0018109636148437858\n",
            "step: 70, loss: 0.003133026883006096\n",
            "step: 80, loss: 0.0014099081745371222\n",
            "step: 90, loss: 0.01124762836843729\n",
            "step: 100, loss: 0.0014029167359694839\n",
            "step: 110, loss: 0.0005592048401013017\n",
            "step: 120, loss: 0.00937657617032528\n",
            "step: 130, loss: 0.019461773335933685\n",
            "step: 140, loss: 0.0006266534910537302\n",
            "step: 150, loss: 0.0007796703139320016\n",
            "step: 160, loss: 0.0011303500505164266\n",
            "step: 170, loss: 0.11606352776288986\n",
            "step: 180, loss: 0.12078585475683212\n",
            "step: 190, loss: 0.003643014468252659\n",
            "step: 200, loss: 0.023317262530326843\n",
            "step: 210, loss: 0.0011409125290811062\n",
            "step: 220, loss: 0.0003858719137497246\n",
            "step: 230, loss: 0.0033663902431726456\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9876543209876544, f1=0.9821029082774049, best_f1=0.9821029082774049\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009044855833053589\n",
            "step: 10, loss: 0.0058761779218912125\n",
            "step: 20, loss: 0.0010562833631411195\n",
            "step: 30, loss: 0.0004658415273297578\n",
            "step: 40, loss: 0.001630284357815981\n",
            "step: 50, loss: 0.0008075972436927259\n",
            "step: 60, loss: 0.001390896737575531\n",
            "step: 70, loss: 0.002583626192063093\n",
            "step: 80, loss: 0.07010243088006973\n",
            "step: 90, loss: 0.04627065733075142\n",
            "step: 100, loss: 0.0006005041650496423\n",
            "step: 110, loss: 0.0177837572991848\n",
            "step: 120, loss: 0.002018429571762681\n",
            "step: 130, loss: 0.0006459909491240978\n",
            "step: 140, loss: 0.0011848622234538198\n",
            "step: 150, loss: 0.10455508530139923\n",
            "step: 160, loss: 0.0010035211453214288\n",
            "step: 170, loss: 0.0014041168615221977\n",
            "step: 180, loss: 0.0025947033427655697\n",
            "step: 190, loss: 0.00668276147916913\n",
            "step: 200, loss: 0.001683026785030961\n",
            "step: 210, loss: 0.017323032021522522\n",
            "step: 220, loss: 0.0006582676433026791\n",
            "step: 230, loss: 0.004202922340482473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9865470852017937, f1=0.984304932735426, best_f1=0.9821029082774049\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015424530953168869\n",
            "step: 10, loss: 0.0010495560709387064\n",
            "step: 20, loss: 0.0026341050397604704\n",
            "step: 30, loss: 0.0013133998727425933\n",
            "step: 40, loss: 0.0005796718178316951\n",
            "step: 50, loss: 0.0006237680790945888\n",
            "step: 60, loss: 0.0012657905463129282\n",
            "step: 70, loss: 0.00034515498555265367\n",
            "step: 80, loss: 0.0012562331976369023\n",
            "step: 90, loss: 0.003974954131990671\n",
            "step: 100, loss: 0.0010540225775912404\n",
            "step: 110, loss: 0.007259138859808445\n",
            "step: 120, loss: 0.0004458418698050082\n",
            "step: 130, loss: 0.0006364120054058731\n",
            "step: 140, loss: 0.0011242295149713755\n",
            "step: 150, loss: 0.00030548000358976424\n",
            "step: 160, loss: 0.04272085428237915\n",
            "step: 170, loss: 0.0005069730104878545\n",
            "step: 180, loss: 0.0002049948088824749\n",
            "step: 190, loss: 0.00036896273377351463\n",
            "step: 200, loss: 0.002012969460338354\n",
            "step: 210, loss: 0.06289268285036087\n",
            "step: 220, loss: 0.015184693969786167\n",
            "step: 230, loss: 0.0025791707448661327\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.988814317673378, f1=0.9799554565701558, best_f1=0.9799554565701558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019305545138195157\n",
            "step: 10, loss: 0.0007999589433893561\n",
            "step: 20, loss: 0.0011045782594010234\n",
            "step: 30, loss: 0.0005673918058164418\n",
            "step: 40, loss: 0.0018239174969494343\n",
            "step: 50, loss: 0.0009913714602589607\n",
            "step: 60, loss: 0.0004955098265781999\n",
            "step: 70, loss: 0.0005470045143738389\n",
            "step: 80, loss: 0.0006496031419374049\n",
            "step: 90, loss: 0.06560084223747253\n",
            "step: 100, loss: 0.0005230549140833318\n",
            "step: 110, loss: 0.0013855358120054007\n",
            "step: 120, loss: 0.0008934124489314854\n",
            "step: 130, loss: 0.0025507549289613962\n",
            "step: 140, loss: 0.0019074113806709647\n",
            "step: 150, loss: 0.007187935058027506\n",
            "step: 160, loss: 0.004452361259609461\n",
            "step: 170, loss: 0.0005371003062464297\n",
            "step: 180, loss: 0.00033585919300094247\n",
            "step: 190, loss: 0.0009358323877677321\n",
            "step: 200, loss: 0.0006380395498126745\n",
            "step: 210, loss: 0.014889808371663094\n",
            "step: 220, loss: 0.001417951425537467\n",
            "step: 230, loss: 0.0663522332906723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9876265466816648, f1=0.9810055865921787, best_f1=0.9799554565701558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0024885740131139755\n",
            "step: 10, loss: 0.0029163481667637825\n",
            "step: 20, loss: 0.002770679770037532\n",
            "step: 30, loss: 0.001321515184827149\n",
            "step: 40, loss: 0.0013461180496960878\n",
            "step: 50, loss: 0.0007934514433145523\n",
            "step: 60, loss: 0.0009270465816371143\n",
            "step: 70, loss: 0.00033038645051419735\n",
            "step: 80, loss: 0.00047175641520880163\n",
            "step: 90, loss: 0.0005870468448847532\n",
            "step: 100, loss: 0.0013269629562273622\n",
            "step: 110, loss: 0.0016604817938059568\n",
            "step: 120, loss: 0.0016919111367315054\n",
            "step: 130, loss: 0.0015916073461994529\n",
            "step: 140, loss: 0.0008135454263538122\n",
            "step: 150, loss: 0.014217502437531948\n",
            "step: 160, loss: 0.00071366922929883\n",
            "step: 170, loss: 0.010036608204245567\n",
            "step: 180, loss: 0.0004700912395492196\n",
            "step: 190, loss: 0.00048751739086583257\n",
            "step: 200, loss: 0.016871437430381775\n",
            "step: 210, loss: 0.0049136122688651085\n",
            "step: 220, loss: 0.0009975965367630124\n",
            "step: 230, loss: 0.0006426554173231125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9854748603351955, f1=0.9821029082774049, best_f1=0.9799554565701558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004915558383800089\n",
            "step: 10, loss: 0.0009601548663340509\n",
            "step: 20, loss: 0.010857946239411831\n",
            "step: 30, loss: 0.004539794288575649\n",
            "step: 40, loss: 0.0007869211258366704\n",
            "step: 50, loss: 0.0013106483966112137\n",
            "step: 60, loss: 0.00038054437027312815\n",
            "step: 70, loss: 0.05782892554998398\n",
            "step: 80, loss: 0.0069616809487342834\n",
            "step: 90, loss: 0.01567225530743599\n",
            "step: 100, loss: 0.00031128356931731105\n",
            "step: 110, loss: 0.0002849942829925567\n",
            "step: 120, loss: 0.010820221155881882\n",
            "step: 130, loss: 0.016112273558974266\n",
            "step: 140, loss: 0.0009254488977603614\n",
            "step: 150, loss: 0.0005666646757163107\n",
            "step: 160, loss: 0.0041363914497196674\n",
            "step: 170, loss: 0.0002446429571136832\n",
            "step: 180, loss: 0.0004091874579899013\n",
            "step: 190, loss: 0.00022567635460291058\n",
            "step: 200, loss: 0.0007837151642888784\n",
            "step: 210, loss: 0.0016297245165333152\n",
            "step: 220, loss: 0.0008079588878899813\n",
            "step: 230, loss: 0.0006169406697154045\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9876819708846584, f1=0.984304932735426, best_f1=0.9799554565701558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009333347552455962\n",
            "step: 10, loss: 0.0022656794171780348\n",
            "step: 20, loss: 0.0007425628136843443\n",
            "step: 30, loss: 0.00014388997806236148\n",
            "step: 40, loss: 0.0025805195327848196\n",
            "step: 50, loss: 0.00039016350638121367\n",
            "step: 60, loss: 0.003582475008442998\n",
            "step: 70, loss: 0.0005925934528931975\n",
            "step: 80, loss: 0.0002334695600438863\n",
            "step: 90, loss: 0.0001642491261009127\n",
            "step: 100, loss: 0.0001377372827846557\n",
            "step: 110, loss: 0.012043178081512451\n",
            "step: 120, loss: 0.00010711365757742897\n",
            "step: 130, loss: 0.0006215700414031744\n",
            "step: 140, loss: 0.0007688191253691912\n",
            "step: 150, loss: 0.0012189527042210102\n",
            "step: 160, loss: 0.00010614383791107684\n",
            "step: 170, loss: 0.00040009545045904815\n",
            "step: 180, loss: 0.0007886952953413129\n",
            "step: 190, loss: 0.00042850463069044054\n",
            "step: 200, loss: 0.000860086118336767\n",
            "step: 210, loss: 0.0028550836723297834\n",
            "step: 220, loss: 0.016854045912623405\n",
            "step: 230, loss: 0.00602483656257391\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9887892376681614, f1=0.9798657718120806, best_f1=0.9799554565701558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00028836651472374797\n",
            "step: 10, loss: 0.000491812766995281\n",
            "step: 20, loss: 0.0004026828391943127\n",
            "step: 30, loss: 0.0002522034919820726\n",
            "step: 40, loss: 0.00013435645087156445\n",
            "step: 50, loss: 0.0011775277089327574\n",
            "step: 60, loss: 0.0030789675656706095\n",
            "step: 70, loss: 0.00022508615802507848\n",
            "step: 80, loss: 0.00042466906597837806\n",
            "step: 90, loss: 0.14484375715255737\n",
            "step: 100, loss: 0.0098029263317585\n",
            "step: 110, loss: 0.0029284716583788395\n",
            "step: 120, loss: 0.0005468653980642557\n",
            "step: 130, loss: 0.0008285484509542584\n",
            "step: 140, loss: 0.0034447633661329746\n",
            "step: 150, loss: 0.0006832918734289706\n",
            "step: 160, loss: 0.003081829287111759\n",
            "step: 170, loss: 0.0009186399984173477\n",
            "step: 180, loss: 0.0008329841075465083\n",
            "step: 190, loss: 0.0007878874312154949\n",
            "step: 200, loss: 0.005823030136525631\n",
            "step: 210, loss: 0.00099461548961699\n",
            "step: 220, loss: 0.004990404471755028\n",
            "step: 230, loss: 0.000813242862932384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9899216125419933, f1=0.9810479375696767, best_f1=0.9810479375696767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008520496194250882\n",
            "step: 10, loss: 0.00029252850799821317\n",
            "step: 20, loss: 0.008399982936680317\n",
            "step: 30, loss: 0.03408580645918846\n",
            "step: 40, loss: 0.000898837810382247\n",
            "step: 50, loss: 0.0010357009014114738\n",
            "step: 60, loss: 0.0010593285551294684\n",
            "step: 70, loss: 0.0006649998831562698\n",
            "step: 80, loss: 0.0004962744424119592\n",
            "step: 90, loss: 0.001034822198562324\n",
            "step: 100, loss: 0.00039151209057308733\n",
            "step: 110, loss: 0.0003955198626499623\n",
            "step: 120, loss: 0.0006835878593847156\n",
            "step: 130, loss: 0.00035381974885240197\n",
            "step: 140, loss: 0.0005538556142710149\n",
            "step: 150, loss: 0.0005788073176518083\n",
            "step: 160, loss: 0.0004601191612891853\n",
            "step: 170, loss: 0.0005728827090933919\n",
            "step: 180, loss: 0.00042187259532511234\n",
            "step: 190, loss: 0.001974481390789151\n",
            "step: 200, loss: 0.00043280329555273056\n",
            "step: 210, loss: 0.0008214066037908196\n",
            "step: 220, loss: 0.002282349858433008\n",
            "step: 230, loss: 0.0011578588746488094\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9888641425389755, f1=0.9777777777777777, best_f1=0.9810479375696767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001415032660588622\n",
            "step: 10, loss: 0.0008079293766058981\n",
            "step: 20, loss: 0.0012458949349820614\n",
            "step: 30, loss: 0.0056993067264556885\n",
            "step: 40, loss: 0.0007944183889776468\n",
            "step: 50, loss: 0.0016705194720998406\n",
            "step: 60, loss: 0.0005046273581683636\n",
            "step: 70, loss: 0.0033868090249598026\n",
            "step: 80, loss: 0.005683518014848232\n",
            "step: 90, loss: 0.0003328614984638989\n",
            "step: 100, loss: 0.0015750883612781763\n",
            "step: 110, loss: 0.008142647333443165\n",
            "step: 120, loss: 0.003253455273807049\n",
            "step: 130, loss: 0.0003491574025247246\n",
            "step: 140, loss: 0.00018142179760616273\n",
            "step: 150, loss: 0.00010769737127702683\n",
            "step: 160, loss: 0.0019773344974964857\n",
            "step: 170, loss: 0.0008549299091100693\n",
            "step: 180, loss: 0.0168751310557127\n",
            "step: 190, loss: 0.00021640094928443432\n",
            "step: 200, loss: 0.0027532526291906834\n",
            "step: 210, loss: 0.00029343910864554346\n",
            "step: 220, loss: 0.0004948753630742431\n",
            "step: 230, loss: 0.0006424959283322096\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.987709497206704, f1=0.978865406006674, best_f1=0.9810479375696767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003617156180553138\n",
            "step: 10, loss: 0.00012554881686810404\n",
            "step: 20, loss: 0.0005490662297233939\n",
            "step: 30, loss: 0.0004348708607722074\n",
            "step: 40, loss: 0.0004922565422020853\n",
            "step: 50, loss: 0.00011045186693081632\n",
            "step: 60, loss: 0.0007197794620878994\n",
            "step: 70, loss: 0.009425771422684193\n",
            "step: 80, loss: 0.0004928044509142637\n",
            "step: 90, loss: 0.0010958645725622773\n",
            "step: 100, loss: 0.001381073845550418\n",
            "step: 110, loss: 0.00045026428415440023\n",
            "step: 120, loss: 0.0003069079539272934\n",
            "step: 130, loss: 0.00021582850604318082\n",
            "step: 140, loss: 0.0001197152232634835\n",
            "step: 150, loss: 4.738852658192627e-05\n",
            "step: 160, loss: 0.00024616284645162523\n",
            "step: 170, loss: 0.00012612405407708138\n",
            "step: 180, loss: 0.00013694955850951374\n",
            "step: 190, loss: 0.00011680438910843804\n",
            "step: 200, loss: 0.00015736426576040685\n",
            "step: 210, loss: 0.00025287922471761703\n",
            "step: 220, loss: 0.00043942141928710043\n",
            "step: 230, loss: 0.0004533810424618423\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9888641425389755, f1=0.9778270509977827, best_f1=0.9810479375696767\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009813091019168496\n",
            "step: 10, loss: 0.00014287879457697272\n",
            "step: 20, loss: 0.0011181074660271406\n",
            "step: 30, loss: 0.00012931124365422875\n",
            "step: 40, loss: 6.400729762390256e-05\n",
            "step: 50, loss: 0.00013288197806105018\n",
            "step: 60, loss: 0.028200244531035423\n",
            "step: 70, loss: 0.00011316404561512172\n",
            "step: 80, loss: 0.0001143789995694533\n",
            "step: 90, loss: 0.00023222086019814014\n",
            "step: 100, loss: 7.790594827383757e-05\n",
            "step: 110, loss: 0.0002048733294941485\n",
            "step: 120, loss: 0.018364375457167625\n",
            "step: 130, loss: 0.00037737531238235533\n",
            "step: 140, loss: 0.01333856675773859\n",
            "step: 150, loss: 0.0007425449439324439\n",
            "step: 160, loss: 0.014932889491319656\n",
            "step: 170, loss: 6.151487468741834e-05\n",
            "step: 180, loss: 0.0001681731519056484\n",
            "step: 190, loss: 0.0003143683716189116\n",
            "step: 200, loss: 0.0004465229285415262\n",
            "step: 210, loss: 0.14351888000965118\n",
            "step: 220, loss: 0.0001278347772313282\n",
            "step: 230, loss: 0.00015661769430153072\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.9899665551839464, f1=0.98, best_f1=0.98\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:16, 147.59it/s]\n",
            "load_f1 = 0.9877641824249165\n",
            "real_f1 = 0.9899665551839464\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:32, 136.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "ck7uL6uPgNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "YyyxG2qpgNFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8312c806-b436-4b75-e211-42c2f6a7284d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6263348460197449\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4973161816596985\n",
            "step: 20, loss: 0.2833632230758667\n",
            "step: 30, loss: 0.3593776524066925\n",
            "step: 40, loss: 0.34269315004348755\n",
            "step: 50, loss: 0.39669230580329895\n",
            "step: 60, loss: 0.3837755024433136\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 70, loss: 0.3018675148487091\n",
            "step: 80, loss: 0.21441593766212463\n",
            "step: 90, loss: 0.17384760081768036\n",
            "step: 100, loss: 0.22428159415721893\n",
            "step: 110, loss: 0.18347381055355072\n",
            "step: 120, loss: 0.11018988490104675\n",
            "step: 130, loss: 0.1379026621580124\n",
            "step: 140, loss: 0.28511324524879456\n",
            "step: 150, loss: 0.1095525473356247\n",
            "step: 160, loss: 0.1203758716583252\n",
            "step: 170, loss: 0.07205317169427872\n",
            "step: 180, loss: 0.07004734873771667\n",
            "step: 190, loss: 0.04175862297415733\n",
            "step: 200, loss: 0.04107574000954628\n",
            "step: 210, loss: 0.04300704970955849\n",
            "step: 220, loss: 0.05804172158241272\n",
            "step: 230, loss: 0.18291741609573364\n",
            "step: 240, loss: 0.06447173655033112\n",
            "step: 250, loss: 0.04456693306565285\n",
            "step: 260, loss: 0.3131154775619507\n",
            "step: 270, loss: 0.2612079679965973\n",
            "step: 280, loss: 0.05350590869784355\n",
            "step: 290, loss: 0.04354057461023331\n",
            "step: 300, loss: 0.05866860970854759\n",
            "step: 310, loss: 0.09950673580169678\n",
            "step: 320, loss: 0.06844452768564224\n",
            "step: 330, loss: 0.09541264921426773\n",
            "step: 340, loss: 0.32008102536201477\n",
            "step: 350, loss: 0.11585213989019394\n",
            "step: 360, loss: 0.0651768147945404\n",
            "step: 370, loss: 0.23941285908222198\n",
            "step: 380, loss: 0.1378161609172821\n",
            "step: 390, loss: 0.006768961902707815\n",
            "step: 400, loss: 0.027717486023902893\n",
            "step: 410, loss: 0.28167030215263367\n",
            "step: 420, loss: 0.010725093074142933\n",
            "step: 430, loss: 0.013808106072247028\n",
            "step: 440, loss: 0.08096048980951309\n",
            "step: 450, loss: 0.008807875216007233\n",
            "step: 460, loss: 0.058876264840364456\n",
            "step: 470, loss: 0.02842910774052143\n",
            "step: 480, loss: 0.06486668437719345\n",
            "step: 490, loss: 0.053147703409194946\n",
            "step: 500, loss: 0.02434937097132206\n",
            "step: 510, loss: 0.055842600762844086\n",
            "step: 520, loss: 0.1365514099597931\n",
            "step: 530, loss: 0.06530742347240448\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.93796992481203, f1=0.9250936329588015, best_f1=0.9250936329588015\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03944693133234978\n",
            "step: 10, loss: 0.03317610174417496\n",
            "step: 20, loss: 0.00678984122350812\n",
            "step: 30, loss: 0.10393403470516205\n",
            "step: 40, loss: 0.0744871124625206\n",
            "step: 50, loss: 0.022533614188432693\n",
            "step: 60, loss: 0.020579051226377487\n",
            "step: 70, loss: 0.03416626155376434\n",
            "step: 80, loss: 0.027936525642871857\n",
            "step: 90, loss: 0.003809636691585183\n",
            "step: 100, loss: 0.0933891162276268\n",
            "step: 110, loss: 0.008617025800049305\n",
            "step: 120, loss: 0.198206827044487\n",
            "step: 130, loss: 0.005584754515439272\n",
            "step: 140, loss: 0.11160261929035187\n",
            "step: 150, loss: 0.003612494096159935\n",
            "step: 160, loss: 0.01600664295256138\n",
            "step: 170, loss: 0.02976212464272976\n",
            "step: 180, loss: 0.01697417162358761\n",
            "step: 190, loss: 0.005604600999504328\n",
            "step: 200, loss: 0.0733979195356369\n",
            "step: 210, loss: 0.02063019759953022\n",
            "step: 220, loss: 0.0016193118644878268\n",
            "step: 230, loss: 0.035175807774066925\n",
            "step: 240, loss: 0.08112572133541107\n",
            "step: 250, loss: 0.012529250234365463\n",
            "step: 260, loss: 0.010045447386801243\n",
            "step: 270, loss: 0.04539019986987114\n",
            "step: 280, loss: 0.07083974033594131\n",
            "step: 290, loss: 0.00926421582698822\n",
            "step: 300, loss: 0.03618483617901802\n",
            "step: 310, loss: 0.009489256888628006\n",
            "step: 320, loss: 0.008955627679824829\n",
            "step: 330, loss: 0.06990694999694824\n",
            "step: 340, loss: 0.0750921294093132\n",
            "step: 350, loss: 0.0020099012181162834\n",
            "step: 360, loss: 0.13188712298870087\n",
            "step: 370, loss: 0.017567291855812073\n",
            "step: 380, loss: 0.10909925401210785\n",
            "step: 390, loss: 0.0037479239981621504\n",
            "step: 400, loss: 0.02608453296124935\n",
            "step: 410, loss: 0.00699173379689455\n",
            "step: 420, loss: 0.027912596240639687\n",
            "step: 430, loss: 0.16620869934558868\n",
            "step: 440, loss: 0.021188508719205856\n",
            "step: 450, loss: 0.16266892850399017\n",
            "step: 460, loss: 0.07063189148902893\n",
            "step: 470, loss: 0.08947452157735825\n",
            "step: 480, loss: 0.009427743032574654\n",
            "step: 490, loss: 0.06880935281515121\n",
            "step: 500, loss: 0.0030357937794178724\n",
            "step: 510, loss: 0.011780834756791592\n",
            "step: 520, loss: 0.3757750391960144\n",
            "step: 530, loss: 0.08865208178758621\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9475138121546962, f1=0.944954128440367, best_f1=0.944954128440367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15370526909828186\n",
            "step: 10, loss: 0.03287733718752861\n",
            "step: 20, loss: 0.003759226994588971\n",
            "step: 30, loss: 0.0248874481767416\n",
            "step: 40, loss: 0.008828647434711456\n",
            "step: 50, loss: 0.05233617126941681\n",
            "step: 60, loss: 0.011494728736579418\n",
            "step: 70, loss: 0.003129238961264491\n",
            "step: 80, loss: 0.0699615329504013\n",
            "step: 90, loss: 0.0013681353302672505\n",
            "step: 100, loss: 0.09921572357416153\n",
            "step: 110, loss: 0.011414390057325363\n",
            "step: 120, loss: 0.08355653285980225\n",
            "step: 130, loss: 0.12691199779510498\n",
            "step: 140, loss: 0.007877463474869728\n",
            "step: 150, loss: 0.00786168035119772\n",
            "step: 160, loss: 0.015748558565974236\n",
            "step: 170, loss: 0.02879943512380123\n",
            "step: 180, loss: 0.00794746819883585\n",
            "step: 190, loss: 0.0019380536396056414\n",
            "step: 200, loss: 0.01895117573440075\n",
            "step: 210, loss: 0.032254740595817566\n",
            "step: 220, loss: 0.05944984406232834\n",
            "step: 230, loss: 0.014080232009291649\n",
            "step: 240, loss: 0.007296237628906965\n",
            "step: 250, loss: 0.035379279404878616\n",
            "step: 260, loss: 0.017136525362730026\n",
            "step: 270, loss: 0.014353403821587563\n",
            "step: 280, loss: 0.0036163204349577427\n",
            "step: 290, loss: 0.02147928811609745\n",
            "step: 300, loss: 0.2593390643596649\n",
            "step: 310, loss: 0.06302858144044876\n",
            "step: 320, loss: 0.03504505753517151\n",
            "step: 330, loss: 0.076636403799057\n",
            "step: 340, loss: 0.05632772669196129\n",
            "step: 350, loss: 0.0573115311563015\n",
            "step: 360, loss: 0.041603267192840576\n",
            "step: 370, loss: 0.03347015008330345\n",
            "step: 380, loss: 0.006328487768769264\n",
            "step: 390, loss: 0.012964210473001003\n",
            "step: 400, loss: 0.17149673402309418\n",
            "step: 410, loss: 0.07126011699438095\n",
            "step: 420, loss: 0.006263755261898041\n",
            "step: 430, loss: 0.014844579622149467\n",
            "step: 440, loss: 0.05665489658713341\n",
            "step: 450, loss: 0.00945302750915289\n",
            "step: 460, loss: 0.1355389654636383\n",
            "step: 470, loss: 0.06507526338100433\n",
            "step: 480, loss: 0.22667457163333893\n",
            "step: 490, loss: 0.006806590128690004\n",
            "step: 500, loss: 0.01163868885487318\n",
            "step: 510, loss: 0.004002810921519995\n",
            "step: 520, loss: 0.006253946106880903\n",
            "step: 530, loss: 0.00917065143585205\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.940677966101695, f1=0.9347724073205067, best_f1=0.944954128440367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03507433459162712\n",
            "step: 10, loss: 0.0015883909072726965\n",
            "step: 20, loss: 0.008514279499650002\n",
            "step: 30, loss: 0.04627411812543869\n",
            "step: 40, loss: 0.008653289638459682\n",
            "step: 50, loss: 0.00857322197407484\n",
            "step: 60, loss: 0.002305032452568412\n",
            "step: 70, loss: 0.059984318912029266\n",
            "step: 80, loss: 0.004187441430985928\n",
            "step: 90, loss: 0.021083399653434753\n",
            "step: 100, loss: 0.005515010096132755\n",
            "step: 110, loss: 0.11356570571660995\n",
            "step: 120, loss: 0.006510508246719837\n",
            "step: 130, loss: 0.024743905290961266\n",
            "step: 140, loss: 0.0038225497119128704\n",
            "step: 150, loss: 0.00830626580864191\n",
            "step: 160, loss: 0.0006634774035774171\n",
            "step: 170, loss: 0.018619215115904808\n",
            "step: 180, loss: 0.03582095354795456\n",
            "step: 190, loss: 0.0831371545791626\n",
            "step: 200, loss: 0.007948186248540878\n",
            "step: 210, loss: 0.0021317750215530396\n",
            "step: 220, loss: 0.004434855654835701\n",
            "step: 230, loss: 0.002757107140496373\n",
            "step: 240, loss: 0.011614036746323109\n",
            "step: 250, loss: 0.06693655997514725\n",
            "step: 260, loss: 0.01793641224503517\n",
            "step: 270, loss: 0.012915664352476597\n",
            "step: 280, loss: 0.003219541860744357\n",
            "step: 290, loss: 0.017376471310853958\n",
            "step: 300, loss: 0.0009600566700100899\n",
            "step: 310, loss: 0.00331440893933177\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 320, loss: 0.13199810683727264\n",
            "step: 330, loss: 0.019244644790887833\n",
            "step: 340, loss: 0.002560913795605302\n",
            "step: 350, loss: 0.004322261083871126\n",
            "step: 360, loss: 0.019219014793634415\n",
            "step: 370, loss: 0.0007525355322286487\n",
            "step: 380, loss: 0.0008256121072918177\n",
            "step: 390, loss: 0.0002805322001222521\n",
            "step: 400, loss: 0.0016435307916253805\n",
            "step: 410, loss: 0.0048783449456095695\n",
            "step: 420, loss: 0.004225256387144327\n",
            "step: 430, loss: 0.07269539684057236\n",
            "step: 440, loss: 0.006214291788637638\n",
            "step: 450, loss: 0.043326299637556076\n",
            "step: 460, loss: 0.11475323885679245\n",
            "step: 470, loss: 0.0022102785296738148\n",
            "step: 480, loss: 0.003802276449277997\n",
            "step: 490, loss: 0.0013818868901580572\n",
            "step: 500, loss: 0.044637762010097504\n",
            "step: 510, loss: 0.07492522150278091\n",
            "step: 520, loss: 0.008791007101535797\n",
            "step: 530, loss: 0.035699933767318726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9485873089393237, f1=0.9456221198156683, best_f1=0.9456221198156683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0019358722493052483\n",
            "step: 10, loss: 0.011355835013091564\n",
            "step: 20, loss: 0.005152831319719553\n",
            "step: 30, loss: 0.004713123664259911\n",
            "step: 40, loss: 0.00031360378488898277\n",
            "step: 50, loss: 0.0686824768781662\n",
            "step: 60, loss: 0.003157183527946472\n",
            "step: 70, loss: 0.01976865716278553\n",
            "step: 80, loss: 0.003562558675184846\n",
            "step: 90, loss: 0.12863436341285706\n",
            "step: 100, loss: 0.07980790734291077\n",
            "step: 110, loss: 0.0007455796003341675\n",
            "step: 120, loss: 0.2623266875743866\n",
            "step: 130, loss: 0.011044860817492008\n",
            "step: 140, loss: 0.08915965259075165\n",
            "step: 150, loss: 0.037766795605421066\n",
            "step: 160, loss: 0.02775431238114834\n",
            "step: 170, loss: 0.03895983844995499\n",
            "step: 180, loss: 0.0046665361151099205\n",
            "step: 190, loss: 0.0008655605488456786\n",
            "step: 200, loss: 0.0014702416956424713\n",
            "step: 210, loss: 0.0012151331175118685\n",
            "step: 220, loss: 0.004093277268111706\n",
            "step: 230, loss: 0.007205755449831486\n",
            "step: 240, loss: 0.004865738097578287\n",
            "step: 250, loss: 0.05847563594579697\n",
            "step: 260, loss: 0.0008870845194905996\n",
            "step: 270, loss: 0.0052370112389326096\n",
            "step: 280, loss: 0.0027063372544944286\n",
            "step: 290, loss: 0.0016490034759044647\n",
            "step: 300, loss: 0.01910649612545967\n",
            "step: 310, loss: 0.04932970181107521\n",
            "step: 320, loss: 0.011712828651070595\n",
            "step: 330, loss: 0.03796180337667465\n",
            "step: 340, loss: 0.0004703276790678501\n",
            "step: 350, loss: 0.0009537564474157989\n",
            "step: 360, loss: 4.652145435102284e-05\n",
            "step: 370, loss: 8.601158333476633e-05\n",
            "step: 380, loss: 0.0019393251277506351\n",
            "step: 390, loss: 0.006169174797832966\n",
            "step: 400, loss: 0.002991320099681616\n",
            "step: 410, loss: 0.024220865219831467\n",
            "step: 420, loss: 0.2687503695487976\n",
            "step: 430, loss: 0.0007684594020247459\n",
            "step: 440, loss: 0.0022838895674794912\n",
            "step: 450, loss: 0.0028489213436841965\n",
            "step: 460, loss: 0.011061116121709347\n",
            "step: 470, loss: 0.020732877776026726\n",
            "step: 480, loss: 0.02561073936522007\n",
            "step: 490, loss: 0.002688839565962553\n",
            "step: 500, loss: 0.003667506854981184\n",
            "step: 510, loss: 0.002444045851007104\n",
            "step: 520, loss: 0.05305091291666031\n",
            "step: 530, loss: 0.0038104429841041565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9542725173210163, f1=0.9443678160919541, best_f1=0.9443678160919541\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005529404152184725\n",
            "step: 10, loss: 0.0009775900980457664\n",
            "step: 20, loss: 0.07224899530410767\n",
            "step: 30, loss: 0.008879968896508217\n",
            "step: 40, loss: 0.0013485124800354242\n",
            "step: 50, loss: 0.0005549369379878044\n",
            "step: 60, loss: 0.010602624155580997\n",
            "step: 70, loss: 0.0007424473878927529\n",
            "step: 80, loss: 0.00035856233444064856\n",
            "step: 90, loss: 0.0006688113790005445\n",
            "step: 100, loss: 0.01042536087334156\n",
            "step: 110, loss: 0.008386376313865185\n",
            "step: 120, loss: 0.0030543815810233355\n",
            "step: 130, loss: 0.0006019863649271429\n",
            "step: 140, loss: 0.0007063047378323972\n",
            "step: 150, loss: 0.00020947790471836925\n",
            "step: 160, loss: 0.0014051563339307904\n",
            "step: 170, loss: 0.0017311120172962546\n",
            "step: 180, loss: 0.00035303013282828033\n",
            "step: 190, loss: 0.041197579354047775\n",
            "step: 200, loss: 0.022210512310266495\n",
            "step: 210, loss: 0.00810728408396244\n",
            "step: 220, loss: 0.003652711398899555\n",
            "step: 230, loss: 0.0022469654213637114\n",
            "step: 240, loss: 0.00016070243145804852\n",
            "step: 250, loss: 0.11833884567022324\n",
            "step: 260, loss: 0.0030890400521457195\n",
            "step: 270, loss: 0.04233158379793167\n",
            "step: 280, loss: 0.12402670085430145\n",
            "step: 290, loss: 0.011711574159562588\n",
            "step: 300, loss: 0.005866894032806158\n",
            "step: 310, loss: 0.05014616996049881\n",
            "step: 320, loss: 0.08516576141119003\n",
            "step: 330, loss: 0.02823386713862419\n",
            "step: 340, loss: 0.000533841724973172\n",
            "step: 350, loss: 0.001213539857417345\n",
            "step: 360, loss: 0.10150207579135895\n",
            "step: 370, loss: 0.006982052698731422\n",
            "step: 380, loss: 0.0008848788565956056\n",
            "step: 390, loss: 0.020280612632632256\n",
            "step: 400, loss: 0.0031468148808926344\n",
            "step: 410, loss: 0.000853105157148093\n",
            "step: 420, loss: 0.0011993101797997952\n",
            "step: 430, loss: 0.0004442096105776727\n",
            "step: 440, loss: 0.021312009543180466\n",
            "step: 450, loss: 0.2910473644733429\n",
            "step: 460, loss: 0.005771875847131014\n",
            "step: 470, loss: 0.0019577518105506897\n",
            "step: 480, loss: 0.005419893190264702\n",
            "step: 490, loss: 0.008704054169356823\n",
            "step: 500, loss: 0.015706488862633705\n",
            "step: 510, loss: 0.2991597056388855\n",
            "step: 520, loss: 0.0005759337218478322\n",
            "step: 530, loss: 0.005581557750701904\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9496535796766744, f1=0.9401473296500922, best_f1=0.9443678160919541\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0027270030695945024\n",
            "step: 10, loss: 0.012207675725221634\n",
            "step: 20, loss: 0.0009849285706877708\n",
            "step: 30, loss: 0.004188664723187685\n",
            "step: 40, loss: 0.002131872344762087\n",
            "step: 50, loss: 0.000563754525501281\n",
            "step: 60, loss: 0.0009597527678124607\n",
            "step: 70, loss: 0.00031458292505703866\n",
            "step: 80, loss: 0.00022810063092038035\n",
            "step: 90, loss: 0.005712633021175861\n",
            "step: 100, loss: 0.00023862195666879416\n",
            "step: 110, loss: 0.00019706838065758348\n",
            "step: 120, loss: 0.00044637342216446996\n",
            "step: 130, loss: 0.0005713290302082896\n",
            "step: 140, loss: 8.56950573506765e-05\n",
            "step: 150, loss: 9.473740647081286e-05\n",
            "step: 160, loss: 0.0008409758447669446\n",
            "step: 170, loss: 0.006444387137889862\n",
            "step: 180, loss: 0.006303183734416962\n",
            "step: 190, loss: 0.0022702652495354414\n",
            "step: 200, loss: 0.002335494151338935\n",
            "step: 210, loss: 0.11927134543657303\n",
            "step: 220, loss: 0.0007150303572416306\n",
            "step: 230, loss: 0.0003487040230538696\n",
            "step: 240, loss: 0.0010505920508876443\n",
            "step: 250, loss: 0.01612623780965805\n",
            "step: 260, loss: 0.003886917605996132\n",
            "step: 270, loss: 0.0009117809822782874\n",
            "step: 280, loss: 0.0024485911708325148\n",
            "step: 290, loss: 0.0036626968067139387\n",
            "step: 300, loss: 0.00017416698392480612\n",
            "step: 310, loss: 0.00022114213788881898\n",
            "step: 320, loss: 0.0010113188764080405\n",
            "step: 330, loss: 0.0007516102632507682\n",
            "step: 340, loss: 0.00044857640750706196\n",
            "step: 350, loss: 0.0017814188031479716\n",
            "step: 360, loss: 0.0035618171095848083\n",
            "step: 370, loss: 0.004019510000944138\n",
            "step: 380, loss: 0.0008847266435623169\n",
            "step: 390, loss: 0.0011753769358620048\n",
            "step: 400, loss: 0.006187814753502607\n",
            "step: 410, loss: 0.00022973763407208025\n",
            "step: 420, loss: 0.18618375062942505\n",
            "step: 430, loss: 0.0006903507164679468\n",
            "step: 440, loss: 0.02396620251238346\n",
            "step: 450, loss: 0.004528460558503866\n",
            "step: 460, loss: 0.001702727866359055\n",
            "step: 470, loss: 0.18295322358608246\n",
            "step: 480, loss: 0.009335954673588276\n",
            "step: 490, loss: 0.002042989945039153\n",
            "step: 500, loss: 0.0028280094265937805\n",
            "step: 510, loss: 0.00028350710636004806\n",
            "step: 520, loss: 0.0014411471784114838\n",
            "step: 530, loss: 0.0005457318620756269\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9561975768872321, f1=0.9471243042671614, best_f1=0.9471243042671614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003907237551175058\n",
            "step: 10, loss: 0.002722564386203885\n",
            "step: 20, loss: 0.0005056365043856204\n",
            "step: 30, loss: 0.004366529639810324\n",
            "step: 40, loss: 0.00012513043475337327\n",
            "step: 50, loss: 0.00012409340706653893\n",
            "step: 60, loss: 0.0008139932178892195\n",
            "step: 70, loss: 0.00011498099775053561\n",
            "step: 80, loss: 0.010183071717619896\n",
            "step: 90, loss: 0.0001082465605577454\n",
            "step: 100, loss: 0.00038738694274798036\n",
            "step: 110, loss: 0.0001512881281087175\n",
            "step: 120, loss: 0.0012851323699578643\n",
            "step: 130, loss: 0.016764769330620766\n",
            "step: 140, loss: 5.177991624805145e-05\n",
            "step: 150, loss: 0.008470816537737846\n",
            "step: 160, loss: 7.144411210902035e-05\n",
            "step: 170, loss: 0.2571895718574524\n",
            "step: 180, loss: 0.0026966682635247707\n",
            "step: 190, loss: 0.002227236283943057\n",
            "step: 200, loss: 0.008242227137088776\n",
            "step: 210, loss: 0.05832676962018013\n",
            "step: 220, loss: 0.0014456245116889477\n",
            "step: 230, loss: 0.013917211443185806\n",
            "step: 240, loss: 0.0013473380822688341\n",
            "step: 250, loss: 0.0005832742317579687\n",
            "step: 260, loss: 5.631961903418414e-05\n",
            "step: 270, loss: 0.00041197205428034067\n",
            "step: 280, loss: 0.0001339373702649027\n",
            "step: 290, loss: 0.00016315252287313342\n",
            "step: 300, loss: 6.930858944542706e-05\n",
            "step: 310, loss: 0.0015812907367944717\n",
            "step: 320, loss: 0.007622165139764547\n",
            "step: 330, loss: 0.08344846963882446\n",
            "step: 340, loss: 0.004381783772259951\n",
            "step: 350, loss: 0.000511987425852567\n",
            "step: 360, loss: 0.09770669788122177\n",
            "step: 370, loss: 0.08913105726242065\n",
            "step: 380, loss: 0.001403079368174076\n",
            "step: 390, loss: 0.07027441263198853\n",
            "step: 400, loss: 0.0020634003449231386\n",
            "step: 410, loss: 0.0064681582152843475\n",
            "step: 420, loss: 0.0003451485827099532\n",
            "step: 430, loss: 0.0003355229855515063\n",
            "step: 440, loss: 0.01412061508744955\n",
            "step: 450, loss: 0.0004759422445204109\n",
            "step: 460, loss: 0.0005877430667169392\n",
            "step: 470, loss: 0.06421177089214325\n",
            "step: 480, loss: 0.0011566831963136792\n",
            "step: 490, loss: 0.0034572086296975613\n",
            "step: 500, loss: 0.0004244967130944133\n",
            "step: 510, loss: 0.0006283335969783366\n",
            "step: 520, loss: 0.00029219617135822773\n",
            "step: 530, loss: 0.15796300768852234\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9494949494949496, f1=0.943758573388203, best_f1=0.9471243042671614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000363372964784503\n",
            "step: 10, loss: 0.000489253958221525\n",
            "step: 20, loss: 0.0003210904833395034\n",
            "step: 30, loss: 0.08701508492231369\n",
            "step: 40, loss: 0.0006645518587902188\n",
            "step: 50, loss: 0.0007633932982571423\n",
            "step: 60, loss: 0.000688070198521018\n",
            "step: 70, loss: 0.025727545842528343\n",
            "step: 80, loss: 0.06157407909631729\n",
            "step: 90, loss: 0.05511412397027016\n",
            "step: 100, loss: 0.0007238587131723762\n",
            "step: 110, loss: 0.009635805152356625\n",
            "step: 120, loss: 0.006952681113034487\n",
            "step: 130, loss: 0.0015520998276770115\n",
            "step: 140, loss: 0.00026591046480461955\n",
            "step: 150, loss: 0.00042230874532833695\n",
            "step: 160, loss: 0.0011070739710703492\n",
            "step: 170, loss: 0.013342779129743576\n",
            "step: 180, loss: 0.00013487313117366284\n",
            "step: 190, loss: 5.859455995960161e-05\n",
            "step: 200, loss: 0.000464351789560169\n",
            "step: 210, loss: 0.0007214761571958661\n",
            "step: 220, loss: 0.0008802501833997667\n",
            "step: 230, loss: 0.0005444721318781376\n",
            "step: 240, loss: 0.00038075039628893137\n",
            "step: 250, loss: 0.0004898448241874576\n",
            "step: 260, loss: 0.00022208041627891362\n",
            "step: 270, loss: 0.00043375923996791244\n",
            "step: 280, loss: 0.015320554375648499\n",
            "step: 290, loss: 0.00018727866699919105\n",
            "step: 300, loss: 0.0014862969983369112\n",
            "step: 310, loss: 0.004830300807952881\n",
            "step: 320, loss: 0.0001668889744905755\n",
            "step: 330, loss: 0.00043681106762960553\n",
            "step: 340, loss: 0.00038573029451072216\n",
            "step: 350, loss: 0.0058491225354373455\n",
            "step: 360, loss: 0.0006128688110038638\n",
            "step: 370, loss: 0.000513209670316428\n",
            "step: 380, loss: 0.000425867474405095\n",
            "step: 390, loss: 3.646362529252656e-05\n",
            "step: 400, loss: 0.00020757279708050191\n",
            "step: 410, loss: 0.00022302249271888286\n",
            "step: 420, loss: 0.0001560504169901833\n",
            "step: 430, loss: 0.0008881050744093955\n",
            "step: 440, loss: 5.286662053549662e-05\n",
            "step: 450, loss: 0.002105261664837599\n",
            "step: 460, loss: 4.6639295760542154e-05\n",
            "step: 470, loss: 3.620285860961303e-05\n",
            "step: 480, loss: 4.709874701802619e-05\n",
            "step: 490, loss: 0.03766992688179016\n",
            "step: 500, loss: 0.00015215099847409874\n",
            "step: 510, loss: 0.00010092469165101647\n",
            "step: 520, loss: 0.014857657253742218\n",
            "step: 530, loss: 0.01608290523290634\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.948405253283302, f1=0.9427640763145649, best_f1=0.9471243042671614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00047244224697351456\n",
            "step: 10, loss: 0.001008383696898818\n",
            "step: 20, loss: 5.515876546269283e-05\n",
            "step: 30, loss: 0.001858410076238215\n",
            "step: 40, loss: 4.548938522930257e-05\n",
            "step: 50, loss: 0.00036529090721160173\n",
            "step: 60, loss: 0.008571546524763107\n",
            "step: 70, loss: 0.008730224333703518\n",
            "step: 80, loss: 0.0026706992648541927\n",
            "step: 90, loss: 0.0006557951564900577\n",
            "step: 100, loss: 0.10551457107067108\n",
            "step: 110, loss: 0.0663413479924202\n",
            "step: 120, loss: 0.00027669177507050335\n",
            "step: 130, loss: 0.0003168070106767118\n",
            "step: 140, loss: 0.0011456306092441082\n",
            "step: 150, loss: 0.006783670745790005\n",
            "step: 160, loss: 0.08598891645669937\n",
            "step: 170, loss: 7.954165630508214e-05\n",
            "step: 180, loss: 0.00012503853940870613\n",
            "step: 190, loss: 4.4367119699018076e-05\n",
            "step: 200, loss: 7.948206621222198e-05\n",
            "step: 210, loss: 0.003983668051660061\n",
            "step: 220, loss: 0.0005411064485087991\n",
            "step: 230, loss: 0.027526361867785454\n",
            "step: 240, loss: 6.939529703231528e-05\n",
            "step: 250, loss: 4.992407775716856e-05\n",
            "step: 260, loss: 0.010155759751796722\n",
            "step: 270, loss: 5.3700081480201334e-05\n",
            "step: 280, loss: 0.003382977331057191\n",
            "step: 290, loss: 0.0025294062215834856\n",
            "step: 300, loss: 0.007323050871491432\n",
            "step: 310, loss: 0.08090304583311081\n",
            "step: 320, loss: 0.01861836388707161\n",
            "step: 330, loss: 0.0007352142711170018\n",
            "step: 340, loss: 0.0001220603589899838\n",
            "step: 350, loss: 0.0005347086698748171\n",
            "step: 360, loss: 0.00015788013115525246\n",
            "step: 370, loss: 0.0011795145692303777\n",
            "step: 380, loss: 0.0014417872298508883\n",
            "step: 390, loss: 0.00015524282935075462\n",
            "step: 400, loss: 7.249390910146758e-05\n",
            "step: 410, loss: 0.0009415920940227807\n",
            "step: 420, loss: 4.6451510570477694e-05\n",
            "step: 430, loss: 8.081840496743098e-05\n",
            "step: 440, loss: 0.00025062309578061104\n",
            "step: 450, loss: 9.073725232155994e-05\n",
            "step: 460, loss: 4.0388404158875346e-05\n",
            "step: 470, loss: 0.025314049795269966\n",
            "step: 480, loss: 0.00010482700599823147\n",
            "step: 490, loss: 0.0003002967860084027\n",
            "step: 500, loss: 0.00030812682234682143\n",
            "step: 510, loss: 0.0001381646143272519\n",
            "step: 520, loss: 0.0003050520608667284\n",
            "step: 530, loss: 0.017924839630723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9548447789275636, f1=0.9453526389537599, best_f1=0.9471243042671614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021631539857480675\n",
            "step: 10, loss: 0.00020829896675422788\n",
            "step: 20, loss: 0.00018456987163517624\n",
            "step: 30, loss: 0.0001238123222719878\n",
            "step: 40, loss: 3.283928890596144e-05\n",
            "step: 50, loss: 0.0006043485482223332\n",
            "step: 60, loss: 0.00041152103221975267\n",
            "step: 70, loss: 4.4290729420026764e-05\n",
            "step: 80, loss: 0.000681036151945591\n",
            "step: 90, loss: 0.00037473271368071437\n",
            "step: 100, loss: 5.3687883337261155e-05\n",
            "step: 110, loss: 3.694135375553742e-05\n",
            "step: 120, loss: 4.3073003325844184e-05\n",
            "step: 130, loss: 2.677236989256926e-05\n",
            "step: 140, loss: 4.440512566361576e-05\n",
            "step: 150, loss: 9.504861372988671e-05\n",
            "step: 160, loss: 0.001206119661219418\n",
            "step: 170, loss: 3.4592379961395636e-05\n",
            "step: 180, loss: 3.729837044375017e-05\n",
            "step: 190, loss: 0.023129304870963097\n",
            "step: 200, loss: 6.522507464978844e-05\n",
            "step: 210, loss: 3.8008914998499677e-05\n",
            "step: 220, loss: 0.031145257875323296\n",
            "step: 230, loss: 0.006241777911782265\n",
            "step: 240, loss: 0.0016075781313702464\n",
            "step: 250, loss: 0.00026353797875344753\n",
            "step: 260, loss: 0.0006836463580839336\n",
            "step: 270, loss: 0.002042514504864812\n",
            "step: 280, loss: 0.1335996687412262\n",
            "step: 290, loss: 0.0002544767630752176\n",
            "step: 300, loss: 0.00024206758826039732\n",
            "step: 310, loss: 0.0002238776214653626\n",
            "step: 320, loss: 0.0004822692135348916\n",
            "step: 330, loss: 2.3781209165463224e-05\n",
            "step: 340, loss: 0.00034354557283222675\n",
            "step: 350, loss: 0.0004031905555166304\n",
            "step: 360, loss: 0.00021028699120506644\n",
            "step: 370, loss: 0.001732174656353891\n",
            "step: 380, loss: 0.0009627981926314533\n",
            "step: 390, loss: 0.0018673281883820891\n",
            "step: 400, loss: 0.00018249715503770858\n",
            "step: 410, loss: 0.0012991678668186069\n",
            "step: 420, loss: 0.00015037419507279992\n",
            "step: 430, loss: 0.0011226708302274346\n",
            "step: 440, loss: 0.0008390527218580246\n",
            "step: 450, loss: 0.0008269201498478651\n",
            "step: 460, loss: 0.0024487001355737448\n",
            "step: 470, loss: 0.0004767078207805753\n",
            "step: 480, loss: 0.00017236908024642617\n",
            "step: 490, loss: 8.037563384277746e-05\n",
            "step: 500, loss: 0.0005959414411336184\n",
            "step: 510, loss: 0.0002016128128161654\n",
            "step: 520, loss: 0.00011640186130534858\n",
            "step: 530, loss: 0.053541604429483414\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9556696220251982, f1=0.9527340129749768, best_f1=0.9471243042671614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 3.504462802084163e-05\n",
            "step: 10, loss: 0.0005772520089522004\n",
            "step: 20, loss: 0.000549144227989018\n",
            "step: 30, loss: 0.0017896555364131927\n",
            "step: 40, loss: 0.0002343309170100838\n",
            "step: 50, loss: 9.485489135840908e-05\n",
            "step: 60, loss: 0.000619693542830646\n",
            "step: 70, loss: 0.00017899676458910108\n",
            "step: 80, loss: 0.10728106647729874\n",
            "step: 90, loss: 0.00036632880801334977\n",
            "step: 100, loss: 0.0207046065479517\n",
            "step: 110, loss: 6.482416938524693e-05\n",
            "step: 120, loss: 0.0002720987831708044\n",
            "step: 130, loss: 0.0005897378432564437\n",
            "step: 140, loss: 0.0003244457475375384\n",
            "step: 150, loss: 6.204300734680146e-05\n",
            "step: 160, loss: 0.007069696672260761\n",
            "step: 170, loss: 0.00011564139276742935\n",
            "step: 180, loss: 5.215683995629661e-05\n",
            "step: 190, loss: 0.0004924538661725819\n",
            "step: 200, loss: 0.0011215978302061558\n",
            "step: 210, loss: 0.0003141481429338455\n",
            "step: 220, loss: 5.017990406486206e-05\n",
            "step: 230, loss: 0.00013076962204650044\n",
            "step: 240, loss: 0.017301466315984726\n",
            "step: 250, loss: 5.96999452682212e-05\n",
            "step: 260, loss: 2.605328700155951e-05\n",
            "step: 270, loss: 0.0014245028141885996\n",
            "step: 280, loss: 5.871981556992978e-05\n",
            "step: 290, loss: 9.659455099608749e-05\n",
            "step: 300, loss: 0.0009250009898096323\n",
            "step: 310, loss: 4.1459628846496344e-05\n",
            "step: 320, loss: 0.0008038975647650659\n",
            "step: 330, loss: 0.0014617303386330605\n",
            "step: 340, loss: 4.255982639733702e-05\n",
            "step: 350, loss: 3.311686668894254e-05\n",
            "step: 360, loss: 0.00011108165926998481\n",
            "step: 370, loss: 2.1333553377189673e-05\n",
            "step: 380, loss: 0.012988357804715633\n",
            "step: 390, loss: 0.0014421362429857254\n",
            "step: 400, loss: 2.3747690647724085e-05\n",
            "step: 410, loss: 0.00026235170662403107\n",
            "step: 420, loss: 0.00012125380453653634\n",
            "step: 430, loss: 9.182259964291006e-05\n",
            "step: 440, loss: 0.002778193447738886\n",
            "step: 450, loss: 0.00016536911425646394\n",
            "step: 460, loss: 0.00019463409262243658\n",
            "step: 470, loss: 0.0006041573942638934\n",
            "step: 480, loss: 0.000785279436968267\n",
            "step: 490, loss: 3.579805343179032e-05\n",
            "step: 500, loss: 0.00016190475434996188\n",
            "step: 510, loss: 0.007029622793197632\n",
            "step: 520, loss: 0.00019667006563395262\n",
            "step: 530, loss: 5.6875993323046714e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9546296296296296, f1=0.9479981592268752, best_f1=0.9471243042671614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 5.01203176099807e-05\n",
            "step: 10, loss: 0.00017383653903380036\n",
            "step: 20, loss: 4.416376759763807e-05\n",
            "step: 30, loss: 2.169896470149979e-05\n",
            "step: 40, loss: 0.00010868276149267331\n",
            "step: 50, loss: 0.0006799794500693679\n",
            "step: 60, loss: 0.00023060146486386657\n",
            "step: 70, loss: 0.00011799567437265068\n",
            "step: 80, loss: 0.0004472198197618127\n",
            "step: 90, loss: 9.46956715779379e-05\n",
            "step: 100, loss: 5.4927724704612046e-05\n",
            "step: 110, loss: 1.8666760297492146e-05\n",
            "step: 120, loss: 4.806354991160333e-05\n",
            "step: 130, loss: 0.0005057569942437112\n",
            "step: 140, loss: 0.0003330439212732017\n",
            "step: 150, loss: 0.0006233245367184281\n",
            "step: 160, loss: 5.684678762918338e-05\n",
            "step: 170, loss: 0.00013669686450157315\n",
            "step: 180, loss: 0.008953381329774857\n",
            "step: 190, loss: 3.6043325962964445e-05\n",
            "step: 200, loss: 7.440806075464934e-05\n",
            "step: 210, loss: 3.672325692605227e-05\n",
            "step: 220, loss: 2.4488888811902143e-05\n",
            "step: 230, loss: 0.0010336068226024508\n",
            "step: 240, loss: 0.0005022204713895917\n",
            "step: 250, loss: 8.895158680388704e-05\n",
            "step: 260, loss: 5.302633508108556e-05\n",
            "step: 270, loss: 0.00015978280862327665\n",
            "step: 280, loss: 0.001561817480251193\n",
            "step: 290, loss: 1.566058926982805e-05\n",
            "step: 300, loss: 3.156192906317301e-05\n",
            "step: 310, loss: 2.5851839382085018e-05\n",
            "step: 320, loss: 0.0002060453116428107\n",
            "step: 330, loss: 4.972704118699767e-05\n",
            "step: 340, loss: 3.166665555909276e-05\n",
            "step: 350, loss: 1.9310253264848143e-05\n",
            "step: 360, loss: 0.10800308734178543\n",
            "step: 370, loss: 5.607270577456802e-05\n",
            "step: 380, loss: 2.2491840354632586e-05\n",
            "step: 390, loss: 2.430657514196355e-05\n",
            "step: 400, loss: 2.734641748247668e-05\n",
            "step: 410, loss: 5.270304609439336e-05\n",
            "step: 420, loss: 0.0003147353127133101\n",
            "step: 430, loss: 3.5052664316026494e-05\n",
            "step: 440, loss: 1.1656168680929113e-05\n",
            "step: 450, loss: 9.031100489664823e-05\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 460, loss: 0.005790526047348976\n",
            "step: 470, loss: 0.0004138429940212518\n",
            "step: 480, loss: 8.657470971229486e-06\n",
            "step: 490, loss: 3.2290110539179295e-05\n",
            "step: 500, loss: 1.4338148503156845e-05\n",
            "step: 510, loss: 0.00024409868638031185\n",
            "step: 520, loss: 2.6675248591345735e-05\n",
            "step: 530, loss: 0.0003953198902308941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9552656104380243, f1=0.9475638051044084, best_f1=0.9471243042671614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001136064762249589\n",
            "step: 10, loss: 0.0024626206140965223\n",
            "step: 20, loss: 1.5303161490010098e-05\n",
            "step: 30, loss: 0.0011347560212016106\n",
            "step: 40, loss: 6.455412221839651e-05\n",
            "step: 50, loss: 6.85074701323174e-05\n",
            "step: 60, loss: 2.8664377168752253e-05\n",
            "step: 70, loss: 3.26008812407963e-05\n",
            "step: 80, loss: 0.00017582708096597344\n",
            "step: 90, loss: 4.829854515264742e-05\n",
            "step: 100, loss: 3.6288955016061664e-05\n",
            "step: 110, loss: 3.817009201156907e-05\n",
            "step: 120, loss: 4.775720299221575e-05\n",
            "step: 130, loss: 1.8491897208150476e-05\n",
            "step: 140, loss: 3.455749902059324e-05\n",
            "step: 150, loss: 0.0005892177578061819\n",
            "step: 160, loss: 4.526145494310185e-05\n",
            "step: 170, loss: 1.8793529307004064e-05\n",
            "step: 180, loss: 2.892926386266481e-05\n",
            "step: 190, loss: 0.00014525503502227366\n",
            "step: 200, loss: 3.2054027542471886e-05\n",
            "step: 210, loss: 4.229361002217047e-05\n",
            "step: 220, loss: 4.516686749411747e-05\n",
            "step: 230, loss: 2.441791002638638e-05\n",
            "step: 240, loss: 0.0006357752135954797\n",
            "step: 250, loss: 0.00012931933451909572\n",
            "step: 260, loss: 2.5587543859728612e-05\n",
            "step: 270, loss: 0.00034710849286057055\n",
            "step: 280, loss: 2.5479361283942126e-05\n",
            "step: 290, loss: 9.886784937407356e-06\n",
            "step: 300, loss: 4.634396100300364e-05\n",
            "step: 310, loss: 3.779317921726033e-05\n",
            "step: 320, loss: 3.6388850276125595e-05\n",
            "step: 330, loss: 0.0012225676327943802\n",
            "step: 340, loss: 0.00044268398778513074\n",
            "step: 350, loss: 1.4520880540658254e-05\n",
            "step: 360, loss: 2.071179733320605e-05\n",
            "step: 370, loss: 8.737706957617775e-05\n",
            "step: 380, loss: 0.0002338382473681122\n",
            "step: 390, loss: 1.2721538951154798e-05\n",
            "step: 400, loss: 0.0023848488926887512\n",
            "step: 410, loss: 9.320549906988163e-06\n",
            "step: 420, loss: 1.5727604477433488e-05\n",
            "step: 430, loss: 3.522250335663557e-05\n",
            "step: 440, loss: 1.892762702482287e-05\n",
            "step: 450, loss: 1.5012609765108209e-05\n",
            "step: 460, loss: 0.00018731319869402796\n",
            "step: 470, loss: 2.9084563720971346e-05\n",
            "step: 480, loss: 1.450973650207743e-05\n",
            "step: 490, loss: 4.119515142519958e-05\n",
            "step: 500, loss: 0.0021394367795437574\n",
            "step: 510, loss: 0.00018657572218216956\n",
            "step: 520, loss: 5.393085302785039e-05\n",
            "step: 530, loss: 0.0003247068088967353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9551820728291317, f1=0.9468779123951537, best_f1=0.9471243042671614\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.9747987355221994e-05\n",
            "step: 10, loss: 0.0005938876420259476\n",
            "step: 20, loss: 0.00015338478260673583\n",
            "step: 30, loss: 0.0017464427510276437\n",
            "step: 40, loss: 2.6004940082202666e-05\n",
            "step: 50, loss: 0.0001268385531147942\n",
            "step: 60, loss: 3.9983140595722944e-05\n",
            "step: 70, loss: 2.1628158719977364e-05\n",
            "step: 80, loss: 2.5620931410230696e-05\n",
            "step: 90, loss: 2.5036215447471477e-05\n",
            "step: 100, loss: 1.0803166333062109e-05\n",
            "step: 110, loss: 2.8809414288843982e-05\n",
            "step: 120, loss: 1.8599836039356887e-05\n",
            "step: 130, loss: 1.9646622604341246e-05\n",
            "step: 140, loss: 2.601748929009773e-05\n",
            "step: 150, loss: 2.3077072910382412e-05\n",
            "step: 160, loss: 2.8074720830773003e-05\n",
            "step: 170, loss: 2.8716800443362445e-05\n",
            "step: 180, loss: 6.191983266035095e-05\n",
            "step: 190, loss: 0.0026829584967345\n",
            "step: 200, loss: 0.0013407740043476224\n",
            "step: 210, loss: 2.2111236830824055e-05\n",
            "step: 220, loss: 1.7195488908328116e-05\n",
            "step: 230, loss: 0.0003794324875343591\n",
            "step: 240, loss: 1.5686850019847043e-05\n",
            "step: 250, loss: 1.1335855560901109e-05\n",
            "step: 260, loss: 1.3399620911513921e-05\n",
            "step: 270, loss: 1.995575803448446e-05\n",
            "step: 280, loss: 1.210327900480479e-05\n",
            "step: 290, loss: 2.1644676962750964e-05\n",
            "step: 300, loss: 1.5172736311797053e-05\n",
            "step: 310, loss: 0.027323201298713684\n",
            "step: 320, loss: 2.9571718187071383e-05\n",
            "step: 330, loss: 2.4623303033877164e-05\n",
            "step: 340, loss: 2.3561577108921483e-05\n",
            "step: 350, loss: 0.0005803278181701899\n",
            "step: 360, loss: 3.7511221307795495e-05\n",
            "step: 370, loss: 2.9294513296918012e-05\n",
            "step: 380, loss: 0.00010728052438935265\n",
            "step: 390, loss: 3.438362182350829e-05\n",
            "step: 400, loss: 0.005160119384527206\n",
            "step: 410, loss: 8.912247722037137e-05\n",
            "step: 420, loss: 1.5247140254359692e-05\n",
            "step: 430, loss: 9.585038242221344e-06\n",
            "step: 440, loss: 0.0009688138961791992\n",
            "step: 450, loss: 0.001367163727991283\n",
            "step: 460, loss: 0.0008695179130882025\n",
            "step: 470, loss: 1.5247208466462325e-05\n",
            "step: 480, loss: 0.002361948136240244\n",
            "step: 490, loss: 0.00010925287642749026\n",
            "step: 500, loss: 0.0005928142927587032\n",
            "step: 510, loss: 4.31550397479441e-05\n",
            "step: 520, loss: 6.196799949975684e-05\n",
            "step: 530, loss: 1.577985312906094e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9556282111163008, f1=0.9468779123951537, best_f1=0.9471243042671614\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:27, 207.49it/s]\n",
            "load_f1 = 0.9559573481687529\n",
            "real_f1 = 0.9536178107606679\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:26, 166.52it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "tb_EWW7DgNFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "-oQ7ANLogNFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af6d7a0-5a22-424d-cde3-0ba25133d797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5155778527259827\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4549299478530884\n",
            "step: 20, loss: 0.4210590124130249\n",
            "step: 30, loss: 0.3467528223991394\n",
            "step: 40, loss: 0.35602065920829773\n",
            "step: 50, loss: 0.41927188634872437\n",
            "step: 60, loss: 0.4874627888202667\n",
            "step: 70, loss: 0.30477967858314514\n",
            "step: 80, loss: 0.3670080304145813\n",
            "step: 90, loss: 0.2171422690153122\n",
            "step: 100, loss: 0.2738848328590393\n",
            "step: 110, loss: 0.2638636529445648\n",
            "step: 120, loss: 0.3125879466533661\n",
            "step: 130, loss: 0.268472284078598\n",
            "step: 140, loss: 0.4236496686935425\n",
            "step: 150, loss: 0.18624068796634674\n",
            "step: 160, loss: 0.2984656095504761\n",
            "step: 170, loss: 0.10571030527353287\n",
            "step: 180, loss: 0.25917384028434753\n",
            "step: 190, loss: 0.4443119764328003\n",
            "step: 200, loss: 0.26302197575569153\n",
            "step: 210, loss: 0.32049360871315\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5580448065173116, f1=0.6021505376344086, best_f1=0.6021505376344086\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2562345564365387\n",
            "step: 10, loss: 0.07473249733448029\n",
            "step: 20, loss: 0.3290693759918213\n",
            "step: 30, loss: 0.24858036637306213\n",
            "step: 40, loss: 0.3928782343864441\n",
            "step: 50, loss: 0.13339802622795105\n",
            "step: 60, loss: 0.3200238049030304\n",
            "step: 70, loss: 0.17974668741226196\n",
            "step: 80, loss: 0.2757015824317932\n",
            "step: 90, loss: 0.20416006445884705\n",
            "step: 100, loss: 0.44972139596939087\n",
            "step: 110, loss: 0.3422737419605255\n",
            "step: 120, loss: 0.0958266630768776\n",
            "step: 130, loss: 0.26221784949302673\n",
            "step: 140, loss: 0.10715440660715103\n",
            "step: 150, loss: 0.2984318733215332\n",
            "step: 160, loss: 0.07618987560272217\n",
            "step: 170, loss: 0.2587047517299652\n",
            "step: 180, loss: 0.13406042754650116\n",
            "step: 190, loss: 0.2423725575208664\n",
            "step: 200, loss: 0.09407778829336166\n",
            "step: 210, loss: 0.09281625598669052\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.6211180124223603, f1=0.6495049504950495, best_f1=0.6495049504950495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05826778709888458\n",
            "step: 10, loss: 0.07293475419282913\n",
            "step: 20, loss: 0.29122474789619446\n",
            "step: 30, loss: 0.09951261430978775\n",
            "step: 40, loss: 0.18458281457424164\n",
            "step: 50, loss: 0.12796422839164734\n",
            "step: 60, loss: 0.19532734155654907\n",
            "step: 70, loss: 0.0559188649058342\n",
            "step: 80, loss: 0.13691014051437378\n",
            "step: 90, loss: 0.04730089381337166\n",
            "step: 100, loss: 0.11861715465784073\n",
            "step: 110, loss: 0.13230367004871368\n",
            "step: 120, loss: 0.12355025857686996\n",
            "step: 130, loss: 0.14565418660640717\n",
            "step: 140, loss: 0.08644027262926102\n",
            "step: 150, loss: 0.0984036773443222\n",
            "step: 160, loss: 0.1656700074672699\n",
            "step: 170, loss: 0.22207826375961304\n",
            "step: 180, loss: 0.10968732088804245\n",
            "step: 190, loss: 0.03209807351231575\n",
            "step: 200, loss: 0.14467589557170868\n",
            "step: 210, loss: 0.09719985723495483\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.6194331983805668, f1=0.6268656716417911, best_f1=0.6495049504950495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05604707449674606\n",
            "step: 10, loss: 0.0844237357378006\n",
            "step: 20, loss: 0.1981213390827179\n",
            "step: 30, loss: 0.11955415457487106\n",
            "step: 40, loss: 0.02689272351562977\n",
            "step: 50, loss: 0.10619001090526581\n",
            "step: 60, loss: 0.09742650389671326\n",
            "step: 70, loss: 0.08714254200458527\n",
            "step: 80, loss: 0.08191680163145065\n",
            "step: 90, loss: 0.028596075251698494\n",
            "step: 100, loss: 0.14794030785560608\n",
            "step: 110, loss: 0.2448480874300003\n",
            "step: 120, loss: 0.17996545135974884\n",
            "step: 130, loss: 0.21500681340694427\n",
            "step: 140, loss: 0.17128226161003113\n",
            "step: 150, loss: 0.19281503558158875\n",
            "step: 160, loss: 0.0665188729763031\n",
            "step: 170, loss: 0.06345128268003464\n",
            "step: 180, loss: 0.024969002231955528\n",
            "step: 190, loss: 0.19598563015460968\n",
            "step: 200, loss: 0.1179322823882103\n",
            "step: 210, loss: 0.28677377104759216\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.6205607476635514, f1=0.6469428007889546, best_f1=0.6495049504950495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09146839380264282\n",
            "step: 10, loss: 0.01393413357436657\n",
            "step: 20, loss: 0.022417984902858734\n",
            "step: 30, loss: 0.0343298502266407\n",
            "step: 40, loss: 0.05592191591858864\n",
            "step: 50, loss: 0.14227190613746643\n",
            "step: 60, loss: 0.06748957186937332\n",
            "step: 70, loss: 0.36031389236450195\n",
            "step: 80, loss: 0.06186017766594887\n",
            "step: 90, loss: 0.04791367053985596\n",
            "step: 100, loss: 0.201520636677742\n",
            "step: 110, loss: 0.23018178343772888\n",
            "step: 120, loss: 0.09179428219795227\n",
            "step: 130, loss: 0.03844389691948891\n",
            "step: 140, loss: 0.1782013326883316\n",
            "step: 150, loss: 0.08067953586578369\n",
            "step: 160, loss: 0.05448570102453232\n",
            "step: 170, loss: 0.028607646003365517\n",
            "step: 180, loss: 0.18256275355815887\n",
            "step: 190, loss: 0.17674468457698822\n",
            "step: 200, loss: 0.09939305484294891\n",
            "step: 210, loss: 0.02697477675974369\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.6195899772209568, f1=0.64, best_f1=0.6495049504950495\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014090974815189838\n",
            "step: 10, loss: 0.08234016597270966\n",
            "step: 20, loss: 0.013842378742992878\n",
            "step: 30, loss: 0.16951802372932434\n",
            "step: 40, loss: 0.030039899051189423\n",
            "step: 50, loss: 0.029522927477955818\n",
            "step: 60, loss: 0.1345161646604538\n",
            "step: 70, loss: 0.05797918885946274\n",
            "step: 80, loss: 0.16371388733386993\n",
            "step: 90, loss: 0.013280712068080902\n",
            "step: 100, loss: 0.062232181429862976\n",
            "step: 110, loss: 0.011154682375490665\n",
            "step: 120, loss: 0.007420635316520929\n",
            "step: 130, loss: 0.013572599738836288\n",
            "step: 140, loss: 0.07603595405817032\n",
            "step: 150, loss: 0.04982564598321915\n",
            "step: 160, loss: 0.09939340502023697\n",
            "step: 170, loss: 0.0771949291229248\n",
            "step: 180, loss: 0.08326806128025055\n",
            "step: 190, loss: 0.0746644139289856\n",
            "step: 200, loss: 0.2636849582195282\n",
            "step: 210, loss: 0.0740513727068901\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.6330935251798561, f1=0.6791044776119403, best_f1=0.6791044776119403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0467936247587204\n",
            "step: 10, loss: 0.0031366439070552588\n",
            "step: 20, loss: 0.044848937541246414\n",
            "step: 30, loss: 0.09173718094825745\n",
            "step: 40, loss: 0.036165300756692886\n",
            "step: 50, loss: 0.12875673174858093\n",
            "step: 60, loss: 0.08605720102787018\n",
            "step: 70, loss: 0.14492319524288177\n",
            "step: 80, loss: 0.05187069624662399\n",
            "step: 90, loss: 0.08592003583908081\n",
            "step: 100, loss: 0.015270018018782139\n",
            "step: 110, loss: 0.08661326766014099\n",
            "step: 120, loss: 0.031904496252536774\n",
            "step: 130, loss: 0.09759752452373505\n",
            "step: 140, loss: 0.032417573034763336\n",
            "step: 150, loss: 0.025669490918517113\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 160, loss: 0.202272430062294\n",
            "step: 170, loss: 0.06953588873147964\n",
            "step: 180, loss: 0.01935150846838951\n",
            "step: 190, loss: 0.04871245101094246\n",
            "step: 200, loss: 0.012845885939896107\n",
            "step: 210, loss: 0.18136243522167206\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.606425702811245, f1=0.6598778004073319, best_f1=0.6791044776119403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.06370304524898529\n",
            "step: 10, loss: 0.0029892725870013237\n",
            "step: 20, loss: 0.0020594075322151184\n",
            "step: 30, loss: 0.004129985347390175\n",
            "step: 40, loss: 0.010907850228250027\n",
            "step: 50, loss: 0.0005164567264728248\n",
            "step: 60, loss: 0.01460691262036562\n",
            "step: 70, loss: 0.03664705157279968\n",
            "step: 80, loss: 0.11457548290491104\n",
            "step: 90, loss: 0.014430524781346321\n",
            "step: 100, loss: 0.1408519595861435\n",
            "step: 110, loss: 0.011408142745494843\n",
            "step: 120, loss: 0.05065353587269783\n",
            "step: 130, loss: 0.003978252876549959\n",
            "step: 140, loss: 0.030349375680088997\n",
            "step: 150, loss: 0.062485065311193466\n",
            "step: 160, loss: 0.054153501987457275\n",
            "step: 170, loss: 0.14880135655403137\n",
            "step: 180, loss: 0.024286920204758644\n",
            "step: 190, loss: 0.00465440982952714\n",
            "step: 200, loss: 0.02883940376341343\n",
            "step: 210, loss: 0.06088022142648697\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.6362098138747885, f1=0.6308724832214765, best_f1=0.6308724832214765\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03650864213705063\n",
            "step: 10, loss: 0.022350018844008446\n",
            "step: 20, loss: 0.11507356911897659\n",
            "step: 30, loss: 0.003893547225743532\n",
            "step: 40, loss: 0.00713984714820981\n",
            "step: 50, loss: 0.04833120480179787\n",
            "step: 60, loss: 0.006528419908136129\n",
            "step: 70, loss: 0.003538592718541622\n",
            "step: 80, loss: 0.43777501583099365\n",
            "step: 90, loss: 0.02076655812561512\n",
            "step: 100, loss: 0.06621020287275314\n",
            "step: 110, loss: 0.030348701402544975\n",
            "step: 120, loss: 0.035650692880153656\n",
            "step: 130, loss: 0.08792688697576523\n",
            "step: 140, loss: 0.05886368826031685\n",
            "step: 150, loss: 0.1446310579776764\n",
            "step: 160, loss: 0.06500979512929916\n",
            "step: 170, loss: 0.008998574689030647\n",
            "step: 180, loss: 0.015772460028529167\n",
            "step: 190, loss: 0.10775304585695267\n",
            "step: 200, loss: 0.02159402333199978\n",
            "step: 210, loss: 0.007951818406581879\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.6460905349794239, f1=0.6872427983539094, best_f1=0.6872427983539094\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007946825935505331\n",
            "step: 10, loss: 0.014281034469604492\n",
            "step: 20, loss: 0.06531878560781479\n",
            "step: 30, loss: 0.001954765524715185\n",
            "step: 40, loss: 0.006046005990356207\n",
            "step: 50, loss: 0.03896232321858406\n",
            "step: 60, loss: 0.0004035198944620788\n",
            "step: 70, loss: 0.0033294600434601307\n",
            "step: 80, loss: 0.0022967655677348375\n",
            "step: 90, loss: 0.013368036597967148\n",
            "step: 100, loss: 0.0019880631007254124\n",
            "step: 110, loss: 0.0009769027819857001\n",
            "step: 120, loss: 0.006812750827521086\n",
            "step: 130, loss: 0.018560148775577545\n",
            "step: 140, loss: 0.0013402783079072833\n",
            "step: 150, loss: 0.0002617717836983502\n",
            "step: 160, loss: 0.0012007065815851092\n",
            "step: 170, loss: 0.017706220969557762\n",
            "step: 180, loss: 0.004200418945401907\n",
            "step: 190, loss: 0.0012072839308530092\n",
            "step: 200, loss: 0.02610691823065281\n",
            "step: 210, loss: 0.037203699350357056\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.6487523992322457, f1=0.6691449814126395, best_f1=0.6691449814126395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07392851263284683\n",
            "step: 10, loss: 0.0010850524995476007\n",
            "step: 20, loss: 0.0006368468166328967\n",
            "step: 30, loss: 0.0035688993521034718\n",
            "step: 40, loss: 0.023724844679236412\n",
            "step: 50, loss: 0.03230703994631767\n",
            "step: 60, loss: 0.003816422773525119\n",
            "step: 70, loss: 0.0019243961432948709\n",
            "step: 80, loss: 0.01626993529498577\n",
            "step: 90, loss: 0.17873656749725342\n",
            "step: 100, loss: 0.02455108053982258\n",
            "step: 110, loss: 0.005556050222367048\n",
            "step: 120, loss: 0.04406498000025749\n",
            "step: 130, loss: 0.000672699126880616\n",
            "step: 140, loss: 0.0236563291400671\n",
            "step: 150, loss: 0.009032146073877811\n",
            "step: 160, loss: 0.00078672164818272\n",
            "step: 170, loss: 0.03731634095311165\n",
            "step: 180, loss: 0.17712868750095367\n",
            "step: 190, loss: 0.0739326998591423\n",
            "step: 200, loss: 0.0018096267012879252\n",
            "step: 210, loss: 0.029065126553177834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6475409836065574, f1=0.6680327868852459, best_f1=0.6691449814126395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007470162236131728\n",
            "step: 10, loss: 0.0028912269044667482\n",
            "step: 20, loss: 0.055608585476875305\n",
            "step: 30, loss: 0.002365159336477518\n",
            "step: 40, loss: 0.0010938779450953007\n",
            "step: 50, loss: 0.007347205653786659\n",
            "step: 60, loss: 0.0013006675289943814\n",
            "step: 70, loss: 0.06257250159978867\n",
            "step: 80, loss: 0.021837159991264343\n",
            "step: 90, loss: 0.00683048740029335\n",
            "step: 100, loss: 0.0013096665497869253\n",
            "step: 110, loss: 0.06596965342760086\n",
            "step: 120, loss: 0.0002917995734605938\n",
            "step: 130, loss: 0.0018416373059153557\n",
            "step: 140, loss: 0.01808067224919796\n",
            "step: 150, loss: 8.733119466342032e-05\n",
            "step: 160, loss: 0.0028132153674960136\n",
            "step: 170, loss: 0.00174841214902699\n",
            "step: 180, loss: 0.014916813932359219\n",
            "step: 190, loss: 0.000921492523048073\n",
            "step: 200, loss: 0.009275502525269985\n",
            "step: 210, loss: 0.00025954667944461107\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.6224066390041494, f1=0.6652892561983471, best_f1=0.6691449814126395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005721098277717829\n",
            "step: 10, loss: 0.0754278302192688\n",
            "step: 20, loss: 0.0004934130702167749\n",
            "step: 30, loss: 0.0006636235048063099\n",
            "step: 40, loss: 0.0006342585547827184\n",
            "step: 50, loss: 0.032672978937625885\n",
            "step: 60, loss: 0.0028503250796347857\n",
            "step: 70, loss: 0.0022064491640776396\n",
            "step: 80, loss: 0.05686541274189949\n",
            "step: 90, loss: 0.004521395545452833\n",
            "step: 100, loss: 0.010096589103341103\n",
            "step: 110, loss: 0.16394998133182526\n",
            "step: 120, loss: 0.001051162602379918\n",
            "step: 130, loss: 0.000122404468129389\n",
            "step: 140, loss: 0.0018838188843801618\n",
            "step: 150, loss: 0.00013624754501506686\n",
            "step: 160, loss: 0.011759239248931408\n",
            "step: 170, loss: 0.0004815109714400023\n",
            "step: 180, loss: 0.015852320939302444\n",
            "step: 190, loss: 0.03226740285754204\n",
            "step: 200, loss: 0.0004893321311101317\n",
            "step: 210, loss: 0.00045045174192637205\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.6359832635983262, f1=0.6540084388185654, best_f1=0.6691449814126395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004535629414021969\n",
            "step: 10, loss: 0.01225584652274847\n",
            "step: 20, loss: 0.0012840701965615153\n",
            "step: 30, loss: 0.012922386638820171\n",
            "step: 40, loss: 0.0006413118680939078\n",
            "step: 50, loss: 0.0032047717832028866\n",
            "step: 60, loss: 0.0038719007279723883\n",
            "step: 70, loss: 0.0002213678671978414\n",
            "step: 80, loss: 0.03359135612845421\n",
            "step: 90, loss: 0.004350944887846708\n",
            "step: 100, loss: 0.0018070843070745468\n",
            "step: 110, loss: 0.00015332929615397006\n",
            "step: 120, loss: 0.00017839166685007513\n",
            "step: 130, loss: 0.018970375880599022\n",
            "step: 140, loss: 0.0005133681115694344\n",
            "step: 150, loss: 0.08049193024635315\n",
            "step: 160, loss: 0.012999029830098152\n",
            "step: 170, loss: 0.032794248312711716\n",
            "step: 180, loss: 0.004714951850473881\n",
            "step: 190, loss: 0.0031855173874646425\n",
            "step: 200, loss: 0.020278265699744225\n",
            "step: 210, loss: 0.05018579587340355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6464646464646464, f1=0.676, best_f1=0.6691449814126395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002418714575469494\n",
            "step: 10, loss: 0.0025451271794736385\n",
            "step: 20, loss: 0.0021405501756817102\n",
            "step: 30, loss: 0.018899299204349518\n",
            "step: 40, loss: 0.0003134789294563234\n",
            "step: 50, loss: 7.295682735275477e-05\n",
            "step: 60, loss: 0.014263210818171501\n",
            "step: 70, loss: 0.019906731322407722\n",
            "step: 80, loss: 0.0072339195758104324\n",
            "step: 90, loss: 0.00012581197370309383\n",
            "step: 100, loss: 0.0006886543706059456\n",
            "step: 110, loss: 0.02487279288470745\n",
            "step: 120, loss: 0.0009317626245319843\n",
            "step: 130, loss: 0.00012559734750539064\n",
            "step: 140, loss: 0.009036689065396786\n",
            "step: 150, loss: 0.0017047615256160498\n",
            "step: 160, loss: 0.009899711236357689\n",
            "step: 170, loss: 0.02314736880362034\n",
            "step: 180, loss: 0.001206891844049096\n",
            "step: 190, loss: 0.00010162828402826563\n",
            "step: 200, loss: 0.0025260986294597387\n",
            "step: 210, loss: 0.02702757529914379\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6432989690721649, f1=0.6666666666666666, best_f1=0.6691449814126395\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:06, 348.50it/s]\n",
            "load_f1 = 0.6407766990291262\n",
            "real_f1 = 0.6513409961685824\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:26, 166.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "NC7Q_ekTgNFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "iIIoASlugNFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168c6375-c17c-44e6-c8bf-091a93b2f0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4524528682231903\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4907338619232178\n",
            "step: 20, loss: 0.27211910486221313\n",
            "step: 30, loss: 0.39673420786857605\n",
            "step: 40, loss: 0.30527472496032715\n",
            "step: 50, loss: 0.28430041670799255\n",
            "step: 60, loss: 0.4247785806655884\n",
            "step: 70, loss: 0.42897483706474304\n",
            "step: 80, loss: 0.16505342721939087\n",
            "step: 90, loss: 0.29101839661598206\n",
            "step: 100, loss: 0.4487752616405487\n",
            "step: 110, loss: 0.2445240467786789\n",
            "step: 120, loss: 0.3446422517299652\n",
            "step: 130, loss: 0.31203383207321167\n",
            "step: 140, loss: 0.1749337911605835\n",
            "step: 150, loss: 0.29575711488723755\n",
            "step: 160, loss: 0.22638320922851562\n",
            "step: 170, loss: 0.34926751255989075\n",
            "step: 180, loss: 0.16595710813999176\n",
            "step: 190, loss: 0.19163964688777924\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.24048913043478262, f1=0.25844346549192365, best_f1=0.25844346549192365\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32897746562957764\n",
            "step: 10, loss: 0.29385897517204285\n",
            "step: 20, loss: 0.7880050539970398\n",
            "step: 30, loss: 0.14498350024223328\n",
            "step: 40, loss: 0.3696051239967346\n",
            "step: 50, loss: 0.35615187883377075\n",
            "step: 60, loss: 0.24302446842193604\n",
            "step: 70, loss: 0.31367021799087524\n",
            "step: 80, loss: 0.04597628116607666\n",
            "step: 90, loss: 0.20745450258255005\n",
            "step: 100, loss: 0.1384061872959137\n",
            "step: 110, loss: 0.1571856141090393\n",
            "step: 120, loss: 0.04739963263273239\n",
            "step: 130, loss: 0.1797734647989273\n",
            "step: 140, loss: 0.25446003675460815\n",
            "step: 150, loss: 0.2253463715314865\n",
            "step: 160, loss: 0.3204880952835083\n",
            "step: 170, loss: 0.04249170795083046\n",
            "step: 180, loss: 0.03518984094262123\n",
            "step: 190, loss: 0.07188571244478226\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7861271676300577, f1=0.7988165680473371, best_f1=0.7988165680473371\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14519381523132324\n",
            "step: 10, loss: 0.1213764175772667\n",
            "step: 20, loss: 0.13455531001091003\n",
            "step: 30, loss: 0.0952657014131546\n",
            "step: 40, loss: 0.024308007210493088\n",
            "step: 50, loss: 0.1472628265619278\n",
            "step: 60, loss: 0.07685523480176926\n",
            "step: 70, loss: 0.11076179891824722\n",
            "step: 80, loss: 0.1212930753827095\n",
            "step: 90, loss: 0.05316886678338051\n",
            "step: 100, loss: 0.021097306162118912\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 110, loss: 0.15113277733325958\n",
            "step: 120, loss: 0.2707178592681885\n",
            "step: 130, loss: 0.16122573614120483\n",
            "step: 140, loss: 0.023192953318357468\n",
            "step: 150, loss: 0.1745539754629135\n",
            "step: 160, loss: 0.07499146461486816\n",
            "step: 170, loss: 0.2276066094636917\n",
            "step: 180, loss: 0.17159993946552277\n",
            "step: 190, loss: 0.11122038960456848\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8136482939632546, f1=0.8083989501312335, best_f1=0.8083989501312335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022534560412168503\n",
            "step: 10, loss: 0.1352987289428711\n",
            "step: 20, loss: 0.040068238973617554\n",
            "step: 30, loss: 0.013148462399840355\n",
            "step: 40, loss: 0.14025504887104034\n",
            "step: 50, loss: 0.01124374009668827\n",
            "step: 60, loss: 0.04349396377801895\n",
            "step: 70, loss: 0.02749166637659073\n",
            "step: 80, loss: 0.008862743154168129\n",
            "step: 90, loss: 0.028663162142038345\n",
            "step: 100, loss: 0.15582405030727386\n",
            "step: 110, loss: 0.14893163740634918\n",
            "step: 120, loss: 0.08504307270050049\n",
            "step: 130, loss: 0.0334814116358757\n",
            "step: 140, loss: 0.1351631134748459\n",
            "step: 150, loss: 0.009092418476939201\n",
            "step: 160, loss: 0.01869615912437439\n",
            "step: 170, loss: 0.13003908097743988\n",
            "step: 180, loss: 0.03491974622011185\n",
            "step: 190, loss: 0.01418572012335062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8324607329842932, f1=0.8494623655913978, best_f1=0.8494623655913978\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.017391661182045937\n",
            "step: 10, loss: 0.07477036118507385\n",
            "step: 20, loss: 0.011140014044940472\n",
            "step: 30, loss: 0.08745303004980087\n",
            "step: 40, loss: 0.02060439996421337\n",
            "step: 50, loss: 0.018678108230233192\n",
            "step: 60, loss: 0.08724801242351532\n",
            "step: 70, loss: 0.019251735880970955\n",
            "step: 80, loss: 0.0014659682055935264\n",
            "step: 90, loss: 0.2125636637210846\n",
            "step: 100, loss: 0.17145968973636627\n",
            "step: 110, loss: 0.09067215025424957\n",
            "step: 120, loss: 0.0365484394133091\n",
            "step: 130, loss: 0.2884424924850464\n",
            "step: 140, loss: 0.029751883819699287\n",
            "step: 150, loss: 0.2382117211818695\n",
            "step: 160, loss: 0.04066860303282738\n",
            "step: 170, loss: 0.10581749677658081\n",
            "step: 180, loss: 0.016484569758176804\n",
            "step: 190, loss: 0.013713584281504154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.8264462809917356, f1=0.8351648351648351, best_f1=0.8494623655913978\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003224472515285015\n",
            "step: 10, loss: 0.014792942441999912\n",
            "step: 20, loss: 0.08349142223596573\n",
            "step: 30, loss: 0.04143908992409706\n",
            "step: 40, loss: 0.007846100255846977\n",
            "step: 50, loss: 0.05932077020406723\n",
            "step: 60, loss: 0.04909524321556091\n",
            "step: 70, loss: 0.021928641945123672\n",
            "step: 80, loss: 0.11015064269304276\n",
            "step: 90, loss: 0.009451083838939667\n",
            "step: 100, loss: 0.09431928396224976\n",
            "step: 110, loss: 0.007132353261113167\n",
            "step: 120, loss: 0.03173824027180672\n",
            "step: 130, loss: 0.03999459743499756\n",
            "step: 140, loss: 0.0015092118410393596\n",
            "step: 150, loss: 0.008557706139981747\n",
            "step: 160, loss: 0.05700521171092987\n",
            "step: 170, loss: 0.036688465625047684\n",
            "step: 180, loss: 0.03188788890838623\n",
            "step: 190, loss: 0.003936567343771458\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8369565217391304, f1=0.8337874659400545, best_f1=0.8337874659400545\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008806012570858002\n",
            "step: 10, loss: 0.03935234621167183\n",
            "step: 20, loss: 0.009343760088086128\n",
            "step: 30, loss: 0.026387128978967667\n",
            "step: 40, loss: 0.002684509614482522\n",
            "step: 50, loss: 0.10411889851093292\n",
            "step: 60, loss: 0.04248470440506935\n",
            "step: 70, loss: 0.0035682932939380407\n",
            "step: 80, loss: 0.00499187596142292\n",
            "step: 90, loss: 0.0013496403116732836\n",
            "step: 100, loss: 0.22968028485774994\n",
            "step: 110, loss: 0.05447835847735405\n",
            "step: 120, loss: 0.07894329726696014\n",
            "step: 130, loss: 0.03985690698027611\n",
            "step: 140, loss: 0.009223251603543758\n",
            "step: 150, loss: 0.021707020699977875\n",
            "step: 160, loss: 0.10034465789794922\n",
            "step: 170, loss: 0.051806677132844925\n",
            "step: 180, loss: 0.0035703438334167004\n",
            "step: 190, loss: 0.017221763730049133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8146067415730337, f1=0.8409090909090909, best_f1=0.8337874659400545\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.050191182643175125\n",
            "step: 10, loss: 0.01789926551282406\n",
            "step: 20, loss: 0.0015603273641318083\n",
            "step: 30, loss: 0.2533629536628723\n",
            "step: 40, loss: 0.04932699725031853\n",
            "step: 50, loss: 0.03417586162686348\n",
            "step: 60, loss: 0.019867878407239914\n",
            "step: 70, loss: 0.010573500767350197\n",
            "step: 80, loss: 0.009977566078305244\n",
            "step: 90, loss: 0.0009323521517217159\n",
            "step: 100, loss: 0.0013634904753416777\n",
            "step: 110, loss: 0.011114557273685932\n",
            "step: 120, loss: 0.0016037049936130643\n",
            "step: 130, loss: 0.0017963576829060912\n",
            "step: 140, loss: 0.013204214163124561\n",
            "step: 150, loss: 0.002057113917544484\n",
            "step: 160, loss: 0.0031921351328492165\n",
            "step: 170, loss: 0.0011507606832310557\n",
            "step: 180, loss: 0.19511404633522034\n",
            "step: 190, loss: 0.0023152311332523823\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8355795148247979, f1=0.8484848484848484, best_f1=0.8337874659400545\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005188150680623949\n",
            "step: 10, loss: 0.001319206552579999\n",
            "step: 20, loss: 0.0053605083376169205\n",
            "step: 30, loss: 0.0010015040170401335\n",
            "step: 40, loss: 0.12253192067146301\n",
            "step: 50, loss: 0.01081763580441475\n",
            "step: 60, loss: 0.04294270649552345\n",
            "step: 70, loss: 0.003155095735564828\n",
            "step: 80, loss: 0.006782537326216698\n",
            "step: 90, loss: 0.011205809190869331\n",
            "step: 100, loss: 0.0005591197405010462\n",
            "step: 110, loss: 0.00037901653558947146\n",
            "step: 120, loss: 0.0013078255578875542\n",
            "step: 130, loss: 0.11948736757040024\n",
            "step: 140, loss: 0.01134692132472992\n",
            "step: 150, loss: 0.0016569209983572364\n",
            "step: 160, loss: 0.0020877490751445293\n",
            "step: 170, loss: 0.003609568113461137\n",
            "step: 180, loss: 0.07667506486177444\n",
            "step: 190, loss: 0.0006326072034426033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8337595907928389, f1=0.8494623655913978, best_f1=0.8337874659400545\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002131679095327854\n",
            "step: 10, loss: 0.0004263495502527803\n",
            "step: 20, loss: 0.001966915326192975\n",
            "step: 30, loss: 0.2815450429916382\n",
            "step: 40, loss: 0.0026914412155747414\n",
            "step: 50, loss: 0.0025167304556816816\n",
            "step: 60, loss: 0.0009985195938497782\n",
            "step: 70, loss: 0.000855988881085068\n",
            "step: 80, loss: 0.0015976761933416128\n",
            "step: 90, loss: 0.0003698104410432279\n",
            "step: 100, loss: 0.02567587047815323\n",
            "step: 110, loss: 0.0013943869853392243\n",
            "step: 120, loss: 0.000962807098403573\n",
            "step: 130, loss: 0.00796717219054699\n",
            "step: 140, loss: 0.0005162382731214166\n",
            "step: 150, loss: 0.00044685660395771265\n",
            "step: 160, loss: 0.0005195031990297139\n",
            "step: 170, loss: 0.019152633845806122\n",
            "step: 180, loss: 0.009658312425017357\n",
            "step: 190, loss: 0.0004969689762219787\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.8378378378378378, f1=0.8324022346368716, best_f1=0.8324022346368716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008024988346733153\n",
            "step: 10, loss: 0.001802169717848301\n",
            "step: 20, loss: 0.000561979366466403\n",
            "step: 30, loss: 0.0017122544813901186\n",
            "step: 40, loss: 0.0004310287768021226\n",
            "step: 50, loss: 0.0005994862876832485\n",
            "step: 60, loss: 0.004688428249210119\n",
            "step: 70, loss: 0.0006364152650348842\n",
            "step: 80, loss: 0.003338287118822336\n",
            "step: 90, loss: 0.0005982921575196087\n",
            "step: 100, loss: 0.0005334258312359452\n",
            "step: 110, loss: 0.23941093683242798\n",
            "step: 120, loss: 0.0007154257618822157\n",
            "step: 130, loss: 0.0008126737084239721\n",
            "step: 140, loss: 0.14616768062114716\n",
            "step: 150, loss: 0.000703020894434303\n",
            "step: 160, loss: 0.0016103534726426005\n",
            "step: 170, loss: 0.002420293167233467\n",
            "step: 180, loss: 0.002739283721894026\n",
            "step: 190, loss: 0.0020353677682578564\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8238482384823848, f1=0.8222222222222222, best_f1=0.8324022346368716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017361396457999945\n",
            "step: 10, loss: 0.0012073327088728547\n",
            "step: 20, loss: 0.0021266101393848658\n",
            "step: 30, loss: 0.0017217686399817467\n",
            "step: 40, loss: 0.0006146621308289468\n",
            "step: 50, loss: 0.0034591336734592915\n",
            "step: 60, loss: 0.15541177988052368\n",
            "step: 70, loss: 0.002414047485217452\n",
            "step: 80, loss: 0.002442194614559412\n",
            "step: 90, loss: 0.0007013115100562572\n",
            "step: 100, loss: 0.0008286202792078257\n",
            "step: 110, loss: 0.010561969131231308\n",
            "step: 120, loss: 0.003898903261870146\n",
            "step: 130, loss: 0.001975443447008729\n",
            "step: 140, loss: 0.0012863982701674104\n",
            "step: 150, loss: 0.0018359303940087557\n",
            "step: 160, loss: 0.001220298116095364\n",
            "step: 170, loss: 0.03301326185464859\n",
            "step: 180, loss: 0.0005484000430442393\n",
            "step: 190, loss: 0.0060249860398471355\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.847645429362881, f1=0.8441926345609065, best_f1=0.8441926345609065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007238429388962686\n",
            "step: 10, loss: 0.0008522921125404537\n",
            "step: 20, loss: 0.0010536123299971223\n",
            "step: 30, loss: 0.007968918420374393\n",
            "step: 40, loss: 0.0009080563322640955\n",
            "step: 50, loss: 0.0007999810623005033\n",
            "step: 60, loss: 0.000802733819000423\n",
            "step: 70, loss: 0.0029882495291531086\n",
            "step: 80, loss: 0.0005649300292134285\n",
            "step: 90, loss: 0.002086290856823325\n",
            "step: 100, loss: 0.00035952453617937863\n",
            "step: 110, loss: 0.0005499051185324788\n",
            "step: 120, loss: 0.0003910073428414762\n",
            "step: 130, loss: 0.0005928543978370726\n",
            "step: 140, loss: 0.00135558913461864\n",
            "step: 150, loss: 0.00040070750401355326\n",
            "step: 160, loss: 0.0013693647924810648\n",
            "step: 170, loss: 0.0003493904077913612\n",
            "step: 180, loss: 0.00047068309504538774\n",
            "step: 190, loss: 0.23384273052215576\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8502673796791445, f1=0.8446866485013624, best_f1=0.8446866485013624\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003100736648775637\n",
            "step: 10, loss: 0.00030023540602996945\n",
            "step: 20, loss: 0.0010315906256437302\n",
            "step: 30, loss: 0.0003508291265461594\n",
            "step: 40, loss: 0.000605689303483814\n",
            "step: 50, loss: 0.0004288745403755456\n",
            "step: 60, loss: 0.0006283089169301093\n",
            "step: 70, loss: 0.0005877601215615869\n",
            "step: 80, loss: 0.0007920641801320016\n",
            "step: 90, loss: 0.0003101295151282102\n",
            "step: 100, loss: 0.00031585185206495225\n",
            "step: 110, loss: 0.00046427000779658556\n",
            "step: 120, loss: 0.00042231837869621813\n",
            "step: 130, loss: 0.0003747475566342473\n",
            "step: 140, loss: 0.0005883436533622444\n",
            "step: 150, loss: 0.000545683316886425\n",
            "step: 160, loss: 0.0014972533099353313\n",
            "step: 170, loss: 0.0002384866529610008\n",
            "step: 180, loss: 0.0003403039008844644\n",
            "step: 190, loss: 0.0006743537960574031\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.8511749347258485, f1=0.8457446808510638, best_f1=0.8457446808510638\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00040967241511680186\n",
            "step: 10, loss: 0.00025176480994559824\n",
            "step: 20, loss: 0.0003250802110414952\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 30, loss: 0.0003382269642315805\n",
            "step: 40, loss: 0.0006465643527917564\n",
            "step: 50, loss: 0.00115263054613024\n",
            "step: 60, loss: 0.00030110549414530396\n",
            "step: 70, loss: 0.0003117382002528757\n",
            "step: 80, loss: 0.00026170045020990074\n",
            "step: 90, loss: 0.001055542379617691\n",
            "step: 100, loss: 0.0006865981267765164\n",
            "step: 110, loss: 0.00043705798452720046\n",
            "step: 120, loss: 0.00028607359854504466\n",
            "step: 130, loss: 0.00035346552613191307\n",
            "step: 140, loss: 0.0005744096706621349\n",
            "step: 150, loss: 0.0002357392368139699\n",
            "step: 160, loss: 0.0003909103979822248\n",
            "step: 170, loss: 0.0004509420832619071\n",
            "step: 180, loss: 0.000289744115434587\n",
            "step: 190, loss: 0.016078295186161995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8511749347258485, f1=0.8533333333333334, best_f1=0.8457446808510638\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:10, 197.00it/s]\n",
            "load_f1 = 0.8556149732620322\n",
            "real_f1 = 0.8486486486486486\n",
            "733it [00:00, 3406.57it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:26, 164.10it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE TEXTUAL"
      ],
      "metadata": {
        "id": "vWkqC6MWgNFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "dtPR9KRSgNFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "62jt5GiEgNFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900a4f12-30c5-4e58-c9d0-3f76886f5bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4928053319454193\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4349179267883301\n",
            "step: 20, loss: 0.33063292503356934\n",
            "step: 30, loss: 0.40531593561172485\n",
            "step: 40, loss: 0.5542669296264648\n",
            "step: 50, loss: 0.31882157921791077\n",
            "step: 60, loss: 0.5818251371383667\n",
            "step: 70, loss: 0.3471697270870209\n",
            "step: 80, loss: 0.2287585288286209\n",
            "step: 90, loss: 0.20188120007514954\n",
            "step: 100, loss: 0.17280900478363037\n",
            "step: 110, loss: 0.41780826449394226\n",
            "step: 120, loss: 0.31236588954925537\n",
            "step: 130, loss: 0.3141826391220093\n",
            "step: 140, loss: 0.4067370891571045\n",
            "step: 150, loss: 0.31920233368873596\n",
            "step: 160, loss: 0.398556649684906\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 170, loss: 0.3228471875190735\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32559365034103394\n",
            "step: 10, loss: 0.49822139739990234\n",
            "step: 20, loss: 0.3404408395290375\n",
            "step: 30, loss: 0.3393236994743347\n",
            "step: 40, loss: 0.07340560108423233\n",
            "step: 50, loss: 0.46622246503829956\n",
            "step: 60, loss: 0.19561553001403809\n",
            "step: 70, loss: 0.5166041254997253\n",
            "step: 80, loss: 0.23688791692256927\n",
            "step: 90, loss: 0.24644088745117188\n",
            "step: 100, loss: 0.5339024066925049\n",
            "step: 110, loss: 0.26294073462486267\n",
            "step: 120, loss: 0.2570662200450897\n",
            "step: 130, loss: 0.5252088904380798\n",
            "step: 140, loss: 0.5281322002410889\n",
            "step: 150, loss: 0.4463338553905487\n",
            "step: 160, loss: 0.4327203035354614\n",
            "step: 170, loss: 0.38180431723594666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6099401712417603\n",
            "step: 10, loss: 0.31569191813468933\n",
            "step: 20, loss: 0.234957754611969\n",
            "step: 30, loss: 0.24711769819259644\n",
            "step: 40, loss: 0.3734431564807892\n",
            "step: 50, loss: 0.5845890045166016\n",
            "step: 60, loss: 0.31050196290016174\n",
            "step: 70, loss: 0.23684018850326538\n",
            "step: 80, loss: 0.38218942284584045\n",
            "step: 90, loss: 0.5281054973602295\n",
            "step: 100, loss: 0.3052397072315216\n",
            "step: 110, loss: 0.18621434271335602\n",
            "step: 120, loss: 0.5757997035980225\n",
            "step: 130, loss: 0.5119825601577759\n",
            "step: 140, loss: 0.44268882274627686\n",
            "step: 150, loss: 0.19158978760242462\n",
            "step: 160, loss: 0.16297093033790588\n",
            "step: 170, loss: 0.3243035674095154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3867289125919342\n",
            "step: 10, loss: 0.5340400338172913\n",
            "step: 20, loss: 0.25765371322631836\n",
            "step: 30, loss: 0.4281710982322693\n",
            "step: 40, loss: 0.24596863985061646\n",
            "step: 50, loss: 0.3878962993621826\n",
            "step: 60, loss: 0.6871850490570068\n",
            "step: 70, loss: 0.3394131064414978\n",
            "step: 80, loss: 0.4719897210597992\n",
            "step: 90, loss: 0.31421464681625366\n",
            "step: 100, loss: 0.38599371910095215\n",
            "step: 110, loss: 0.4356541633605957\n",
            "step: 120, loss: 0.4794921875\n",
            "step: 130, loss: 0.312163770198822\n",
            "step: 140, loss: 0.2408446967601776\n",
            "step: 150, loss: 0.680359423160553\n",
            "step: 160, loss: 0.12367522716522217\n",
            "step: 170, loss: 0.23533591628074646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.378680020570755\n",
            "step: 10, loss: 0.3231655955314636\n",
            "step: 20, loss: 0.31657493114471436\n",
            "step: 30, loss: 0.30738696455955505\n",
            "step: 40, loss: 0.2504223585128784\n",
            "step: 50, loss: 0.2397632598876953\n",
            "step: 60, loss: 0.3061055839061737\n",
            "step: 70, loss: 0.34375855326652527\n",
            "step: 80, loss: 0.0818040743470192\n",
            "step: 90, loss: 0.5570517182350159\n",
            "step: 100, loss: 0.2815834879875183\n",
            "step: 110, loss: 0.2552187442779541\n",
            "step: 120, loss: 0.12900099158287048\n",
            "step: 130, loss: 0.23849014937877655\n",
            "step: 140, loss: 0.16912981867790222\n",
            "step: 150, loss: 0.31275832653045654\n",
            "step: 160, loss: 0.2439342439174652\n",
            "step: 170, loss: 0.3759259581565857\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.23356231599607455, f1=0.23814541622760801, best_f1=0.23814541622760801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3059719204902649\n",
            "step: 10, loss: 0.10533169656991959\n",
            "step: 20, loss: 0.3565874993801117\n",
            "step: 30, loss: 0.24812135100364685\n",
            "step: 40, loss: 0.4786665737628937\n",
            "step: 50, loss: 0.24954348802566528\n",
            "step: 60, loss: 0.3985234797000885\n",
            "step: 70, loss: 0.37315046787261963\n",
            "step: 80, loss: 0.21431003510951996\n",
            "step: 90, loss: 0.30784642696380615\n",
            "step: 100, loss: 0.18381322920322418\n",
            "step: 110, loss: 0.448807030916214\n",
            "step: 120, loss: 0.43727946281433105\n",
            "step: 130, loss: 0.49232590198516846\n",
            "step: 140, loss: 0.25271695852279663\n",
            "step: 150, loss: 0.18028776347637177\n",
            "step: 160, loss: 0.31032586097717285\n",
            "step: 170, loss: 0.2544368803501129\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.23814541622760801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2379506230354309\n",
            "step: 10, loss: 0.45694729685783386\n",
            "step: 20, loss: 0.46228885650634766\n",
            "step: 30, loss: 0.5348737835884094\n",
            "step: 40, loss: 0.12381000816822052\n",
            "step: 50, loss: 0.45130106806755066\n",
            "step: 60, loss: 0.5584824085235596\n",
            "step: 70, loss: 0.3721004128456116\n",
            "step: 80, loss: 0.1702057421207428\n",
            "step: 90, loss: 0.45268258452415466\n",
            "step: 100, loss: 0.32235440611839294\n",
            "step: 110, loss: 0.37733983993530273\n",
            "step: 120, loss: 0.3715292513370514\n",
            "step: 130, loss: 0.19262515008449554\n",
            "step: 140, loss: 0.12275274842977524\n",
            "step: 150, loss: 0.30880534648895264\n",
            "step: 160, loss: 0.38340285420417786\n",
            "step: 170, loss: 0.30660945177078247\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.23814541622760801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.46944504976272583\n",
            "step: 10, loss: 0.5199652314186096\n",
            "step: 20, loss: 0.24454271793365479\n",
            "step: 30, loss: 0.3177546262741089\n",
            "step: 40, loss: 0.24150945246219635\n",
            "step: 50, loss: 0.3874492943286896\n",
            "step: 60, loss: 0.38419806957244873\n",
            "step: 70, loss: 0.18955938518047333\n",
            "step: 80, loss: 0.31028884649276733\n",
            "step: 90, loss: 0.5762788653373718\n",
            "step: 100, loss: 0.25511854887008667\n",
            "step: 110, loss: 0.5150011777877808\n",
            "step: 120, loss: 0.32464152574539185\n",
            "step: 130, loss: 0.31521978974342346\n",
            "step: 140, loss: 0.5103778839111328\n",
            "step: 150, loss: 0.2455732673406601\n",
            "step: 160, loss: 0.17339982092380524\n",
            "step: 170, loss: 0.45518940687179565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.23814541622760801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3797317445278168\n",
            "step: 10, loss: 0.5951831936836243\n",
            "step: 20, loss: 0.38609758019447327\n",
            "step: 30, loss: 0.18192936480045319\n",
            "step: 40, loss: 0.37623530626296997\n",
            "step: 50, loss: 0.25748634338378906\n",
            "step: 60, loss: 0.3784882426261902\n",
            "step: 70, loss: 0.3056439459323883\n",
            "step: 80, loss: 0.1691942662000656\n",
            "step: 90, loss: 0.45615389943122864\n",
            "step: 100, loss: 0.505968451499939\n",
            "step: 110, loss: 0.3145592510700226\n",
            "step: 120, loss: 0.4425269663333893\n",
            "step: 130, loss: 0.3141370713710785\n",
            "step: 140, loss: 0.37432143092155457\n",
            "step: 150, loss: 0.3696785271167755\n",
            "step: 160, loss: 0.26204344630241394\n",
            "step: 170, loss: 0.4262712299823761\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.23814541622760801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4414629340171814\n",
            "step: 10, loss: 0.3164246678352356\n",
            "step: 20, loss: 0.24695025384426117\n",
            "step: 30, loss: 0.6404058933258057\n",
            "step: 40, loss: 0.37901148200035095\n",
            "step: 50, loss: 0.4469989538192749\n",
            "step: 60, loss: 0.5657257437705994\n",
            "step: 70, loss: 0.2637825906276703\n",
            "step: 80, loss: 0.2564784288406372\n",
            "step: 90, loss: 0.2432769536972046\n",
            "step: 100, loss: 0.2440928965806961\n",
            "step: 110, loss: 0.31510108709335327\n",
            "step: 120, loss: 0.30644553899765015\n",
            "step: 130, loss: 0.17600540816783905\n",
            "step: 140, loss: 0.2359827160835266\n",
            "step: 150, loss: 0.3118997812271118\n",
            "step: 160, loss: 0.2378024309873581\n",
            "step: 170, loss: 0.5119420886039734\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.23814541622760801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.377928227186203\n",
            "step: 10, loss: 0.30820509791374207\n",
            "step: 20, loss: 0.1662551313638687\n",
            "step: 30, loss: 0.5170382261276245\n",
            "step: 40, loss: 0.23943538963794708\n",
            "step: 50, loss: 0.23921331763267517\n",
            "step: 60, loss: 0.3642449378967285\n",
            "step: 70, loss: 0.3301851749420166\n",
            "step: 80, loss: 0.187226340174675\n",
            "step: 90, loss: 0.37177208065986633\n",
            "step: 100, loss: 0.5050495862960815\n",
            "step: 110, loss: 0.37545180320739746\n",
            "step: 120, loss: 0.37578368186950684\n",
            "step: 130, loss: 0.37798723578453064\n",
            "step: 140, loss: 0.506903886795044\n",
            "step: 150, loss: 0.4390481114387512\n",
            "step: 160, loss: 0.24898476898670197\n",
            "step: 170, loss: 0.18357622623443604\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.23814541622760801\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3846483528614044\n",
            "step: 10, loss: 0.3179028630256653\n",
            "step: 20, loss: 0.2463928908109665\n",
            "step: 30, loss: 0.30856892466545105\n",
            "step: 40, loss: 0.31138285994529724\n",
            "step: 50, loss: 0.3073675334453583\n",
            "step: 60, loss: 0.3119404911994934\n",
            "step: 70, loss: 0.17576274275779724\n",
            "step: 80, loss: 0.2346687763929367\n",
            "step: 90, loss: 0.2895848751068115\n",
            "step: 100, loss: 0.2599300742149353\n",
            "step: 110, loss: 0.44523489475250244\n",
            "step: 120, loss: 0.3130645751953125\n",
            "step: 130, loss: 0.3740246891975403\n",
            "step: 140, loss: 0.30112215876579285\n",
            "step: 150, loss: 0.2862653434276581\n",
            "step: 160, loss: 0.7669490575790405\n",
            "step: 170, loss: 0.37412089109420776\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.26315789473684215, f1=0.25, best_f1=0.25\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3340521454811096\n",
            "step: 10, loss: 0.220183864235878\n",
            "step: 20, loss: 0.20277050137519836\n",
            "step: 30, loss: 0.4042738974094391\n",
            "step: 40, loss: 0.40386316180229187\n",
            "step: 50, loss: 0.20588071644306183\n",
            "step: 60, loss: 0.2064208984375\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 70, loss: 0.3707588315010071\n",
            "step: 80, loss: 0.1837206929922104\n",
            "step: 90, loss: 0.2551865577697754\n",
            "step: 100, loss: 0.19398687779903412\n",
            "step: 110, loss: 0.4151850640773773\n",
            "step: 120, loss: 0.2193908393383026\n",
            "step: 130, loss: 0.2223810851573944\n",
            "step: 140, loss: 0.4917833209037781\n",
            "step: 150, loss: 0.3159846067428589\n",
            "step: 160, loss: 0.31010109186172485\n",
            "step: 170, loss: 0.29665496945381165\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.35491606714628293, f1=0.3270718232044199, best_f1=0.3270718232044199\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.34222808480262756\n",
            "step: 10, loss: 0.19693240523338318\n",
            "step: 20, loss: 0.2968979477882385\n",
            "step: 30, loss: 0.3656150698661804\n",
            "step: 40, loss: 0.2769557237625122\n",
            "step: 50, loss: 0.14439575374126434\n",
            "step: 60, loss: 0.5129988193511963\n",
            "step: 70, loss: 0.2033102661371231\n",
            "step: 80, loss: 0.12573906779289246\n",
            "step: 90, loss: 0.22990663349628448\n",
            "step: 100, loss: 0.36983564496040344\n",
            "step: 110, loss: 0.1635352522134781\n",
            "step: 120, loss: 0.20766359567642212\n",
            "step: 130, loss: 0.25195884704589844\n",
            "step: 140, loss: 0.2898542582988739\n",
            "step: 150, loss: 0.26189061999320984\n",
            "step: 160, loss: 0.296896368265152\n",
            "step: 170, loss: 0.20318573713302612\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.6203703703703705, f1=0.5766590389016018, best_f1=0.5766590389016018\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.044707316905260086\n",
            "step: 10, loss: 0.2615838050842285\n",
            "step: 20, loss: 0.18870510160923004\n",
            "step: 30, loss: 0.4776771366596222\n",
            "step: 40, loss: 0.0842880979180336\n",
            "step: 50, loss: 0.06618955731391907\n",
            "step: 60, loss: 0.1655178964138031\n",
            "step: 70, loss: 0.1805242896080017\n",
            "step: 80, loss: 0.06301921606063843\n",
            "step: 90, loss: 0.15423960983753204\n",
            "step: 100, loss: 0.37700939178466797\n",
            "step: 110, loss: 0.22283610701560974\n",
            "step: 120, loss: 0.17163115739822388\n",
            "step: 130, loss: 0.2045041024684906\n",
            "step: 140, loss: 0.09434111416339874\n",
            "step: 150, loss: 0.0794941857457161\n",
            "step: 160, loss: 0.07934844493865967\n",
            "step: 170, loss: 0.1596963107585907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.6559633027522935, f1=0.6272727272727273, best_f1=0.6272727272727273\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 236.37it/s]\n",
            "load_f1 = 0.6535626535626535\n",
            "real_f1 = 0.6437346437346438\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 144.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BASELINE DIRTY"
      ],
      "metadata": {
        "id": "djX3yHRNgNFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "b011EMgogNFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model  \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "5_ai4a3YgNFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c57ca5e-d344-4aa0-9ce1-3d1d94143a89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5938824415206909\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.4344732463359833\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.5291913747787476\n",
            "step: 30, loss: 0.3597757816314697\n",
            "step: 40, loss: 0.34163030982017517\n",
            "step: 50, loss: 0.5929827690124512\n",
            "step: 60, loss: 0.4895910322666168\n",
            "step: 70, loss: 0.10770280659198761\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 80, loss: 1.4392887353897095\n",
            "step: 90, loss: 0.2676054835319519\n",
            "step: 100, loss: 0.20344911515712738\n",
            "step: 110, loss: 0.2753446698188782\n",
            "step: 120, loss: 0.23964405059814453\n",
            "step: 130, loss: 0.03420074284076691\n",
            "step: 140, loss: 0.03319314867258072\n",
            "step: 150, loss: 0.1698465347290039\n",
            "step: 160, loss: 0.30529266595840454\n",
            "step: 170, loss: 0.16950112581253052\n",
            "step: 180, loss: 0.21108286082744598\n",
            "step: 190, loss: 0.06678152084350586\n",
            "step: 200, loss: 0.1004483550786972\n",
            "step: 210, loss: 0.13208593428134918\n",
            "step: 220, loss: 0.05433306843042374\n",
            "step: 230, loss: 0.004899165127426386\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9542920847268673, f1=0.9635535307517085, best_f1=0.9635535307517085\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002245693700388074\n",
            "step: 10, loss: 0.12693218886852264\n",
            "step: 20, loss: 0.0356370285153389\n",
            "step: 30, loss: 0.019969012588262558\n",
            "step: 40, loss: 0.015283117070794106\n",
            "step: 50, loss: 0.03440125286579132\n",
            "step: 60, loss: 0.007714845240116119\n",
            "step: 70, loss: 0.023798998445272446\n",
            "step: 80, loss: 0.03721480071544647\n",
            "step: 90, loss: 0.17699825763702393\n",
            "step: 100, loss: 0.015402919612824917\n",
            "step: 110, loss: 0.10658532381057739\n",
            "step: 120, loss: 0.04078937694430351\n",
            "step: 130, loss: 0.036639969795942307\n",
            "step: 140, loss: 0.007676809094846249\n",
            "step: 150, loss: 0.25826171040534973\n",
            "step: 160, loss: 0.03473823517560959\n",
            "step: 170, loss: 0.0010106794070452452\n",
            "step: 180, loss: 0.008487689308822155\n",
            "step: 190, loss: 0.03470682352781296\n",
            "step: 200, loss: 0.03568645194172859\n",
            "step: 210, loss: 0.04780500382184982\n",
            "step: 220, loss: 0.008497631177306175\n",
            "step: 230, loss: 0.006327452138066292\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9611542730299667, f1=0.961625282167043, best_f1=0.961625282167043\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.015604588203132153\n",
            "step: 10, loss: 0.009812377393245697\n",
            "step: 20, loss: 0.15119704604148865\n",
            "step: 30, loss: 0.004912353120744228\n",
            "step: 40, loss: 0.007398274261504412\n",
            "step: 50, loss: 0.027637140825390816\n",
            "step: 60, loss: 0.0602424293756485\n",
            "step: 70, loss: 0.010767459869384766\n",
            "step: 80, loss: 0.16118605434894562\n",
            "step: 90, loss: 0.008989744819700718\n",
            "step: 100, loss: 0.055976491421461105\n",
            "step: 110, loss: 0.009317193180322647\n",
            "step: 120, loss: 0.009042806923389435\n",
            "step: 130, loss: 0.0038800158072263002\n",
            "step: 140, loss: 0.0025244764983654022\n",
            "step: 150, loss: 0.11706563085317612\n",
            "step: 160, loss: 0.019334709271788597\n",
            "step: 170, loss: 0.005602532997727394\n",
            "step: 180, loss: 0.021311815828084946\n",
            "step: 190, loss: 0.005309188272804022\n",
            "step: 200, loss: 0.031281549483537674\n",
            "step: 210, loss: 0.02923642471432686\n",
            "step: 220, loss: 0.11814562231302261\n",
            "step: 230, loss: 0.022983616217970848\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9752808988764046, f1=0.9624573378839592, best_f1=0.9624573378839592\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.014348003081977367\n",
            "step: 10, loss: 0.0022349797654896975\n",
            "step: 20, loss: 0.0018296042690053582\n",
            "step: 30, loss: 0.0021317084319889545\n",
            "step: 40, loss: 0.0336337648332119\n",
            "step: 50, loss: 0.021363873034715652\n",
            "step: 60, loss: 0.0027833234053105116\n",
            "step: 70, loss: 0.06051080301403999\n",
            "step: 80, loss: 0.13905811309814453\n",
            "step: 90, loss: 0.21652060747146606\n",
            "step: 100, loss: 0.057392969727516174\n",
            "step: 110, loss: 0.0010714296950027347\n",
            "step: 120, loss: 0.01459306851029396\n",
            "step: 130, loss: 0.01989440619945526\n",
            "step: 140, loss: 0.004813509993255138\n",
            "step: 150, loss: 0.007896467112004757\n",
            "step: 160, loss: 0.002238625194877386\n",
            "step: 170, loss: 0.0019910139963030815\n",
            "step: 180, loss: 0.09023474901914597\n",
            "step: 190, loss: 0.0011880146339535713\n",
            "step: 200, loss: 0.13543355464935303\n",
            "step: 210, loss: 0.006040820851922035\n",
            "step: 220, loss: 0.0025342742446810007\n",
            "step: 230, loss: 0.0034970082342624664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9751131221719457, f1=0.9727272727272728, best_f1=0.9624573378839592\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005408337339758873\n",
            "step: 10, loss: 0.0015762008260935545\n",
            "step: 20, loss: 0.002114564646035433\n",
            "step: 30, loss: 0.00397872319445014\n",
            "step: 40, loss: 0.002192205749452114\n",
            "step: 50, loss: 0.0014171323273330927\n",
            "step: 60, loss: 0.005165181588381529\n",
            "step: 70, loss: 0.007668002042919397\n",
            "step: 80, loss: 0.013527180068194866\n",
            "step: 90, loss: 0.019780956208705902\n",
            "step: 100, loss: 0.004697431344538927\n",
            "step: 110, loss: 0.0017702047480270267\n",
            "step: 120, loss: 0.0006836746470071375\n",
            "step: 130, loss: 0.0028682525735348463\n",
            "step: 140, loss: 0.003549022600054741\n",
            "step: 150, loss: 0.010519033297896385\n",
            "step: 160, loss: 0.0014448069268837571\n",
            "step: 170, loss: 0.06127874553203583\n",
            "step: 180, loss: 0.0027902342844754457\n",
            "step: 190, loss: 0.03431466221809387\n",
            "step: 200, loss: 0.01706733927130699\n",
            "step: 210, loss: 0.0026192613877356052\n",
            "step: 220, loss: 0.03468545153737068\n",
            "step: 230, loss: 0.010512310080230236\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9755555555555556, f1=0.9753363228699552, best_f1=0.9753363228699552\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005810215952806175\n",
            "step: 10, loss: 0.001228701788932085\n",
            "step: 20, loss: 0.0010871124686673284\n",
            "step: 30, loss: 0.0005371551378630102\n",
            "step: 40, loss: 0.00020156164828222245\n",
            "step: 50, loss: 0.0003360146947670728\n",
            "step: 60, loss: 0.012544243596494198\n",
            "step: 70, loss: 0.04513880982995033\n",
            "step: 80, loss: 0.000457750225905329\n",
            "step: 90, loss: 0.035912930965423584\n",
            "step: 100, loss: 0.00036008298047818244\n",
            "step: 110, loss: 0.010859716683626175\n",
            "step: 120, loss: 0.0009840615093708038\n",
            "step: 130, loss: 0.0024912827648222446\n",
            "step: 140, loss: 0.00149907183367759\n",
            "step: 150, loss: 0.00020082674745935947\n",
            "step: 160, loss: 0.0013273241929709911\n",
            "step: 170, loss: 0.018406735733151436\n",
            "step: 180, loss: 0.0005720107001252472\n",
            "step: 190, loss: 0.0017898220103234053\n",
            "step: 200, loss: 0.0017371535068377852\n",
            "step: 210, loss: 0.0008916400838643312\n",
            "step: 220, loss: 0.011194156482815742\n",
            "step: 230, loss: 0.0007877881871536374\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.9820627802690582, f1=0.9818594104308391, best_f1=0.9818594104308391\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011405491968616843\n",
            "step: 10, loss: 0.0005382021772675216\n",
            "step: 20, loss: 0.00022652087500318885\n",
            "step: 30, loss: 0.0003339348768349737\n",
            "step: 40, loss: 0.0015395993832498789\n",
            "step: 50, loss: 0.0015834345249459147\n",
            "step: 60, loss: 0.0006589479162357748\n",
            "step: 70, loss: 0.0012703175889328122\n",
            "step: 80, loss: 0.004100162070244551\n",
            "step: 90, loss: 0.0009834419470280409\n",
            "step: 100, loss: 0.001166854053735733\n",
            "step: 110, loss: 0.0008968852926045656\n",
            "step: 120, loss: 0.0021537449210882187\n",
            "step: 130, loss: 0.0055900514125823975\n",
            "step: 140, loss: 0.0006669766735285521\n",
            "step: 150, loss: 0.035457100719213486\n",
            "step: 160, loss: 0.0016224257415160537\n",
            "step: 170, loss: 0.00042198013397865\n",
            "step: 180, loss: 0.0003943217743653804\n",
            "step: 190, loss: 0.0002724641526583582\n",
            "step: 200, loss: 0.0005502633284777403\n",
            "step: 210, loss: 0.003246542764827609\n",
            "step: 220, loss: 0.00037354230880737305\n",
            "step: 230, loss: 0.0009888748172670603\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9778270509977827, f1=0.9753914988814317, best_f1=0.9818594104308391\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021744809055235237\n",
            "step: 10, loss: 0.0010546285193413496\n",
            "step: 20, loss: 0.0013599650701507926\n",
            "step: 30, loss: 0.15195679664611816\n",
            "step: 40, loss: 0.01786908693611622\n",
            "step: 50, loss: 0.015297586098313332\n",
            "step: 60, loss: 0.0019456250593066216\n",
            "step: 70, loss: 0.0005769685376435518\n",
            "step: 80, loss: 0.12029172480106354\n",
            "step: 90, loss: 0.0009158574976027012\n",
            "step: 100, loss: 0.00029035855550318956\n",
            "step: 110, loss: 0.001718413201160729\n",
            "step: 120, loss: 0.0023605271708220243\n",
            "step: 130, loss: 0.001087366952560842\n",
            "step: 140, loss: 0.00020536633383017033\n",
            "step: 150, loss: 0.10987379401922226\n",
            "step: 160, loss: 0.00507896114140749\n",
            "step: 170, loss: 0.0514310821890831\n",
            "step: 180, loss: 0.01723391003906727\n",
            "step: 190, loss: 0.00748762022703886\n",
            "step: 200, loss: 0.011952051892876625\n",
            "step: 210, loss: 0.0018295121844857931\n",
            "step: 220, loss: 0.0009113281266763806\n",
            "step: 230, loss: 0.0009805161971598864\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.983050847457627, f1=0.9737742303306728, best_f1=0.9737742303306728\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005492418073117733\n",
            "step: 10, loss: 0.0028648825827986\n",
            "step: 20, loss: 0.00031897996086627245\n",
            "step: 30, loss: 0.0007523577660322189\n",
            "step: 40, loss: 0.000310759583953768\n",
            "step: 50, loss: 0.00047944337711669505\n",
            "step: 60, loss: 0.00011130112397950143\n",
            "step: 70, loss: 0.027080891653895378\n",
            "step: 80, loss: 0.0014067280571907759\n",
            "step: 90, loss: 0.013270996510982513\n",
            "step: 100, loss: 0.0004923649248667061\n",
            "step: 110, loss: 0.000388192042009905\n",
            "step: 120, loss: 0.014652473852038383\n",
            "step: 130, loss: 0.0008035200880840421\n",
            "step: 140, loss: 0.0002615304256323725\n",
            "step: 150, loss: 0.00020484364358708262\n",
            "step: 160, loss: 0.000159167087986134\n",
            "step: 170, loss: 0.0002028070157393813\n",
            "step: 180, loss: 0.0004575435013975948\n",
            "step: 190, loss: 0.0010985435219481587\n",
            "step: 200, loss: 0.00023947993759065866\n",
            "step: 210, loss: 0.00047921083751134574\n",
            "step: 220, loss: 0.0011710900580510497\n",
            "step: 230, loss: 0.00021309591829776764\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9831649831649831, f1=0.9750566893424036, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003138341708108783\n",
            "step: 10, loss: 0.0010961273219436407\n",
            "step: 20, loss: 0.00030654590227641165\n",
            "step: 30, loss: 0.00010156352072954178\n",
            "step: 40, loss: 0.0007349670631811023\n",
            "step: 50, loss: 0.00012540302122943103\n",
            "step: 60, loss: 0.0001591600594110787\n",
            "step: 70, loss: 0.00112350529525429\n",
            "step: 80, loss: 0.00013804312038701028\n",
            "step: 90, loss: 0.000148212187923491\n",
            "step: 100, loss: 0.001646299147978425\n",
            "step: 110, loss: 0.0012738811783492565\n",
            "step: 120, loss: 0.0008397971978411078\n",
            "step: 130, loss: 0.0003490816743578762\n",
            "step: 140, loss: 0.0007347817881964147\n",
            "step: 150, loss: 0.00034990248968824744\n",
            "step: 160, loss: 6.341841071844101e-05\n",
            "step: 170, loss: 0.00040551283746026456\n",
            "step: 180, loss: 0.00043851183727383614\n",
            "step: 190, loss: 0.0001376967557007447\n",
            "step: 200, loss: 0.0003306887811049819\n",
            "step: 210, loss: 0.061558809131383896\n",
            "step: 220, loss: 0.018229439854621887\n",
            "step: 230, loss: 0.00010615790961310267\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9819819819819819, f1=0.979591836734694, best_f1=0.9750566893424036\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00010282474977429956\n",
            "step: 10, loss: 0.00022042309865355492\n",
            "step: 20, loss: 0.00011931249173358083\n",
            "step: 30, loss: 0.00026072515174746513\n",
            "step: 40, loss: 4.8071950004668906e-05\n",
            "step: 50, loss: 7.803809421602637e-05\n",
            "step: 60, loss: 0.003248602384701371\n",
            "step: 70, loss: 0.00027242532814852893\n",
            "step: 80, loss: 0.001438664272427559\n",
            "step: 90, loss: 0.01568811573088169\n",
            "step: 100, loss: 0.0001316911802859977\n",
            "step: 110, loss: 0.002383829327300191\n",
            "step: 120, loss: 0.00011038028605980799\n",
            "step: 130, loss: 0.00012826977763324976\n",
            "step: 140, loss: 0.0026641525328159332\n",
            "step: 150, loss: 0.0016867013182491064\n",
            "step: 160, loss: 0.0009056479902938008\n",
            "step: 170, loss: 0.0004449528641998768\n",
            "step: 180, loss: 0.0010195012437179685\n",
            "step: 190, loss: 0.00012887058255728334\n",
            "step: 200, loss: 0.003366270335391164\n",
            "step: 210, loss: 0.00013989288709126413\n",
            "step: 220, loss: 0.0005034170462749898\n",
            "step: 230, loss: 9.4149487267714e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9832026875699889, f1=0.9787234042553192, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00038362087798304856\n",
            "step: 10, loss: 0.00012160736514488235\n",
            "step: 20, loss: 0.012173991650342941\n",
            "step: 30, loss: 0.033772360533475876\n",
            "step: 40, loss: 0.00015747110592201352\n",
            "step: 50, loss: 0.0011008454021066427\n",
            "step: 60, loss: 0.0003281717945355922\n",
            "step: 70, loss: 0.00022071183775551617\n",
            "step: 80, loss: 0.00010869279503822327\n",
            "step: 90, loss: 0.003935604821890593\n",
            "step: 100, loss: 7.89165060268715e-05\n",
            "step: 110, loss: 7.392139377770945e-05\n",
            "step: 120, loss: 0.00022278731921687722\n",
            "step: 130, loss: 0.0003722895635291934\n",
            "step: 140, loss: 9.689209400676191e-05\n",
            "step: 150, loss: 0.00010032518184743822\n",
            "step: 160, loss: 0.00012069158401573077\n",
            "step: 170, loss: 0.00010923810623353347\n",
            "step: 180, loss: 0.0001580766256665811\n",
            "step: 190, loss: 0.0005660004098899662\n",
            "step: 200, loss: 9.05152628547512e-05\n",
            "step: 210, loss: 0.00012187920947326347\n",
            "step: 220, loss: 0.013294676318764687\n",
            "step: 230, loss: 0.00030917645199224353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9820224719101124, f1=0.9786276715410572, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00021453876979649067\n",
            "step: 10, loss: 0.00012122245971113443\n",
            "step: 20, loss: 0.00010363764886278659\n",
            "step: 30, loss: 0.00013207487063482404\n",
            "step: 40, loss: 0.00013533436867874116\n",
            "step: 50, loss: 0.00024193670833483338\n",
            "step: 60, loss: 8.773730223765597e-05\n",
            "step: 70, loss: 0.00012439244892448187\n",
            "step: 80, loss: 0.0015393901849165559\n",
            "step: 90, loss: 0.00015344239363912493\n",
            "step: 100, loss: 0.00011764196824515238\n",
            "step: 110, loss: 0.0004009093972854316\n",
            "step: 120, loss: 0.00011122879368485883\n",
            "step: 130, loss: 0.00010747146734502167\n",
            "step: 140, loss: 0.00010668267350411043\n",
            "step: 150, loss: 0.00038954155752435327\n",
            "step: 160, loss: 0.002218734472990036\n",
            "step: 170, loss: 8.266131771961227e-05\n",
            "step: 180, loss: 0.005815069656819105\n",
            "step: 190, loss: 0.00010118507634615526\n",
            "step: 200, loss: 4.7987039579311386e-05\n",
            "step: 210, loss: 0.00017224568000528961\n",
            "step: 220, loss: 0.0001507151173427701\n",
            "step: 230, loss: 0.00014124641893431544\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9820224719101124, f1=0.9751693002257337, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00011904740676982328\n",
            "step: 10, loss: 6.86475177644752e-05\n",
            "step: 20, loss: 9.787904855329543e-05\n",
            "step: 30, loss: 0.0003205237735528499\n",
            "step: 40, loss: 0.00027934799436479807\n",
            "step: 50, loss: 6.968862726353109e-05\n",
            "step: 60, loss: 0.0003785028529819101\n",
            "step: 70, loss: 0.000193051018868573\n",
            "step: 80, loss: 0.00026253523537889123\n",
            "step: 90, loss: 0.0012409326154738665\n",
            "step: 100, loss: 0.00018128554802387953\n",
            "step: 110, loss: 0.0001008811013889499\n",
            "step: 120, loss: 4.600878310156986e-05\n",
            "step: 130, loss: 0.000998022616840899\n",
            "step: 140, loss: 0.00010726352775236592\n",
            "step: 150, loss: 5.364017852116376e-05\n",
            "step: 160, loss: 0.0002214660489698872\n",
            "step: 170, loss: 0.00016946540563367307\n",
            "step: 180, loss: 0.00011109936895081773\n",
            "step: 190, loss: 7.567612192360684e-05\n",
            "step: 200, loss: 0.00014260775060392916\n",
            "step: 210, loss: 7.263618317665532e-05\n",
            "step: 220, loss: 0.00017073970229830593\n",
            "step: 230, loss: 0.0001987374562304467\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9821029082774049, f1=0.9764837625979844, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00012729925219900906\n",
            "step: 10, loss: 0.00019114650785923004\n",
            "step: 20, loss: 0.0012809338513761759\n",
            "step: 30, loss: 9.592855349183083e-05\n",
            "step: 40, loss: 0.00017861739615909755\n",
            "step: 50, loss: 7.766341150272638e-05\n",
            "step: 60, loss: 0.009548150934278965\n",
            "step: 70, loss: 0.00018352035840507597\n",
            "step: 80, loss: 0.00020272917754482478\n",
            "step: 90, loss: 5.7757028116611764e-05\n",
            "step: 100, loss: 4.963627361576073e-05\n",
            "step: 110, loss: 0.0001605569414095953\n",
            "step: 120, loss: 0.03259432688355446\n",
            "step: 130, loss: 0.00031594003667123616\n",
            "step: 140, loss: 0.015560006722807884\n",
            "step: 150, loss: 0.0001630835613468662\n",
            "step: 160, loss: 0.014262950979173183\n",
            "step: 170, loss: 4.464119047042914e-05\n",
            "step: 180, loss: 8.412451279582456e-05\n",
            "step: 190, loss: 0.00012561213225126266\n",
            "step: 200, loss: 0.0002634548000060022\n",
            "step: 210, loss: 0.006528892088681459\n",
            "step: 220, loss: 6.207792466739193e-05\n",
            "step: 230, loss: 8.046008588280529e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9821428571428571, f1=0.9743016759776536, best_f1=0.9787234042553192\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 155.67it/s]\n",
            "load_f1 = 0.9831271091113611\n",
            "real_f1 = 0.9808773903262092\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 142.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "h62Yut_pgNFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QGEElkeagNFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0802e30-315a-4e41-fdae-f840d39f820b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6631510257720947\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.47177523374557495\n",
            "step: 20, loss: 0.3426883816719055\n",
            "step: 30, loss: 0.3258879780769348\n",
            "step: 40, loss: 0.400151789188385\n",
            "step: 50, loss: 0.7301903963088989\n",
            "step: 60, loss: 0.3551722466945648\n",
            "step: 70, loss: 0.42770692706108093\n",
            "step: 80, loss: 0.5230494737625122\n",
            "step: 90, loss: 0.42419689893722534\n",
            "step: 100, loss: 0.5550311207771301\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 110, loss: 0.17287564277648926\n",
            "step: 120, loss: 0.41307204961776733\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
            "step: 130, loss: 0.38983041048049927\n",
            "step: 140, loss: 0.19072894752025604\n",
            "step: 150, loss: 0.07777298986911774\n",
            "step: 160, loss: 0.2191697359085083\n",
            "step: 170, loss: 0.10708991438150406\n",
            "step: 180, loss: 0.08277495950460434\n",
            "step: 190, loss: 0.11903510242700577\n",
            "step: 200, loss: 0.0673302561044693\n",
            "step: 210, loss: 0.051074933260679245\n",
            "step: 220, loss: 0.0993175134062767\n",
            "step: 230, loss: 0.23800188302993774\n",
            "step: 240, loss: 0.07323738932609558\n",
            "step: 250, loss: 0.03546852990984917\n",
            "step: 260, loss: 0.2488759458065033\n",
            "step: 270, loss: 0.43791094422340393\n",
            "step: 280, loss: 0.06684650480747223\n",
            "step: 290, loss: 0.04484562948346138\n",
            "step: 300, loss: 0.07493212819099426\n",
            "step: 310, loss: 0.06775878369808197\n",
            "step: 320, loss: 0.026020344346761703\n",
            "step: 330, loss: 0.06580441445112228\n",
            "step: 340, loss: 0.23236718773841858\n",
            "step: 350, loss: 0.05738032981753349\n",
            "step: 360, loss: 0.08351290971040726\n",
            "step: 370, loss: 0.11178679019212723\n",
            "step: 380, loss: 0.08410966396331787\n",
            "step: 390, loss: 0.027698833495378494\n",
            "step: 400, loss: 0.09989239275455475\n",
            "step: 410, loss: 0.23884496092796326\n",
            "step: 420, loss: 0.03703567013144493\n",
            "step: 430, loss: 0.015448938123881817\n",
            "step: 440, loss: 0.0386492982506752\n",
            "step: 450, loss: 0.10136707872152328\n",
            "step: 460, loss: 0.03800095245242119\n",
            "step: 470, loss: 0.03865994140505791\n",
            "step: 480, loss: 0.2167428731918335\n",
            "step: 490, loss: 0.08855915814638138\n",
            "step: 500, loss: 0.013617843389511108\n",
            "step: 510, loss: 0.02724895253777504\n",
            "step: 520, loss: 0.15517203509807587\n",
            "step: 530, loss: 0.2753509283065796\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9329529243937231, f1=0.9342230695900858, best_f1=0.9342230695900858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11741320043802261\n",
            "step: 10, loss: 0.037177059799432755\n",
            "step: 20, loss: 0.030042795464396477\n",
            "step: 30, loss: 0.037504490464925766\n",
            "step: 40, loss: 0.01968027465045452\n",
            "step: 50, loss: 0.031043076887726784\n",
            "step: 60, loss: 0.008911840617656708\n",
            "step: 70, loss: 0.03694292530417442\n",
            "step: 80, loss: 0.0215435940772295\n",
            "step: 90, loss: 0.013324897736310959\n",
            "step: 100, loss: 0.172823965549469\n",
            "step: 110, loss: 0.007198247127234936\n",
            "step: 120, loss: 0.020577656105160713\n",
            "step: 130, loss: 0.0042045218870043755\n",
            "step: 140, loss: 0.042688049376010895\n",
            "step: 150, loss: 0.0223309975117445\n",
            "step: 160, loss: 0.08446191251277924\n",
            "step: 170, loss: 0.01998022384941578\n",
            "step: 180, loss: 0.06287482380867004\n",
            "step: 190, loss: 0.017250949516892433\n",
            "step: 200, loss: 0.17706921696662903\n",
            "step: 210, loss: 0.03353685513138771\n",
            "step: 220, loss: 0.0019856984727084637\n",
            "step: 230, loss: 0.020695405080914497\n",
            "step: 240, loss: 0.14342951774597168\n",
            "step: 250, loss: 0.039421163499355316\n",
            "step: 260, loss: 0.04823979735374451\n",
            "step: 270, loss: 0.00937049649655819\n",
            "step: 280, loss: 0.025321755558252335\n",
            "step: 290, loss: 0.049613550305366516\n",
            "step: 300, loss: 0.04987654462456703\n",
            "step: 310, loss: 0.10378684848546982\n",
            "step: 320, loss: 0.03331734612584114\n",
            "step: 330, loss: 0.06808716058731079\n",
            "step: 340, loss: 0.02636868879199028\n",
            "step: 350, loss: 0.0012826458550989628\n",
            "step: 360, loss: 0.09114451706409454\n",
            "step: 370, loss: 0.012522518634796143\n",
            "step: 380, loss: 0.1145838126540184\n",
            "step: 390, loss: 0.005202979780733585\n",
            "step: 400, loss: 0.03594474866986275\n",
            "step: 410, loss: 0.031046373769640923\n",
            "step: 420, loss: 0.14367258548736572\n",
            "step: 430, loss: 0.22288541495800018\n",
            "step: 440, loss: 0.027146486565470695\n",
            "step: 450, loss: 0.1520404815673828\n",
            "step: 460, loss: 0.025053299963474274\n",
            "step: 470, loss: 0.02021789737045765\n",
            "step: 480, loss: 0.004809098318219185\n",
            "step: 490, loss: 0.04834045097231865\n",
            "step: 500, loss: 0.009646335616707802\n",
            "step: 510, loss: 0.06104521453380585\n",
            "step: 520, loss: 0.3249950110912323\n",
            "step: 530, loss: 0.10730385780334473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.942008486562942, f1=0.935831381733021, best_f1=0.935831381733021\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09492763876914978\n",
            "step: 10, loss: 0.06386493146419525\n",
            "step: 20, loss: 0.005874177906662226\n",
            "step: 30, loss: 0.02630016952753067\n",
            "step: 40, loss: 0.0800306499004364\n",
            "step: 50, loss: 0.01171767245978117\n",
            "step: 60, loss: 0.003278432646766305\n",
            "step: 70, loss: 0.006444238126277924\n",
            "step: 80, loss: 0.012089035473763943\n",
            "step: 90, loss: 0.0017573523800820112\n",
            "step: 100, loss: 0.045429155230522156\n",
            "step: 110, loss: 0.018781842663884163\n",
            "step: 120, loss: 0.21475189924240112\n",
            "step: 130, loss: 0.07576549798250198\n",
            "step: 140, loss: 0.02529771439731121\n",
            "step: 150, loss: 0.01976662687957287\n",
            "step: 160, loss: 0.0066005331464111805\n",
            "step: 170, loss: 0.0029781272169202566\n",
            "step: 180, loss: 0.009653093293309212\n",
            "step: 190, loss: 0.004499378614127636\n",
            "step: 200, loss: 0.03257245197892189\n",
            "step: 210, loss: 0.041036393493413925\n",
            "step: 220, loss: 0.11555071920156479\n",
            "step: 230, loss: 0.013496031984686852\n",
            "step: 240, loss: 0.0787348821759224\n",
            "step: 250, loss: 0.16280405223369598\n",
            "step: 260, loss: 0.0257598664611578\n",
            "step: 270, loss: 0.03332644701004028\n",
            "step: 280, loss: 0.005068054422736168\n",
            "step: 290, loss: 0.027704689651727676\n",
            "step: 300, loss: 0.1994352787733078\n",
            "step: 310, loss: 0.13068503141403198\n",
            "step: 320, loss: 0.01702943816781044\n",
            "step: 330, loss: 0.045870814472436905\n",
            "step: 340, loss: 0.008745224215090275\n",
            "step: 350, loss: 0.2161153256893158\n",
            "step: 360, loss: 0.02596789412200451\n",
            "step: 370, loss: 0.03497958555817604\n",
            "step: 380, loss: 0.001234946888871491\n",
            "step: 390, loss: 0.002914954209700227\n",
            "step: 400, loss: 0.12924158573150635\n",
            "step: 410, loss: 0.04715823009610176\n",
            "step: 420, loss: 0.011758486740291119\n",
            "step: 430, loss: 0.030820196494460106\n",
            "step: 440, loss: 0.23072078824043274\n",
            "step: 450, loss: 0.055702753365039825\n",
            "step: 460, loss: 0.07309554517269135\n",
            "step: 470, loss: 0.02527722530066967\n",
            "step: 480, loss: 0.05328521877527237\n",
            "step: 490, loss: 0.04282884672284126\n",
            "step: 500, loss: 0.010646277107298374\n",
            "step: 510, loss: 0.06086812913417816\n",
            "step: 520, loss: 0.008778432384133339\n",
            "step: 530, loss: 0.008214782923460007\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9446529080675422, f1=0.9397363465160076, best_f1=0.9397363465160076\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013798082247376442\n",
            "step: 10, loss: 0.0014289660612121224\n",
            "step: 20, loss: 0.049307312816381454\n",
            "step: 30, loss: 0.11050814390182495\n",
            "step: 40, loss: 0.009346251375973225\n",
            "step: 50, loss: 0.036634042859077454\n",
            "step: 60, loss: 0.0035497320350259542\n",
            "step: 70, loss: 0.017578493803739548\n",
            "step: 80, loss: 0.03214704617857933\n",
            "step: 90, loss: 0.07489942014217377\n",
            "step: 100, loss: 0.001239457749761641\n",
            "step: 110, loss: 0.09689787775278091\n",
            "step: 120, loss: 0.0013953594025224447\n",
            "step: 130, loss: 0.02037951350212097\n",
            "step: 140, loss: 0.012009136378765106\n",
            "step: 150, loss: 0.030704624950885773\n",
            "step: 160, loss: 0.030406642705202103\n",
            "step: 170, loss: 0.02667299658060074\n",
            "step: 180, loss: 0.0302715040743351\n",
            "step: 190, loss: 0.015301985666155815\n",
            "step: 200, loss: 0.047616105526685715\n",
            "step: 210, loss: 0.0002764402888715267\n",
            "step: 220, loss: 0.0026403130032122135\n",
            "step: 230, loss: 0.002883258508518338\n",
            "step: 240, loss: 0.007472814992070198\n",
            "step: 250, loss: 0.11454064399003983\n",
            "step: 260, loss: 0.009659616276621819\n",
            "step: 270, loss: 0.03713047504425049\n",
            "step: 280, loss: 0.0028192722238600254\n",
            "step: 290, loss: 0.05555151030421257\n",
            "step: 300, loss: 0.004830810707062483\n",
            "step: 310, loss: 0.0047620791010558605\n",
            "step: 320, loss: 0.11295825988054276\n",
            "step: 330, loss: 0.11485271900892258\n",
            "step: 340, loss: 0.014272892847657204\n",
            "step: 350, loss: 0.11374890059232712\n",
            "step: 360, loss: 0.051107484847307205\n",
            "step: 370, loss: 0.012627721764147282\n",
            "step: 380, loss: 0.008094137534499168\n",
            "step: 390, loss: 0.02112513594329357\n",
            "step: 400, loss: 0.02775552123785019\n",
            "step: 410, loss: 0.0005523646832443774\n",
            "step: 420, loss: 0.0012461802689358592\n",
            "step: 430, loss: 0.0003174465091433376\n",
            "step: 440, loss: 0.005062011536210775\n",
            "step: 450, loss: 0.0066785006783902645\n",
            "step: 460, loss: 0.0221197921782732\n",
            "step: 470, loss: 0.0018323991680517793\n",
            "step: 480, loss: 0.024139977991580963\n",
            "step: 490, loss: 0.052377909421920776\n",
            "step: 500, loss: 0.04152441397309303\n",
            "step: 510, loss: 0.08880618959665298\n",
            "step: 520, loss: 0.008666587062180042\n",
            "step: 530, loss: 0.049044832587242126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9498141263940519, f1=0.9473193473193473, best_f1=0.9473193473193473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013290145434439182\n",
            "step: 10, loss: 0.005465212743729353\n",
            "step: 20, loss: 0.013340276665985584\n",
            "step: 30, loss: 0.027282986789941788\n",
            "step: 40, loss: 0.0004135115013923496\n",
            "step: 50, loss: 0.005080326460301876\n",
            "step: 60, loss: 0.004476367495954037\n",
            "step: 70, loss: 0.022427095100283623\n",
            "step: 80, loss: 0.003404591465368867\n",
            "step: 90, loss: 0.034644853323698044\n",
            "step: 100, loss: 0.03629877045750618\n",
            "step: 110, loss: 0.002940659411251545\n",
            "step: 120, loss: 0.24315473437309265\n",
            "step: 130, loss: 0.0023439819924533367\n",
            "step: 140, loss: 0.0016066976822912693\n",
            "step: 150, loss: 0.021519988775253296\n",
            "step: 160, loss: 0.00500071095302701\n",
            "step: 170, loss: 0.011984122917056084\n",
            "step: 180, loss: 0.003253692528232932\n",
            "step: 190, loss: 0.0029588595498353243\n",
            "step: 200, loss: 0.003561790566891432\n",
            "step: 210, loss: 0.0013655063230544329\n",
            "step: 220, loss: 0.0329941026866436\n",
            "step: 230, loss: 0.0010917647741734982\n",
            "step: 240, loss: 0.017646703869104385\n",
            "step: 250, loss: 0.15547069907188416\n",
            "step: 260, loss: 0.006685519125312567\n",
            "step: 270, loss: 0.0032809912227094173\n",
            "step: 280, loss: 0.007153820246458054\n",
            "step: 290, loss: 0.048815857619047165\n",
            "step: 300, loss: 0.0060916622169315815\n",
            "step: 310, loss: 0.06033644452691078\n",
            "step: 320, loss: 0.010573655366897583\n",
            "step: 330, loss: 0.0018753716722130775\n",
            "step: 340, loss: 0.017361054196953773\n",
            "step: 350, loss: 0.00045872191549278796\n",
            "step: 360, loss: 0.005833501927554607\n",
            "step: 370, loss: 0.0010522163938730955\n",
            "step: 380, loss: 0.0002270216355100274\n",
            "step: 390, loss: 0.0027107109781354666\n",
            "step: 400, loss: 0.01065738033503294\n",
            "step: 410, loss: 0.16572217643260956\n",
            "step: 420, loss: 0.14345499873161316\n",
            "step: 430, loss: 0.025042623281478882\n",
            "step: 440, loss: 0.004541922360658646\n",
            "step: 450, loss: 0.01554618775844574\n",
            "step: 460, loss: 0.0024080458097159863\n",
            "step: 470, loss: 0.03968465328216553\n",
            "step: 480, loss: 0.0275479294359684\n",
            "step: 490, loss: 0.0019364075269550085\n",
            "step: 500, loss: 0.010702281259000301\n",
            "step: 510, loss: 0.0010413742857053876\n",
            "step: 520, loss: 0.06918098777532578\n",
            "step: 530, loss: 0.0030176672153174877\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9484440315838365, f1=0.9407372841810545, best_f1=0.9473193473193473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012392089702188969\n",
            "step: 10, loss: 0.0008790647843852639\n",
            "step: 20, loss: 0.00047428059042431414\n",
            "step: 30, loss: 0.0008878656663000584\n",
            "step: 40, loss: 0.0027225506491959095\n",
            "step: 50, loss: 0.00022200928651727736\n",
            "step: 60, loss: 0.0028720484115183353\n",
            "step: 70, loss: 0.0008533311192877591\n",
            "step: 80, loss: 0.003666578559204936\n",
            "step: 90, loss: 0.020606625825166702\n",
            "step: 100, loss: 0.16398361325263977\n",
            "step: 110, loss: 0.0044881422072649\n",
            "step: 120, loss: 0.19584739208221436\n",
            "step: 130, loss: 0.023192783817648888\n",
            "step: 140, loss: 0.004151188302785158\n",
            "step: 150, loss: 0.00117881887126714\n",
            "step: 160, loss: 0.00912188645452261\n",
            "step: 170, loss: 0.0001632446510484442\n",
            "step: 180, loss: 0.0009463286842219532\n",
            "step: 190, loss: 0.12737220525741577\n",
            "step: 200, loss: 0.005594970658421516\n",
            "step: 210, loss: 0.014945878647267818\n",
            "step: 220, loss: 0.025173043832182884\n",
            "step: 230, loss: 0.008362723514437675\n",
            "step: 240, loss: 0.0024500482250005007\n",
            "step: 250, loss: 0.0245102159678936\n",
            "step: 260, loss: 0.0009090399253182113\n",
            "step: 270, loss: 0.0022692016791552305\n",
            "step: 280, loss: 0.001759371254593134\n",
            "step: 290, loss: 0.01705995202064514\n",
            "step: 300, loss: 0.018259292468428612\n",
            "step: 310, loss: 0.029677700251340866\n",
            "step: 320, loss: 4.0957678720587865e-05\n",
            "step: 330, loss: 0.016056755557656288\n",
            "step: 340, loss: 0.0010372198885306716\n",
            "step: 350, loss: 0.0014993326039984822\n",
            "step: 360, loss: 0.010933885350823402\n",
            "step: 370, loss: 0.008065136149525642\n",
            "step: 380, loss: 0.002893275348469615\n",
            "step: 390, loss: 0.001494131051003933\n",
            "step: 400, loss: 0.021758753806352615\n",
            "step: 410, loss: 0.0004196331719867885\n",
            "step: 420, loss: 0.012921527028083801\n",
            "step: 430, loss: 0.0008424405241385102\n",
            "step: 440, loss: 0.008473072201013565\n",
            "step: 450, loss: 0.13727985322475433\n",
            "step: 460, loss: 0.0027804269921034575\n",
            "step: 470, loss: 0.0026715544518083334\n",
            "step: 480, loss: 0.0037504108622670174\n",
            "step: 490, loss: 0.0015587739180773497\n",
            "step: 500, loss: 0.00020651589147746563\n",
            "step: 510, loss: 0.2644132673740387\n",
            "step: 520, loss: 0.0023704422637820244\n",
            "step: 530, loss: 0.0019154378678649664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.94200187090739, f1=0.9354686764013189, best_f1=0.9473193473193473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0037738101091235876\n",
            "step: 10, loss: 0.001670857542194426\n",
            "step: 20, loss: 0.006850619800388813\n",
            "step: 30, loss: 0.011266738176345825\n",
            "step: 40, loss: 0.006375796627253294\n",
            "step: 50, loss: 0.0021202918142080307\n",
            "step: 60, loss: 0.08890272676944733\n",
            "step: 70, loss: 0.00037468387745320797\n",
            "step: 80, loss: 0.00034363195300102234\n",
            "step: 90, loss: 0.0005162478191778064\n",
            "step: 100, loss: 0.000881291285622865\n",
            "step: 110, loss: 6.685828702757135e-05\n",
            "step: 120, loss: 0.020940368995070457\n",
            "step: 130, loss: 0.0006322991685010493\n",
            "step: 140, loss: 0.00016880307521205395\n",
            "step: 150, loss: 0.02977008745074272\n",
            "step: 160, loss: 0.00029123705462552607\n",
            "step: 170, loss: 0.01390268374234438\n",
            "step: 180, loss: 0.018306532874703407\n",
            "step: 190, loss: 0.002590804360806942\n",
            "step: 200, loss: 0.0044114734046161175\n",
            "step: 210, loss: 0.012627040967345238\n",
            "step: 220, loss: 0.0003471104719210416\n",
            "step: 230, loss: 0.000660971796605736\n",
            "step: 240, loss: 0.00123393046669662\n",
            "step: 250, loss: 0.0525011382997036\n",
            "step: 260, loss: 0.0023890738375484943\n",
            "step: 270, loss: 0.0005840189987793565\n",
            "step: 280, loss: 0.027425602078437805\n",
            "step: 290, loss: 0.0014800478238612413\n",
            "step: 300, loss: 0.00016370137745980173\n",
            "step: 310, loss: 0.001657145912759006\n",
            "step: 320, loss: 0.0008810590370558202\n",
            "step: 330, loss: 0.0031597046181559563\n",
            "step: 340, loss: 0.003798386547714472\n",
            "step: 350, loss: 0.0006445776089094579\n",
            "step: 360, loss: 0.0014434515032917261\n",
            "step: 370, loss: 0.0059719872660934925\n",
            "step: 380, loss: 0.0006443258607760072\n",
            "step: 390, loss: 0.017080310732126236\n",
            "step: 400, loss: 0.07625246047973633\n",
            "step: 410, loss: 0.00013957677583675832\n",
            "step: 420, loss: 0.011643619276583195\n",
            "step: 430, loss: 0.000516659754794091\n",
            "step: 440, loss: 0.005214490462094545\n",
            "step: 450, loss: 0.0019645385909825563\n",
            "step: 460, loss: 0.033829204738140106\n",
            "step: 470, loss: 0.13054367899894714\n",
            "step: 480, loss: 0.007297556381672621\n",
            "step: 490, loss: 0.1038021370768547\n",
            "step: 500, loss: 0.0030694182496517897\n",
            "step: 510, loss: 0.00022378764697350562\n",
            "step: 520, loss: 0.0035071575548499823\n",
            "step: 530, loss: 0.0017552707577124238\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9521597770552717, f1=0.9464867380176826, best_f1=0.9464867380176826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009342527482658625\n",
            "step: 10, loss: 0.0033070382196456194\n",
            "step: 20, loss: 0.0005260029574856162\n",
            "step: 30, loss: 0.0005950990598648787\n",
            "step: 40, loss: 0.00063158362172544\n",
            "step: 50, loss: 0.0013482669601216912\n",
            "step: 60, loss: 0.0009306251304224133\n",
            "step: 70, loss: 0.0004437978786882013\n",
            "step: 80, loss: 0.04592753201723099\n",
            "step: 90, loss: 0.00046504929196089506\n",
            "step: 100, loss: 0.0003863434540107846\n",
            "step: 110, loss: 0.0004118476063013077\n",
            "step: 120, loss: 0.0014418604550883174\n",
            "step: 130, loss: 0.0001427927054464817\n",
            "step: 140, loss: 0.0031610336154699326\n",
            "step: 150, loss: 0.00019511398568283767\n",
            "step: 160, loss: 0.0001262393780052662\n",
            "step: 170, loss: 0.0015734938206151128\n",
            "step: 180, loss: 0.0002871699689421803\n",
            "step: 190, loss: 0.01123350765556097\n",
            "step: 200, loss: 0.0008525022421963513\n",
            "step: 210, loss: 0.09746523201465607\n",
            "step: 220, loss: 0.00021685297542717308\n",
            "step: 230, loss: 0.06529227644205093\n",
            "step: 240, loss: 0.0016446999507024884\n",
            "step: 250, loss: 0.0010039351182058454\n",
            "step: 260, loss: 0.0002549363998696208\n",
            "step: 270, loss: 0.006911277770996094\n",
            "step: 280, loss: 0.0007572845788672566\n",
            "step: 290, loss: 0.0016101045766845345\n",
            "step: 300, loss: 0.00034924442297779024\n",
            "step: 310, loss: 0.0013306026812642813\n",
            "step: 320, loss: 0.0010472690919414163\n",
            "step: 330, loss: 0.0013676612870767713\n",
            "step: 340, loss: 0.09953443706035614\n",
            "step: 350, loss: 0.0004193224594928324\n",
            "step: 360, loss: 0.028177538886666298\n",
            "step: 370, loss: 0.002031471114605665\n",
            "step: 380, loss: 7.237166573759168e-05\n",
            "step: 390, loss: 0.0017144945450127125\n",
            "step: 400, loss: 0.0016251642955467105\n",
            "step: 410, loss: 0.0006582658970728517\n",
            "step: 420, loss: 0.00020020786905661225\n",
            "step: 430, loss: 0.001270404551178217\n",
            "step: 440, loss: 0.003434128360822797\n",
            "step: 450, loss: 0.000942566548474133\n",
            "step: 460, loss: 0.012674743309617043\n",
            "step: 470, loss: 0.1585671454668045\n",
            "step: 480, loss: 0.019380580633878708\n",
            "step: 490, loss: 0.023488158360123634\n",
            "step: 500, loss: 0.0013240569969639182\n",
            "step: 510, loss: 0.004268900025635958\n",
            "step: 520, loss: 0.0002647778019309044\n",
            "step: 530, loss: 0.002416247734799981\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9451360073766714, f1=0.9416126042632066, best_f1=0.9464867380176826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00029330188408493996\n",
            "step: 10, loss: 0.0028728158213198185\n",
            "step: 20, loss: 0.000138981340569444\n",
            "step: 30, loss: 0.09799385070800781\n",
            "step: 40, loss: 0.0012810751795768738\n",
            "step: 50, loss: 0.0001866332459030673\n",
            "step: 60, loss: 0.0006658528582192957\n",
            "step: 70, loss: 0.0024769166484475136\n",
            "step: 80, loss: 0.0054280818440020084\n",
            "step: 90, loss: 0.04577948898077011\n",
            "step: 100, loss: 0.00019327946938574314\n",
            "step: 110, loss: 0.03247964754700661\n",
            "step: 120, loss: 0.019028740003705025\n",
            "step: 130, loss: 0.04394541680812836\n",
            "step: 140, loss: 0.0019278832478448749\n",
            "step: 150, loss: 0.024595508351922035\n",
            "step: 160, loss: 0.0002193194377468899\n",
            "step: 170, loss: 0.0015647285617887974\n",
            "step: 180, loss: 0.0002543670707382262\n",
            "step: 190, loss: 9.711356688058004e-05\n",
            "step: 200, loss: 0.011477278545498848\n",
            "step: 210, loss: 0.0004221297276671976\n",
            "step: 220, loss: 0.0013038819888606668\n",
            "step: 230, loss: 0.001984015805646777\n",
            "step: 240, loss: 0.0013548951828852296\n",
            "step: 250, loss: 0.0017511994810774922\n",
            "step: 260, loss: 0.0017146007157862186\n",
            "step: 270, loss: 0.0017759122420102358\n",
            "step: 280, loss: 0.000521720910910517\n",
            "step: 290, loss: 0.000131415348732844\n",
            "step: 300, loss: 0.011189262382686138\n",
            "step: 310, loss: 0.001590463682077825\n",
            "step: 320, loss: 0.00017144737648777664\n",
            "step: 330, loss: 0.0024736125487834215\n",
            "step: 340, loss: 0.001820942503400147\n",
            "step: 350, loss: 0.058346472680568695\n",
            "step: 360, loss: 0.00034319443511776626\n",
            "step: 370, loss: 0.00023920492094475776\n",
            "step: 380, loss: 0.0007703941082581878\n",
            "step: 390, loss: 0.0002529993071220815\n",
            "step: 400, loss: 0.12750551104545593\n",
            "step: 410, loss: 0.00035729468800127506\n",
            "step: 420, loss: 0.007432352285832167\n",
            "step: 430, loss: 0.007169617339968681\n",
            "step: 440, loss: 7.198414095910266e-05\n",
            "step: 450, loss: 0.0007664108416065574\n",
            "step: 460, loss: 0.0015664247330278158\n",
            "step: 470, loss: 0.0004658494144678116\n",
            "step: 480, loss: 0.001127629424445331\n",
            "step: 490, loss: 0.0004520677321124822\n",
            "step: 500, loss: 0.0020705973729491234\n",
            "step: 510, loss: 0.0008083077264018357\n",
            "step: 520, loss: 0.0051104044541716576\n",
            "step: 530, loss: 0.003850232809782028\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9506057781919852, f1=0.9434843531060252, best_f1=0.9464867380176826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003380195703357458\n",
            "step: 10, loss: 0.0015208980767056346\n",
            "step: 20, loss: 0.0018975718412548304\n",
            "step: 30, loss: 0.0017015708144754171\n",
            "step: 40, loss: 0.0007364371558651328\n",
            "step: 50, loss: 0.002436678158119321\n",
            "step: 60, loss: 0.005972565617412329\n",
            "step: 70, loss: 0.0001263960439246148\n",
            "step: 80, loss: 0.0047851670533418655\n",
            "step: 90, loss: 4.0150745917344466e-05\n",
            "step: 100, loss: 0.00017420918447896838\n",
            "step: 110, loss: 0.011783311143517494\n",
            "step: 120, loss: 0.0009520664461888373\n",
            "step: 130, loss: 0.00015215255552902818\n",
            "step: 140, loss: 0.00010473771544639021\n",
            "step: 150, loss: 0.0002748311962932348\n",
            "step: 160, loss: 0.04325684532523155\n",
            "step: 170, loss: 0.0005567934131249785\n",
            "step: 180, loss: 0.00031835175468586385\n",
            "step: 190, loss: 2.6347279344918206e-05\n",
            "step: 200, loss: 4.414805880514905e-05\n",
            "step: 210, loss: 0.0009870269568637013\n",
            "step: 220, loss: 0.000758049194701016\n",
            "step: 230, loss: 2.438501178403385e-05\n",
            "step: 240, loss: 4.319414802012034e-05\n",
            "step: 250, loss: 0.003805846208706498\n",
            "step: 260, loss: 0.0015103628393262625\n",
            "step: 270, loss: 5.4282962082652375e-05\n",
            "step: 280, loss: 0.03481755033135414\n",
            "step: 290, loss: 0.00010128906433237717\n",
            "step: 300, loss: 0.000721063872333616\n",
            "step: 310, loss: 0.01930287852883339\n",
            "step: 320, loss: 0.00021739781368523836\n",
            "step: 330, loss: 0.006806053686887026\n",
            "step: 340, loss: 7.280929276021197e-05\n",
            "step: 350, loss: 0.00039852719055488706\n",
            "step: 360, loss: 5.8624038501875475e-05\n",
            "step: 370, loss: 0.0259723961353302\n",
            "step: 380, loss: 0.00034824971226044\n",
            "step: 390, loss: 0.11684215068817139\n",
            "step: 400, loss: 0.002479265909641981\n",
            "step: 410, loss: 0.0016014180146157742\n",
            "step: 420, loss: 0.00011303351493552327\n",
            "step: 430, loss: 0.00019492699357215315\n",
            "step: 440, loss: 8.466611325275153e-05\n",
            "step: 450, loss: 0.0006085888016968966\n",
            "step: 460, loss: 0.0006405851454474032\n",
            "step: 470, loss: 0.01430224534124136\n",
            "step: 480, loss: 0.0008675642893649638\n",
            "step: 490, loss: 0.0038884999230504036\n",
            "step: 500, loss: 0.0015177371678873897\n",
            "step: 510, loss: 0.00032085395650938153\n",
            "step: 520, loss: 0.0005258123273961246\n",
            "step: 530, loss: 0.008069991134107113\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9501630181648812, f1=0.9471221338324755, best_f1=0.9464867380176826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004948289133608341\n",
            "step: 10, loss: 0.00012992705160286278\n",
            "step: 20, loss: 0.001501885592006147\n",
            "step: 30, loss: 0.00022811256349086761\n",
            "step: 40, loss: 7.154076592996716e-05\n",
            "step: 50, loss: 0.0005239497986622155\n",
            "step: 60, loss: 0.00011300771438982338\n",
            "step: 70, loss: 0.00035566333099268377\n",
            "step: 80, loss: 0.0009985121432691813\n",
            "step: 90, loss: 0.0012269638245925307\n",
            "step: 100, loss: 0.000240931156440638\n",
            "step: 110, loss: 5.466167567647062e-05\n",
            "step: 120, loss: 0.00017036426288541406\n",
            "step: 130, loss: 5.656334542436525e-05\n",
            "step: 140, loss: 4.9422811571275815e-05\n",
            "step: 150, loss: 0.002079392783343792\n",
            "step: 160, loss: 0.00029020648798905313\n",
            "step: 170, loss: 3.449102587183006e-05\n",
            "step: 180, loss: 0.0011587119661271572\n",
            "step: 190, loss: 0.14592324197292328\n",
            "step: 200, loss: 6.34294847259298e-05\n",
            "step: 210, loss: 0.0001668710756348446\n",
            "step: 220, loss: 0.005717751104384661\n",
            "step: 230, loss: 0.001304592820815742\n",
            "step: 240, loss: 0.000522451417054981\n",
            "step: 250, loss: 0.0036480792332440615\n",
            "step: 260, loss: 0.001417059451341629\n",
            "step: 270, loss: 0.0026302188634872437\n",
            "step: 280, loss: 0.003880593925714493\n",
            "step: 290, loss: 3.3161264582304284e-05\n",
            "step: 300, loss: 3.9742251829011366e-05\n",
            "step: 310, loss: 0.024966195225715637\n",
            "step: 320, loss: 0.00011113385698990896\n",
            "step: 330, loss: 2.7681569918058813e-05\n",
            "step: 340, loss: 0.003548920853063464\n",
            "step: 350, loss: 0.00013795559061691165\n",
            "step: 360, loss: 0.0002696604933589697\n",
            "step: 370, loss: 0.002125052735209465\n",
            "step: 380, loss: 0.0006017002160660923\n",
            "step: 390, loss: 0.0009862409206107259\n",
            "step: 400, loss: 0.039306577295064926\n",
            "step: 410, loss: 0.0018950001103803515\n",
            "step: 420, loss: 0.000777191249653697\n",
            "step: 430, loss: 0.0012645235983654857\n",
            "step: 440, loss: 0.00017229080549441278\n",
            "step: 450, loss: 0.000231521378736943\n",
            "step: 460, loss: 0.004533964209258556\n",
            "step: 470, loss: 0.00034494505962356925\n",
            "step: 480, loss: 0.0006135429721325636\n",
            "step: 490, loss: 0.004648908041417599\n",
            "step: 500, loss: 0.0005149584612809122\n",
            "step: 510, loss: 0.0001736019621603191\n",
            "step: 520, loss: 0.0002594335819594562\n",
            "step: 530, loss: 0.02286817878484726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9496944052656323, f1=0.9413953488372093, best_f1=0.9464867380176826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0091921491548419\n",
            "step: 10, loss: 0.0004095975018572062\n",
            "step: 20, loss: 0.011544519104063511\n",
            "step: 30, loss: 3.039947296201717e-05\n",
            "step: 40, loss: 0.0003726030408870429\n",
            "step: 50, loss: 2.511135426175315e-05\n",
            "step: 60, loss: 4.719338539871387e-05\n",
            "step: 70, loss: 6.626672256970778e-05\n",
            "step: 80, loss: 0.00503385066986084\n",
            "step: 90, loss: 4.9057947762776166e-05\n",
            "step: 100, loss: 0.003937715198844671\n",
            "step: 110, loss: 0.0005039493553340435\n",
            "step: 120, loss: 7.728115451755002e-05\n",
            "step: 130, loss: 0.00156929693184793\n",
            "step: 140, loss: 2.947663415397983e-05\n",
            "step: 150, loss: 0.0001268247578991577\n",
            "step: 160, loss: 8.601349691161886e-05\n",
            "step: 170, loss: 0.00016984695685096085\n",
            "step: 180, loss: 2.1311643649823964e-05\n",
            "step: 190, loss: 4.697880649473518e-05\n",
            "step: 200, loss: 0.0002884110144805163\n",
            "step: 210, loss: 4.7385834477609023e-05\n",
            "step: 220, loss: 4.338540747994557e-05\n",
            "step: 230, loss: 2.8791184377041645e-05\n",
            "step: 240, loss: 0.0011362670920789242\n",
            "step: 250, loss: 2.9806617021677084e-05\n",
            "step: 260, loss: 4.6865960030118003e-05\n",
            "step: 270, loss: 0.00012161157792434096\n",
            "step: 280, loss: 0.00036775710759684443\n",
            "step: 290, loss: 2.3293392587220296e-05\n",
            "step: 300, loss: 0.00014314422151073813\n",
            "step: 310, loss: 0.15885916352272034\n",
            "step: 320, loss: 0.007796788588166237\n",
            "step: 330, loss: 0.055851049721241\n",
            "step: 340, loss: 0.0012045593466609716\n",
            "step: 350, loss: 0.00011653387628030032\n",
            "step: 360, loss: 0.0001321962772635743\n",
            "step: 370, loss: 0.0006296260980889201\n",
            "step: 380, loss: 0.0003986964584328234\n",
            "step: 390, loss: 0.004584207199513912\n",
            "step: 400, loss: 2.2786758563597687e-05\n",
            "step: 410, loss: 5.7923673011828214e-05\n",
            "step: 420, loss: 0.000269654905423522\n",
            "step: 430, loss: 0.009690248407423496\n",
            "step: 440, loss: 4.078376878169365e-05\n",
            "step: 450, loss: 0.00021975785784889013\n",
            "step: 460, loss: 0.00012635134044103324\n",
            "step: 470, loss: 0.0010468311375007033\n",
            "step: 480, loss: 0.00045617198338732123\n",
            "step: 490, loss: 4.359573722467758e-05\n",
            "step: 500, loss: 9.248782589565963e-05\n",
            "step: 510, loss: 0.002812970196828246\n",
            "step: 520, loss: 0.002568027237430215\n",
            "step: 530, loss: 7.89305631769821e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.95004712535344, f1=0.9441052137153593, best_f1=0.9464867380176826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012446236796677113\n",
            "step: 10, loss: 0.00011112452921224758\n",
            "step: 20, loss: 5.313123983796686e-05\n",
            "step: 30, loss: 1.4848676073597744e-05\n",
            "step: 40, loss: 2.4566645151935518e-05\n",
            "step: 50, loss: 0.0006025693146511912\n",
            "step: 60, loss: 0.00010244004806736484\n",
            "step: 70, loss: 0.0012210299028083682\n",
            "step: 80, loss: 7.282780279638246e-05\n",
            "step: 90, loss: 3.126484079984948e-05\n",
            "step: 100, loss: 0.0011443570256233215\n",
            "step: 110, loss: 1.2311938917264342e-05\n",
            "step: 120, loss: 1.996699211304076e-05\n",
            "step: 130, loss: 6.753401248715818e-05\n",
            "step: 140, loss: 0.00010981885861838236\n",
            "step: 150, loss: 0.00011168952187290415\n",
            "step: 160, loss: 3.9168677176348865e-05\n",
            "step: 170, loss: 3.13156378979329e-05\n",
            "step: 180, loss: 2.2582054953090847e-05\n",
            "step: 190, loss: 0.00034895483986474574\n",
            "step: 200, loss: 2.2399128283723257e-05\n",
            "step: 210, loss: 0.0001229621411766857\n",
            "step: 220, loss: 2.115092138410546e-05\n",
            "step: 230, loss: 2.4679249690962024e-05\n",
            "step: 240, loss: 0.001257053343579173\n",
            "step: 250, loss: 3.888268838636577e-05\n",
            "step: 260, loss: 0.00014701149484608322\n",
            "step: 270, loss: 0.0001218021716340445\n",
            "step: 280, loss: 0.008684376254677773\n",
            "step: 290, loss: 0.0025675962679088116\n",
            "step: 300, loss: 1.9341163351782598e-05\n",
            "step: 310, loss: 9.641284123063087e-05\n",
            "step: 320, loss: 8.386738772969693e-05\n",
            "step: 330, loss: 1.9061975763179362e-05\n",
            "step: 340, loss: 2.433607733109966e-05\n",
            "step: 350, loss: 9.123138625000138e-06\n",
            "step: 360, loss: 0.07255271822214127\n",
            "step: 370, loss: 0.00019951301510445774\n",
            "step: 380, loss: 2.3833197701605968e-05\n",
            "step: 390, loss: 1.8596158042782918e-05\n",
            "step: 400, loss: 3.43524370691739e-05\n",
            "step: 410, loss: 0.0248210858553648\n",
            "step: 420, loss: 0.002154873451218009\n",
            "step: 430, loss: 0.0003095695283263922\n",
            "step: 440, loss: 2.3143633370636962e-05\n",
            "step: 450, loss: 0.016674183309078217\n",
            "step: 460, loss: 0.0006924314657226205\n",
            "step: 470, loss: 0.0008834194159135222\n",
            "step: 480, loss: 1.2918983884446789e-05\n",
            "step: 490, loss: 0.006347615737468004\n",
            "step: 500, loss: 1.9597604477894492e-05\n",
            "step: 510, loss: 0.00017393435700796545\n",
            "step: 520, loss: 7.610849570482969e-05\n",
            "step: 530, loss: 0.00012907024938613176\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9482596425211665, f1=0.942615239887112, best_f1=0.9464867380176826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014112950302660465\n",
            "step: 10, loss: 5.517063254956156e-05\n",
            "step: 20, loss: 8.692011761013418e-05\n",
            "step: 30, loss: 0.00014675605052616447\n",
            "step: 40, loss: 0.00011914374772459269\n",
            "step: 50, loss: 0.0030389949679374695\n",
            "step: 60, loss: 5.730077828047797e-05\n",
            "step: 70, loss: 0.00011124341835966334\n",
            "step: 80, loss: 0.00010184250277234241\n",
            "step: 90, loss: 1.5221112334984355e-05\n",
            "step: 100, loss: 9.695412882138044e-05\n",
            "step: 110, loss: 4.149025335209444e-05\n",
            "step: 120, loss: 1.893891931104008e-05\n",
            "step: 130, loss: 1.5929050277918577e-05\n",
            "step: 140, loss: 5.214691918808967e-05\n",
            "step: 150, loss: 7.192888733698055e-05\n",
            "step: 160, loss: 2.2686155716655776e-05\n",
            "step: 170, loss: 0.0007830546819604933\n",
            "step: 180, loss: 7.622817065566778e-05\n",
            "step: 190, loss: 3.6377092328621075e-05\n",
            "step: 200, loss: 2.3576660169055685e-05\n",
            "step: 210, loss: 7.289170025615022e-05\n",
            "step: 220, loss: 4.634939614334144e-05\n",
            "step: 230, loss: 3.9163973269751295e-05\n",
            "step: 240, loss: 0.0001641005219426006\n",
            "step: 250, loss: 0.001957208849489689\n",
            "step: 260, loss: 2.5829316655290313e-05\n",
            "step: 270, loss: 0.00047806446673348546\n",
            "step: 280, loss: 1.3518771993403789e-05\n",
            "step: 290, loss: 2.8810871299356222e-05\n",
            "step: 300, loss: 7.227327296277508e-05\n",
            "step: 310, loss: 1.9630662791314535e-05\n",
            "step: 320, loss: 2.8578329875017516e-05\n",
            "step: 330, loss: 2.6790097763296217e-05\n",
            "step: 340, loss: 0.0001607692101970315\n",
            "step: 350, loss: 1.9801966118393466e-05\n",
            "step: 360, loss: 1.5537807485088706e-05\n",
            "step: 370, loss: 0.002788426587358117\n",
            "step: 380, loss: 0.00011607866326812655\n",
            "step: 390, loss: 0.0002534578670747578\n",
            "step: 400, loss: 0.0016377083957195282\n",
            "step: 410, loss: 1.002462795440806e-05\n",
            "step: 420, loss: 1.7355714589939453e-05\n",
            "step: 430, loss: 0.0003766241134144366\n",
            "step: 440, loss: 5.889981912332587e-05\n",
            "step: 450, loss: 2.5351451768074185e-05\n",
            "step: 460, loss: 0.020973041653633118\n",
            "step: 470, loss: 1.9117420379188843e-05\n",
            "step: 480, loss: 1.4237628420232795e-05\n",
            "step: 490, loss: 5.042196426074952e-05\n",
            "step: 500, loss: 0.061101283878088\n",
            "step: 510, loss: 0.007580767851322889\n",
            "step: 520, loss: 4.244318915880285e-05\n",
            "step: 530, loss: 0.0012121936306357384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9504672897196261, f1=0.9453015427769986, best_f1=0.9464867380176826\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 2.2022391931386665e-05\n",
            "step: 10, loss: 0.0006574745057150722\n",
            "step: 20, loss: 0.0001231592905241996\n",
            "step: 30, loss: 0.00485201133415103\n",
            "step: 40, loss: 2.1046740585006773e-05\n",
            "step: 50, loss: 0.00020327542733866721\n",
            "step: 60, loss: 0.0013512088917195797\n",
            "step: 70, loss: 1.9899300241377205e-05\n",
            "step: 80, loss: 1.2971291653229855e-05\n",
            "step: 90, loss: 2.544150993344374e-05\n",
            "step: 100, loss: 7.670313607377466e-06\n",
            "step: 110, loss: 0.00040068128146231174\n",
            "step: 120, loss: 1.2743970728479326e-05\n",
            "step: 130, loss: 3.3577824069652706e-05\n",
            "step: 140, loss: 1.5686789993196726e-05\n",
            "step: 150, loss: 2.6645355319487862e-05\n",
            "step: 160, loss: 2.362384293519426e-05\n",
            "step: 170, loss: 1.3030736226937734e-05\n",
            "step: 180, loss: 1.92553416127339e-05\n",
            "step: 190, loss: 4.263272421667352e-05\n",
            "step: 200, loss: 0.00010143612598767504\n",
            "step: 210, loss: 2.0386498363222927e-05\n",
            "step: 220, loss: 1.8812079360941425e-05\n",
            "step: 230, loss: 0.01337635051459074\n",
            "step: 240, loss: 2.2301870558294468e-05\n",
            "step: 250, loss: 8.083809916570317e-06\n",
            "step: 260, loss: 9.842104191193357e-06\n",
            "step: 270, loss: 2.223037154180929e-05\n",
            "step: 280, loss: 1.1600402103795204e-05\n",
            "step: 290, loss: 9.179003427561838e-06\n",
            "step: 300, loss: 1.4937862943043001e-05\n",
            "step: 310, loss: 0.007150289136916399\n",
            "step: 320, loss: 2.000023414439056e-05\n",
            "step: 330, loss: 0.00031311606289818883\n",
            "step: 340, loss: 2.8708847821690142e-05\n",
            "step: 350, loss: 2.011559445236344e-05\n",
            "step: 360, loss: 0.00029084988636896014\n",
            "step: 370, loss: 0.0002430541644571349\n",
            "step: 380, loss: 4.416201409185305e-05\n",
            "step: 390, loss: 3.922516407328658e-05\n",
            "step: 400, loss: 2.4876228053472005e-05\n",
            "step: 410, loss: 3.7512047128984705e-05\n",
            "step: 420, loss: 1.3485228009813e-05\n",
            "step: 430, loss: 8.072620403254405e-06\n",
            "step: 440, loss: 1.131354110839311e-05\n",
            "step: 450, loss: 0.005462229251861572\n",
            "step: 460, loss: 0.00018487458874005824\n",
            "step: 470, loss: 1.2174056791991461e-05\n",
            "step: 480, loss: 0.0013142938259989023\n",
            "step: 490, loss: 8.730898116482422e-05\n",
            "step: 500, loss: 0.00011440498201409355\n",
            "step: 510, loss: 1.194305150420405e-05\n",
            "step: 520, loss: 5.734053047490306e-05\n",
            "step: 530, loss: 1.4651153833256103e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9506517690875234, f1=0.9433085501858736, best_f1=0.9464867380176826\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:33, 171.46it/s]\n",
            "load_f1 = 0.9522041763341067\n",
            "real_f1 = 0.9521152952115295\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 143.19it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DK"
      ],
      "metadata": {
        "id": "10svv34hgw7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para executar dk necesita\n",
        "#!pip install -r requirements.txt\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "id": "xShW56LQs006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b107a421-d01a-4d88-8247-036e41e930e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.7)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.10.1)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.25.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=b8ee3362a0d56ed0b4cfb15d1ec1d0fb67fd413bfb9e8e0af0f3b3fe66437b9e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1t9kalor/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK STRUCTURED"
      ],
      "metadata": {
        "id": "pdNk8ikFgw7-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yLxbfdggw7_"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCvdP9vMgw7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7313639c-d482-455e-ef6e-cbe23e33a518"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4464835822582245\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.2666666666666667, f1=0.2666666666666667, best_f1=0.2666666666666667\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3882683515548706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.37837837837837834, f1=0.26262626262626265, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.40121328830718994\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3524555265903473\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.30721378326416016\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2896624803543091\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6813454627990723\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5024064183235168\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37582382559776306\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.2692307692307693, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.48656052350997925\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.27184466019417475, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.506280243396759\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.28, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.44152024388313293\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.2916666666666667, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4162924289703369\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.288659793814433, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.35893523693084717\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.288659793814433, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.40308016538619995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.288659793814433, f1=0.2692307692307693, best_f1=0.26262626262626265\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 125925.99it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.33333333333333337\n",
            "real_f1 = 0.30434782608695654\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 216.17it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "TWZ1NvUvgw8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "6VIiiAcAgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7318a88-4f19-41b3-86b6-2e887a6877f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.642425000667572\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5017406344413757\n",
            "step: 20, loss: 0.5103674530982971\n",
            "step: 30, loss: 0.26089468598365784\n",
            "step: 40, loss: 0.3818041682243347\n",
            "step: 50, loss: 0.668758749961853\n",
            "step: 60, loss: 0.4791797995567322\n",
            "step: 70, loss: 0.37628480792045593\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 80, loss: 0.3796873092651367\n",
            "step: 90, loss: 0.0978928804397583\n",
            "step: 100, loss: 0.15923985838890076\n",
            "step: 110, loss: 0.08144884556531906\n",
            "step: 120, loss: 0.10255473852157593\n",
            "step: 130, loss: 0.024221794679760933\n",
            "step: 140, loss: 0.014062696136534214\n",
            "step: 150, loss: 0.1743423491716385\n",
            "step: 160, loss: 0.012653644196689129\n",
            "step: 170, loss: 0.1481861025094986\n",
            "step: 180, loss: 0.07103492319583893\n",
            "step: 190, loss: 0.023944532498717308\n",
            "step: 200, loss: 0.03059191256761551\n",
            "step: 210, loss: 0.013501688838005066\n",
            "step: 220, loss: 0.050308212637901306\n",
            "step: 230, loss: 0.007186539005488157\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9743589743589743, f1=0.9709821428571428, best_f1=0.9709821428571428\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.013825001195073128\n",
            "step: 10, loss: 0.06415978074073792\n",
            "step: 20, loss: 0.010028814896941185\n",
            "step: 30, loss: 0.007613536436110735\n",
            "step: 40, loss: 0.043118804693222046\n",
            "step: 50, loss: 0.001932749291881919\n",
            "step: 60, loss: 0.0065463874489068985\n",
            "step: 70, loss: 0.007705240044742823\n",
            "step: 80, loss: 0.002561191562563181\n",
            "step: 90, loss: 0.01105353981256485\n",
            "step: 100, loss: 0.04220879450440407\n",
            "step: 110, loss: 0.019783522933721542\n",
            "step: 120, loss: 0.0012342592235654593\n",
            "step: 130, loss: 0.054859939962625504\n",
            "step: 140, loss: 0.0016032621497288346\n",
            "step: 150, loss: 0.05695808306336403\n",
            "step: 160, loss: 0.09995504468679428\n",
            "step: 170, loss: 0.017613552510738373\n",
            "step: 180, loss: 0.005648380611091852\n",
            "step: 190, loss: 0.026212861761450768\n",
            "step: 200, loss: 0.0038839494809508324\n",
            "step: 210, loss: 0.02364102192223072\n",
            "step: 220, loss: 0.002109998371452093\n",
            "step: 230, loss: 0.0017560218693688512\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9819004524886877, f1=0.9759999999999999, best_f1=0.9759999999999999\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00975214596837759\n",
            "step: 10, loss: 0.012295710854232311\n",
            "step: 20, loss: 0.0023515603970736265\n",
            "step: 30, loss: 0.024020466953516006\n",
            "step: 40, loss: 0.01965656504034996\n",
            "step: 50, loss: 0.0684603899717331\n",
            "step: 60, loss: 0.0012475531548261642\n",
            "step: 70, loss: 0.0008373182499781251\n",
            "step: 80, loss: 0.0003917946887668222\n",
            "step: 90, loss: 0.005513705778867006\n",
            "step: 100, loss: 0.0009700815426185727\n",
            "step: 110, loss: 0.007343923673033714\n",
            "step: 120, loss: 0.0002742062497418374\n",
            "step: 130, loss: 0.003801067592576146\n",
            "step: 140, loss: 0.000866077549289912\n",
            "step: 150, loss: 0.006922523491084576\n",
            "step: 160, loss: 0.0066132089123129845\n",
            "step: 170, loss: 0.007128482218831778\n",
            "step: 180, loss: 0.008291102945804596\n",
            "step: 190, loss: 0.02285112626850605\n",
            "step: 200, loss: 0.01119040697813034\n",
            "step: 210, loss: 0.003248992608860135\n",
            "step: 220, loss: 0.005212882999330759\n",
            "step: 230, loss: 0.006613322999328375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9841628959276018, f1=0.9817767653758542, best_f1=0.9817767653758542\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.007583811413496733\n",
            "step: 10, loss: 0.001373698585666716\n",
            "step: 20, loss: 0.0017009630100801587\n",
            "step: 30, loss: 0.0011821433436125517\n",
            "step: 40, loss: 0.1014334037899971\n",
            "step: 50, loss: 0.009383843280375004\n",
            "step: 60, loss: 0.011903120204806328\n",
            "step: 70, loss: 0.007983568124473095\n",
            "step: 80, loss: 0.0007251294446177781\n",
            "step: 90, loss: 0.0034628533758223057\n",
            "step: 100, loss: 0.001198501675389707\n",
            "step: 110, loss: 0.0007024130900390446\n",
            "step: 120, loss: 0.0016079447232186794\n",
            "step: 130, loss: 0.00957517884671688\n",
            "step: 140, loss: 0.0031223262194544077\n",
            "step: 150, loss: 0.0003345843288116157\n",
            "step: 160, loss: 0.000890760391484946\n",
            "step: 170, loss: 0.003190930699929595\n",
            "step: 180, loss: 0.17122255265712738\n",
            "step: 190, loss: 0.0060203587636351585\n",
            "step: 200, loss: 0.0019636410288512707\n",
            "step: 210, loss: 0.0006054681143723428\n",
            "step: 220, loss: 0.0006189013365656137\n",
            "step: 230, loss: 0.004168540704995394\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9898762654668166, f1=0.9854096520763187, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011390057625249028\n",
            "step: 10, loss: 0.0019692606292665005\n",
            "step: 20, loss: 0.0012602244969457388\n",
            "step: 30, loss: 0.0007584878476336598\n",
            "step: 40, loss: 0.0012641869252547622\n",
            "step: 50, loss: 0.009406768716871738\n",
            "step: 60, loss: 0.32990872859954834\n",
            "step: 70, loss: 0.001613251632079482\n",
            "step: 80, loss: 0.08294341713190079\n",
            "step: 90, loss: 0.03838430345058441\n",
            "step: 100, loss: 0.0004959430661983788\n",
            "step: 110, loss: 0.04047269746661186\n",
            "step: 120, loss: 0.004568095784634352\n",
            "step: 130, loss: 0.0011925563449040055\n",
            "step: 140, loss: 0.003099958412349224\n",
            "step: 150, loss: 0.15344658493995667\n",
            "step: 160, loss: 0.0014114173827692866\n",
            "step: 170, loss: 0.0035601069685071707\n",
            "step: 180, loss: 0.0134193766862154\n",
            "step: 190, loss: 0.010896381922066212\n",
            "step: 200, loss: 0.0009666422847658396\n",
            "step: 210, loss: 0.007130152080208063\n",
            "step: 220, loss: 0.0010894632432609797\n",
            "step: 230, loss: 0.01655147783458233\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9854748603351955, f1=0.9887133182844244, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001039210706949234\n",
            "step: 10, loss: 0.0013619466917589307\n",
            "step: 20, loss: 0.0013389494270086288\n",
            "step: 30, loss: 0.0008213629480451345\n",
            "step: 40, loss: 0.00041643521399237216\n",
            "step: 50, loss: 0.0015926542691886425\n",
            "step: 60, loss: 0.0008094740333035588\n",
            "step: 70, loss: 0.00041037038317881525\n",
            "step: 80, loss: 0.0016776411794126034\n",
            "step: 90, loss: 0.013464372605085373\n",
            "step: 100, loss: 0.0007202615379355848\n",
            "step: 110, loss: 0.020474523305892944\n",
            "step: 120, loss: 0.0004686861939262599\n",
            "step: 130, loss: 0.0004612489137798548\n",
            "step: 140, loss: 0.0005610320949926972\n",
            "step: 150, loss: 0.00010268326877849177\n",
            "step: 160, loss: 0.030357204377651215\n",
            "step: 170, loss: 0.000409951142501086\n",
            "step: 180, loss: 0.0016846274957060814\n",
            "step: 190, loss: 0.0005928885657340288\n",
            "step: 200, loss: 0.004244954325258732\n",
            "step: 210, loss: 0.0031360520515590906\n",
            "step: 220, loss: 0.0661851167678833\n",
            "step: 230, loss: 0.0009360030526295304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9887892376681614, f1=0.9876265466816648, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014375094324350357\n",
            "step: 10, loss: 0.004966686014086008\n",
            "step: 20, loss: 0.0004405365907587111\n",
            "step: 30, loss: 0.0009448733180761337\n",
            "step: 40, loss: 0.0023988732136785984\n",
            "step: 50, loss: 0.0006069926312193274\n",
            "step: 60, loss: 0.0006414172239601612\n",
            "step: 70, loss: 0.0017863760003820062\n",
            "step: 80, loss: 0.0003319327952340245\n",
            "step: 90, loss: 0.0002732092689257115\n",
            "step: 100, loss: 0.000467059260699898\n",
            "step: 110, loss: 0.0007066680118441582\n",
            "step: 120, loss: 0.0016971349250525236\n",
            "step: 130, loss: 0.004045458976179361\n",
            "step: 140, loss: 0.0004324638575781137\n",
            "step: 150, loss: 0.018132738769054413\n",
            "step: 160, loss: 0.0004243534349370748\n",
            "step: 170, loss: 0.0005134929670020938\n",
            "step: 180, loss: 0.0002461016410961747\n",
            "step: 190, loss: 0.00039906188612803817\n",
            "step: 200, loss: 0.02335965260863304\n",
            "step: 210, loss: 0.00021230966376606375\n",
            "step: 220, loss: 0.0006113977869972587\n",
            "step: 230, loss: 0.0007983265677466989\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9865470852017937, f1=0.9898534385569334, best_f1=0.9854096520763187\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0007676382083445787\n",
            "step: 10, loss: 0.007765226531773806\n",
            "step: 20, loss: 0.002901819534599781\n",
            "step: 30, loss: 0.0006829558406025171\n",
            "step: 40, loss: 0.00051949976477772\n",
            "step: 50, loss: 0.001476836041547358\n",
            "step: 60, loss: 0.0003574073198251426\n",
            "step: 70, loss: 0.0001952205057023093\n",
            "step: 80, loss: 0.0004320907173678279\n",
            "step: 90, loss: 0.00043643455137498677\n",
            "step: 100, loss: 0.0006240651709958911\n",
            "step: 110, loss: 0.0005139712593518198\n",
            "step: 120, loss: 0.0022956125903874636\n",
            "step: 130, loss: 0.0008265624055638909\n",
            "step: 140, loss: 0.0003466866328381002\n",
            "step: 150, loss: 0.11724685877561569\n",
            "step: 160, loss: 0.022033432498574257\n",
            "step: 170, loss: 0.03855443000793457\n",
            "step: 180, loss: 0.0005812786403112113\n",
            "step: 190, loss: 0.014773928560316563\n",
            "step: 200, loss: 0.007046978920698166\n",
            "step: 210, loss: 0.0012361397966742516\n",
            "step: 220, loss: 0.0005695613799616694\n",
            "step: 230, loss: 0.000590082781855017\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9898989898989898, f1=0.9831271091113611, best_f1=0.9831271091113611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000425478327088058\n",
            "step: 10, loss: 0.008282983675599098\n",
            "step: 20, loss: 0.0006905407644808292\n",
            "step: 30, loss: 0.0013126143021509051\n",
            "step: 40, loss: 0.0007154716877266765\n",
            "step: 50, loss: 0.0008065300062298775\n",
            "step: 60, loss: 0.0018369709141552448\n",
            "step: 70, loss: 0.04417150467634201\n",
            "step: 80, loss: 0.0001325635239481926\n",
            "step: 90, loss: 0.010572846047580242\n",
            "step: 100, loss: 0.00022088302648626268\n",
            "step: 110, loss: 0.00017490806931164116\n",
            "step: 120, loss: 0.01065025757998228\n",
            "step: 130, loss: 0.0005525926244445145\n",
            "step: 140, loss: 0.00045215917634777725\n",
            "step: 150, loss: 0.0010679508559405804\n",
            "step: 160, loss: 0.00137861049734056\n",
            "step: 170, loss: 0.0004948487039655447\n",
            "step: 180, loss: 0.0021537276916205883\n",
            "step: 190, loss: 0.0002769882557913661\n",
            "step: 200, loss: 0.0004119597433600575\n",
            "step: 210, loss: 0.0036184233613312244\n",
            "step: 220, loss: 0.0005307663232088089\n",
            "step: 230, loss: 0.000341481645591557\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9854748603351955, f1=0.9854096520763187, best_f1=0.9831271091113611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009103349293582141\n",
            "step: 10, loss: 0.0020700758323073387\n",
            "step: 20, loss: 0.0005596668925136328\n",
            "step: 30, loss: 0.00038873523590154946\n",
            "step: 40, loss: 0.0014280484756454825\n",
            "step: 50, loss: 0.00031027282238937914\n",
            "step: 60, loss: 0.00041427984251640737\n",
            "step: 70, loss: 0.0009211570140905678\n",
            "step: 80, loss: 0.0002195816778112203\n",
            "step: 90, loss: 0.0002469484752509743\n",
            "step: 100, loss: 0.0001652469945838675\n",
            "step: 110, loss: 0.00028491736156865954\n",
            "step: 120, loss: 0.0001770404342096299\n",
            "step: 130, loss: 0.0005420050583779812\n",
            "step: 140, loss: 0.00045483617577701807\n",
            "step: 150, loss: 0.002367848763242364\n",
            "step: 160, loss: 0.00021355622448027134\n",
            "step: 170, loss: 0.00039031117921695113\n",
            "step: 180, loss: 0.005482246167957783\n",
            "step: 190, loss: 0.0006904475740157068\n",
            "step: 200, loss: 0.0006232013693079352\n",
            "step: 210, loss: 0.0008714560535736382\n",
            "step: 220, loss: 0.0001760378945618868\n",
            "step: 230, loss: 0.0002215367858298123\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.987709497206704, f1=0.9865771812080537, best_f1=0.9831271091113611\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00019753210654016584\n",
            "step: 10, loss: 0.0036594851408153772\n",
            "step: 20, loss: 0.0003764512948691845\n",
            "step: 30, loss: 0.0002448961604386568\n",
            "step: 40, loss: 9.657604095991701e-05\n",
            "step: 50, loss: 0.00014799443306401372\n",
            "step: 60, loss: 0.0070287734270095825\n",
            "step: 70, loss: 0.00014100289263296872\n",
            "step: 80, loss: 0.0006586425588466227\n",
            "step: 90, loss: 0.0048077162355184555\n",
            "step: 100, loss: 0.0023100809194147587\n",
            "step: 110, loss: 0.0022427989169955254\n",
            "step: 120, loss: 0.0006817023968324065\n",
            "step: 130, loss: 0.0004948003916069865\n",
            "step: 140, loss: 0.009164690040051937\n",
            "step: 150, loss: 0.0015668851556256413\n",
            "step: 160, loss: 0.03281458094716072\n",
            "step: 170, loss: 0.01885787770152092\n",
            "step: 180, loss: 0.000667267304379493\n",
            "step: 190, loss: 0.00044515958870761096\n",
            "step: 200, loss: 0.005036630667746067\n",
            "step: 210, loss: 0.0003425592731218785\n",
            "step: 220, loss: 0.0013879985781386495\n",
            "step: 230, loss: 0.0004942481173202395\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.992108229988726, f1=0.9887133182844244, best_f1=0.9887133182844244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000283715664409101\n",
            "step: 10, loss: 0.0001677164837019518\n",
            "step: 20, loss: 0.013975896872580051\n",
            "step: 30, loss: 0.02996145375072956\n",
            "step: 40, loss: 0.0008043526322580874\n",
            "step: 50, loss: 0.0006920191226527095\n",
            "step: 60, loss: 0.001203193562105298\n",
            "step: 70, loss: 0.0007264676969498396\n",
            "step: 80, loss: 0.00016816990682855248\n",
            "step: 90, loss: 0.002199603710323572\n",
            "step: 100, loss: 0.00019984458049293607\n",
            "step: 110, loss: 0.00014716126315761358\n",
            "step: 120, loss: 0.00018890330102294683\n",
            "step: 130, loss: 0.00012017120752716437\n",
            "step: 140, loss: 0.000460757699329406\n",
            "step: 150, loss: 0.0002493517240509391\n",
            "step: 160, loss: 0.006999438162893057\n",
            "step: 170, loss: 0.00020266325736884028\n",
            "step: 180, loss: 0.0001579675736138597\n",
            "step: 190, loss: 0.004355217795819044\n",
            "step: 200, loss: 0.00016674326616339386\n",
            "step: 210, loss: 0.00027442339342087507\n",
            "step: 220, loss: 0.006711889524012804\n",
            "step: 230, loss: 0.0017209643265232444\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9921436588103255, f1=0.984304932735426, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013659364776685834\n",
            "step: 10, loss: 0.00031585790566168725\n",
            "step: 20, loss: 0.0008338844636455178\n",
            "step: 30, loss: 0.0004189653554931283\n",
            "step: 40, loss: 0.0011647335486486554\n",
            "step: 50, loss: 0.007552823517471552\n",
            "step: 60, loss: 0.00042577803833410144\n",
            "step: 70, loss: 0.0005084963631816208\n",
            "step: 80, loss: 0.0004337663121987134\n",
            "step: 90, loss: 0.0001558627118356526\n",
            "step: 100, loss: 0.0015805287985131145\n",
            "step: 110, loss: 0.000893113377969712\n",
            "step: 120, loss: 0.00020512549963314086\n",
            "step: 130, loss: 0.00020589045016095042\n",
            "step: 140, loss: 0.00012355307990219444\n",
            "step: 150, loss: 6.376688543241471e-05\n",
            "step: 160, loss: 0.00022254962823353708\n",
            "step: 170, loss: 0.00014739031030330807\n",
            "step: 180, loss: 0.009847572073340416\n",
            "step: 190, loss: 0.0002571415388956666\n",
            "step: 200, loss: 3.867768100462854e-05\n",
            "step: 210, loss: 0.005773370619863272\n",
            "step: 220, loss: 0.00011620506847975776\n",
            "step: 230, loss: 0.00016839485033415258\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9910313901345291, f1=0.9854423292273236, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.854962314013392e-05\n",
            "step: 10, loss: 9.133284038398415e-05\n",
            "step: 20, loss: 0.00011773577716667205\n",
            "step: 30, loss: 0.00016900579794310033\n",
            "step: 40, loss: 0.000107829604530707\n",
            "step: 50, loss: 6.38978453935124e-05\n",
            "step: 60, loss: 9.689160651760176e-05\n",
            "step: 70, loss: 0.00015548487135674804\n",
            "step: 80, loss: 0.00010986236884491518\n",
            "step: 90, loss: 0.0004798285663127899\n",
            "step: 100, loss: 0.0002060107362922281\n",
            "step: 110, loss: 0.00029075221391394734\n",
            "step: 120, loss: 6.016324186930433e-05\n",
            "step: 130, loss: 0.0003563356585800648\n",
            "step: 140, loss: 7.093021122273058e-05\n",
            "step: 150, loss: 5.261049227556214e-05\n",
            "step: 160, loss: 0.0009601161582395434\n",
            "step: 170, loss: 0.00013744279567617923\n",
            "step: 180, loss: 0.00012511477689258754\n",
            "step: 190, loss: 7.48715756344609e-05\n",
            "step: 200, loss: 0.0008335883030667901\n",
            "step: 210, loss: 0.00010969580034725368\n",
            "step: 220, loss: 0.00014754087897017598\n",
            "step: 230, loss: 8.156183321261778e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9910313901345291, f1=0.9854096520763187, best_f1=0.984304932735426\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002877477090805769\n",
            "step: 10, loss: 0.00013701235002372414\n",
            "step: 20, loss: 0.0011643999023362994\n",
            "step: 30, loss: 8.244549826486036e-05\n",
            "step: 40, loss: 5.779182174592279e-05\n",
            "step: 50, loss: 7.02483594068326e-05\n",
            "step: 60, loss: 0.06856506317853928\n",
            "step: 70, loss: 0.00011189009092049673\n",
            "step: 80, loss: 6.217105692485347e-05\n",
            "step: 90, loss: 5.350727224140428e-05\n",
            "step: 100, loss: 0.00011758998152799904\n",
            "step: 110, loss: 0.000198557652765885\n",
            "step: 120, loss: 0.015836775302886963\n",
            "step: 130, loss: 5.674786734743975e-05\n",
            "step: 140, loss: 0.01004179660230875\n",
            "step: 150, loss: 0.00010817413567565382\n",
            "step: 160, loss: 0.00956691149622202\n",
            "step: 170, loss: 3.553222632035613e-05\n",
            "step: 180, loss: 0.0005517103127203882\n",
            "step: 190, loss: 0.00253272638656199\n",
            "step: 200, loss: 9.619855700293556e-05\n",
            "step: 210, loss: 0.0431685633957386\n",
            "step: 220, loss: 0.0001009062398225069\n",
            "step: 230, loss: 0.00010814137931447476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.990990990990991, f1=0.9898534385569334, best_f1=0.984304932735426\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:11, 218.45it/s]\n",
            "load_f1 = 0.9943630214205187\n",
            "real_f1 = 0.9898534385569334\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:22, 199.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "S4v1tmXbgw8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "qUUIV1IBgw8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bff7b252-348e-4255-8a53-7048f18c8904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6089612245559692\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4016510248184204\n",
            "step: 20, loss: 0.31712397933006287\n",
            "step: 30, loss: 0.372112900018692\n",
            "step: 40, loss: 0.34108850359916687\n",
            "step: 50, loss: 0.08926969766616821\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 60, loss: 0.13151559233665466\n",
            "step: 70, loss: 0.24695783853530884\n",
            "step: 80, loss: 0.11330589652061462\n",
            "step: 90, loss: 0.18827638030052185\n",
            "step: 100, loss: 0.1722632348537445\n",
            "step: 110, loss: 0.14077606797218323\n",
            "step: 120, loss: 0.14251145720481873\n",
            "step: 130, loss: 0.259803831577301\n",
            "step: 140, loss: 0.3016637861728668\n",
            "step: 150, loss: 0.1206800788640976\n",
            "step: 160, loss: 0.2311892807483673\n",
            "step: 170, loss: 0.06388650834560394\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 180, loss: 0.021601304411888123\n",
            "step: 190, loss: 0.12284837663173676\n",
            "step: 200, loss: 0.03699744865298271\n",
            "step: 210, loss: 0.03647492825984955\n",
            "step: 220, loss: 0.07315949350595474\n",
            "step: 230, loss: 0.1511138677597046\n",
            "step: 240, loss: 0.02041328325867653\n",
            "step: 250, loss: 0.11886411905288696\n",
            "step: 260, loss: 0.2537340819835663\n",
            "step: 270, loss: 0.2752493619918823\n",
            "step: 280, loss: 0.045938290655612946\n",
            "step: 290, loss: 0.13688188791275024\n",
            "step: 300, loss: 0.09582068026065826\n",
            "step: 310, loss: 0.11906357854604721\n",
            "step: 320, loss: 0.0617513544857502\n",
            "step: 330, loss: 0.077440045773983\n",
            "step: 340, loss: 0.356437623500824\n",
            "step: 350, loss: 0.0876178964972496\n",
            "step: 360, loss: 0.10020691901445389\n",
            "step: 370, loss: 0.08274967968463898\n",
            "step: 380, loss: 0.09306110441684723\n",
            "step: 390, loss: 0.0078098708763718605\n",
            "step: 400, loss: 0.057483673095703125\n",
            "step: 410, loss: 0.2621709406375885\n",
            "step: 420, loss: 0.04580499976873398\n",
            "step: 430, loss: 0.06847318261861801\n",
            "step: 440, loss: 0.04297994449734688\n",
            "step: 450, loss: 0.011385372839868069\n",
            "step: 460, loss: 0.01477331854403019\n",
            "step: 470, loss: 0.014120325446128845\n",
            "step: 480, loss: 0.10617470741271973\n",
            "step: 490, loss: 0.12974843382835388\n",
            "step: 500, loss: 0.03273019939661026\n",
            "step: 510, loss: 0.018962595611810684\n",
            "step: 520, loss: 0.2411423921585083\n",
            "step: 530, loss: 0.030779512599110603\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9343735658558971, f1=0.9372423270728356, best_f1=0.9372423270728356\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0972883552312851\n",
            "step: 10, loss: 0.0316658578813076\n",
            "step: 20, loss: 0.04629868268966675\n",
            "step: 30, loss: 0.11915736645460129\n",
            "step: 40, loss: 0.0741962268948555\n",
            "step: 50, loss: 0.030807267874479294\n",
            "step: 60, loss: 0.029400324448943138\n",
            "step: 70, loss: 0.040451645851135254\n",
            "step: 80, loss: 0.024475038051605225\n",
            "step: 90, loss: 0.01082471664994955\n",
            "step: 100, loss: 0.15096919238567352\n",
            "step: 110, loss: 0.013290183618664742\n",
            "step: 120, loss: 0.2682948410511017\n",
            "step: 130, loss: 0.033416200429201126\n",
            "step: 140, loss: 0.05682038515806198\n",
            "step: 150, loss: 0.022475028410553932\n",
            "step: 160, loss: 0.006962081417441368\n",
            "step: 170, loss: 0.02391129918396473\n",
            "step: 180, loss: 0.03983677923679352\n",
            "step: 190, loss: 0.03798910602927208\n",
            "step: 200, loss: 0.19492414593696594\n",
            "step: 210, loss: 0.03684442117810249\n",
            "step: 220, loss: 0.0010576056083664298\n",
            "step: 230, loss: 0.046231575310230255\n",
            "step: 240, loss: 0.12334544956684113\n",
            "step: 250, loss: 0.09374584257602692\n",
            "step: 260, loss: 0.05963636934757233\n",
            "step: 270, loss: 0.009003497660160065\n",
            "step: 280, loss: 0.03202337026596069\n",
            "step: 290, loss: 0.014480789192020893\n",
            "step: 300, loss: 0.011458178982138634\n",
            "step: 310, loss: 0.04805297777056694\n",
            "step: 320, loss: 0.026231421157717705\n",
            "step: 330, loss: 0.05190826952457428\n",
            "step: 340, loss: 0.0670299082994461\n",
            "step: 350, loss: 0.0025494894944131374\n",
            "step: 360, loss: 0.02439262717962265\n",
            "step: 370, loss: 0.017327435314655304\n",
            "step: 380, loss: 0.13336478173732758\n",
            "step: 390, loss: 0.007565770763903856\n",
            "step: 400, loss: 0.07867047190666199\n",
            "step: 410, loss: 0.01929369382560253\n",
            "step: 420, loss: 0.009878741577267647\n",
            "step: 430, loss: 0.1641150563955307\n",
            "step: 440, loss: 0.08742599189281464\n",
            "step: 450, loss: 0.03763822093605995\n",
            "step: 460, loss: 0.056781619787216187\n",
            "step: 470, loss: 0.034485358744859695\n",
            "step: 480, loss: 0.02490108646452427\n",
            "step: 490, loss: 0.17578420042991638\n",
            "step: 500, loss: 0.008340057916939259\n",
            "step: 510, loss: 0.0219767726957798\n",
            "step: 520, loss: 0.35816752910614014\n",
            "step: 530, loss: 0.13343289494514465\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9436947417403444, f1=0.9404096834264433, best_f1=0.9404096834264433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14337767660617828\n",
            "step: 10, loss: 0.02861062064766884\n",
            "step: 20, loss: 0.0037493640556931496\n",
            "step: 30, loss: 0.06681866943836212\n",
            "step: 40, loss: 0.02986249141395092\n",
            "step: 50, loss: 0.06075010448694229\n",
            "step: 60, loss: 0.013322013430297375\n",
            "step: 70, loss: 0.007422365248203278\n",
            "step: 80, loss: 0.020761841908097267\n",
            "step: 90, loss: 0.016984155401587486\n",
            "step: 100, loss: 0.24530573189258575\n",
            "step: 110, loss: 0.022914189845323563\n",
            "step: 120, loss: 0.13962498307228088\n",
            "step: 130, loss: 0.015649668872356415\n",
            "step: 140, loss: 0.003879171796143055\n",
            "step: 150, loss: 0.03833119571208954\n",
            "step: 160, loss: 0.02237282507121563\n",
            "step: 170, loss: 0.023375531658530235\n",
            "step: 180, loss: 0.0168803371489048\n",
            "step: 190, loss: 0.0061256168410182\n",
            "step: 200, loss: 0.01817593351006508\n",
            "step: 210, loss: 0.08632976561784744\n",
            "step: 220, loss: 0.1375247985124588\n",
            "step: 230, loss: 0.04292796924710274\n",
            "step: 240, loss: 0.07236658781766891\n",
            "step: 250, loss: 0.04711786285042763\n",
            "step: 260, loss: 0.10006499290466309\n",
            "step: 270, loss: 0.036699019372463226\n",
            "step: 280, loss: 0.02499477006494999\n",
            "step: 290, loss: 0.006564737297594547\n",
            "step: 300, loss: 0.13059619069099426\n",
            "step: 310, loss: 0.03486108407378197\n",
            "step: 320, loss: 0.031602129340171814\n",
            "step: 330, loss: 0.039549026638269424\n",
            "step: 340, loss: 0.014266104437410831\n",
            "step: 350, loss: 0.051969900727272034\n",
            "step: 360, loss: 0.0030975532718002796\n",
            "step: 370, loss: 0.02464665286242962\n",
            "step: 380, loss: 0.07051780819892883\n",
            "step: 390, loss: 0.019598081707954407\n",
            "step: 400, loss: 0.2434217780828476\n",
            "step: 410, loss: 0.0759233608841896\n",
            "step: 420, loss: 0.006586997304111719\n",
            "step: 430, loss: 0.014667063020169735\n",
            "step: 440, loss: 0.22918832302093506\n",
            "step: 450, loss: 0.062224168330430984\n",
            "step: 460, loss: 0.041979677975177765\n",
            "step: 470, loss: 0.08932089805603027\n",
            "step: 480, loss: 0.12551918625831604\n",
            "step: 490, loss: 0.007557259406894445\n",
            "step: 500, loss: 0.05375609174370766\n",
            "step: 510, loss: 0.05619954317808151\n",
            "step: 520, loss: 0.005751168355345726\n",
            "step: 530, loss: 0.004565411247313023\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9433962264150944, f1=0.9399527186761228, best_f1=0.9404096834264433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.024954143911600113\n",
            "step: 10, loss: 0.0011167444754391909\n",
            "step: 20, loss: 0.044858019798994064\n",
            "step: 30, loss: 0.020094314590096474\n",
            "step: 40, loss: 0.041448045521974564\n",
            "step: 50, loss: 0.12475957721471786\n",
            "step: 60, loss: 0.0032713774126023054\n",
            "step: 70, loss: 0.02288908138871193\n",
            "step: 80, loss: 0.01990930177271366\n",
            "step: 90, loss: 0.013755979016423225\n",
            "step: 100, loss: 0.004922434221953154\n",
            "step: 110, loss: 0.0671735405921936\n",
            "step: 120, loss: 0.0024781590327620506\n",
            "step: 130, loss: 0.007642771117389202\n",
            "step: 140, loss: 0.005945139564573765\n",
            "step: 150, loss: 0.006982822436839342\n",
            "step: 160, loss: 0.04271283000707626\n",
            "step: 170, loss: 0.016866430640220642\n",
            "step: 180, loss: 0.13958178460597992\n",
            "step: 190, loss: 0.05831108242273331\n",
            "step: 200, loss: 0.05553830415010452\n",
            "step: 210, loss: 0.0029633580707013607\n",
            "step: 220, loss: 0.011327115818858147\n",
            "step: 230, loss: 0.01319810375571251\n",
            "step: 240, loss: 0.008945898152887821\n",
            "step: 250, loss: 0.09204365313053131\n",
            "step: 260, loss: 0.0016071901191025972\n",
            "step: 270, loss: 0.11499883234500885\n",
            "step: 280, loss: 0.003190105315297842\n",
            "step: 290, loss: 0.005142547190189362\n",
            "step: 300, loss: 0.003231661394238472\n",
            "step: 310, loss: 0.0069673736579716206\n",
            "step: 320, loss: 0.08667398244142532\n",
            "step: 330, loss: 0.05328160151839256\n",
            "step: 340, loss: 0.001869620056822896\n",
            "step: 350, loss: 0.012089907191693783\n",
            "step: 360, loss: 0.007795896381139755\n",
            "step: 370, loss: 0.0020241448655724525\n",
            "step: 380, loss: 0.007504928857088089\n",
            "step: 390, loss: 0.003974901977926493\n",
            "step: 400, loss: 0.002857252024114132\n",
            "step: 410, loss: 0.0018647804390639067\n",
            "step: 420, loss: 0.0036382453981786966\n",
            "step: 430, loss: 0.0411624051630497\n",
            "step: 440, loss: 0.003761813510209322\n",
            "step: 450, loss: 0.022352585569024086\n",
            "step: 460, loss: 0.014157080091536045\n",
            "step: 470, loss: 0.0028673307970166206\n",
            "step: 480, loss: 0.012196121737360954\n",
            "step: 490, loss: 0.009077120572328568\n",
            "step: 500, loss: 0.023836927488446236\n",
            "step: 510, loss: 0.20543009042739868\n",
            "step: 520, loss: 0.007039960473775864\n",
            "step: 530, loss: 0.10835722833871841\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9383974062065772, f1=0.9400369003690037, best_f1=0.9404096834264433\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0015874895034357905\n",
            "step: 10, loss: 0.010793488472700119\n",
            "step: 20, loss: 0.0031174037139862776\n",
            "step: 30, loss: 0.005681683775037527\n",
            "step: 40, loss: 0.0011616022093221545\n",
            "step: 50, loss: 0.03284046798944473\n",
            "step: 60, loss: 0.005954340565949678\n",
            "step: 70, loss: 0.0022264081053435802\n",
            "step: 80, loss: 0.000887762289494276\n",
            "step: 90, loss: 0.13189764320850372\n",
            "step: 100, loss: 0.02387685887515545\n",
            "step: 110, loss: 0.020900703966617584\n",
            "step: 120, loss: 0.18705390393733978\n",
            "step: 130, loss: 0.027182800695300102\n",
            "step: 140, loss: 0.0030806618742644787\n",
            "step: 150, loss: 0.0037601622752845287\n",
            "step: 160, loss: 0.003608164144679904\n",
            "step: 170, loss: 0.03950970619916916\n",
            "step: 180, loss: 0.011919633485376835\n",
            "step: 190, loss: 0.01010801550000906\n",
            "step: 200, loss: 0.042239148169755936\n",
            "step: 210, loss: 0.0027351106982678175\n",
            "step: 220, loss: 0.0036225812509655952\n",
            "step: 230, loss: 0.002401405945420265\n",
            "step: 240, loss: 0.024915920570492744\n",
            "step: 250, loss: 0.039857909083366394\n",
            "step: 260, loss: 0.000571323384065181\n",
            "step: 270, loss: 0.013778243213891983\n",
            "step: 280, loss: 0.01629810221493244\n",
            "step: 290, loss: 0.010015456937253475\n",
            "step: 300, loss: 0.0313040055334568\n",
            "step: 310, loss: 0.08394245058298111\n",
            "step: 320, loss: 0.011238766834139824\n",
            "step: 330, loss: 0.0036923668812960386\n",
            "step: 340, loss: 0.005118487402796745\n",
            "step: 350, loss: 0.0008642902248539031\n",
            "step: 360, loss: 0.004011403303593397\n",
            "step: 370, loss: 0.000465579389128834\n",
            "step: 380, loss: 0.00022423797054216266\n",
            "step: 390, loss: 0.00038616755045950413\n",
            "step: 400, loss: 0.08269092440605164\n",
            "step: 410, loss: 0.017430784180760384\n",
            "step: 420, loss: 0.22986245155334473\n",
            "step: 430, loss: 0.023962536826729774\n",
            "step: 440, loss: 0.0009949398227036\n",
            "step: 450, loss: 0.01313697174191475\n",
            "step: 460, loss: 0.009588822722434998\n",
            "step: 470, loss: 0.13237397372722626\n",
            "step: 480, loss: 0.03584165871143341\n",
            "step: 490, loss: 0.03109557367861271\n",
            "step: 500, loss: 0.0063224975019693375\n",
            "step: 510, loss: 0.001752092270180583\n",
            "step: 520, loss: 0.028596477583050728\n",
            "step: 530, loss: 0.056013643741607666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9441860465116279, f1=0.9459459459459459, best_f1=0.9459459459459459\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.027730001136660576\n",
            "step: 10, loss: 0.0010029241675511003\n",
            "step: 20, loss: 0.0020240293815732002\n",
            "step: 30, loss: 0.0022503535728901625\n",
            "step: 40, loss: 0.0018492323579266667\n",
            "step: 50, loss: 0.0002623196633066982\n",
            "step: 60, loss: 0.000926628359593451\n",
            "step: 70, loss: 0.00017194345127791166\n",
            "step: 80, loss: 0.00035989086609333754\n",
            "step: 90, loss: 0.020938169211149216\n",
            "step: 100, loss: 0.005359154660254717\n",
            "step: 110, loss: 0.0020932441111654043\n",
            "step: 120, loss: 0.006843896582722664\n",
            "step: 130, loss: 0.02937605232000351\n",
            "step: 140, loss: 0.0015153357526287436\n",
            "step: 150, loss: 0.0006472717504948378\n",
            "step: 160, loss: 0.03017158806324005\n",
            "step: 170, loss: 0.0009434616658836603\n",
            "step: 180, loss: 0.0011555773671716452\n",
            "step: 190, loss: 0.14617039263248444\n",
            "step: 200, loss: 0.019945688545703888\n",
            "step: 210, loss: 0.004170006141066551\n",
            "step: 220, loss: 0.003073420375585556\n",
            "step: 230, loss: 0.020184693858027458\n",
            "step: 240, loss: 0.0031038899905979633\n",
            "step: 250, loss: 0.033016253262758255\n",
            "step: 260, loss: 0.0008701165788806975\n",
            "step: 270, loss: 0.0012432244839146733\n",
            "step: 280, loss: 0.08259213715791702\n",
            "step: 290, loss: 0.0015887421322986484\n",
            "step: 300, loss: 0.002627661218866706\n",
            "step: 310, loss: 0.001544304541312158\n",
            "step: 320, loss: 0.0009005212923511863\n",
            "step: 330, loss: 0.00020416968618519604\n",
            "step: 340, loss: 0.00042720482451841235\n",
            "step: 350, loss: 0.009860367514193058\n",
            "step: 360, loss: 0.009875779040157795\n",
            "step: 370, loss: 0.0054396395571529865\n",
            "step: 380, loss: 0.003170388052240014\n",
            "step: 390, loss: 0.0017430087318643928\n",
            "step: 400, loss: 0.0008122275467030704\n",
            "step: 410, loss: 0.0013594237389042974\n",
            "step: 420, loss: 0.0017618769779801369\n",
            "step: 430, loss: 0.0383133664727211\n",
            "step: 440, loss: 0.00476841302588582\n",
            "step: 450, loss: 0.151334747672081\n",
            "step: 460, loss: 0.0026172969955950975\n",
            "step: 470, loss: 0.001639084191992879\n",
            "step: 480, loss: 0.007662672083824873\n",
            "step: 490, loss: 0.0027061845175921917\n",
            "step: 500, loss: 0.0028598057106137276\n",
            "step: 510, loss: 0.20600421726703644\n",
            "step: 520, loss: 0.0040522669441998005\n",
            "step: 530, loss: 0.0046122511848807335\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9432029795158287, f1=0.940795559666975, best_f1=0.9459459459459459\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0051259128376841545\n",
            "step: 10, loss: 0.002780412556603551\n",
            "step: 20, loss: 0.007880650460720062\n",
            "step: 30, loss: 0.0036861563567072153\n",
            "step: 40, loss: 0.0022542388178408146\n",
            "step: 50, loss: 0.004514184780418873\n",
            "step: 60, loss: 0.03051207959651947\n",
            "step: 70, loss: 0.0013830700190737844\n",
            "step: 80, loss: 0.0008888870943337679\n",
            "step: 90, loss: 0.00038524382398463786\n",
            "step: 100, loss: 0.0006251009763218462\n",
            "step: 110, loss: 0.0002523771545384079\n",
            "step: 120, loss: 0.0004433807043824345\n",
            "step: 130, loss: 0.00012028997298330069\n",
            "step: 140, loss: 0.0005535971140488982\n",
            "step: 150, loss: 0.010228010825812817\n",
            "step: 160, loss: 0.0059777782298624516\n",
            "step: 170, loss: 0.0033657292369753122\n",
            "step: 180, loss: 0.09715139120817184\n",
            "step: 190, loss: 0.0017219781875610352\n",
            "step: 200, loss: 0.00038920415681786835\n",
            "step: 210, loss: 0.002092239446938038\n",
            "step: 220, loss: 0.000694401329383254\n",
            "step: 230, loss: 0.0012954944977536798\n",
            "step: 240, loss: 0.043064992874860764\n",
            "step: 250, loss: 0.04129005968570709\n",
            "step: 260, loss: 0.0023542619310319424\n",
            "step: 270, loss: 0.0005683128256350756\n",
            "step: 280, loss: 0.004024135880172253\n",
            "step: 290, loss: 0.002661231206730008\n",
            "step: 300, loss: 0.005510171875357628\n",
            "step: 310, loss: 0.0007091195438988507\n",
            "step: 320, loss: 0.0018499352736398578\n",
            "step: 330, loss: 0.009631969965994358\n",
            "step: 340, loss: 0.001002300065010786\n",
            "step: 350, loss: 0.010273615829646587\n",
            "step: 360, loss: 0.08686962723731995\n",
            "step: 370, loss: 0.09962131828069687\n",
            "step: 380, loss: 0.02392657846212387\n",
            "step: 390, loss: 0.002891409443691373\n",
            "step: 400, loss: 0.007562188897281885\n",
            "step: 410, loss: 0.0003456571139395237\n",
            "step: 420, loss: 0.026990283280611038\n",
            "step: 430, loss: 0.011140036396682262\n",
            "step: 440, loss: 0.0063084447756409645\n",
            "step: 450, loss: 0.0021313356701284647\n",
            "step: 460, loss: 0.0001628733880352229\n",
            "step: 470, loss: 0.10077625513076782\n",
            "step: 480, loss: 0.011078369803726673\n",
            "step: 490, loss: 0.0010809262748807669\n",
            "step: 500, loss: 0.0006910167867317796\n",
            "step: 510, loss: 0.0001496447075624019\n",
            "step: 520, loss: 0.09142327308654785\n",
            "step: 530, loss: 0.0006702787359245121\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9490740740740741, f1=0.9365446966188051, best_f1=0.9365446966188051\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000932642724364996\n",
            "step: 10, loss: 0.014652889221906662\n",
            "step: 20, loss: 0.021390248090028763\n",
            "step: 30, loss: 0.0009695531334728003\n",
            "step: 40, loss: 0.0004427703970577568\n",
            "step: 50, loss: 0.0010849999962374568\n",
            "step: 60, loss: 0.0005626128404401243\n",
            "step: 70, loss: 0.0006735817296430469\n",
            "step: 80, loss: 0.0004348883521743119\n",
            "step: 90, loss: 0.0035218987613916397\n",
            "step: 100, loss: 0.0002583850873634219\n",
            "step: 110, loss: 0.00014288508100435138\n",
            "step: 120, loss: 0.0002589219366200268\n",
            "step: 130, loss: 0.0002381915837759152\n",
            "step: 140, loss: 0.00015853032527957112\n",
            "step: 150, loss: 0.0008261111797764897\n",
            "step: 160, loss: 0.00011663122131722048\n",
            "step: 170, loss: 0.0021721196826547384\n",
            "step: 180, loss: 0.0007750015356577933\n",
            "step: 190, loss: 0.001663114526309073\n",
            "step: 200, loss: 0.007908567786216736\n",
            "step: 210, loss: 0.1423148810863495\n",
            "step: 220, loss: 0.0020215471740812063\n",
            "step: 230, loss: 0.011040530167520046\n",
            "step: 240, loss: 0.0031894019339233637\n",
            "step: 250, loss: 0.0021885009482502937\n",
            "step: 260, loss: 0.0009186223614960909\n",
            "step: 270, loss: 0.001152166398242116\n",
            "step: 280, loss: 0.0013692151987925172\n",
            "step: 290, loss: 0.0003481886233203113\n",
            "step: 300, loss: 0.00010423688945593312\n",
            "step: 310, loss: 0.00022196384088601917\n",
            "step: 320, loss: 0.00029474368784576654\n",
            "step: 330, loss: 0.0582464300096035\n",
            "step: 340, loss: 0.09289698302745819\n",
            "step: 350, loss: 0.00469879200682044\n",
            "step: 360, loss: 0.004297854844480753\n",
            "step: 370, loss: 0.007460962515324354\n",
            "step: 380, loss: 0.00047640452976338565\n",
            "step: 390, loss: 0.0024372972548007965\n",
            "step: 400, loss: 0.0014916092623025179\n",
            "step: 410, loss: 0.0009466689662076533\n",
            "step: 420, loss: 0.0004290770157240331\n",
            "step: 430, loss: 0.006396720185875893\n",
            "step: 440, loss: 0.006073642522096634\n",
            "step: 450, loss: 0.001780266291461885\n",
            "step: 460, loss: 0.00101346499286592\n",
            "step: 470, loss: 0.24781650304794312\n",
            "step: 480, loss: 0.007225663401186466\n",
            "step: 490, loss: 0.0052864584140479565\n",
            "step: 500, loss: 0.07084193080663681\n",
            "step: 510, loss: 0.0011537447571754456\n",
            "step: 520, loss: 0.0005910302279517055\n",
            "step: 530, loss: 0.01038189698010683\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9490268767377201, f1=0.9460083064143978, best_f1=0.9365446966188051\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0013654688373208046\n",
            "step: 10, loss: 0.0042361728847026825\n",
            "step: 20, loss: 0.00032978522358462214\n",
            "step: 30, loss: 0.09450706094503403\n",
            "step: 40, loss: 0.001988018164411187\n",
            "step: 50, loss: 0.0014558570692315698\n",
            "step: 60, loss: 0.005543766543269157\n",
            "step: 70, loss: 0.00649621756747365\n",
            "step: 80, loss: 0.0048213740810751915\n",
            "step: 90, loss: 0.02882956527173519\n",
            "step: 100, loss: 0.0007187425508163869\n",
            "step: 110, loss: 0.0075227306224405766\n",
            "step: 120, loss: 0.0005896607181057334\n",
            "step: 130, loss: 0.00035219627898186445\n",
            "step: 140, loss: 0.000331234245095402\n",
            "step: 150, loss: 0.0006696748896501958\n",
            "step: 160, loss: 0.0004739435389637947\n",
            "step: 170, loss: 0.008558874949812889\n",
            "step: 180, loss: 0.00014940112305339426\n",
            "step: 190, loss: 0.0004516390326898545\n",
            "step: 200, loss: 0.00015169694961514324\n",
            "step: 210, loss: 0.0007835079450160265\n",
            "step: 220, loss: 0.0005790751310996711\n",
            "step: 230, loss: 0.0006691170856356621\n",
            "step: 240, loss: 0.0012815133668482304\n",
            "step: 250, loss: 0.03988127410411835\n",
            "step: 260, loss: 0.0013586211716756225\n",
            "step: 270, loss: 0.0031863730400800705\n",
            "step: 280, loss: 0.032487835735082626\n",
            "step: 290, loss: 0.0002299973857589066\n",
            "step: 300, loss: 0.00215065386146307\n",
            "step: 310, loss: 0.006178102921694517\n",
            "step: 320, loss: 0.0005901475669816136\n",
            "step: 330, loss: 0.0017599075799807906\n",
            "step: 340, loss: 0.005171274766325951\n",
            "step: 350, loss: 0.06132163852453232\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 360, loss: 0.003724693786352873\n",
            "step: 370, loss: 0.0020674136467278004\n",
            "step: 380, loss: 0.0005881093093194067\n",
            "step: 390, loss: 0.00782088190317154\n",
            "step: 400, loss: 0.0004850148980040103\n",
            "step: 410, loss: 0.000537665793672204\n",
            "step: 420, loss: 0.0005395965999923646\n",
            "step: 430, loss: 0.0005092878709547222\n",
            "step: 440, loss: 9.102689364226535e-05\n",
            "step: 450, loss: 0.003227694658562541\n",
            "step: 460, loss: 0.0032601249404251575\n",
            "step: 470, loss: 0.0006130889523774385\n",
            "step: 480, loss: 0.0006655485485680401\n",
            "step: 490, loss: 0.00088942312868312\n",
            "step: 500, loss: 0.0018806083826348186\n",
            "step: 510, loss: 0.0035992173943668604\n",
            "step: 520, loss: 0.010503503493964672\n",
            "step: 530, loss: 0.0032467786222696304\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9461966604823748, f1=0.9439555349698935, best_f1=0.9365446966188051\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008057790109887719\n",
            "step: 10, loss: 0.0005867477739229798\n",
            "step: 20, loss: 0.015679210424423218\n",
            "step: 30, loss: 0.0013476255116984248\n",
            "step: 40, loss: 0.0034700389951467514\n",
            "step: 50, loss: 0.0011726301163434982\n",
            "step: 60, loss: 0.00025851192185655236\n",
            "step: 70, loss: 0.000540428445674479\n",
            "step: 80, loss: 0.0005714800790883601\n",
            "step: 90, loss: 0.0005844431580044329\n",
            "step: 100, loss: 0.0013156337663531303\n",
            "step: 110, loss: 0.014932222664356232\n",
            "step: 120, loss: 0.00015237851766869426\n",
            "step: 130, loss: 0.011869003996253014\n",
            "step: 140, loss: 0.0002952419454231858\n",
            "step: 150, loss: 0.00048708554822951555\n",
            "step: 160, loss: 0.08077961206436157\n",
            "step: 170, loss: 0.006189426872879267\n",
            "step: 180, loss: 0.0007740459404885769\n",
            "step: 190, loss: 0.0001334466942353174\n",
            "step: 200, loss: 0.0002972946094814688\n",
            "step: 210, loss: 0.009283735416829586\n",
            "step: 220, loss: 0.0011981434654444456\n",
            "step: 230, loss: 0.00038043843233026564\n",
            "step: 240, loss: 0.000693033856805414\n",
            "step: 250, loss: 0.0002524142328184098\n",
            "step: 260, loss: 0.0008327924879267812\n",
            "step: 270, loss: 0.0004448821709956974\n",
            "step: 280, loss: 0.016293341293931007\n",
            "step: 290, loss: 0.0035308152437210083\n",
            "step: 300, loss: 0.011249348521232605\n",
            "step: 310, loss: 0.0285439845174551\n",
            "step: 320, loss: 0.0023083891719579697\n",
            "step: 330, loss: 0.0014003815595060587\n",
            "step: 340, loss: 6.0025024140486494e-05\n",
            "step: 350, loss: 0.0004916936159133911\n",
            "step: 360, loss: 7.383363845292479e-05\n",
            "step: 370, loss: 0.0006360887200571597\n",
            "step: 380, loss: 0.00026735535357147455\n",
            "step: 390, loss: 0.00014494518109131604\n",
            "step: 400, loss: 0.0002489531470928341\n",
            "step: 410, loss: 0.00025896611623466015\n",
            "step: 420, loss: 5.4185729823075235e-05\n",
            "step: 430, loss: 0.0011101111304014921\n",
            "step: 440, loss: 0.0012645383831113577\n",
            "step: 450, loss: 0.0005774840246886015\n",
            "step: 460, loss: 0.00021810819453094155\n",
            "step: 470, loss: 0.0007116320775821805\n",
            "step: 480, loss: 0.00013383050099946558\n",
            "step: 490, loss: 0.03424801677465439\n",
            "step: 500, loss: 0.000845833623316139\n",
            "step: 510, loss: 0.0003200212959200144\n",
            "step: 520, loss: 0.007259285543113947\n",
            "step: 530, loss: 0.001687456271611154\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9559099437148217, f1=0.9451476793248945, best_f1=0.9451476793248945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002526551252231002\n",
            "step: 10, loss: 0.0003889760992024094\n",
            "step: 20, loss: 0.00023121933918446302\n",
            "step: 30, loss: 0.00022327392071019858\n",
            "step: 40, loss: 0.0001136160281021148\n",
            "step: 50, loss: 9.494777623331174e-05\n",
            "step: 60, loss: 7.754279795335606e-05\n",
            "step: 70, loss: 0.00799601711332798\n",
            "step: 80, loss: 0.0001141631364589557\n",
            "step: 90, loss: 0.0004133519541937858\n",
            "step: 100, loss: 0.0006734919152222574\n",
            "step: 110, loss: 0.0004093205789104104\n",
            "step: 120, loss: 0.00020717098959721625\n",
            "step: 130, loss: 0.0001841755147324875\n",
            "step: 140, loss: 7.021163037279621e-05\n",
            "step: 150, loss: 0.004263550043106079\n",
            "step: 160, loss: 0.04631773754954338\n",
            "step: 170, loss: 0.0002320078492630273\n",
            "step: 180, loss: 0.0007078485796228051\n",
            "step: 190, loss: 0.012806098908185959\n",
            "step: 200, loss: 0.00015793894999660552\n",
            "step: 210, loss: 0.00022495724260807037\n",
            "step: 220, loss: 0.01759902946650982\n",
            "step: 230, loss: 7.609061140101403e-05\n",
            "step: 240, loss: 0.0034493200946599245\n",
            "step: 250, loss: 0.00031327197211794555\n",
            "step: 260, loss: 0.000759469170589\n",
            "step: 270, loss: 0.0030973409302532673\n",
            "step: 280, loss: 0.001569350017234683\n",
            "step: 290, loss: 0.004438537172973156\n",
            "step: 300, loss: 0.00020780088379979134\n",
            "step: 310, loss: 0.010525580495595932\n",
            "step: 320, loss: 0.0003038956201635301\n",
            "step: 330, loss: 0.0003938609443139285\n",
            "step: 340, loss: 0.0007320090662688017\n",
            "step: 350, loss: 0.011606894433498383\n",
            "step: 360, loss: 0.0033932740334421396\n",
            "step: 370, loss: 0.0016285997116938233\n",
            "step: 380, loss: 0.002255247673019767\n",
            "step: 390, loss: 0.00031730299815535545\n",
            "step: 400, loss: 0.0038606866728514433\n",
            "step: 410, loss: 0.0015621044440194964\n",
            "step: 420, loss: 0.0006005085306242108\n",
            "step: 430, loss: 0.0022311778739094734\n",
            "step: 440, loss: 0.00030986653291620314\n",
            "step: 450, loss: 0.0005935419467277825\n",
            "step: 460, loss: 0.00040541135240346193\n",
            "step: 470, loss: 0.0007750296499580145\n",
            "step: 480, loss: 0.0006148662650957704\n",
            "step: 490, loss: 0.001951193786226213\n",
            "step: 500, loss: 0.0009467832860536873\n",
            "step: 510, loss: 0.012675965204834938\n",
            "step: 520, loss: 0.0028960651252418756\n",
            "step: 530, loss: 0.006072956137359142\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9524253731343284, f1=0.9361305361305362, best_f1=0.9451476793248945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00046989330439828336\n",
            "step: 10, loss: 0.00025482228375039995\n",
            "step: 20, loss: 0.0004381704202387482\n",
            "step: 30, loss: 6.06147623329889e-05\n",
            "step: 40, loss: 0.00011781398643506691\n",
            "step: 50, loss: 0.0004548810247797519\n",
            "step: 60, loss: 9.196218161378056e-05\n",
            "step: 70, loss: 0.001957214204594493\n",
            "step: 80, loss: 0.0005391589365899563\n",
            "step: 90, loss: 0.00030514979152940214\n",
            "step: 100, loss: 0.19653815031051636\n",
            "step: 110, loss: 0.0016945756506174803\n",
            "step: 120, loss: 0.0034504348877817392\n",
            "step: 130, loss: 0.0006271819001995027\n",
            "step: 140, loss: 0.0003280468808952719\n",
            "step: 150, loss: 7.942577940411866e-05\n",
            "step: 160, loss: 0.0007953417371027172\n",
            "step: 170, loss: 0.0001354240084765479\n",
            "step: 180, loss: 0.00012133840937167406\n",
            "step: 190, loss: 0.00014862576790619642\n",
            "step: 200, loss: 0.004273626487702131\n",
            "step: 210, loss: 0.00038500470691360533\n",
            "step: 220, loss: 0.00020302488701418042\n",
            "step: 230, loss: 0.00010490958084119484\n",
            "step: 240, loss: 0.0009187753894366324\n",
            "step: 250, loss: 7.427745003951713e-05\n",
            "step: 260, loss: 0.0001789149537216872\n",
            "step: 270, loss: 0.0007522394298575819\n",
            "step: 280, loss: 0.00015133156557567418\n",
            "step: 290, loss: 0.00010093392484122887\n",
            "step: 300, loss: 0.00021585194917861372\n",
            "step: 310, loss: 0.007106791716068983\n",
            "step: 320, loss: 0.0001828910899348557\n",
            "step: 330, loss: 0.0007987716817297041\n",
            "step: 340, loss: 0.001271153916604817\n",
            "step: 350, loss: 0.00013121233496349305\n",
            "step: 360, loss: 0.00011225731577724218\n",
            "step: 370, loss: 0.00011145209282403812\n",
            "step: 380, loss: 0.00012458219134714454\n",
            "step: 390, loss: 0.0014147255569696426\n",
            "step: 400, loss: 0.00010249249316984788\n",
            "step: 410, loss: 0.00043131865095347166\n",
            "step: 420, loss: 0.00025884638307616115\n",
            "step: 430, loss: 0.0008074079523794353\n",
            "step: 440, loss: 0.005629927385598421\n",
            "step: 450, loss: 0.0004341899766586721\n",
            "step: 460, loss: 0.000223902243305929\n",
            "step: 470, loss: 0.004696945194154978\n",
            "step: 480, loss: 0.007911836728453636\n",
            "step: 490, loss: 0.0002817553759086877\n",
            "step: 500, loss: 4.760422234539874e-05\n",
            "step: 510, loss: 0.0012824343284592032\n",
            "step: 520, loss: 0.0010365542257204652\n",
            "step: 530, loss: 8.395557961193845e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9544408651633686, f1=0.944649446494465, best_f1=0.9451476793248945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 4.807809091289528e-05\n",
            "step: 10, loss: 0.00013843612396158278\n",
            "step: 20, loss: 5.214686825638637e-05\n",
            "step: 30, loss: 3.5600245610112324e-05\n",
            "step: 40, loss: 0.00012020234862575307\n",
            "step: 50, loss: 0.0007354166591539979\n",
            "step: 60, loss: 0.00012188324035378173\n",
            "step: 70, loss: 0.00015662013902328908\n",
            "step: 80, loss: 0.00015086514758877456\n",
            "step: 90, loss: 0.00012435272219590843\n",
            "step: 100, loss: 9.563824278302491e-05\n",
            "step: 110, loss: 8.157412958098575e-05\n",
            "step: 120, loss: 8.669335511513054e-05\n",
            "step: 130, loss: 0.00013254574150778353\n",
            "step: 140, loss: 9.143104398390278e-05\n",
            "step: 150, loss: 0.004373867996037006\n",
            "step: 160, loss: 0.0004996961215510964\n",
            "step: 170, loss: 0.0015142099000513554\n",
            "step: 180, loss: 8.26673349365592e-05\n",
            "step: 190, loss: 0.00023607671028003097\n",
            "step: 200, loss: 0.00019094179151579738\n",
            "step: 210, loss: 0.00021188758546486497\n",
            "step: 220, loss: 6.0860824305564165e-05\n",
            "step: 230, loss: 0.001048530451953411\n",
            "step: 240, loss: 0.0023516230285167694\n",
            "step: 250, loss: 0.00015002096188254654\n",
            "step: 260, loss: 0.021919531747698784\n",
            "step: 270, loss: 0.00022942826035432518\n",
            "step: 280, loss: 7.54507418605499e-05\n",
            "step: 290, loss: 7.90016638347879e-05\n",
            "step: 300, loss: 0.0354212187230587\n",
            "step: 310, loss: 0.00011167916818521917\n",
            "step: 320, loss: 0.00010242947610095143\n",
            "step: 330, loss: 0.0007350342930294573\n",
            "step: 340, loss: 9.28804074646905e-05\n",
            "step: 350, loss: 0.0007301230798475444\n",
            "step: 360, loss: 0.07772528380155563\n",
            "step: 370, loss: 0.026941582560539246\n",
            "step: 380, loss: 0.00038404547376558185\n",
            "step: 390, loss: 0.00014420058869291097\n",
            "step: 400, loss: 0.00014804632519371808\n",
            "step: 410, loss: 0.00021330261370167136\n",
            "step: 420, loss: 0.00010690921772038564\n",
            "step: 430, loss: 0.0009671669686213136\n",
            "step: 440, loss: 5.823792889714241e-05\n",
            "step: 450, loss: 0.00011336689203744754\n",
            "step: 460, loss: 0.0005856252973899245\n",
            "step: 470, loss: 0.0028226415161043406\n",
            "step: 480, loss: 3.263584949309006e-05\n",
            "step: 490, loss: 0.0001699943677522242\n",
            "step: 500, loss: 0.0003416196268517524\n",
            "step: 510, loss: 0.00039307516999542713\n",
            "step: 520, loss: 6.970463437028229e-05\n",
            "step: 530, loss: 0.00018117961008101702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9533146591970122, f1=0.9463369108726085, best_f1=0.9451476793248945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00024880070122890174\n",
            "step: 10, loss: 0.00019306466856505722\n",
            "step: 20, loss: 3.9476173697039485e-05\n",
            "step: 30, loss: 0.002406616695225239\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 40, loss: 6.222397496458143e-05\n",
            "step: 50, loss: 0.005626068450510502\n",
            "step: 60, loss: 5.537055403692648e-05\n",
            "step: 70, loss: 5.391345621319488e-05\n",
            "step: 80, loss: 0.00017224899784196168\n",
            "step: 90, loss: 4.537480344879441e-05\n",
            "step: 100, loss: 8.576217805966735e-05\n",
            "step: 110, loss: 4.7146448196144775e-05\n",
            "step: 120, loss: 4.0208262362284586e-05\n",
            "step: 130, loss: 5.063408752903342e-05\n",
            "step: 140, loss: 0.00031048653181642294\n",
            "step: 150, loss: 0.0003833411610685289\n",
            "step: 160, loss: 5.021879405830987e-05\n",
            "step: 170, loss: 0.00012244886602275074\n",
            "step: 180, loss: 7.994702900759876e-05\n",
            "step: 190, loss: 0.0003200224309694022\n",
            "step: 200, loss: 0.00012917126878164709\n",
            "step: 210, loss: 7.50578474253416e-05\n",
            "step: 220, loss: 3.635916073108092e-05\n",
            "step: 230, loss: 4.921030267723836e-05\n",
            "step: 240, loss: 4.062103835167363e-05\n",
            "step: 250, loss: 0.00038533174665644765\n",
            "step: 260, loss: 4.887238537776284e-05\n",
            "step: 270, loss: 0.0006442677695304155\n",
            "step: 280, loss: 0.00010783699690364301\n",
            "step: 290, loss: 3.528840534272604e-05\n",
            "step: 300, loss: 0.00013871790724806488\n",
            "step: 310, loss: 0.0003323113196529448\n",
            "step: 320, loss: 8.817019988782704e-05\n",
            "step: 330, loss: 0.0033119446597993374\n",
            "step: 340, loss: 7.194100908236578e-05\n",
            "step: 350, loss: 2.7204290745430626e-05\n",
            "step: 360, loss: 4.2393749026814476e-05\n",
            "step: 370, loss: 0.0005207439535297453\n",
            "step: 380, loss: 0.0018338047666475177\n",
            "step: 390, loss: 0.004203092772513628\n",
            "step: 400, loss: 0.0026678605936467648\n",
            "step: 410, loss: 2.3306258299271576e-05\n",
            "step: 420, loss: 0.00010771519009722397\n",
            "step: 430, loss: 0.0007240719860419631\n",
            "step: 440, loss: 0.00028009136440232396\n",
            "step: 450, loss: 3.682517490233295e-05\n",
            "step: 460, loss: 0.00043287183507345617\n",
            "step: 470, loss: 4.665169399231672e-05\n",
            "step: 480, loss: 5.087145109428093e-05\n",
            "step: 490, loss: 0.002382913837209344\n",
            "step: 500, loss: 0.0003030076331924647\n",
            "step: 510, loss: 0.00030936990515328944\n",
            "step: 520, loss: 4.182835255051032e-05\n",
            "step: 530, loss: 0.00013785381452180445\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9541029207232267, f1=0.9476124246638852, best_f1=0.9451476793248945\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.323858830612153e-05\n",
            "step: 10, loss: 7.091977749951184e-05\n",
            "step: 20, loss: 0.0007313434034585953\n",
            "step: 30, loss: 0.007978050038218498\n",
            "step: 40, loss: 3.855051181744784e-05\n",
            "step: 50, loss: 0.004538755398243666\n",
            "step: 60, loss: 0.0012263285461813211\n",
            "step: 70, loss: 4.762950266012922e-05\n",
            "step: 80, loss: 5.223086918704212e-05\n",
            "step: 90, loss: 5.3880597988609225e-05\n",
            "step: 100, loss: 3.982616544817574e-05\n",
            "step: 110, loss: 0.00024237690377049148\n",
            "step: 120, loss: 8.682003681315109e-05\n",
            "step: 130, loss: 5.5739361414453015e-05\n",
            "step: 140, loss: 5.055499786976725e-05\n",
            "step: 150, loss: 6.221527291927487e-05\n",
            "step: 160, loss: 0.00013637995289172977\n",
            "step: 170, loss: 1.9720499039976858e-05\n",
            "step: 180, loss: 0.00033333527971990407\n",
            "step: 190, loss: 0.004386389162391424\n",
            "step: 200, loss: 0.0006553312414325774\n",
            "step: 210, loss: 0.00023238245921675116\n",
            "step: 220, loss: 4.6574485168093815e-05\n",
            "step: 230, loss: 4.6905588533263654e-05\n",
            "step: 240, loss: 6.799285620218143e-05\n",
            "step: 250, loss: 2.2573580281459726e-05\n",
            "step: 260, loss: 2.654081617947668e-05\n",
            "step: 270, loss: 0.0008523553842678666\n",
            "step: 280, loss: 7.203916902653873e-05\n",
            "step: 290, loss: 6.319824751699343e-05\n",
            "step: 300, loss: 2.3486754798796028e-05\n",
            "step: 310, loss: 0.04995815083384514\n",
            "step: 320, loss: 4.727295890916139e-05\n",
            "step: 330, loss: 0.00010463776561664417\n",
            "step: 340, loss: 5.317356044542976e-05\n",
            "step: 350, loss: 0.001258639502339065\n",
            "step: 360, loss: 4.145497950958088e-05\n",
            "step: 370, loss: 8.33231897559017e-05\n",
            "step: 380, loss: 5.0286722398595884e-05\n",
            "step: 390, loss: 0.0004725692269857973\n",
            "step: 400, loss: 0.0022329941857606173\n",
            "step: 410, loss: 0.00016578730719629675\n",
            "step: 420, loss: 2.8250611649127677e-05\n",
            "step: 430, loss: 1.676653300819453e-05\n",
            "step: 440, loss: 6.444154132623225e-05\n",
            "step: 450, loss: 0.0014136980753391981\n",
            "step: 460, loss: 0.0002189625083701685\n",
            "step: 470, loss: 3.066466160817072e-05\n",
            "step: 480, loss: 0.0022803654428571463\n",
            "step: 490, loss: 4.1245311876991764e-05\n",
            "step: 500, loss: 0.00042567020864225924\n",
            "step: 510, loss: 3.882153396261856e-05\n",
            "step: 520, loss: 7.593755435664207e-05\n",
            "step: 530, loss: 2.499830225133337e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9547785547785548, f1=0.9471221338324755, best_f1=0.9451476793248945\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:22, 251.77it/s]\n",
            "load_f1 = 0.9544186046511628\n",
            "real_f1 = 0.951366373320982\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 208.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "Zbv_H8sHgw8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "oqkZ1fXggw8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5ecfea-07c1-4c7b-e55a-02cf397e5e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5256560444831848\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5046546459197998\n",
            "step: 20, loss: 0.41702619194984436\n",
            "step: 30, loss: 0.41729527711868286\n",
            "step: 40, loss: 0.32436081767082214\n",
            "step: 50, loss: 0.4694766402244568\n",
            "step: 60, loss: 0.44174709916114807\n",
            "step: 70, loss: 0.3558689057826996\n",
            "step: 80, loss: 0.387315034866333\n",
            "step: 90, loss: 0.26610028743743896\n",
            "step: 100, loss: 0.22170816361904144\n",
            "step: 110, loss: 0.2703292965888977\n",
            "step: 120, loss: 0.49102187156677246\n",
            "step: 130, loss: 0.2604563534259796\n",
            "step: 140, loss: 0.4658019244670868\n",
            "step: 150, loss: 0.3830311596393585\n",
            "step: 160, loss: 0.5066589713096619\n",
            "step: 170, loss: 0.24289780855178833\n",
            "step: 180, loss: 0.38947343826293945\n",
            "step: 190, loss: 0.6326875686645508\n",
            "step: 200, loss: 0.4032491445541382\n",
            "step: 210, loss: 0.5167878270149231\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3795267939567566\n",
            "step: 10, loss: 0.20477545261383057\n",
            "step: 20, loss: 0.4814799726009369\n",
            "step: 30, loss: 0.47475650906562805\n",
            "step: 40, loss: 0.43443700671195984\n",
            "step: 50, loss: 0.23990701138973236\n",
            "step: 60, loss: 0.30496153235435486\n",
            "step: 70, loss: 0.4190770089626312\n",
            "step: 80, loss: 0.4001544117927551\n",
            "step: 90, loss: 0.45558497309684753\n",
            "step: 100, loss: 0.49704673886299133\n",
            "step: 110, loss: 0.38014549016952515\n",
            "step: 120, loss: 0.25123053789138794\n",
            "step: 130, loss: 0.16730180382728577\n",
            "step: 140, loss: 0.24157042801380157\n",
            "step: 150, loss: 0.44513070583343506\n",
            "step: 160, loss: 0.15692442655563354\n",
            "step: 170, loss: 0.6010239720344543\n",
            "step: 180, loss: 0.3245784044265747\n",
            "step: 190, loss: 0.30898019671440125\n",
            "step: 200, loss: 0.15316303074359894\n",
            "step: 210, loss: 0.31064727902412415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25453004240989685\n",
            "step: 10, loss: 0.23234710097312927\n",
            "step: 20, loss: 0.4750593304634094\n",
            "step: 30, loss: 0.25556230545043945\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 40, loss: 0.46512728929519653\n",
            "step: 50, loss: 0.505531907081604\n",
            "step: 60, loss: 0.5045008659362793\n",
            "step: 70, loss: 0.18389621376991272\n",
            "step: 80, loss: 0.477409303188324\n",
            "step: 90, loss: 0.25102514028549194\n",
            "step: 100, loss: 0.3845066428184509\n",
            "step: 110, loss: 0.2589631974697113\n",
            "step: 120, loss: 0.2504101097583771\n",
            "step: 130, loss: 0.15915681421756744\n",
            "step: 140, loss: 0.36631473898887634\n",
            "step: 150, loss: 0.3260229825973511\n",
            "step: 160, loss: 0.2048550248146057\n",
            "step: 170, loss: 0.3828948140144348\n",
            "step: 180, loss: 0.24429501593112946\n",
            "step: 190, loss: 0.166031613945961\n",
            "step: 200, loss: 0.23814575374126434\n",
            "step: 210, loss: 0.269571453332901\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3219221830368042\n",
            "step: 10, loss: 0.3103790283203125\n",
            "step: 20, loss: 0.32134369015693665\n",
            "step: 30, loss: 0.2388373762369156\n",
            "step: 40, loss: 0.23847505450248718\n",
            "step: 50, loss: 0.241471529006958\n",
            "step: 60, loss: 0.493104487657547\n",
            "step: 70, loss: 0.2515726387500763\n",
            "step: 80, loss: 0.24514512717723846\n",
            "step: 90, loss: 0.47127532958984375\n",
            "step: 100, loss: 0.37451601028442383\n",
            "step: 110, loss: 0.5925753116607666\n",
            "step: 120, loss: 0.35345786809921265\n",
            "step: 130, loss: 0.6620739102363586\n",
            "step: 140, loss: 0.5068488717079163\n",
            "step: 150, loss: 0.388830304145813\n",
            "step: 160, loss: 0.3318195044994354\n",
            "step: 170, loss: 0.17553310096263885\n",
            "step: 180, loss: 0.07519423961639404\n",
            "step: 190, loss: 0.16405481100082397\n",
            "step: 200, loss: 0.316340833902359\n",
            "step: 210, loss: 0.3979664444923401\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3838878870010376\n",
            "step: 10, loss: 0.3028911054134369\n",
            "step: 20, loss: 0.26162728667259216\n",
            "step: 30, loss: 0.210226908326149\n",
            "step: 40, loss: 0.4070267379283905\n",
            "step: 50, loss: 0.41737720370292664\n",
            "step: 60, loss: 0.3614498972892761\n",
            "step: 70, loss: 0.23278431594371796\n",
            "step: 80, loss: 0.3515598475933075\n",
            "step: 90, loss: 0.4606224000453949\n",
            "step: 100, loss: 0.23534250259399414\n",
            "step: 110, loss: 0.16430963575839996\n",
            "step: 120, loss: 0.24305568635463715\n",
            "step: 130, loss: 0.27656394243240356\n",
            "step: 140, loss: 0.5465062856674194\n",
            "step: 150, loss: 0.3270464837551117\n",
            "step: 160, loss: 0.2447238266468048\n",
            "step: 170, loss: 0.38959574699401855\n",
            "step: 180, loss: 0.24110472202301025\n",
            "step: 190, loss: 0.5077227354049683\n",
            "step: 200, loss: 0.3623637557029724\n",
            "step: 210, loss: 0.2357654869556427\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.18519984170953702, f1=0.18519984170953702, best_f1=0.18519984170953702\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15676714479923248\n",
            "step: 10, loss: 0.34317460656166077\n",
            "step: 20, loss: 0.3134663701057434\n",
            "step: 30, loss: 0.2398395836353302\n",
            "step: 40, loss: 0.2715475261211395\n",
            "step: 50, loss: 0.5287100672721863\n",
            "step: 60, loss: 0.2296798825263977\n",
            "step: 70, loss: 0.3795630633831024\n",
            "step: 80, loss: 0.2594280242919922\n",
            "step: 90, loss: 0.2961295247077942\n",
            "step: 100, loss: 0.3499734103679657\n",
            "step: 110, loss: 0.3208400309085846\n",
            "step: 120, loss: 0.2070361077785492\n",
            "step: 130, loss: 0.21928708255290985\n",
            "step: 140, loss: 0.37782102823257446\n",
            "step: 150, loss: 0.18218636512756348\n",
            "step: 160, loss: 0.19533614814281464\n",
            "step: 170, loss: 0.3731187880039215\n",
            "step: 180, loss: 0.4076756238937378\n",
            "step: 190, loss: 0.33035409450531006\n",
            "step: 200, loss: 0.27843624353408813\n",
            "step: 210, loss: 0.30935221910476685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.28797468354430383, f1=0.28892455858747995, best_f1=0.28892455858747995\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.43738850951194763\n",
            "step: 10, loss: 0.27474358677864075\n",
            "step: 20, loss: 0.3851282596588135\n",
            "step: 30, loss: 0.20236268639564514\n",
            "step: 40, loss: 0.23509439826011658\n",
            "step: 50, loss: 0.3198070228099823\n",
            "step: 60, loss: 0.2715081572532654\n",
            "step: 70, loss: 0.23963260650634766\n",
            "step: 80, loss: 0.42127588391304016\n",
            "step: 90, loss: 0.3719950318336487\n",
            "step: 100, loss: 0.39628615975379944\n",
            "step: 110, loss: 0.2812681496143341\n",
            "step: 120, loss: 0.2726941406726837\n",
            "step: 130, loss: 0.4120584726333618\n",
            "step: 140, loss: 0.3174207806587219\n",
            "step: 150, loss: 0.2946838438510895\n",
            "step: 160, loss: 0.5371550917625427\n",
            "step: 170, loss: 0.5601440072059631\n",
            "step: 180, loss: 0.20696474611759186\n",
            "step: 190, loss: 0.2968573272228241\n",
            "step: 200, loss: 0.29638367891311646\n",
            "step: 210, loss: 0.29332712292671204\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.3028169014084507, f1=0.2998236331569665, best_f1=0.2998236331569665\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5124892592430115\n",
            "step: 10, loss: 0.36760661005973816\n",
            "step: 20, loss: 0.19725148379802704\n",
            "step: 30, loss: 0.2140347957611084\n",
            "step: 40, loss: 0.24670933187007904\n",
            "step: 50, loss: 0.14087416231632233\n",
            "step: 60, loss: 0.18135428428649902\n",
            "step: 70, loss: 0.34444424510002136\n",
            "step: 80, loss: 0.26108816266059875\n",
            "step: 90, loss: 0.33581453561782837\n",
            "step: 100, loss: 0.5297002792358398\n",
            "step: 110, loss: 0.2961474359035492\n",
            "step: 120, loss: 0.30617567896842957\n",
            "step: 130, loss: 0.11538037657737732\n",
            "step: 140, loss: 0.3177794814109802\n",
            "step: 150, loss: 0.532988429069519\n",
            "step: 160, loss: 0.40076714754104614\n",
            "step: 170, loss: 0.4823712706565857\n",
            "step: 180, loss: 0.21328765153884888\n",
            "step: 190, loss: 0.1865435689687729\n",
            "step: 200, loss: 0.3850748538970947\n",
            "step: 210, loss: 0.34763476252555847\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.32232496697490093, f1=0.3289817232375979, best_f1=0.3289817232375979\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4705325961112976\n",
            "step: 10, loss: 0.19493147730827332\n",
            "step: 20, loss: 0.3726638853549957\n",
            "step: 30, loss: 0.13524852693080902\n",
            "step: 40, loss: 0.12701833248138428\n",
            "step: 50, loss: 0.4922756552696228\n",
            "step: 60, loss: 0.15215672552585602\n",
            "step: 70, loss: 0.3523675203323364\n",
            "step: 80, loss: 0.2666992247104645\n",
            "step: 90, loss: 0.3218724727630615\n",
            "step: 100, loss: 0.3832259774208069\n",
            "step: 110, loss: 0.4873022437095642\n",
            "step: 120, loss: 0.4201198220252991\n",
            "step: 130, loss: 0.20984207093715668\n",
            "step: 140, loss: 0.47306570410728455\n",
            "step: 150, loss: 0.11819647997617722\n",
            "step: 160, loss: 0.3191278576850891\n",
            "step: 170, loss: 0.27250632643699646\n",
            "step: 180, loss: 0.4602178931236267\n",
            "step: 190, loss: 0.2265533208847046\n",
            "step: 200, loss: 0.2479306310415268\n",
            "step: 210, loss: 0.2392304688692093\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.3401506996770721, f1=0.3307607497243661, best_f1=0.3307607497243661\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23555728793144226\n",
            "step: 10, loss: 0.44004198908805847\n",
            "step: 20, loss: 0.238517165184021\n",
            "step: 30, loss: 0.21025805175304413\n",
            "step: 40, loss: 0.3485206961631775\n",
            "step: 50, loss: 0.38766616582870483\n",
            "step: 60, loss: 0.15969495475292206\n",
            "step: 70, loss: 0.3868013322353363\n",
            "step: 80, loss: 0.13976719975471497\n",
            "step: 90, loss: 0.3735732138156891\n",
            "step: 100, loss: 0.41184669733047485\n",
            "step: 110, loss: 0.13727697730064392\n",
            "step: 120, loss: 0.34281864762306213\n",
            "step: 130, loss: 0.1684083789587021\n",
            "step: 140, loss: 0.3075522184371948\n",
            "step: 150, loss: 0.2079710215330124\n",
            "step: 160, loss: 0.2439766824245453\n",
            "step: 170, loss: 0.1823650449514389\n",
            "step: 180, loss: 0.28792330622673035\n",
            "step: 190, loss: 0.17614398896694183\n",
            "step: 200, loss: 0.5507678389549255\n",
            "step: 210, loss: 0.20344777405261993\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.3529411764705882, f1=0.3934426229508197, best_f1=0.3934426229508197\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.21694974601268768\n",
            "step: 10, loss: 0.3366050124168396\n",
            "step: 20, loss: 0.2545435428619385\n",
            "step: 30, loss: 0.18532642722129822\n",
            "step: 40, loss: 0.39878466725349426\n",
            "step: 50, loss: 0.39481401443481445\n",
            "step: 60, loss: 0.23125791549682617\n",
            "step: 70, loss: 0.1884276419878006\n",
            "step: 80, loss: 0.29818829894065857\n",
            "step: 90, loss: 0.3882178068161011\n",
            "step: 100, loss: 0.40720197558403015\n",
            "step: 110, loss: 0.448878675699234\n",
            "step: 120, loss: 0.31517890095710754\n",
            "step: 130, loss: 0.1864912062883377\n",
            "step: 140, loss: 0.2854679524898529\n",
            "step: 150, loss: 0.28265607357025146\n",
            "step: 160, loss: 0.13969376683235168\n",
            "step: 170, loss: 0.21194937825202942\n",
            "step: 180, loss: 0.21708986163139343\n",
            "step: 190, loss: 0.4463649392127991\n",
            "step: 200, loss: 0.16457191109657288\n",
            "step: 210, loss: 0.3442610502243042\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.3695652173913043, f1=0.371571072319202, best_f1=0.371571072319202\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.23244182765483856\n",
            "step: 10, loss: 0.25150302052497864\n",
            "step: 20, loss: 0.4044765532016754\n",
            "step: 30, loss: 0.24890559911727905\n",
            "step: 40, loss: 0.11383330076932907\n",
            "step: 50, loss: 0.48832404613494873\n",
            "step: 60, loss: 0.10398933291435242\n",
            "step: 70, loss: 0.3661366105079651\n",
            "step: 80, loss: 0.27194029092788696\n",
            "step: 90, loss: 0.44080016016960144\n",
            "step: 100, loss: 0.05651477724313736\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 110, loss: 0.25310537219047546\n",
            "step: 120, loss: 0.15455995500087738\n",
            "step: 130, loss: 0.4096345901489258\n",
            "step: 140, loss: 0.42885246872901917\n",
            "step: 150, loss: 0.08249706774950027\n",
            "step: 160, loss: 0.15154163539409637\n",
            "step: 170, loss: 0.16611728072166443\n",
            "step: 180, loss: 0.32904887199401855\n",
            "step: 190, loss: 0.34893715381622314\n",
            "step: 200, loss: 0.09276235848665237\n",
            "step: 210, loss: 0.12013549357652664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.4716981132075472, f1=0.5440313111545988, best_f1=0.5440313111545988\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1787831038236618\n",
            "step: 10, loss: 0.10371901839971542\n",
            "step: 20, loss: 0.1079833060503006\n",
            "step: 30, loss: 0.08989761769771576\n",
            "step: 40, loss: 0.16405589878559113\n",
            "step: 50, loss: 0.2167740911245346\n",
            "step: 60, loss: 0.38423651456832886\n",
            "step: 70, loss: 0.1750698834657669\n",
            "step: 80, loss: 0.2915131151676178\n",
            "step: 90, loss: 0.35258209705352783\n",
            "step: 100, loss: 0.2089955061674118\n",
            "step: 110, loss: 0.20067116618156433\n",
            "step: 120, loss: 0.19702701270580292\n",
            "step: 130, loss: 0.13981249928474426\n",
            "step: 140, loss: 0.15195083618164062\n",
            "step: 150, loss: 0.17570459842681885\n",
            "step: 160, loss: 0.2536356747150421\n",
            "step: 170, loss: 0.09684058278799057\n",
            "step: 180, loss: 0.1500312089920044\n",
            "step: 190, loss: 0.17817027866840363\n",
            "step: 200, loss: 0.2225930392742157\n",
            "step: 210, loss: 0.304394006729126\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.5621414913957935, f1=0.6242990654205607, best_f1=0.6242990654205607\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10010382533073425\n",
            "step: 10, loss: 0.17755407094955444\n",
            "step: 20, loss: 0.29133495688438416\n",
            "step: 30, loss: 0.07378628849983215\n",
            "step: 40, loss: 0.26404738426208496\n",
            "step: 50, loss: 0.15320442616939545\n",
            "step: 60, loss: 0.20733290910720825\n",
            "step: 70, loss: 0.18909591436386108\n",
            "step: 80, loss: 0.12395182996988297\n",
            "step: 90, loss: 0.06848897784948349\n",
            "step: 100, loss: 0.12066859006881714\n",
            "step: 110, loss: 0.22648069262504578\n",
            "step: 120, loss: 0.08058451861143112\n",
            "step: 130, loss: 0.07471349090337753\n",
            "step: 140, loss: 0.19448965787887573\n",
            "step: 150, loss: 0.1707676649093628\n",
            "step: 160, loss: 0.08018394559621811\n",
            "step: 170, loss: 0.17534787952899933\n",
            "step: 180, loss: 0.12549225986003876\n",
            "step: 190, loss: 0.24002479016780853\n",
            "step: 200, loss: 0.0587749183177948\n",
            "step: 210, loss: 0.21132014691829681\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.5466448445171849, f1=0.598705501618123, best_f1=0.6242990654205607\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11314454674720764\n",
            "step: 10, loss: 0.15695719420909882\n",
            "step: 20, loss: 0.38505110144615173\n",
            "step: 30, loss: 0.09841132164001465\n",
            "step: 40, loss: 0.24590028822422028\n",
            "step: 50, loss: 0.0736171305179596\n",
            "step: 60, loss: 0.18609566986560822\n",
            "step: 70, loss: 0.21023496985435486\n",
            "step: 80, loss: 0.19693569839000702\n",
            "step: 90, loss: 0.2136196792125702\n",
            "step: 100, loss: 0.1466730237007141\n",
            "step: 110, loss: 0.16490691900253296\n",
            "step: 120, loss: 0.33058595657348633\n",
            "step: 130, loss: 0.10386569052934647\n",
            "step: 140, loss: 0.26794424653053284\n",
            "step: 150, loss: 0.2414316087961197\n",
            "step: 160, loss: 0.3738042414188385\n",
            "step: 170, loss: 0.15438510477542877\n",
            "step: 180, loss: 0.17529620230197906\n",
            "step: 190, loss: 0.14733977615833282\n",
            "step: 200, loss: 0.08047953993082047\n",
            "step: 210, loss: 0.24369680881500244\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.5625, f1=0.5954692556634305, best_f1=0.5954692556634305\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:04, 535.04it/s]\n",
            "load_f1 = 0.5357142857142857\n",
            "real_f1 = 0.5273833671399594\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:20, 210.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "nXvTChDGgw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "IwR6Lg5Ygw8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "482fc58c-c318-4391-afcc-cd8af9ad531b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.4949153959751129\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5073251724243164\n",
            "step: 20, loss: 0.2675057649612427\n",
            "step: 30, loss: 0.3595905303955078\n",
            "step: 40, loss: 0.252562016248703\n",
            "step: 50, loss: 0.34240806102752686\n",
            "step: 60, loss: 0.48125511407852173\n",
            "step: 70, loss: 0.4460850656032562\n",
            "step: 80, loss: 0.17734165489673615\n",
            "step: 90, loss: 0.3094157576560974\n",
            "step: 100, loss: 0.4194526970386505\n",
            "step: 110, loss: 0.248111754655838\n",
            "step: 120, loss: 0.31536978483200073\n",
            "step: 130, loss: 0.3110385835170746\n",
            "step: 140, loss: 0.1658197045326233\n",
            "step: 150, loss: 0.2971802055835724\n",
            "step: 160, loss: 0.22114045917987823\n",
            "step: 170, loss: 0.4590890407562256\n",
            "step: 180, loss: 0.13520756363868713\n",
            "step: 190, loss: 0.12907493114471436\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5612472160356347, f1=0.5785876993166287, best_f1=0.5785876993166287\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3277832269668579\n",
            "step: 10, loss: 0.3266008496284485\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 20, loss: 0.8541995286941528\n",
            "step: 30, loss: 0.15774479508399963\n",
            "step: 40, loss: 0.29823967814445496\n",
            "step: 50, loss: 0.3550439178943634\n",
            "step: 60, loss: 0.123057521879673\n",
            "step: 70, loss: 0.28412431478500366\n",
            "step: 80, loss: 0.05308227241039276\n",
            "step: 90, loss: 0.11014040559530258\n",
            "step: 100, loss: 0.15302957594394684\n",
            "step: 110, loss: 0.21917416155338287\n",
            "step: 120, loss: 0.05570093169808388\n",
            "step: 130, loss: 0.1251225620508194\n",
            "step: 140, loss: 0.20552265644073486\n",
            "step: 150, loss: 0.24295878410339355\n",
            "step: 160, loss: 0.1895386129617691\n",
            "step: 170, loss: 0.038265421986579895\n",
            "step: 180, loss: 0.08991725742816925\n",
            "step: 190, loss: 0.05657688155770302\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7419354838709677, f1=0.7731092436974789, best_f1=0.7731092436974789\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16619235277175903\n",
            "step: 10, loss: 0.14552336931228638\n",
            "step: 20, loss: 0.10445481538772583\n",
            "step: 30, loss: 0.02164752408862114\n",
            "step: 40, loss: 0.1167815700173378\n",
            "step: 50, loss: 0.14244021475315094\n",
            "step: 60, loss: 0.019945835694670677\n",
            "step: 70, loss: 0.2335360050201416\n",
            "step: 80, loss: 0.25774726271629333\n",
            "step: 90, loss: 0.11031216382980347\n",
            "step: 100, loss: 0.07831598073244095\n",
            "step: 110, loss: 0.20851899683475494\n",
            "step: 120, loss: 0.2086750864982605\n",
            "step: 130, loss: 0.07572148740291595\n",
            "step: 140, loss: 0.0626305416226387\n",
            "step: 150, loss: 0.16141168773174286\n",
            "step: 160, loss: 0.04211818799376488\n",
            "step: 170, loss: 0.29638680815696716\n",
            "step: 180, loss: 0.13413850963115692\n",
            "step: 190, loss: 0.08928581327199936\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7759036144578314, f1=0.7852193995381062, best_f1=0.7852193995381062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.023135900497436523\n",
            "step: 10, loss: 0.02447669580578804\n",
            "step: 20, loss: 0.08667381852865219\n",
            "step: 30, loss: 0.13765409588813782\n",
            "step: 40, loss: 0.22274374961853027\n",
            "step: 50, loss: 0.07785145193338394\n",
            "step: 60, loss: 0.0438796691596508\n",
            "step: 70, loss: 0.0685327798128128\n",
            "step: 80, loss: 0.017795991152524948\n",
            "step: 90, loss: 0.03248830512166023\n",
            "step: 100, loss: 0.16374874114990234\n",
            "step: 110, loss: 0.1930035650730133\n",
            "step: 120, loss: 0.05853065103292465\n",
            "step: 130, loss: 0.09795606881380081\n",
            "step: 140, loss: 0.2138664573431015\n",
            "step: 150, loss: 0.013411096297204494\n",
            "step: 160, loss: 0.02548850327730179\n",
            "step: 170, loss: 0.07941025495529175\n",
            "step: 180, loss: 0.12135205417871475\n",
            "step: 190, loss: 0.04758143424987793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7658536585365853, f1=0.769607843137255, best_f1=0.7852193995381062\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2339976280927658\n",
            "step: 10, loss: 0.14797788858413696\n",
            "step: 20, loss: 0.05784142017364502\n",
            "step: 30, loss: 0.028381144627928734\n",
            "step: 40, loss: 0.02248363196849823\n",
            "step: 50, loss: 0.018782738596200943\n",
            "step: 60, loss: 0.16574031114578247\n",
            "step: 70, loss: 0.1257181167602539\n",
            "step: 80, loss: 0.01322004571557045\n",
            "step: 90, loss: 0.23733848333358765\n",
            "step: 100, loss: 0.13438546657562256\n",
            "step: 110, loss: 0.11342757195234299\n",
            "step: 120, loss: 0.03839763253927231\n",
            "step: 130, loss: 0.3085111677646637\n",
            "step: 140, loss: 0.017781995236873627\n",
            "step: 150, loss: 0.2774507701396942\n",
            "step: 160, loss: 0.012199895456433296\n",
            "step: 170, loss: 0.021687636151909828\n",
            "step: 180, loss: 0.035476818680763245\n",
            "step: 190, loss: 0.02089606784284115\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.7989556135770235, f1=0.7916666666666666, best_f1=0.7916666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.01760876178741455\n",
            "step: 10, loss: 0.0654076337814331\n",
            "step: 20, loss: 0.003914030268788338\n",
            "step: 30, loss: 0.10583187639713287\n",
            "step: 40, loss: 0.012281776405870914\n",
            "step: 50, loss: 0.02503105439245701\n",
            "step: 60, loss: 0.05736769735813141\n",
            "step: 70, loss: 0.04406176880002022\n",
            "step: 80, loss: 0.07139932364225388\n",
            "step: 90, loss: 0.053164951503276825\n",
            "step: 100, loss: 0.02393747679889202\n",
            "step: 110, loss: 0.024076830595731735\n",
            "step: 120, loss: 0.01650143973529339\n",
            "step: 130, loss: 0.10519401729106903\n",
            "step: 140, loss: 0.0035142956767231226\n",
            "step: 150, loss: 0.024227125570178032\n",
            "step: 160, loss: 0.1916133612394333\n",
            "step: 170, loss: 0.1701589971780777\n",
            "step: 180, loss: 0.07530733942985535\n",
            "step: 190, loss: 0.016203319653868675\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.7921760391198045, f1=0.8077858880778588, best_f1=0.7916666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020294692367315292\n",
            "step: 10, loss: 0.07334255427122116\n",
            "step: 20, loss: 0.003911056090146303\n",
            "step: 30, loss: 0.034917935729026794\n",
            "step: 40, loss: 0.06039974093437195\n",
            "step: 50, loss: 0.00678511057049036\n",
            "step: 60, loss: 0.006766526959836483\n",
            "step: 70, loss: 0.004499088507145643\n",
            "step: 80, loss: 0.0034701991826295853\n",
            "step: 90, loss: 0.0017976914532482624\n",
            "step: 100, loss: 0.20394732058048248\n",
            "step: 110, loss: 0.10717611759901047\n",
            "step: 120, loss: 0.07891780883073807\n",
            "step: 130, loss: 0.007952417246997356\n",
            "step: 140, loss: 0.016374222934246063\n",
            "step: 150, loss: 0.004293127451092005\n",
            "step: 160, loss: 0.08445660024881363\n",
            "step: 170, loss: 0.0062193856574594975\n",
            "step: 180, loss: 0.01470264419913292\n",
            "step: 190, loss: 0.02327943779528141\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.7919799498746868, f1=0.8113695090439276, best_f1=0.7916666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02707202173769474\n",
            "step: 10, loss: 0.004506107419729233\n",
            "step: 20, loss: 0.0026872309390455484\n",
            "step: 30, loss: 0.1276949942111969\n",
            "step: 40, loss: 0.04784363508224487\n",
            "step: 50, loss: 0.00913923792541027\n",
            "step: 60, loss: 0.032974209636449814\n",
            "step: 70, loss: 0.005538120865821838\n",
            "step: 80, loss: 0.08477449417114258\n",
            "step: 90, loss: 0.0025869577657431364\n",
            "step: 100, loss: 0.00623583747074008\n",
            "step: 110, loss: 0.09691162407398224\n",
            "step: 120, loss: 0.0035041950177401304\n",
            "step: 130, loss: 0.0015474739484488964\n",
            "step: 140, loss: 0.0037249766755849123\n",
            "step: 150, loss: 0.14563453197479248\n",
            "step: 160, loss: 0.0067129735834896564\n",
            "step: 170, loss: 0.0008382229716517031\n",
            "step: 180, loss: 0.030733970925211906\n",
            "step: 190, loss: 0.0024357398506253958\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.7979539641943734, f1=0.795969773299748, best_f1=0.7916666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.012073909863829613\n",
            "step: 10, loss: 0.0006984818028286099\n",
            "step: 20, loss: 0.0033177933655679226\n",
            "step: 30, loss: 0.002865065587684512\n",
            "step: 40, loss: 0.030016181990504265\n",
            "step: 50, loss: 0.029109599068760872\n",
            "step: 60, loss: 0.011706414632499218\n",
            "step: 70, loss: 0.0026022084057331085\n",
            "step: 80, loss: 0.014934813603758812\n",
            "step: 90, loss: 0.0012583571951836348\n",
            "step: 100, loss: 0.0007392395636998117\n",
            "step: 110, loss: 0.0005864903214387596\n",
            "step: 120, loss: 0.0037463465705513954\n",
            "step: 130, loss: 0.0329907163977623\n",
            "step: 140, loss: 0.056884896010160446\n",
            "step: 150, loss: 0.0021395611111074686\n",
            "step: 160, loss: 0.0009391973144374788\n",
            "step: 170, loss: 0.005628280341625214\n",
            "step: 180, loss: 0.00456096651032567\n",
            "step: 190, loss: 0.00043179019121453166\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8169761273209548, f1=0.8288770053475937, best_f1=0.8288770053475937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003491143463179469\n",
            "step: 10, loss: 0.0010782465105876327\n",
            "step: 20, loss: 0.0009495048434473574\n",
            "step: 30, loss: 0.17523346841335297\n",
            "step: 40, loss: 0.0014192801900207996\n",
            "step: 50, loss: 0.004005652852356434\n",
            "step: 60, loss: 0.0004690038622356951\n",
            "step: 70, loss: 0.0003731635515578091\n",
            "step: 80, loss: 0.001387840136885643\n",
            "step: 90, loss: 0.0013374964473769069\n",
            "step: 100, loss: 0.000494323845487088\n",
            "step: 110, loss: 0.0004965000553056598\n",
            "step: 120, loss: 0.0009014259558171034\n",
            "step: 130, loss: 0.00042120704893022776\n",
            "step: 140, loss: 0.013596657663583755\n",
            "step: 150, loss: 0.00026634763344191015\n",
            "step: 160, loss: 0.0005734710139222443\n",
            "step: 170, loss: 0.04200499877333641\n",
            "step: 180, loss: 0.005664633121341467\n",
            "step: 190, loss: 0.00046727684093639255\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.7806122448979592, f1=0.8226221079691518, best_f1=0.8288770053475937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0033152818214148283\n",
            "step: 10, loss: 0.0007566402782686055\n",
            "step: 20, loss: 0.0020806898828595877\n",
            "step: 30, loss: 0.002761928364634514\n",
            "step: 40, loss: 0.0005963582661934197\n",
            "step: 50, loss: 0.005902057513594627\n",
            "step: 60, loss: 0.001393725979141891\n",
            "step: 70, loss: 0.0033867903985083103\n",
            "step: 80, loss: 0.003818897297605872\n",
            "step: 90, loss: 0.00779363326728344\n",
            "step: 100, loss: 0.0005073242355138063\n",
            "step: 110, loss: 0.18932074308395386\n",
            "step: 120, loss: 0.0006035424885340035\n",
            "step: 130, loss: 0.0008600241271778941\n",
            "step: 140, loss: 0.003433970035985112\n",
            "step: 150, loss: 0.03476765751838684\n",
            "step: 160, loss: 0.002485094126313925\n",
            "step: 170, loss: 0.002461736323311925\n",
            "step: 180, loss: 0.002592360833659768\n",
            "step: 190, loss: 0.0009215666796080768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8129675810473815, f1=0.8, best_f1=0.8288770053475937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006717428914271295\n",
            "step: 10, loss: 0.02279203198850155\n",
            "step: 20, loss: 0.0004831039987038821\n",
            "step: 30, loss: 0.0007741758599877357\n",
            "step: 40, loss: 0.00022451297263614833\n",
            "step: 50, loss: 0.025252828374505043\n",
            "step: 60, loss: 0.013665142469108105\n",
            "step: 70, loss: 0.0007288993801921606\n",
            "step: 80, loss: 0.0005817125202156603\n",
            "step: 90, loss: 0.0009856689721345901\n",
            "step: 100, loss: 0.000423167715780437\n",
            "step: 110, loss: 0.002485520439222455\n",
            "step: 120, loss: 0.000249385746428743\n",
            "step: 130, loss: 0.0016085215611383319\n",
            "step: 140, loss: 0.0022897496819496155\n",
            "step: 150, loss: 0.0006236285553313792\n",
            "step: 160, loss: 0.0004061888321302831\n",
            "step: 170, loss: 0.0006144448998384178\n",
            "step: 180, loss: 0.00069764896761626\n",
            "step: 190, loss: 0.0001978341315407306\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.7977839335180055, f1=0.8179271708683474, best_f1=0.8288770053475937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002590187592431903\n",
            "step: 10, loss: 0.0003519994206726551\n",
            "step: 20, loss: 0.0007068563718348742\n",
            "step: 30, loss: 0.00721993762999773\n",
            "step: 40, loss: 0.0007721178117208183\n",
            "step: 50, loss: 0.0017163777956739068\n",
            "step: 60, loss: 0.0003762743144761771\n",
            "step: 70, loss: 0.009433058090507984\n",
            "step: 80, loss: 0.000892349926289171\n",
            "step: 90, loss: 0.0008665493223816156\n",
            "step: 100, loss: 0.001096282503567636\n",
            "step: 110, loss: 0.0014639555010944605\n",
            "step: 120, loss: 0.0009448418277315795\n",
            "step: 130, loss: 0.011330514214932919\n",
            "step: 140, loss: 0.0013083290541544557\n",
            "step: 150, loss: 0.0003681464877445251\n",
            "step: 160, loss: 0.0006124814390204847\n",
            "step: 170, loss: 0.0001845379447331652\n",
            "step: 180, loss: 0.0005127131589688361\n",
            "step: 190, loss: 0.16450481116771698\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.8179419525065963, f1=0.8213333333333334, best_f1=0.8213333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0021828741300851107\n",
            "step: 10, loss: 0.0002674744464457035\n",
            "step: 20, loss: 0.0004329598159529269\n",
            "step: 30, loss: 0.002014914294704795\n",
            "step: 40, loss: 0.000401488650823012\n",
            "step: 50, loss: 0.00031912472331896424\n",
            "step: 60, loss: 0.00272843474522233\n",
            "step: 70, loss: 0.0011103413999080658\n",
            "step: 80, loss: 0.0007336512207984924\n",
            "step: 90, loss: 0.000192279985640198\n",
            "step: 100, loss: 0.003073622239753604\n",
            "step: 110, loss: 0.001912764273583889\n",
            "step: 120, loss: 0.002083786064758897\n",
            "step: 130, loss: 0.0002698293246794492\n",
            "step: 140, loss: 0.00027859595138579607\n",
            "step: 150, loss: 0.00043768397881649435\n",
            "step: 160, loss: 0.0003184577217325568\n",
            "step: 170, loss: 0.00015021003491710871\n",
            "step: 180, loss: 0.0006455505499616265\n",
            "step: 190, loss: 0.0007668031612411141\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8146067415730337, f1=0.8441926345609065, best_f1=0.8213333333333334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011312311980873346\n",
            "step: 10, loss: 0.0007883823709562421\n",
            "step: 20, loss: 0.0011630072258412838\n",
            "step: 30, loss: 0.002433845540508628\n",
            "step: 40, loss: 0.0007944984245114028\n",
            "step: 50, loss: 0.00018288730643689632\n",
            "step: 60, loss: 0.00016971949662547559\n",
            "step: 70, loss: 0.0006101452745497227\n",
            "step: 80, loss: 0.00016183160187210888\n",
            "step: 90, loss: 0.000895194592885673\n",
            "step: 100, loss: 0.000349442329024896\n",
            "step: 110, loss: 0.00239246035926044\n",
            "step: 120, loss: 0.00013891000708099455\n",
            "step: 130, loss: 0.00031283075804822147\n",
            "step: 140, loss: 0.008531270548701286\n",
            "step: 150, loss: 0.00019069199333898723\n",
            "step: 160, loss: 0.00027313243481330574\n",
            "step: 170, loss: 0.0010509842541068792\n",
            "step: 180, loss: 0.0012855792883783579\n",
            "step: 190, loss: 0.005933270324021578\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8125, f1=0.8376068376068376, best_f1=0.8213333333333334\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:08, 255.70it/s]\n",
            "load_f1 = 0.837696335078534\n",
            "real_f1 = 0.810126582278481\n",
            "733it [00:00, 3277.73it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:21, 203.63it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK TEXTUAL"
      ],
      "metadata": {
        "id": "SSCCmtSggw8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "OAbIZQYfgw8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "n5DZbZADgw8F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c46504-0cbc-4e81-c955-99887d43d55d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 431kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 682kB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 512kB/s]\n",
            "Downloading: 100% 501M/501M [00:07<00:00, 71.1MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5162197351455688\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4652726650238037\n",
            "step: 20, loss: 0.30483829975128174\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 30, loss: 0.43703436851501465\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 40, loss: 0.5278929471969604\n",
            "step: 50, loss: 0.35351303219795227\n",
            "step: 60, loss: 0.6001684665679932\n",
            "step: 70, loss: 0.30587223172187805\n",
            "step: 80, loss: 0.24077089130878448\n",
            "step: 90, loss: 0.19923202693462372\n",
            "step: 100, loss: 0.17677664756774902\n",
            "step: 110, loss: 0.4411058723926544\n",
            "step: 120, loss: 0.29203489422798157\n",
            "step: 130, loss: 0.2932400703430176\n",
            "step: 140, loss: 0.38842636346817017\n",
            "step: 150, loss: 0.29728078842163086\n",
            "step: 160, loss: 0.3779364228248596\n",
            "step: 170, loss: 0.34423866868019104\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.32432037591934204\n",
            "step: 10, loss: 0.4972870945930481\n",
            "step: 20, loss: 0.31009721755981445\n",
            "step: 30, loss: 0.3265571892261505\n",
            "step: 40, loss: 0.08676432073116302\n",
            "step: 50, loss: 0.45236432552337646\n",
            "step: 60, loss: 0.1898670196533203\n",
            "step: 70, loss: 0.5097609758377075\n",
            "step: 80, loss: 0.24591916799545288\n",
            "step: 90, loss: 0.24997738003730774\n",
            "step: 100, loss: 0.5153307318687439\n",
            "step: 110, loss: 0.2572435140609741\n",
            "step: 120, loss: 0.251792311668396\n",
            "step: 130, loss: 0.5463679432868958\n",
            "step: 140, loss: 0.5243008732795715\n",
            "step: 150, loss: 0.44542914628982544\n",
            "step: 160, loss: 0.44386211037635803\n",
            "step: 170, loss: 0.3793767988681793\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.1941564561734213\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5914474129676819\n",
            "step: 10, loss: 0.2885126769542694\n",
            "step: 20, loss: 0.23146218061447144\n",
            "step: 30, loss: 0.24863308668136597\n",
            "step: 40, loss: 0.37419214844703674\n",
            "step: 50, loss: 0.5847966074943542\n",
            "step: 60, loss: 0.30148956179618835\n",
            "step: 70, loss: 0.23383578658103943\n",
            "step: 80, loss: 0.3909297585487366\n",
            "step: 90, loss: 0.5434029698371887\n",
            "step: 100, loss: 0.30677416920661926\n",
            "step: 110, loss: 0.18601615726947784\n",
            "step: 120, loss: 0.5472939610481262\n",
            "step: 130, loss: 0.4982450008392334\n",
            "step: 140, loss: 0.44023215770721436\n",
            "step: 150, loss: 0.20079508423805237\n",
            "step: 160, loss: 0.16690853238105774\n",
            "step: 170, loss: 0.3026820421218872\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.19424799622819425, f1=0.19443133553563002, best_f1=0.19443133553563002\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.362222820520401\n",
            "step: 10, loss: 0.519442617893219\n",
            "step: 20, loss: 0.25359755754470825\n",
            "step: 30, loss: 0.46089547872543335\n",
            "step: 40, loss: 0.25572240352630615\n",
            "step: 50, loss: 0.38359886407852173\n",
            "step: 60, loss: 0.7035394906997681\n",
            "step: 70, loss: 0.325758159160614\n",
            "step: 80, loss: 0.44848328828811646\n",
            "step: 90, loss: 0.30660468339920044\n",
            "step: 100, loss: 0.39245107769966125\n",
            "step: 110, loss: 0.4312525987625122\n",
            "step: 120, loss: 0.5008854866027832\n",
            "step: 130, loss: 0.3065096437931061\n",
            "step: 140, loss: 0.25602373480796814\n",
            "step: 150, loss: 0.7247604727745056\n",
            "step: 160, loss: 0.12260399758815765\n",
            "step: 170, loss: 0.24507850408554077\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.1941564561734213, f1=0.1941564561734213, best_f1=0.19443133553563002\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3816032409667969\n",
            "step: 10, loss: 0.31026172637939453\n",
            "step: 20, loss: 0.3258618116378784\n",
            "step: 30, loss: 0.3154430389404297\n",
            "step: 40, loss: 0.23139777779579163\n",
            "step: 50, loss: 0.2716657221317291\n",
            "step: 60, loss: 0.30525949597358704\n",
            "step: 70, loss: 0.3322230577468872\n",
            "step: 80, loss: 0.06729047745466232\n",
            "step: 90, loss: 0.566967248916626\n",
            "step: 100, loss: 0.2481723129749298\n",
            "step: 110, loss: 0.26505163311958313\n",
            "step: 120, loss: 0.14227613806724548\n",
            "step: 130, loss: 0.24959850311279297\n",
            "step: 140, loss: 0.17272265255451202\n",
            "step: 150, loss: 0.30118030309677124\n",
            "step: 160, loss: 0.25390753149986267\n",
            "step: 170, loss: 0.3412156105041504\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.625, f1=0.6453089244851259, best_f1=0.6453089244851259\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.268867164850235\n",
            "step: 10, loss: 0.12049839645624161\n",
            "step: 20, loss: 0.3748992681503296\n",
            "step: 30, loss: 0.22136712074279785\n",
            "step: 40, loss: 0.29520341753959656\n",
            "step: 50, loss: 0.13439461588859558\n",
            "step: 60, loss: 0.22730869054794312\n",
            "step: 70, loss: 0.39962878823280334\n",
            "step: 80, loss: 0.11782456189393997\n",
            "step: 90, loss: 0.21884785592556\n",
            "step: 100, loss: 0.18687704205513\n",
            "step: 110, loss: 0.1725548952817917\n",
            "step: 120, loss: 0.270090252161026\n",
            "step: 130, loss: 0.2504620850086212\n",
            "step: 140, loss: 0.2843702435493469\n",
            "step: 150, loss: 0.055475201457738876\n",
            "step: 160, loss: 0.22379732131958008\n",
            "step: 170, loss: 0.15027618408203125\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.7037037037037038, f1=0.6810176125244618, best_f1=0.6810176125244618\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.17860601842403412\n",
            "step: 10, loss: 0.23518387973308563\n",
            "step: 20, loss: 0.29857516288757324\n",
            "step: 30, loss: 0.08689091354608536\n",
            "step: 40, loss: 0.0029645864851772785\n",
            "step: 50, loss: 0.06717459112405777\n",
            "step: 60, loss: 0.07899274677038193\n",
            "step: 70, loss: 0.14463907480239868\n",
            "step: 80, loss: 0.1070052832365036\n",
            "step: 90, loss: 0.09890471398830414\n",
            "step: 100, loss: 0.024168971925973892\n",
            "step: 110, loss: 0.08370909839868546\n",
            "step: 120, loss: 0.08570743352174759\n",
            "step: 130, loss: 0.0980653464794159\n",
            "step: 140, loss: 0.08111893385648727\n",
            "step: 150, loss: 0.16866149008274078\n",
            "step: 160, loss: 0.18641069531440735\n",
            "step: 170, loss: 0.07440294325351715\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.7706855791962176, f1=0.7636363636363637, best_f1=0.7636363636363637\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.12209781259298325\n",
            "step: 10, loss: 0.1634446531534195\n",
            "step: 20, loss: 0.15514086186885834\n",
            "step: 30, loss: 0.02502894029021263\n",
            "step: 40, loss: 0.11745693534612656\n",
            "step: 50, loss: 0.02096601203083992\n",
            "step: 60, loss: 0.20637395977973938\n",
            "step: 70, loss: 0.16787883639335632\n",
            "step: 80, loss: 0.021062323823571205\n",
            "step: 90, loss: 0.16484200954437256\n",
            "step: 100, loss: 0.04107052460312843\n",
            "step: 110, loss: 0.16952787339687347\n",
            "step: 120, loss: 0.3910244405269623\n",
            "step: 130, loss: 0.09841425716876984\n",
            "step: 140, loss: 0.10421693325042725\n",
            "step: 150, loss: 0.03143533319234848\n",
            "step: 160, loss: 0.06439757347106934\n",
            "step: 170, loss: 0.1524340957403183\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.7969924812030076, f1=0.7943262411347518, best_f1=0.7943262411347518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09916597604751587\n",
            "step: 10, loss: 0.08272336423397064\n",
            "step: 20, loss: 0.06447123736143112\n",
            "step: 30, loss: 0.043498728424310684\n",
            "step: 40, loss: 0.1130584180355072\n",
            "step: 50, loss: 0.017899710685014725\n",
            "step: 60, loss: 0.062117800116539\n",
            "step: 70, loss: 0.07761258631944656\n",
            "step: 80, loss: 0.05651164799928665\n",
            "step: 90, loss: 0.11434605717658997\n",
            "step: 100, loss: 0.10725453495979309\n",
            "step: 110, loss: 0.15610450506210327\n",
            "step: 120, loss: 0.12068069726228714\n",
            "step: 130, loss: 0.019737184047698975\n",
            "step: 140, loss: 0.16818338632583618\n",
            "step: 150, loss: 0.23829616606235504\n",
            "step: 160, loss: 0.030575282871723175\n",
            "step: 170, loss: 0.06312304735183716\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.8186528497409326, f1=0.8316831683168316, best_f1=0.8316831683168316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11165445297956467\n",
            "step: 10, loss: 0.0352398082613945\n",
            "step: 20, loss: 0.12394160032272339\n",
            "step: 30, loss: 0.07508540153503418\n",
            "step: 40, loss: 0.07967433333396912\n",
            "step: 50, loss: 0.09316997975111008\n",
            "step: 60, loss: 0.09098467975854874\n",
            "step: 70, loss: 0.06480751931667328\n",
            "step: 80, loss: 0.053598128259181976\n",
            "step: 90, loss: 0.007535899057984352\n",
            "step: 100, loss: 0.045659389346838\n",
            "step: 110, loss: 0.04771190136671066\n",
            "step: 120, loss: 0.06126842647790909\n",
            "step: 130, loss: 0.07948238402605057\n",
            "step: 140, loss: 0.015116500668227673\n",
            "step: 150, loss: 0.018015213310718536\n",
            "step: 160, loss: 0.005798035766929388\n",
            "step: 170, loss: 0.05094923451542854\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8115183246073299, f1=0.8333333333333333, best_f1=0.8316831683168316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08161529153585434\n",
            "step: 10, loss: 0.03275800868868828\n",
            "step: 20, loss: 0.03258025273680687\n",
            "step: 30, loss: 0.0677364394068718\n",
            "step: 40, loss: 0.061610084027051926\n",
            "step: 50, loss: 0.17916244268417358\n",
            "step: 60, loss: 0.1519109010696411\n",
            "step: 70, loss: 0.043660905212163925\n",
            "step: 80, loss: 0.0319613479077816\n",
            "step: 90, loss: 0.033204834908246994\n",
            "step: 100, loss: 0.19831311702728271\n",
            "step: 110, loss: 0.02264401875436306\n",
            "step: 120, loss: 0.07905546575784683\n",
            "step: 130, loss: 0.03879585489630699\n",
            "step: 140, loss: 0.02614648826420307\n",
            "step: 150, loss: 0.0028408404905349016\n",
            "step: 160, loss: 0.04955313354730606\n",
            "step: 170, loss: 0.18424341082572937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8177083333333334, f1=0.834567901234568, best_f1=0.8316831683168316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04055623337626457\n",
            "step: 10, loss: 0.024435164406895638\n",
            "step: 20, loss: 0.02495514042675495\n",
            "step: 30, loss: 0.1902477890253067\n",
            "step: 40, loss: 0.004899326711893082\n",
            "step: 50, loss: 0.02622396871447563\n",
            "step: 60, loss: 0.02074681967496872\n",
            "step: 70, loss: 0.031187547370791435\n",
            "step: 80, loss: 0.0037156809121370316\n",
            "step: 90, loss: 0.0755162462592125\n",
            "step: 100, loss: 0.008354008197784424\n",
            "step: 110, loss: 0.04058603197336197\n",
            "step: 120, loss: 0.059135787189006805\n",
            "step: 130, loss: 0.10565447807312012\n",
            "step: 140, loss: 0.003861530451104045\n",
            "step: 150, loss: 0.011128529906272888\n",
            "step: 160, loss: 0.0388069748878479\n",
            "step: 170, loss: 0.29752302169799805\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8116710875331565, f1=0.8287841191066997, best_f1=0.8316831683168316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.033762142062187195\n",
            "step: 10, loss: 0.15921112895011902\n",
            "step: 20, loss: 0.018473612144589424\n",
            "step: 30, loss: 0.073427714407444\n",
            "step: 40, loss: 0.005408929660916328\n",
            "step: 50, loss: 0.11541611701250076\n",
            "step: 60, loss: 0.05802937224507332\n",
            "step: 70, loss: 0.0928603783249855\n",
            "step: 80, loss: 0.0018540788441896439\n",
            "step: 90, loss: 0.08347893506288528\n",
            "step: 100, loss: 0.06818325817584991\n",
            "step: 110, loss: 0.0063418010249733925\n",
            "step: 120, loss: 0.05754885450005531\n",
            "step: 130, loss: 0.004739714786410332\n",
            "step: 140, loss: 0.05755268782377243\n",
            "step: 150, loss: 0.030726799741387367\n",
            "step: 160, loss: 0.0030800416134297848\n",
            "step: 170, loss: 0.009004872292280197\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8159203980099503, f1=0.8314087759815242, best_f1=0.8316831683168316\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008624750189483166\n",
            "step: 10, loss: 0.010562204755842686\n",
            "step: 20, loss: 0.013126160949468613\n",
            "step: 30, loss: 0.03540001064538956\n",
            "step: 40, loss: 0.004660783801227808\n",
            "step: 50, loss: 0.0011097171809524298\n",
            "step: 60, loss: 0.0050000352784991264\n",
            "step: 70, loss: 0.01055472157895565\n",
            "step: 80, loss: 0.0008165116887539625\n",
            "step: 90, loss: 0.001579601550474763\n",
            "step: 100, loss: 0.009006007574498653\n",
            "step: 110, loss: 0.013462717644870281\n",
            "step: 120, loss: 0.09499290585517883\n",
            "step: 130, loss: 0.06939482688903809\n",
            "step: 140, loss: 0.1051851361989975\n",
            "step: 150, loss: 0.05967523902654648\n",
            "step: 160, loss: 0.004395695868879557\n",
            "step: 170, loss: 0.06392832100391388\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 14: dev_f1=0.8191489361702128, f1=0.8341708542713568, best_f1=0.8341708542713568\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.011337080970406532\n",
            "step: 10, loss: 0.01037656795233488\n",
            "step: 20, loss: 0.0185487512499094\n",
            "step: 30, loss: 0.024104349315166473\n",
            "step: 40, loss: 0.010847964324057102\n",
            "step: 50, loss: 0.0014947388553991914\n",
            "step: 60, loss: 0.007714853156358004\n",
            "step: 70, loss: 0.10122599452733994\n",
            "step: 80, loss: 0.010199243202805519\n",
            "step: 90, loss: 0.010936705395579338\n",
            "step: 100, loss: 0.05064341798424721\n",
            "step: 110, loss: 0.023164810612797737\n",
            "step: 120, loss: 0.06038697063922882\n",
            "step: 130, loss: 0.02297557145357132\n",
            "step: 140, loss: 0.02878340519964695\n",
            "step: 150, loss: 0.0007763314060866833\n",
            "step: 160, loss: 0.0016004386125132442\n",
            "step: 170, loss: 0.003147713840007782\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.8203753351206435, f1=0.8226221079691517, best_f1=0.8226221079691517\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:07, 244.90it/s]\n",
            "load_f1 = 0.7658536585365854\n",
            "real_f1 = 0.7482014388489209\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:29, 149.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DK DIRTY"
      ],
      "metadata": {
        "id": "5HZE1zMQgw8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "9jg7qrOQgw8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "fimXO1Yygw8G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c4f8f53-009d-4bbe-eda2-7729f933a13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6428719162940979\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5104607939720154\n",
            "step: 20, loss: 0.5695582032203674\n",
            "step: 30, loss: 0.2913742661476135\n",
            "step: 40, loss: 0.3620990514755249\n",
            "step: 50, loss: 0.6036851406097412\n",
            "step: 60, loss: 0.5134280323982239\n",
            "step: 70, loss: 0.4286361634731293\n",
            "step: 80, loss: 0.5946087837219238\n",
            "step: 90, loss: 0.45967039465904236\n",
            "step: 100, loss: 0.4587825536727905\n",
            "step: 110, loss: 0.6426102519035339\n",
            "step: 120, loss: 0.4088066518306732\n",
            "step: 130, loss: 0.3278732895851135\n",
            "step: 140, loss: 0.3905588388442993\n",
            "step: 150, loss: 0.578973650932312\n",
            "step: 160, loss: 0.49971523880958557\n",
            "step: 170, loss: 0.47726520895957947\n",
            "step: 180, loss: 0.3984268307685852\n",
            "step: 190, loss: 0.2904142439365387\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 200, loss: 0.3940889835357666\n",
            "step: 210, loss: 0.49132266640663147\n",
            "step: 220, loss: 0.6176818013191223\n",
            "step: 230, loss: 0.35527467727661133\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3044223517312307, f1=0.3044223517312307, best_f1=0.3044223517312307\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4869082570075989\n",
            "step: 10, loss: 0.524720311164856\n",
            "step: 20, loss: 0.5583420395851135\n",
            "step: 30, loss: 0.5179626941680908\n",
            "step: 40, loss: 0.537490725517273\n",
            "step: 50, loss: 0.3879796862602234\n",
            "step: 60, loss: 0.2601166069507599\n",
            "step: 70, loss: 0.1309613436460495\n",
            "step: 80, loss: 0.030187496915459633\n",
            "step: 90, loss: 0.14508892595767975\n",
            "step: 100, loss: 0.0512714646756649\n",
            "step: 110, loss: 0.1000601276755333\n",
            "step: 120, loss: 0.09886672347784042\n",
            "step: 130, loss: 0.1940009593963623\n",
            "step: 140, loss: 0.024306731298565865\n",
            "step: 150, loss: 0.029667820781469345\n",
            "step: 160, loss: 0.028672700747847557\n",
            "step: 170, loss: 0.012035577557981014\n",
            "step: 180, loss: 0.010086273774504662\n",
            "step: 190, loss: 0.024762552231550217\n",
            "step: 200, loss: 0.12739530205726624\n",
            "step: 210, loss: 0.03373342007398605\n",
            "step: 220, loss: 0.0030221203342080116\n",
            "step: 230, loss: 0.0035402686335146427\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9496080627099663, f1=0.9524886877828055, best_f1=0.9524886877828055\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.08365448564291\n",
            "step: 10, loss: 0.0484897680580616\n",
            "step: 20, loss: 0.14606156945228577\n",
            "step: 30, loss: 0.015981215983629227\n",
            "step: 40, loss: 0.04396842420101166\n",
            "step: 50, loss: 0.011377297341823578\n",
            "step: 60, loss: 0.04024071618914604\n",
            "step: 70, loss: 0.0801975429058075\n",
            "step: 80, loss: 0.06314227730035782\n",
            "step: 90, loss: 0.06252217292785645\n",
            "step: 100, loss: 0.030802996829152107\n",
            "step: 110, loss: 0.019675131887197495\n",
            "step: 120, loss: 0.0015189606929197907\n",
            "step: 130, loss: 0.05527089536190033\n",
            "step: 140, loss: 0.004025152884423733\n",
            "step: 150, loss: 0.154102623462677\n",
            "step: 160, loss: 0.009134032763540745\n",
            "step: 170, loss: 0.005037502385675907\n",
            "step: 180, loss: 0.013968913815915585\n",
            "step: 190, loss: 0.04223497584462166\n",
            "step: 200, loss: 0.004354025237262249\n",
            "step: 210, loss: 0.0038539497181773186\n",
            "step: 220, loss: 0.0685005709528923\n",
            "step: 230, loss: 0.04631125181913376\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9686800894854586, f1=0.963882618510158, best_f1=0.963882618510158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.022808117792010307\n",
            "step: 10, loss: 0.007249094545841217\n",
            "step: 20, loss: 0.037905484437942505\n",
            "step: 30, loss: 0.0025787476915866137\n",
            "step: 40, loss: 0.11276113241910934\n",
            "step: 50, loss: 0.028411485254764557\n",
            "step: 60, loss: 0.008857461623847485\n",
            "step: 70, loss: 0.047926463186740875\n",
            "step: 80, loss: 0.010615545324981213\n",
            "step: 90, loss: 0.0175553597509861\n",
            "step: 100, loss: 0.024119926616549492\n",
            "step: 110, loss: 0.002574666403234005\n",
            "step: 120, loss: 0.049165356904268265\n",
            "step: 130, loss: 0.04241384565830231\n",
            "step: 140, loss: 0.004982438404113054\n",
            "step: 150, loss: 0.011149407364428043\n",
            "step: 160, loss: 0.012040266767144203\n",
            "step: 170, loss: 0.009022966958582401\n",
            "step: 180, loss: 0.15100277960300446\n",
            "step: 190, loss: 0.004938112106174231\n",
            "step: 200, loss: 0.137749582529068\n",
            "step: 210, loss: 0.11785215884447098\n",
            "step: 220, loss: 0.002203962067142129\n",
            "step: 230, loss: 0.004323951900005341\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9689578713968958, f1=0.9730941704035874, best_f1=0.9730941704035874\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004142193589359522\n",
            "step: 10, loss: 0.007311354856938124\n",
            "step: 20, loss: 0.01651945523917675\n",
            "step: 30, loss: 0.005639621987938881\n",
            "step: 40, loss: 0.0035942448303103447\n",
            "step: 50, loss: 0.006566951517015696\n",
            "step: 60, loss: 0.024736052379012108\n",
            "step: 70, loss: 0.0012392106000334024\n",
            "step: 80, loss: 0.03918059170246124\n",
            "step: 90, loss: 0.24571700394153595\n",
            "step: 100, loss: 0.0038186851888895035\n",
            "step: 110, loss: 0.006878652144223452\n",
            "step: 120, loss: 0.010423251427710056\n",
            "step: 130, loss: 0.002060945611447096\n",
            "step: 140, loss: 0.20732718706130981\n",
            "step: 150, loss: 0.008295971900224686\n",
            "step: 160, loss: 0.0026826029643416405\n",
            "step: 170, loss: 0.006520366296172142\n",
            "step: 180, loss: 0.0026734210550785065\n",
            "step: 190, loss: 0.08401115238666534\n",
            "step: 200, loss: 0.035709962248802185\n",
            "step: 210, loss: 0.07346872985363007\n",
            "step: 220, loss: 0.0011857874924317002\n",
            "step: 230, loss: 0.012093371711671352\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9693654266958425, f1=0.9618320610687023, best_f1=0.9618320610687023\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011451573809608817\n",
            "step: 10, loss: 0.003083331510424614\n",
            "step: 20, loss: 0.001636508502997458\n",
            "step: 30, loss: 0.0014495160430669785\n",
            "step: 40, loss: 0.0005296161398291588\n",
            "step: 50, loss: 0.0005907418089918792\n",
            "step: 60, loss: 0.0010428800014778972\n",
            "step: 70, loss: 0.1342984437942505\n",
            "step: 80, loss: 0.0025658723898231983\n",
            "step: 90, loss: 0.0020947065204381943\n",
            "step: 100, loss: 0.0069968970492482185\n",
            "step: 110, loss: 0.0904054194688797\n",
            "step: 120, loss: 0.0036300141364336014\n",
            "step: 130, loss: 0.0023139268159866333\n",
            "step: 140, loss: 0.0018246641848236322\n",
            "step: 150, loss: 0.0010062218643724918\n",
            "step: 160, loss: 0.003084801137447357\n",
            "step: 170, loss: 0.008459839969873428\n",
            "step: 180, loss: 0.003065630095079541\n",
            "step: 190, loss: 0.0031543858349323273\n",
            "step: 200, loss: 0.010898304171860218\n",
            "step: 210, loss: 0.0023484507109969854\n",
            "step: 220, loss: 0.0044947960413992405\n",
            "step: 230, loss: 0.0010110782459378242\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.978675645342312, f1=0.9752252252252253, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012286331038922071\n",
            "step: 10, loss: 0.0008829715079627931\n",
            "step: 20, loss: 0.008323049172759056\n",
            "step: 30, loss: 0.0003655800537671894\n",
            "step: 40, loss: 0.0006017001578584313\n",
            "step: 50, loss: 0.0008269454119727015\n",
            "step: 60, loss: 0.0008932240889407694\n",
            "step: 70, loss: 0.0018135039135813713\n",
            "step: 80, loss: 0.0007139253430068493\n",
            "step: 90, loss: 0.05458999052643776\n",
            "step: 100, loss: 0.00038398877950385213\n",
            "step: 110, loss: 0.00025359413120895624\n",
            "step: 120, loss: 0.0007990880985744298\n",
            "step: 130, loss: 0.003990247845649719\n",
            "step: 140, loss: 0.0027435917872935534\n",
            "step: 150, loss: 0.022369874641299248\n",
            "step: 160, loss: 0.0033257801551371813\n",
            "step: 170, loss: 0.001556918490678072\n",
            "step: 180, loss: 0.0005262380582280457\n",
            "step: 190, loss: 0.0011414639884606004\n",
            "step: 200, loss: 0.0014956739032641053\n",
            "step: 210, loss: 0.017754197120666504\n",
            "step: 220, loss: 0.00885135680437088\n",
            "step: 230, loss: 0.004218411631882191\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9753363228699552, f1=0.9718785151856018, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0014380983775481582\n",
            "step: 10, loss: 0.0055404589511454105\n",
            "step: 20, loss: 0.004692419897764921\n",
            "step: 30, loss: 0.0011298615718260407\n",
            "step: 40, loss: 0.00143915053922683\n",
            "step: 50, loss: 0.0007032316061668098\n",
            "step: 60, loss: 0.00043803558219224215\n",
            "step: 70, loss: 0.003925844095647335\n",
            "step: 80, loss: 0.028674324974417686\n",
            "step: 90, loss: 0.0031111487187445164\n",
            "step: 100, loss: 0.0008546631434001029\n",
            "step: 110, loss: 0.0009150591213256121\n",
            "step: 120, loss: 0.0006658090278506279\n",
            "step: 130, loss: 0.0027102851308882236\n",
            "step: 140, loss: 0.0013762887101620436\n",
            "step: 150, loss: 0.22670066356658936\n",
            "step: 160, loss: 0.0011558022815734148\n",
            "step: 170, loss: 0.11508937180042267\n",
            "step: 180, loss: 0.002232031896710396\n",
            "step: 190, loss: 0.02351432479918003\n",
            "step: 200, loss: 0.010662727057933807\n",
            "step: 210, loss: 0.0020042159594595432\n",
            "step: 220, loss: 0.004585167393088341\n",
            "step: 230, loss: 0.0012475962284952402\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9786276715410572, f1=0.9752252252252253, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000589379109442234\n",
            "step: 10, loss: 0.0017885713605210185\n",
            "step: 20, loss: 0.00036936436663381755\n",
            "step: 30, loss: 0.0010469343978911638\n",
            "step: 40, loss: 0.0006707963184453547\n",
            "step: 50, loss: 0.0020221476443111897\n",
            "step: 60, loss: 0.0004670898197218776\n",
            "step: 70, loss: 0.052483294159173965\n",
            "step: 80, loss: 0.00017396641487721354\n",
            "step: 90, loss: 0.003373504616320133\n",
            "step: 100, loss: 0.0006214799359440804\n",
            "step: 110, loss: 0.047022659331560135\n",
            "step: 120, loss: 0.017768345773220062\n",
            "step: 130, loss: 0.0009865663014352322\n",
            "step: 140, loss: 0.0011084151919931173\n",
            "step: 150, loss: 0.002189673949033022\n",
            "step: 160, loss: 0.0009067679056897759\n",
            "step: 170, loss: 0.00034030008828267455\n",
            "step: 180, loss: 0.0003808217588812113\n",
            "step: 190, loss: 0.00024203417706303298\n",
            "step: 200, loss: 0.003368593053892255\n",
            "step: 210, loss: 0.0020797057077288628\n",
            "step: 220, loss: 0.0005536901298910379\n",
            "step: 230, loss: 0.0006665863329544663\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9766925638179801, f1=0.9799107142857142, best_f1=0.9752252252252253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004850992118008435\n",
            "step: 10, loss: 0.0006815621163696051\n",
            "step: 20, loss: 0.0010128193534910679\n",
            "step: 30, loss: 0.00019710618653334677\n",
            "step: 40, loss: 0.000981309567578137\n",
            "step: 50, loss: 0.00025376654230058193\n",
            "step: 60, loss: 0.001214535441249609\n",
            "step: 70, loss: 0.004026125650852919\n",
            "step: 80, loss: 0.0009550382965244353\n",
            "step: 90, loss: 0.0005548358312807977\n",
            "step: 100, loss: 0.0002159569994546473\n",
            "step: 110, loss: 0.007302450947463512\n",
            "step: 120, loss: 0.00045171717647463083\n",
            "step: 130, loss: 0.01136760413646698\n",
            "step: 140, loss: 0.0003226593544241041\n",
            "step: 150, loss: 0.0003015224647242576\n",
            "step: 160, loss: 5.0700968131422997e-05\n",
            "step: 170, loss: 0.00025401872699148953\n",
            "step: 180, loss: 0.004697847180068493\n",
            "step: 190, loss: 0.001032471191138029\n",
            "step: 200, loss: 0.0015313458861783147\n",
            "step: 210, loss: 0.010522786527872086\n",
            "step: 220, loss: 0.004408301319926977\n",
            "step: 230, loss: 0.000586775888223201\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9809203142536477, f1=0.9764837625979844, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 9.747833973960951e-05\n",
            "step: 10, loss: 0.0004909625858999789\n",
            "step: 20, loss: 0.0003142375499010086\n",
            "step: 30, loss: 0.0004180852265562862\n",
            "step: 40, loss: 4.200398689135909e-05\n",
            "step: 50, loss: 0.0022062521893531084\n",
            "step: 60, loss: 0.010743875056505203\n",
            "step: 70, loss: 0.0004935219185426831\n",
            "step: 80, loss: 0.0005578084383159876\n",
            "step: 90, loss: 0.23084521293640137\n",
            "step: 100, loss: 0.0005806732806377113\n",
            "step: 110, loss: 0.0005426335264928639\n",
            "step: 120, loss: 0.00036627304507419467\n",
            "step: 130, loss: 0.00027346808928996325\n",
            "step: 140, loss: 0.0027787250000983477\n",
            "step: 150, loss: 0.0004783739277627319\n",
            "step: 160, loss: 0.0006006642361171544\n",
            "step: 170, loss: 0.0005132118239998817\n",
            "step: 180, loss: 0.0004866914823651314\n",
            "step: 190, loss: 0.0001810014946386218\n",
            "step: 200, loss: 0.0008996751275844872\n",
            "step: 210, loss: 0.00017075489449780434\n",
            "step: 220, loss: 0.00025140694924630225\n",
            "step: 230, loss: 0.00024020786804612726\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9797297297297298, f1=0.9774774774774775, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001477016630815342\n",
            "step: 10, loss: 0.00017594904056750238\n",
            "step: 20, loss: 0.0005607671337202191\n",
            "step: 30, loss: 0.0072531793266534805\n",
            "step: 40, loss: 0.0007916066097095609\n",
            "step: 50, loss: 0.0003712291654665023\n",
            "step: 60, loss: 0.0002293606667080894\n",
            "step: 70, loss: 9.263752872357145e-05\n",
            "step: 80, loss: 5.251328184385784e-05\n",
            "step: 90, loss: 0.001764380489476025\n",
            "step: 100, loss: 0.00010748958447948098\n",
            "step: 110, loss: 7.661453128093854e-05\n",
            "step: 120, loss: 0.0009440597495995462\n",
            "step: 130, loss: 0.00039931462379172444\n",
            "step: 140, loss: 0.0003076926514040679\n",
            "step: 150, loss: 0.0002768302510958165\n",
            "step: 160, loss: 0.0008750429842621088\n",
            "step: 170, loss: 0.0007479758933186531\n",
            "step: 180, loss: 0.0005158359417691827\n",
            "step: 190, loss: 0.0007547999266535044\n",
            "step: 200, loss: 0.000220646194065921\n",
            "step: 210, loss: 0.012041430920362473\n",
            "step: 220, loss: 0.008551264181733131\n",
            "step: 230, loss: 0.001090615289285779\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9797752808988766, f1=0.9786276715410572, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00414681201800704\n",
            "step: 10, loss: 0.0012317619984969497\n",
            "step: 20, loss: 0.0004984884290024638\n",
            "step: 30, loss: 0.00016239157412201166\n",
            "step: 40, loss: 0.001957314321771264\n",
            "step: 50, loss: 0.002482266165316105\n",
            "step: 60, loss: 0.00020922553085256368\n",
            "step: 70, loss: 0.0024156083818525076\n",
            "step: 80, loss: 0.0031320438720285892\n",
            "step: 90, loss: 0.0002862394612748176\n",
            "step: 100, loss: 0.0002478977548889816\n",
            "step: 110, loss: 0.00462883897125721\n",
            "step: 120, loss: 0.000526243238709867\n",
            "step: 130, loss: 0.00017498880333732814\n",
            "step: 140, loss: 0.0001646863529458642\n",
            "step: 150, loss: 0.00012171210983069614\n",
            "step: 160, loss: 0.0004068997805006802\n",
            "step: 170, loss: 0.0005819122307002544\n",
            "step: 180, loss: 0.009709845297038555\n",
            "step: 190, loss: 0.00031647837022319436\n",
            "step: 200, loss: 3.9525646570837125e-05\n",
            "step: 210, loss: 0.0003022134187631309\n",
            "step: 220, loss: 0.00038914495962671936\n",
            "step: 230, loss: 0.0005692846025340259\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9718785151856018, f1=0.976324689966178, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003432160010561347\n",
            "step: 10, loss: 0.00019743003940675408\n",
            "step: 20, loss: 0.00018364496645517647\n",
            "step: 30, loss: 0.0004864833317697048\n",
            "step: 40, loss: 0.00025746162282302976\n",
            "step: 50, loss: 0.00010644095164025202\n",
            "step: 60, loss: 0.00011787388939410448\n",
            "step: 70, loss: 6.886095070512965e-05\n",
            "step: 80, loss: 0.0006498168804682791\n",
            "step: 90, loss: 0.004555969499051571\n",
            "step: 100, loss: 0.009916134178638458\n",
            "step: 110, loss: 0.0003030056250281632\n",
            "step: 120, loss: 7.49875107430853e-05\n",
            "step: 130, loss: 0.004201244562864304\n",
            "step: 140, loss: 0.0001964486436918378\n",
            "step: 150, loss: 0.00029504860867746174\n",
            "step: 160, loss: 0.0003678000357467681\n",
            "step: 170, loss: 0.0003209953720215708\n",
            "step: 180, loss: 0.00012812558270525187\n",
            "step: 190, loss: 6.462214514613152e-05\n",
            "step: 200, loss: 0.00026556081138551235\n",
            "step: 210, loss: 0.00010317224223399535\n",
            "step: 220, loss: 0.0001287998748011887\n",
            "step: 230, loss: 7.032915891613811e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9786276715410572, f1=0.9797752808988766, best_f1=0.9764837625979844\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00017790075798984617\n",
            "step: 10, loss: 0.0001526531996205449\n",
            "step: 20, loss: 0.0002153721870854497\n",
            "step: 30, loss: 0.00010920627391897142\n",
            "step: 40, loss: 9.001060243463144e-05\n",
            "step: 50, loss: 6.772070628358051e-05\n",
            "step: 60, loss: 0.0385112538933754\n",
            "step: 70, loss: 0.00034621459781192243\n",
            "step: 80, loss: 0.0003349040634930134\n",
            "step: 90, loss: 0.0001721002336125821\n",
            "step: 100, loss: 9.749310265760869e-05\n",
            "step: 110, loss: 0.0003608346451073885\n",
            "step: 120, loss: 0.0006750551401637495\n",
            "step: 130, loss: 0.00010599695087876171\n",
            "step: 140, loss: 0.00036941509461030364\n",
            "step: 150, loss: 0.00023000742658041418\n",
            "step: 160, loss: 0.007344904821366072\n",
            "step: 170, loss: 8.83882530615665e-05\n",
            "step: 180, loss: 0.0001057112094713375\n",
            "step: 190, loss: 0.0037163496017456055\n",
            "step: 200, loss: 0.0003467377391643822\n",
            "step: 210, loss: 0.0012332132318988442\n",
            "step: 220, loss: 7.765449117869139e-05\n",
            "step: 230, loss: 0.00010413404379505664\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9797297297297298, f1=0.9797752808988766, best_f1=0.9764837625979844\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 156.70it/s]\n",
            "load_f1 = 0.9820224719101124\n",
            "real_f1 = 0.978675645342312\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:29, 149.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "KYEeB2m-gw8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "lz2EPCvvgw8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee247bef-75e9-4ac4-adb6-f0a83e93ca23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.620115339756012\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4311714470386505\n",
            "step: 20, loss: 0.31964048743247986\n",
            "step: 30, loss: 0.37118226289749146\n",
            "step: 40, loss: 0.4153235852718353\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.873443603515625\n",
            "step: 60, loss: 0.36087894439697266\n",
            "step: 70, loss: 0.4752439856529236\n",
            "step: 80, loss: 0.5118817687034607\n",
            "step: 90, loss: 0.45755240321159363\n",
            "step: 100, loss: 0.5601931810379028\n",
            "step: 110, loss: 0.45906680822372437\n",
            "step: 120, loss: 0.6785778999328613\n",
            "step: 130, loss: 0.5236913561820984\n",
            "step: 140, loss: 0.3765549659729004\n",
            "step: 150, loss: 0.4394179582595825\n",
            "step: 160, loss: 0.47050443291664124\n",
            "step: 170, loss: 0.5487673878669739\n",
            "step: 180, loss: 0.31128355860710144\n",
            "step: 190, loss: 0.34980064630508423\n",
            "step: 200, loss: 0.21310926973819733\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 210, loss: 0.4138714373111725\n",
            "step: 220, loss: 0.3942685127258301\n",
            "step: 230, loss: 0.2746538519859314\n",
            "step: 240, loss: 0.20844529569149017\n",
            "step: 250, loss: 0.18245738744735718\n",
            "step: 260, loss: 0.388115793466568\n",
            "step: 270, loss: 0.1498035043478012\n",
            "step: 280, loss: 0.24851514399051666\n",
            "step: 290, loss: 0.29960882663726807\n",
            "step: 300, loss: 0.30501237511634827\n",
            "step: 310, loss: 0.20415829122066498\n",
            "step: 320, loss: 0.1586577147245407\n",
            "step: 330, loss: 0.1949453055858612\n",
            "step: 340, loss: 0.49898141622543335\n",
            "step: 350, loss: 0.42232924699783325\n",
            "step: 360, loss: 0.06626772880554199\n",
            "step: 370, loss: 0.2484072744846344\n",
            "step: 380, loss: 0.27292001247406006\n",
            "step: 390, loss: 0.08321573585271835\n",
            "step: 400, loss: 0.09386128187179565\n",
            "step: 410, loss: 0.2615313231945038\n",
            "step: 420, loss: 0.08552486449480057\n",
            "step: 430, loss: 0.27120092511177063\n",
            "step: 440, loss: 0.12420050799846649\n",
            "step: 450, loss: 0.09447978436946869\n",
            "step: 460, loss: 0.2026575654745102\n",
            "step: 470, loss: 0.10242994129657745\n",
            "step: 480, loss: 0.33627262711524963\n",
            "step: 490, loss: 0.2877534329891205\n",
            "step: 500, loss: 0.12446290254592896\n",
            "step: 510, loss: 0.06208726763725281\n",
            "step: 520, loss: 0.14762504398822784\n",
            "step: 530, loss: 0.20638899505138397\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.8873873873873874, f1=0.8899865289627301, best_f1=0.8899865289627301\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11062570661306381\n",
            "step: 10, loss: 0.07747289538383484\n",
            "step: 20, loss: 0.09498060494661331\n",
            "step: 30, loss: 0.15240968763828278\n",
            "step: 40, loss: 0.21682508289813995\n",
            "step: 50, loss: 0.19594542682170868\n",
            "step: 60, loss: 0.09502038359642029\n",
            "step: 70, loss: 0.09779280424118042\n",
            "step: 80, loss: 0.07480575144290924\n",
            "step: 90, loss: 0.04827239736914635\n",
            "step: 100, loss: 0.3128368556499481\n",
            "step: 110, loss: 0.13457246124744415\n",
            "step: 120, loss: 0.041529517620801926\n",
            "step: 130, loss: 0.009177293628454208\n",
            "step: 140, loss: 0.19070592522621155\n",
            "step: 150, loss: 0.046350881457328796\n",
            "step: 160, loss: 0.06986777484416962\n",
            "step: 170, loss: 0.06222371384501457\n",
            "step: 180, loss: 0.1551697850227356\n",
            "step: 190, loss: 0.08850813657045364\n",
            "step: 200, loss: 0.3604755997657776\n",
            "step: 210, loss: 0.041048564016819\n",
            "step: 220, loss: 0.044967252761125565\n",
            "step: 230, loss: 0.05399801954627037\n",
            "step: 240, loss: 0.17503654956817627\n",
            "step: 250, loss: 0.0753093957901001\n",
            "step: 260, loss: 0.20157799124717712\n",
            "step: 270, loss: 0.011128302663564682\n",
            "step: 280, loss: 0.13672053813934326\n",
            "step: 290, loss: 0.06535913795232773\n",
            "step: 300, loss: 0.07954385876655579\n",
            "step: 310, loss: 0.1119384691119194\n",
            "step: 320, loss: 0.08763442188501358\n",
            "step: 330, loss: 0.07240278273820877\n",
            "step: 340, loss: 0.16294580698013306\n",
            "step: 350, loss: 0.0019352161325514317\n",
            "step: 360, loss: 0.19842633605003357\n",
            "step: 370, loss: 0.03051593154668808\n",
            "step: 380, loss: 0.15420982241630554\n",
            "step: 390, loss: 0.06096306070685387\n",
            "step: 400, loss: 0.10515051335096359\n",
            "step: 410, loss: 0.034721747040748596\n",
            "step: 420, loss: 0.047930944710969925\n",
            "step: 430, loss: 0.08128444105386734\n",
            "step: 440, loss: 0.004836108069866896\n",
            "step: 450, loss: 0.0630631148815155\n",
            "step: 460, loss: 0.09784143418073654\n",
            "step: 470, loss: 0.08435256779193878\n",
            "step: 480, loss: 0.03853129222989082\n",
            "step: 490, loss: 0.09751105308532715\n",
            "step: 500, loss: 0.01757550984621048\n",
            "step: 510, loss: 0.06390825659036636\n",
            "step: 520, loss: 0.32448071241378784\n",
            "step: 530, loss: 0.18457920849323273\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9262564584311883, f1=0.9243856332703214, best_f1=0.9243856332703214\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1903192549943924\n",
            "step: 10, loss: 0.09118395298719406\n",
            "step: 20, loss: 0.018595103174448013\n",
            "step: 30, loss: 0.03843051567673683\n",
            "step: 40, loss: 0.059064775705337524\n",
            "step: 50, loss: 0.09321951866149902\n",
            "step: 60, loss: 0.042590558528900146\n",
            "step: 70, loss: 0.033908661454916\n",
            "step: 80, loss: 0.05000559613108635\n",
            "step: 90, loss: 0.012194342911243439\n",
            "step: 100, loss: 0.06109674274921417\n",
            "step: 110, loss: 0.033359345048666\n",
            "step: 120, loss: 0.29891330003738403\n",
            "step: 130, loss: 0.022645827382802963\n",
            "step: 140, loss: 0.03667449206113815\n",
            "step: 150, loss: 0.02195161022245884\n",
            "step: 160, loss: 0.01576647348701954\n",
            "step: 170, loss: 0.003685307689011097\n",
            "step: 180, loss: 0.0029649685602635145\n",
            "step: 190, loss: 0.013046114705502987\n",
            "step: 200, loss: 0.02414325810968876\n",
            "step: 210, loss: 0.04126037657260895\n",
            "step: 220, loss: 0.1918879598379135\n",
            "step: 230, loss: 0.020340463146567345\n",
            "step: 240, loss: 0.1917167454957962\n",
            "step: 250, loss: 0.31999117136001587\n",
            "step: 260, loss: 0.07838902622461319\n",
            "step: 270, loss: 0.037262026220560074\n",
            "step: 280, loss: 0.014096815139055252\n",
            "step: 290, loss: 0.010436318814754486\n",
            "step: 300, loss: 0.12024743854999542\n",
            "step: 310, loss: 0.1187124028801918\n",
            "step: 320, loss: 0.010587264783680439\n",
            "step: 330, loss: 0.0205099955201149\n",
            "step: 340, loss: 0.02322164922952652\n",
            "step: 350, loss: 0.08004424721002579\n",
            "step: 360, loss: 0.02680172026157379\n",
            "step: 370, loss: 0.08683045208454132\n",
            "step: 380, loss: 0.013790959492325783\n",
            "step: 390, loss: 0.03671788051724434\n",
            "step: 400, loss: 0.22413082420825958\n",
            "step: 410, loss: 0.19509388506412506\n",
            "step: 420, loss: 0.17105814814567566\n",
            "step: 430, loss: 0.14976248145103455\n",
            "step: 440, loss: 0.1711910516023636\n",
            "step: 450, loss: 0.0277034230530262\n",
            "step: 460, loss: 0.11984537541866302\n",
            "step: 470, loss: 0.09056513756513596\n",
            "step: 480, loss: 0.05698157101869583\n",
            "step: 490, loss: 0.03825126588344574\n",
            "step: 500, loss: 0.0403473936021328\n",
            "step: 510, loss: 0.04397078603506088\n",
            "step: 520, loss: 0.01773415505886078\n",
            "step: 530, loss: 0.036605775356292725\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9364055299539169, f1=0.9338805289557685, best_f1=0.9338805289557685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021809175610542297\n",
            "step: 10, loss: 0.008349471725523472\n",
            "step: 20, loss: 0.10056591033935547\n",
            "step: 30, loss: 0.1217704564332962\n",
            "step: 40, loss: 0.11159799993038177\n",
            "step: 50, loss: 0.0997292697429657\n",
            "step: 60, loss: 0.015915581956505775\n",
            "step: 70, loss: 0.07169957458972931\n",
            "step: 80, loss: 0.15728643536567688\n",
            "step: 90, loss: 0.08188606053590775\n",
            "step: 100, loss: 0.0032182075083255768\n",
            "step: 110, loss: 0.15593191981315613\n",
            "step: 120, loss: 0.0367002971470356\n",
            "step: 130, loss: 0.07446243613958359\n",
            "step: 140, loss: 0.04683218151330948\n",
            "step: 150, loss: 0.021707482635974884\n",
            "step: 160, loss: 0.020772811025381088\n",
            "step: 170, loss: 0.03198597580194473\n",
            "step: 180, loss: 0.04885441064834595\n",
            "step: 190, loss: 0.15189315378665924\n",
            "step: 200, loss: 0.10126112401485443\n",
            "step: 210, loss: 0.01151012908667326\n",
            "step: 220, loss: 0.007686942350119352\n",
            "step: 230, loss: 0.0365670807659626\n",
            "step: 240, loss: 0.1606374979019165\n",
            "step: 250, loss: 0.07137513160705566\n",
            "step: 260, loss: 0.008190568536520004\n",
            "step: 270, loss: 0.14485877752304077\n",
            "step: 280, loss: 0.009584451094269753\n",
            "step: 290, loss: 0.1351378709077835\n",
            "step: 300, loss: 0.006096694152802229\n",
            "step: 310, loss: 0.009592791087925434\n",
            "step: 320, loss: 0.1704380214214325\n",
            "step: 330, loss: 0.04238790273666382\n",
            "step: 340, loss: 0.04042026400566101\n",
            "step: 350, loss: 0.10411839932203293\n",
            "step: 360, loss: 0.08602417260408401\n",
            "step: 370, loss: 0.01646536774933338\n",
            "step: 380, loss: 0.007233472540974617\n",
            "step: 390, loss: 0.005138942506164312\n",
            "step: 400, loss: 0.08137886226177216\n",
            "step: 410, loss: 0.00313959876075387\n",
            "step: 420, loss: 0.016772283241152763\n",
            "step: 430, loss: 0.010598125867545605\n",
            "step: 440, loss: 0.014788525179028511\n",
            "step: 450, loss: 0.08730486035346985\n",
            "step: 460, loss: 0.027043547481298447\n",
            "step: 470, loss: 0.008525418117642403\n",
            "step: 480, loss: 0.017309950664639473\n",
            "step: 490, loss: 0.0066529554314911366\n",
            "step: 500, loss: 0.03479648754000664\n",
            "step: 510, loss: 0.06600208580493927\n",
            "step: 520, loss: 0.013158275745809078\n",
            "step: 530, loss: 0.2868507504463196\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9417163836622304, f1=0.9415554532903819, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016591105610132217\n",
            "step: 10, loss: 0.013090177439153194\n",
            "step: 20, loss: 0.0028352041263133287\n",
            "step: 30, loss: 0.08954416215419769\n",
            "step: 40, loss: 0.00231374753639102\n",
            "step: 50, loss: 0.10049070417881012\n",
            "step: 60, loss: 0.06274169683456421\n",
            "step: 70, loss: 0.014099126681685448\n",
            "step: 80, loss: 0.01468626968562603\n",
            "step: 90, loss: 0.13205227255821228\n",
            "step: 100, loss: 0.12837809324264526\n",
            "step: 110, loss: 0.050516288727521896\n",
            "step: 120, loss: 0.16950589418411255\n",
            "step: 130, loss: 0.012788519263267517\n",
            "step: 140, loss: 0.01332942582666874\n",
            "step: 150, loss: 0.006788263563066721\n",
            "step: 160, loss: 0.03826943412423134\n",
            "step: 170, loss: 0.017883814871311188\n",
            "step: 180, loss: 0.0045899031683802605\n",
            "step: 190, loss: 0.009321051649749279\n",
            "step: 200, loss: 0.10454417765140533\n",
            "step: 210, loss: 0.004181544296443462\n",
            "step: 220, loss: 0.02439878135919571\n",
            "step: 230, loss: 0.00548263406381011\n",
            "step: 240, loss: 0.026324747130274773\n",
            "step: 250, loss: 0.11446328461170197\n",
            "step: 260, loss: 0.004196288529783487\n",
            "step: 270, loss: 0.01038012932986021\n",
            "step: 280, loss: 0.06164684146642685\n",
            "step: 290, loss: 0.010100960731506348\n",
            "step: 300, loss: 0.15399505198001862\n",
            "step: 310, loss: 0.01975315436720848\n",
            "step: 320, loss: 0.10164867341518402\n",
            "step: 330, loss: 0.007025393191725016\n",
            "step: 340, loss: 0.03393206372857094\n",
            "step: 350, loss: 0.002754047978669405\n",
            "step: 360, loss: 0.032935552299022675\n",
            "step: 370, loss: 0.003494834993034601\n",
            "step: 380, loss: 0.0010895247105509043\n",
            "step: 390, loss: 0.19983386993408203\n",
            "step: 400, loss: 0.0086705032736063\n",
            "step: 410, loss: 0.0904918909072876\n",
            "step: 420, loss: 0.29228121042251587\n",
            "step: 430, loss: 0.03949315845966339\n",
            "step: 440, loss: 0.017964787781238556\n",
            "step: 450, loss: 0.015166228637099266\n",
            "step: 460, loss: 0.04770543798804283\n",
            "step: 470, loss: 0.03410873934626579\n",
            "step: 480, loss: 0.003875946393236518\n",
            "step: 490, loss: 0.10742808133363724\n",
            "step: 500, loss: 0.12592282891273499\n",
            "step: 510, loss: 0.019307613372802734\n",
            "step: 520, loss: 0.14442600309848785\n",
            "step: 530, loss: 0.03303230181336403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.935602575896964, f1=0.9293954776188279, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.07583492994308472\n",
            "step: 10, loss: 0.001313654938712716\n",
            "step: 20, loss: 0.08604860305786133\n",
            "step: 30, loss: 0.0009305658750236034\n",
            "step: 40, loss: 0.02747304178774357\n",
            "step: 50, loss: 0.005726574454456568\n",
            "step: 60, loss: 0.0083832498639822\n",
            "step: 70, loss: 0.028390642255544662\n",
            "step: 80, loss: 0.0016997656784951687\n",
            "step: 90, loss: 0.1895502507686615\n",
            "step: 100, loss: 0.01884690299630165\n",
            "step: 110, loss: 0.0490533821284771\n",
            "step: 120, loss: 0.10714489966630936\n",
            "step: 130, loss: 0.006592228543013334\n",
            "step: 140, loss: 0.012318931519985199\n",
            "step: 150, loss: 0.014669078402221203\n",
            "step: 160, loss: 0.17168529331684113\n",
            "step: 170, loss: 0.002844876842573285\n",
            "step: 180, loss: 0.04938913509249687\n",
            "step: 190, loss: 0.08938241004943848\n",
            "step: 200, loss: 0.02964102104306221\n",
            "step: 210, loss: 0.01346464641392231\n",
            "step: 220, loss: 0.01240122877061367\n",
            "step: 230, loss: 0.003172086551785469\n",
            "step: 240, loss: 0.006138928234577179\n",
            "step: 250, loss: 0.09828823804855347\n",
            "step: 260, loss: 0.013245129957795143\n",
            "step: 270, loss: 0.003712789388373494\n",
            "step: 280, loss: 0.3612387180328369\n",
            "step: 290, loss: 0.03085186704993248\n",
            "step: 300, loss: 0.1044096127152443\n",
            "step: 310, loss: 0.10366939753293991\n",
            "step: 320, loss: 0.060692399740219116\n",
            "step: 330, loss: 0.060437895357608795\n",
            "step: 340, loss: 0.0015753858024254441\n",
            "step: 350, loss: 0.03836807608604431\n",
            "step: 360, loss: 0.016767559573054314\n",
            "step: 370, loss: 0.0029313932172954082\n",
            "step: 380, loss: 0.009838273748755455\n",
            "step: 390, loss: 0.00489457743242383\n",
            "step: 400, loss: 0.004069631453603506\n",
            "step: 410, loss: 0.0010654703946784139\n",
            "step: 420, loss: 0.10477600246667862\n",
            "step: 430, loss: 0.027260569855570793\n",
            "step: 440, loss: 0.005856361240148544\n",
            "step: 450, loss: 0.30807021260261536\n",
            "step: 460, loss: 0.0063782064244151115\n",
            "step: 470, loss: 0.017063230276107788\n",
            "step: 480, loss: 0.016350559890270233\n",
            "step: 490, loss: 0.017181817442178726\n",
            "step: 500, loss: 0.00832400657236576\n",
            "step: 510, loss: 0.18241478502750397\n",
            "step: 520, loss: 0.0018365789437666535\n",
            "step: 530, loss: 0.05445875972509384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9395604395604396, f1=0.9358560221504384, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0033834530040621758\n",
            "step: 10, loss: 0.003300223732367158\n",
            "step: 20, loss: 0.012116929516196251\n",
            "step: 30, loss: 0.013275359757244587\n",
            "step: 40, loss: 0.004163894336670637\n",
            "step: 50, loss: 0.003690431360155344\n",
            "step: 60, loss: 0.004850556142628193\n",
            "step: 70, loss: 0.002012822777032852\n",
            "step: 80, loss: 0.0010235507506877184\n",
            "step: 90, loss: 0.0003651638689916581\n",
            "step: 100, loss: 0.0010031969286501408\n",
            "step: 110, loss: 0.0010051121935248375\n",
            "step: 120, loss: 0.005091742612421513\n",
            "step: 130, loss: 0.0012376424856483936\n",
            "step: 140, loss: 0.0011677740840241313\n",
            "step: 150, loss: 0.0076930648647248745\n",
            "step: 160, loss: 0.000865665206220001\n",
            "step: 170, loss: 0.0022765775211155415\n",
            "step: 180, loss: 0.0017651169328019023\n",
            "step: 190, loss: 0.05549779161810875\n",
            "step: 200, loss: 0.001246499246917665\n",
            "step: 210, loss: 0.03533460199832916\n",
            "step: 220, loss: 0.002543083392083645\n",
            "step: 230, loss: 0.004120625089854002\n",
            "step: 240, loss: 0.02907324768602848\n",
            "step: 250, loss: 0.022296089679002762\n",
            "step: 260, loss: 0.015981871634721756\n",
            "step: 270, loss: 0.0037525747902691364\n",
            "step: 280, loss: 0.0061273882165551186\n",
            "step: 290, loss: 0.007186051458120346\n",
            "step: 300, loss: 0.003355608554556966\n",
            "step: 310, loss: 0.0013554181205108762\n",
            "step: 320, loss: 0.015397903509438038\n",
            "step: 330, loss: 0.04188456013798714\n",
            "step: 340, loss: 0.0036382062826305628\n",
            "step: 350, loss: 0.014701643027365208\n",
            "step: 360, loss: 0.010465804487466812\n",
            "step: 370, loss: 0.003199332393705845\n",
            "step: 380, loss: 0.0045487224124372005\n",
            "step: 390, loss: 0.01719762571156025\n",
            "step: 400, loss: 0.018688786774873734\n",
            "step: 410, loss: 0.003584792371839285\n",
            "step: 420, loss: 0.03142545372247696\n",
            "step: 430, loss: 0.012355919927358627\n",
            "step: 440, loss: 0.02696460671722889\n",
            "step: 450, loss: 0.012600237503647804\n",
            "step: 460, loss: 0.004672457929700613\n",
            "step: 470, loss: 0.1627059131860733\n",
            "step: 480, loss: 0.0014130235649645329\n",
            "step: 490, loss: 0.04284512624144554\n",
            "step: 500, loss: 0.0066431015729904175\n",
            "step: 510, loss: 0.00040187835111282766\n",
            "step: 520, loss: 0.0013381782919168472\n",
            "step: 530, loss: 0.0079928208142519\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9410664172123478, f1=0.9367924528301887, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011279018362984061\n",
            "step: 10, loss: 0.0014962858986109495\n",
            "step: 20, loss: 0.0008744727238081396\n",
            "step: 30, loss: 0.007873956114053726\n",
            "step: 40, loss: 0.0007362234173342586\n",
            "step: 50, loss: 0.018825329840183258\n",
            "step: 60, loss: 0.0006893634563311934\n",
            "step: 70, loss: 0.032606981694698334\n",
            "step: 80, loss: 0.00454552099108696\n",
            "step: 90, loss: 0.0006607325631193817\n",
            "step: 100, loss: 0.0015633394941687584\n",
            "step: 110, loss: 0.0020495187491178513\n",
            "step: 120, loss: 0.0007348290528170764\n",
            "step: 130, loss: 0.009361767210066319\n",
            "step: 140, loss: 0.00017747646779753268\n",
            "step: 150, loss: 0.00302584795281291\n",
            "step: 160, loss: 0.004616708494722843\n",
            "step: 170, loss: 0.025346355512738228\n",
            "step: 180, loss: 0.003849806496873498\n",
            "step: 190, loss: 0.07483373582363129\n",
            "step: 200, loss: 0.011856251396238804\n",
            "step: 210, loss: 0.031346652656793594\n",
            "step: 220, loss: 0.001578828669153154\n",
            "step: 230, loss: 0.10314050316810608\n",
            "step: 240, loss: 0.0012338242959231138\n",
            "step: 250, loss: 0.022657720372080803\n",
            "step: 260, loss: 0.004154559690505266\n",
            "step: 270, loss: 0.008133409544825554\n",
            "step: 280, loss: 0.00048491384950466454\n",
            "step: 290, loss: 0.005407269112765789\n",
            "step: 300, loss: 0.0024490824434906244\n",
            "step: 310, loss: 0.008500608615577221\n",
            "step: 320, loss: 0.010218190960586071\n",
            "step: 330, loss: 0.003172549419105053\n",
            "step: 340, loss: 0.20896556973457336\n",
            "step: 350, loss: 0.0010341386077925563\n",
            "step: 360, loss: 0.042944394052028656\n",
            "step: 370, loss: 0.2518560290336609\n",
            "step: 380, loss: 0.0015871860086917877\n",
            "step: 390, loss: 0.054266925901174545\n",
            "step: 400, loss: 0.0015840455889701843\n",
            "step: 410, loss: 0.0033459002152085304\n",
            "step: 420, loss: 0.003892812877893448\n",
            "step: 430, loss: 0.008917039260268211\n",
            "step: 440, loss: 0.037978313863277435\n",
            "step: 450, loss: 0.0009825811721384525\n",
            "step: 460, loss: 0.013316957280039787\n",
            "step: 470, loss: 0.04744335636496544\n",
            "step: 480, loss: 0.030284063890576363\n",
            "step: 490, loss: 0.007468405179679394\n",
            "step: 500, loss: 0.0011005999986082315\n",
            "step: 510, loss: 0.012705945409834385\n",
            "step: 520, loss: 0.0007815302815288305\n",
            "step: 530, loss: 0.0014081599656492472\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9389454209065681, f1=0.9343200740055504, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003078277222812176\n",
            "step: 10, loss: 0.005519408732652664\n",
            "step: 20, loss: 0.0010497808689251542\n",
            "step: 30, loss: 0.08004510402679443\n",
            "step: 40, loss: 0.00658252090215683\n",
            "step: 50, loss: 0.002523270668461919\n",
            "step: 60, loss: 0.004355372861027718\n",
            "step: 70, loss: 0.017767934128642082\n",
            "step: 80, loss: 0.19054678082466125\n",
            "step: 90, loss: 0.02074185572564602\n",
            "step: 100, loss: 0.011609543114900589\n",
            "step: 110, loss: 0.10284744203090668\n",
            "step: 120, loss: 0.01012219674885273\n",
            "step: 130, loss: 0.006944979541003704\n",
            "step: 140, loss: 0.006306488532572985\n",
            "step: 150, loss: 0.001864197081886232\n",
            "step: 160, loss: 0.0014631727244704962\n",
            "step: 170, loss: 0.014303985983133316\n",
            "step: 180, loss: 0.004249137826263905\n",
            "step: 190, loss: 0.005100009962916374\n",
            "step: 200, loss: 0.0008990836795419455\n",
            "step: 210, loss: 0.00442300271242857\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 220, loss: 0.039825085550546646\n",
            "step: 230, loss: 0.00766243739053607\n",
            "step: 240, loss: 0.0015307711437344551\n",
            "step: 250, loss: 0.036166705191135406\n",
            "step: 260, loss: 0.002482626587152481\n",
            "step: 270, loss: 0.002911878051236272\n",
            "step: 280, loss: 0.0019110421417281032\n",
            "step: 290, loss: 0.0014446568675339222\n",
            "step: 300, loss: 0.0409519299864769\n",
            "step: 310, loss: 0.0852041095495224\n",
            "step: 320, loss: 0.0040350970812141895\n",
            "step: 330, loss: 0.0011423738906159997\n",
            "step: 340, loss: 0.027526946738362312\n",
            "step: 350, loss: 0.021209094673395157\n",
            "step: 360, loss: 0.0009392725769430399\n",
            "step: 370, loss: 0.018767068162560463\n",
            "step: 380, loss: 0.0006128295208327472\n",
            "step: 390, loss: 0.0002824528201017529\n",
            "step: 400, loss: 0.0006379141705110669\n",
            "step: 410, loss: 0.0036814615596085787\n",
            "step: 420, loss: 0.05024314671754837\n",
            "step: 430, loss: 0.0009043617756105959\n",
            "step: 440, loss: 0.0013052936410531402\n",
            "step: 450, loss: 0.07552976906299591\n",
            "step: 460, loss: 0.0011985008604824543\n",
            "step: 470, loss: 0.08134125918149948\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 480, loss: 0.009641336277127266\n",
            "step: 490, loss: 0.05077467858791351\n",
            "step: 500, loss: 0.0008256454020738602\n",
            "step: 510, loss: 0.16296496987342834\n",
            "step: 520, loss: 0.016931304708123207\n",
            "step: 530, loss: 0.10626773536205292\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.937962962962963, f1=0.9324074074074072, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04906793683767319\n",
            "step: 10, loss: 0.00287756253965199\n",
            "step: 20, loss: 0.007539424579590559\n",
            "step: 30, loss: 0.06392665207386017\n",
            "step: 40, loss: 0.0004820904287043959\n",
            "step: 50, loss: 0.003284689737483859\n",
            "step: 60, loss: 0.004142592195421457\n",
            "step: 70, loss: 0.00250441487878561\n",
            "step: 80, loss: 0.00048749265260994434\n",
            "step: 90, loss: 0.0006269093719311059\n",
            "step: 100, loss: 0.0034182171802967787\n",
            "step: 110, loss: 0.006890783552080393\n",
            "step: 120, loss: 0.0004560283268801868\n",
            "step: 130, loss: 0.0005315706948749721\n",
            "step: 140, loss: 0.0027985484339296818\n",
            "step: 150, loss: 0.00786300003528595\n",
            "step: 160, loss: 0.00862416997551918\n",
            "step: 170, loss: 0.006375395692884922\n",
            "step: 180, loss: 0.03938954323530197\n",
            "step: 190, loss: 0.004402631893754005\n",
            "step: 200, loss: 0.00559247238561511\n",
            "step: 210, loss: 0.006996903568506241\n",
            "step: 220, loss: 0.001382417161948979\n",
            "step: 230, loss: 0.0015742782270535827\n",
            "step: 240, loss: 0.004708945285528898\n",
            "step: 250, loss: 0.003986488562077284\n",
            "step: 260, loss: 0.01679806411266327\n",
            "step: 270, loss: 0.001586925471201539\n",
            "step: 280, loss: 0.06794743239879608\n",
            "step: 290, loss: 0.0008239576127380133\n",
            "step: 300, loss: 0.0026861780788749456\n",
            "step: 310, loss: 0.012498142197728157\n",
            "step: 320, loss: 0.008243697695434093\n",
            "step: 330, loss: 0.00961526483297348\n",
            "step: 340, loss: 0.013387657701969147\n",
            "step: 350, loss: 0.007882033474743366\n",
            "step: 360, loss: 0.0006312616169452667\n",
            "step: 370, loss: 0.004110563546419144\n",
            "step: 380, loss: 0.0025605657137930393\n",
            "step: 390, loss: 0.0017255195416510105\n",
            "step: 400, loss: 0.00261926488019526\n",
            "step: 410, loss: 0.002268081996589899\n",
            "step: 420, loss: 0.0016267335740849376\n",
            "step: 430, loss: 0.0011677963193506002\n",
            "step: 440, loss: 0.00458911806344986\n",
            "step: 450, loss: 0.011162519454956055\n",
            "step: 460, loss: 0.003583453595638275\n",
            "step: 470, loss: 0.005406326614320278\n",
            "step: 480, loss: 0.004217417445033789\n",
            "step: 490, loss: 0.0007011371199041605\n",
            "step: 500, loss: 0.014673775061964989\n",
            "step: 510, loss: 0.0006202482036314905\n",
            "step: 520, loss: 0.047305796295404434\n",
            "step: 530, loss: 0.016169199720025063\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9413953488372093, f1=0.9393090569561158, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005823746905662119\n",
            "step: 10, loss: 0.000523843860719353\n",
            "step: 20, loss: 0.0005126986070536077\n",
            "step: 30, loss: 0.0006940575549378991\n",
            "step: 40, loss: 0.00037494118441827595\n",
            "step: 50, loss: 0.0003723041736520827\n",
            "step: 60, loss: 0.00023922241234686226\n",
            "step: 70, loss: 0.0010346429189667106\n",
            "step: 80, loss: 0.005117656663060188\n",
            "step: 90, loss: 0.0006841076537966728\n",
            "step: 100, loss: 0.001912760315462947\n",
            "step: 110, loss: 0.0014341720379889011\n",
            "step: 120, loss: 0.0013383063487708569\n",
            "step: 130, loss: 0.0007065167301334441\n",
            "step: 140, loss: 0.000672797323204577\n",
            "step: 150, loss: 0.0006408043554984033\n",
            "step: 160, loss: 0.0004737004346679896\n",
            "step: 170, loss: 0.0004101844097021967\n",
            "step: 180, loss: 0.0007849276880733669\n",
            "step: 190, loss: 0.0010126701090484858\n",
            "step: 200, loss: 0.0006976303993724287\n",
            "step: 210, loss: 0.005955908913165331\n",
            "step: 220, loss: 0.051632050424814224\n",
            "step: 230, loss: 0.0009289452573284507\n",
            "step: 240, loss: 0.016429666429758072\n",
            "step: 250, loss: 0.000794425664935261\n",
            "step: 260, loss: 0.012510204687714577\n",
            "step: 270, loss: 0.017591720446944237\n",
            "step: 280, loss: 0.0046396818943321705\n",
            "step: 290, loss: 0.0004156610812060535\n",
            "step: 300, loss: 0.0032960507087409496\n",
            "step: 310, loss: 0.1351713389158249\n",
            "step: 320, loss: 0.0034330347552895546\n",
            "step: 330, loss: 0.00519280880689621\n",
            "step: 340, loss: 0.12164704501628876\n",
            "step: 350, loss: 0.003222484840080142\n",
            "step: 360, loss: 0.03741230070590973\n",
            "step: 370, loss: 0.002262558788061142\n",
            "step: 380, loss: 0.005474877078086138\n",
            "step: 390, loss: 0.023247040808200836\n",
            "step: 400, loss: 0.03988344594836235\n",
            "step: 410, loss: 0.0004442122299224138\n",
            "step: 420, loss: 0.00027656639576889575\n",
            "step: 430, loss: 0.01256740465760231\n",
            "step: 440, loss: 0.0014406356494873762\n",
            "step: 450, loss: 0.012605019845068455\n",
            "step: 460, loss: 0.13426043093204498\n",
            "step: 470, loss: 0.0024661605712026358\n",
            "step: 480, loss: 0.0013525960966944695\n",
            "step: 490, loss: 0.024168651551008224\n",
            "step: 500, loss: 0.0016133161261677742\n",
            "step: 510, loss: 0.16740287840366364\n",
            "step: 520, loss: 0.007021278142929077\n",
            "step: 530, loss: 0.015561302192509174\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9366130558183537, f1=0.9292161520190024, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0016689012991264462\n",
            "step: 10, loss: 0.0010071925353258848\n",
            "step: 20, loss: 0.003251082729548216\n",
            "step: 30, loss: 0.003356020199134946\n",
            "step: 40, loss: 0.0023511196486651897\n",
            "step: 50, loss: 0.0006876944680698216\n",
            "step: 60, loss: 0.0011000022059306502\n",
            "step: 70, loss: 0.0021100633312016726\n",
            "step: 80, loss: 0.1886577308177948\n",
            "step: 90, loss: 0.007049834355711937\n",
            "step: 100, loss: 0.18128031492233276\n",
            "step: 110, loss: 0.001168899005278945\n",
            "step: 120, loss: 0.0024238084442913532\n",
            "step: 130, loss: 0.0024719845969229937\n",
            "step: 140, loss: 0.0015911223599687219\n",
            "step: 150, loss: 0.0027475778479129076\n",
            "step: 160, loss: 0.00204587634652853\n",
            "step: 170, loss: 0.0009773229248821735\n",
            "step: 180, loss: 0.0011672666296362877\n",
            "step: 190, loss: 0.0015337271615862846\n",
            "step: 200, loss: 0.000917085213586688\n",
            "step: 210, loss: 0.008529379963874817\n",
            "step: 220, loss: 0.0013468759134411812\n",
            "step: 230, loss: 0.0009833935182541609\n",
            "step: 240, loss: 0.00830388069152832\n",
            "step: 250, loss: 0.0014681443572044373\n",
            "step: 260, loss: 0.0009922375902533531\n",
            "step: 270, loss: 0.002077815355733037\n",
            "step: 280, loss: 0.0014631027588620782\n",
            "step: 290, loss: 0.0044363937340676785\n",
            "step: 300, loss: 0.03583824634552002\n",
            "step: 310, loss: 0.0008683057385496795\n",
            "step: 320, loss: 0.04128827527165413\n",
            "step: 330, loss: 0.002451017964631319\n",
            "step: 340, loss: 0.0013475617161020637\n",
            "step: 350, loss: 0.0003412748337723315\n",
            "step: 360, loss: 0.008140305057168007\n",
            "step: 370, loss: 0.00034883455373346806\n",
            "step: 380, loss: 0.0006348313763737679\n",
            "step: 390, loss: 0.0036149185616523027\n",
            "step: 400, loss: 0.0003921906172763556\n",
            "step: 410, loss: 0.0037175181787461042\n",
            "step: 420, loss: 0.0008244586060754955\n",
            "step: 430, loss: 0.051635753363370895\n",
            "step: 440, loss: 0.0009634579182602465\n",
            "step: 450, loss: 0.000694962334819138\n",
            "step: 460, loss: 0.0011996423127129674\n",
            "step: 470, loss: 0.0013294161763042212\n",
            "step: 480, loss: 0.0006359133985824883\n",
            "step: 490, loss: 0.001525519648566842\n",
            "step: 500, loss: 0.003011277411133051\n",
            "step: 510, loss: 0.0019832453690469265\n",
            "step: 520, loss: 0.0022234790958464146\n",
            "step: 530, loss: 0.002360390033572912\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9391955098222639, f1=0.9378794955628212, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012053760001435876\n",
            "step: 10, loss: 0.0006678706267848611\n",
            "step: 20, loss: 0.0008358751074410975\n",
            "step: 30, loss: 0.0006113637937232852\n",
            "step: 40, loss: 0.0013092011213302612\n",
            "step: 50, loss: 0.002098648576065898\n",
            "step: 60, loss: 0.0005612092209048569\n",
            "step: 70, loss: 0.0008079408435150981\n",
            "step: 80, loss: 0.001194517477415502\n",
            "step: 90, loss: 0.00802051555365324\n",
            "step: 100, loss: 0.0003848467895295471\n",
            "step: 110, loss: 0.00023320868785958737\n",
            "step: 120, loss: 0.0010269915219396353\n",
            "step: 130, loss: 0.00029709673253819346\n",
            "step: 140, loss: 0.0004071070288773626\n",
            "step: 150, loss: 0.0007041766657494009\n",
            "step: 160, loss: 0.0012171607231721282\n",
            "step: 170, loss: 0.016963399946689606\n",
            "step: 180, loss: 0.0005066000740043819\n",
            "step: 190, loss: 0.00040693345363251865\n",
            "step: 200, loss: 0.0003729159652721137\n",
            "step: 210, loss: 0.0007765774498693645\n",
            "step: 220, loss: 0.0005180197185836732\n",
            "step: 230, loss: 0.0009623540681786835\n",
            "step: 240, loss: 0.044996533542871475\n",
            "step: 250, loss: 0.0008605021284893155\n",
            "step: 260, loss: 0.0007920475327409804\n",
            "step: 270, loss: 0.05069535970687866\n",
            "step: 280, loss: 0.07792457193136215\n",
            "step: 290, loss: 0.0019220003159716725\n",
            "step: 300, loss: 0.0007225267472676933\n",
            "step: 310, loss: 0.0005473473574966192\n",
            "step: 320, loss: 0.0004746364429593086\n",
            "step: 330, loss: 0.0008849063306115568\n",
            "step: 340, loss: 0.000749381142668426\n",
            "step: 350, loss: 0.0003407746844459325\n",
            "step: 360, loss: 0.08996599167585373\n",
            "step: 370, loss: 0.002232619561254978\n",
            "step: 380, loss: 0.00048222194891422987\n",
            "step: 390, loss: 0.0011339070042595267\n",
            "step: 400, loss: 0.0013628537999466062\n",
            "step: 410, loss: 0.009482457302510738\n",
            "step: 420, loss: 0.0022071211133152246\n",
            "step: 430, loss: 0.0010441811755299568\n",
            "step: 440, loss: 0.0003862013982143253\n",
            "step: 450, loss: 0.0007953918538987637\n",
            "step: 460, loss: 0.0028176961932331324\n",
            "step: 470, loss: 0.0008862917311489582\n",
            "step: 480, loss: 0.00023212826636154205\n",
            "step: 490, loss: 0.0005622722092084587\n",
            "step: 500, loss: 0.00167655770201236\n",
            "step: 510, loss: 0.0004302806919440627\n",
            "step: 520, loss: 0.0008292876882478595\n",
            "step: 530, loss: 0.0004408714594319463\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9401947148817803, f1=0.9401947148817803, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005725410883314908\n",
            "step: 10, loss: 0.00044340186286717653\n",
            "step: 20, loss: 0.0003778923419304192\n",
            "step: 30, loss: 0.0010669329203665257\n",
            "step: 40, loss: 0.000803831615485251\n",
            "step: 50, loss: 0.18976500630378723\n",
            "step: 60, loss: 0.0011795349419116974\n",
            "step: 70, loss: 0.002899534534662962\n",
            "step: 80, loss: 0.0012996889417991042\n",
            "step: 90, loss: 0.00047620595432817936\n",
            "step: 100, loss: 0.0006650651339441538\n",
            "step: 110, loss: 0.0007223517750389874\n",
            "step: 120, loss: 0.0006801376584917307\n",
            "step: 130, loss: 0.0007377103320322931\n",
            "step: 140, loss: 0.0012169118272140622\n",
            "step: 150, loss: 0.0010893334401771426\n",
            "step: 160, loss: 0.0009089026134461164\n",
            "step: 170, loss: 0.002542782574892044\n",
            "step: 180, loss: 0.0009990675607696176\n",
            "step: 190, loss: 0.0022442208137363195\n",
            "step: 200, loss: 0.0007688028272241354\n",
            "step: 210, loss: 0.10068739950656891\n",
            "step: 220, loss: 0.00036619516322389245\n",
            "step: 230, loss: 0.0004349799710325897\n",
            "step: 240, loss: 0.0005866821156814694\n",
            "step: 250, loss: 0.0010163176339119673\n",
            "step: 260, loss: 0.00045101065188646317\n",
            "step: 270, loss: 0.0013106069527566433\n",
            "step: 280, loss: 0.00048140756553038955\n",
            "step: 290, loss: 0.00043335993541404605\n",
            "step: 300, loss: 0.0006685142870992422\n",
            "step: 310, loss: 0.0006816714303568006\n",
            "step: 320, loss: 0.0005884146085008979\n",
            "step: 330, loss: 0.0012344351271167397\n",
            "step: 340, loss: 0.000622995721641928\n",
            "step: 350, loss: 0.0005560719291679561\n",
            "step: 360, loss: 0.0004761535965371877\n",
            "step: 370, loss: 0.002262966940179467\n",
            "step: 380, loss: 0.0006786042358726263\n",
            "step: 390, loss: 0.018310274928808212\n",
            "step: 400, loss: 0.0018252965528517962\n",
            "step: 410, loss: 0.00022550803259946406\n",
            "step: 420, loss: 0.0004490474530030042\n",
            "step: 430, loss: 0.0007082626689225435\n",
            "step: 440, loss: 0.001370370271615684\n",
            "step: 450, loss: 0.0004953920724801719\n",
            "step: 460, loss: 0.06643135100603104\n",
            "step: 470, loss: 0.0006735132774338126\n",
            "step: 480, loss: 0.0005584003520198166\n",
            "step: 490, loss: 0.0053179520182311535\n",
            "step: 500, loss: 0.0026392294093966484\n",
            "step: 510, loss: 0.02076626382768154\n",
            "step: 520, loss: 0.0005557896220125258\n",
            "step: 530, loss: 0.0009311999310739338\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9398148148148149, f1=0.9361702127659576, best_f1=0.9415554532903819\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003984327195212245\n",
            "step: 10, loss: 0.001379869063384831\n",
            "step: 20, loss: 0.0009068229701370001\n",
            "step: 30, loss: 0.019328102469444275\n",
            "step: 40, loss: 0.000528152973856777\n",
            "step: 50, loss: 0.0031309095211327076\n",
            "step: 60, loss: 0.013129675760865211\n",
            "step: 70, loss: 0.0004537225468084216\n",
            "step: 80, loss: 0.0005824796389788389\n",
            "step: 90, loss: 0.0003354967338964343\n",
            "step: 100, loss: 0.00024723971728235483\n",
            "step: 110, loss: 0.0009809760376811028\n",
            "step: 120, loss: 0.0003289235755801201\n",
            "step: 130, loss: 0.0003876377595588565\n",
            "step: 140, loss: 0.0003281343379057944\n",
            "step: 150, loss: 0.000885112676769495\n",
            "step: 160, loss: 0.0006801928975619376\n",
            "step: 170, loss: 0.0002490688639227301\n",
            "step: 180, loss: 0.0008758906042203307\n",
            "step: 190, loss: 0.0005710478872060776\n",
            "step: 200, loss: 0.0006326287984848022\n",
            "step: 210, loss: 0.00046181725338101387\n",
            "step: 220, loss: 0.0008845261763781309\n",
            "step: 230, loss: 0.17080196738243103\n",
            "step: 240, loss: 0.0008331039571203291\n",
            "step: 250, loss: 0.00035915596527047455\n",
            "step: 260, loss: 0.00047308707144111395\n",
            "step: 270, loss: 0.0007313016685657203\n",
            "step: 280, loss: 0.0004095419717486948\n",
            "step: 290, loss: 0.004815749824047089\n",
            "step: 300, loss: 0.00047927876585163176\n",
            "step: 310, loss: 0.0563792809844017\n",
            "step: 320, loss: 0.0009088709484785795\n",
            "step: 330, loss: 0.0007530645816586912\n",
            "step: 340, loss: 0.0010717385448515415\n",
            "step: 350, loss: 0.0008264859789051116\n",
            "step: 360, loss: 0.012479661963880062\n",
            "step: 370, loss: 0.0011615701951086521\n",
            "step: 380, loss: 0.012671885080635548\n",
            "step: 390, loss: 0.003195052267983556\n",
            "step: 400, loss: 0.008401889353990555\n",
            "step: 410, loss: 0.0005757251055911183\n",
            "step: 420, loss: 0.0005271135596558452\n",
            "step: 430, loss: 0.00020574375230353326\n",
            "step: 440, loss: 0.00045562098966911435\n",
            "step: 450, loss: 0.054988522082567215\n",
            "step: 460, loss: 0.0008192822569981217\n",
            "step: 470, loss: 0.0004178787348791957\n",
            "step: 480, loss: 0.007287952117621899\n",
            "step: 490, loss: 0.004349939059466124\n",
            "step: 500, loss: 0.0005822041421197355\n",
            "step: 510, loss: 0.0007644628640264273\n",
            "step: 520, loss: 0.0005267994711175561\n",
            "step: 530, loss: 0.0008426797576248646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9408502772643254, f1=0.9367205542725173, best_f1=0.9415554532903819\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:32, 177.62it/s]\n",
            "load_f1 = 0.9377901578458682\n",
            "real_f1 = 0.9299242424242424\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:28, 152.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DITTO"
      ],
      "metadata": {
        "id": "pnXzXaaYhstq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO STRUCTURED"
      ],
      "metadata": {
        "id": "r23AxFPnhstr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCTWC7NUhstr"
      },
      "source": [
        "### Beer - Running the matcher"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpjbjZcRhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8b021a-26da-4da9-9ac2-eaad885e0f92"
      },
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Beer.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Beer \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.43686917424201965\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.3943661971830986, f1=0.2826086956521739, best_f1=0.2826086956521739\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4203495681285858\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 2: dev_f1=0.345679012345679, f1=0.27184466019417475, best_f1=0.2826086956521739\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3736698031425476\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.375, f1=0.3728813559322034, best_f1=0.2826086956521739\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.25810903310775757\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.5106382978723404, f1=0.3823529411764706, best_f1=0.3823529411764706\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3034130930900574\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.5714285714285715, f1=0.3943661971830986, best_f1=0.3943661971830986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2988949418067932\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.5777777777777778, f1=0.45901639344262296, best_f1=0.45901639344262296\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.4438624978065491\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8, f1=0.7000000000000001, best_f1=0.7000000000000001\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.37235701084136963\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.8571428571428571, f1=0.6666666666666666, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.15649902820587158\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8125000000000001, f1=0.6511627906976745, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.16246235370635986\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8125000000000001, f1=0.7000000000000001, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14130587875843048\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8387096774193549, f1=0.7368421052631579, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09526408463716507\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8125000000000001, f1=0.7000000000000001, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03637118265032768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8125000000000001, f1=0.7000000000000001, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.029821613803505898\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8125000000000001, f1=0.7000000000000001, best_f1=0.6666666666666666\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04043501615524292\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8125000000000001, f1=0.7000000000000001, best_f1=0.6666666666666666\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "91it [00:00, 125018.56it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "load_f1 = 0.7142857142857143\n",
            "real_f1 = 0.7096774193548386\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:29, 151.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "6Bang43Lhsts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "QuztJ-lwhsts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacad9d7-343b-4ae5-c5fa-7ebdcb8070e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.601436972618103\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "step: 10, loss: 0.438076376914978\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.49649977684020996\n",
            "step: 30, loss: 0.34297990798950195\n",
            "step: 40, loss: 0.341727077960968\n",
            "step: 50, loss: 0.5359809398651123\n",
            "step: 60, loss: 0.4142535626888275\n",
            "step: 70, loss: 0.1777476817369461\n",
            "step: 80, loss: 0.3532622158527374\n",
            "step: 90, loss: 0.2232976257801056\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.12028683722019196\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 110, loss: 0.1012941300868988\n",
            "step: 120, loss: 0.10007330775260925\n",
            "step: 130, loss: 0.02151414193212986\n",
            "step: 140, loss: 0.033402200788259506\n",
            "step: 150, loss: 0.20620006322860718\n",
            "step: 160, loss: 0.00897417776286602\n",
            "step: 170, loss: 0.07680325955152512\n",
            "step: 180, loss: 0.044045403599739075\n",
            "step: 190, loss: 0.03369678556919098\n",
            "step: 200, loss: 0.0826323851943016\n",
            "step: 210, loss: 0.06161903962492943\n",
            "step: 220, loss: 0.1648285686969757\n",
            "step: 230, loss: 0.021617693826556206\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9527027027027027, f1=0.9515219842164601, best_f1=0.9515219842164601\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0341973602771759\n",
            "step: 10, loss: 0.1507985144853592\n",
            "step: 20, loss: 0.01922895386815071\n",
            "step: 30, loss: 0.05873505398631096\n",
            "step: 40, loss: 0.03399426490068436\n",
            "step: 50, loss: 0.013065723702311516\n",
            "step: 60, loss: 0.055080752819776535\n",
            "step: 70, loss: 0.06769652664661407\n",
            "step: 80, loss: 0.01068596076220274\n",
            "step: 90, loss: 0.009098054841160774\n",
            "step: 100, loss: 0.003219601698219776\n",
            "step: 110, loss: 0.0016680205008015037\n",
            "step: 120, loss: 0.10647735744714737\n",
            "step: 130, loss: 0.0833936408162117\n",
            "step: 140, loss: 0.010867338627576828\n",
            "step: 150, loss: 0.07515305280685425\n",
            "step: 160, loss: 0.007468657102435827\n",
            "step: 170, loss: 0.0069259218871593475\n",
            "step: 180, loss: 0.0019741731230169535\n",
            "step: 190, loss: 0.00836250837892294\n",
            "step: 200, loss: 0.0017911904724314809\n",
            "step: 210, loss: 0.043774332851171494\n",
            "step: 220, loss: 0.0032900446094572544\n",
            "step: 230, loss: 0.00191565474960953\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9819413092550789, f1=0.9783352337514253, best_f1=0.9783352337514253\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021435877308249474\n",
            "step: 10, loss: 0.013371615670621395\n",
            "step: 20, loss: 0.002932582050561905\n",
            "step: 30, loss: 0.00734708970412612\n",
            "step: 40, loss: 0.001492952462285757\n",
            "step: 50, loss: 0.21232353150844574\n",
            "step: 60, loss: 0.003890708787366748\n",
            "step: 70, loss: 0.0022064168006181717\n",
            "step: 80, loss: 0.08829393982887268\n",
            "step: 90, loss: 0.003784741275012493\n",
            "step: 100, loss: 0.001993450103327632\n",
            "step: 110, loss: 0.010266194120049477\n",
            "step: 120, loss: 0.019342990592122078\n",
            "step: 130, loss: 0.09725482016801834\n",
            "step: 140, loss: 0.004704549442976713\n",
            "step: 150, loss: 0.0256541445851326\n",
            "step: 160, loss: 0.0022519396152347326\n",
            "step: 170, loss: 0.00056865019723773\n",
            "step: 180, loss: 0.00254240189678967\n",
            "step: 190, loss: 0.022883053869009018\n",
            "step: 200, loss: 0.01701510325074196\n",
            "step: 210, loss: 0.0019769780337810516\n",
            "step: 220, loss: 0.004315390717238188\n",
            "step: 230, loss: 0.005524769425392151\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9853768278965129, f1=0.9853107344632768, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0031364860478788614\n",
            "step: 10, loss: 0.006892392877489328\n",
            "step: 20, loss: 0.019987359642982483\n",
            "step: 30, loss: 0.00163277518004179\n",
            "step: 40, loss: 0.064165860414505\n",
            "step: 50, loss: 0.004708753898739815\n",
            "step: 60, loss: 0.004159587901085615\n",
            "step: 70, loss: 0.013584217987954617\n",
            "step: 80, loss: 0.0020517325028777122\n",
            "step: 90, loss: 0.019308796152472496\n",
            "step: 100, loss: 0.0014202488819137216\n",
            "step: 110, loss: 0.0008548916666768491\n",
            "step: 120, loss: 0.017783280462026596\n",
            "step: 130, loss: 0.022060109302401543\n",
            "step: 140, loss: 0.004942999221384525\n",
            "step: 150, loss: 0.00036611707764677703\n",
            "step: 160, loss: 0.0011471848702058196\n",
            "step: 170, loss: 0.0060946084558963776\n",
            "step: 180, loss: 0.16200527548789978\n",
            "step: 190, loss: 0.0060466593131423\n",
            "step: 200, loss: 0.007718529086560011\n",
            "step: 210, loss: 0.0019657891243696213\n",
            "step: 220, loss: 0.0016628705197945237\n",
            "step: 230, loss: 0.009454834274947643\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9797752808988766, f1=0.9751131221719457, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008188868523575366\n",
            "step: 10, loss: 0.0036372686736285686\n",
            "step: 20, loss: 0.0034532155841588974\n",
            "step: 30, loss: 0.0009713476174511015\n",
            "step: 40, loss: 0.0017691768007352948\n",
            "step: 50, loss: 0.0030763985123485327\n",
            "step: 60, loss: 0.10631097853183746\n",
            "step: 70, loss: 0.10106409341096878\n",
            "step: 80, loss: 0.06382296979427338\n",
            "step: 90, loss: 0.040915172547101974\n",
            "step: 100, loss: 0.0008599931024946272\n",
            "step: 110, loss: 0.016456928104162216\n",
            "step: 120, loss: 0.0012922856258228421\n",
            "step: 130, loss: 0.0010909802513197064\n",
            "step: 140, loss: 0.0017977155512198806\n",
            "step: 150, loss: 0.025217385962605476\n",
            "step: 160, loss: 0.001615031622350216\n",
            "step: 170, loss: 0.005953832529485226\n",
            "step: 180, loss: 0.02786378376185894\n",
            "step: 190, loss: 0.0220523402094841\n",
            "step: 200, loss: 0.018297597765922546\n",
            "step: 210, loss: 0.01199384219944477\n",
            "step: 220, loss: 0.004687993787229061\n",
            "step: 230, loss: 0.012709815986454487\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.983277591973244, f1=0.9843400447427293, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000945588864851743\n",
            "step: 10, loss: 0.02790111117064953\n",
            "step: 20, loss: 0.0015001478604972363\n",
            "step: 30, loss: 0.0008111781207844615\n",
            "step: 40, loss: 0.0004514936590567231\n",
            "step: 50, loss: 0.0009109880775213242\n",
            "step: 60, loss: 0.003415618324652314\n",
            "step: 70, loss: 0.006854836829006672\n",
            "step: 80, loss: 0.00849850382655859\n",
            "step: 90, loss: 0.009211290627717972\n",
            "step: 100, loss: 0.0007982142269611359\n",
            "step: 110, loss: 0.0690777450799942\n",
            "step: 120, loss: 0.0014174483949318528\n",
            "step: 130, loss: 0.007997419685125351\n",
            "step: 140, loss: 0.0007289572968147695\n",
            "step: 150, loss: 0.0009961440227925777\n",
            "step: 160, loss: 0.026615289971232414\n",
            "step: 170, loss: 0.01155075617134571\n",
            "step: 180, loss: 0.001476111588999629\n",
            "step: 190, loss: 0.0025494364090263844\n",
            "step: 200, loss: 0.004254000727087259\n",
            "step: 210, loss: 0.02038438804447651\n",
            "step: 220, loss: 0.05148817226290703\n",
            "step: 230, loss: 0.003176068188622594\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9830890642615557, f1=0.976054732041049, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003814094001427293\n",
            "step: 10, loss: 0.0009830944472923875\n",
            "step: 20, loss: 0.0005213734111748636\n",
            "step: 30, loss: 0.0006966931978240609\n",
            "step: 40, loss: 0.0029010269790887833\n",
            "step: 50, loss: 0.002641932340338826\n",
            "step: 60, loss: 0.0005059046088717878\n",
            "step: 70, loss: 0.0014130532508715987\n",
            "step: 80, loss: 0.0012669936986640096\n",
            "step: 90, loss: 0.04545332118868828\n",
            "step: 100, loss: 0.0011427513090893626\n",
            "step: 110, loss: 0.0026788462419062853\n",
            "step: 120, loss: 0.0038386215455830097\n",
            "step: 130, loss: 0.0043459320440888405\n",
            "step: 140, loss: 0.000896155193913728\n",
            "step: 150, loss: 0.027404244989156723\n",
            "step: 160, loss: 0.0011177826672792435\n",
            "step: 170, loss: 0.023799598217010498\n",
            "step: 180, loss: 0.0005410680314525962\n",
            "step: 190, loss: 0.006193355657160282\n",
            "step: 200, loss: 0.01069674827158451\n",
            "step: 210, loss: 0.01706995628774166\n",
            "step: 220, loss: 0.00039975775871425867\n",
            "step: 230, loss: 0.0004880241467617452\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9866369710467707, f1=0.9821428571428571, best_f1=0.9821428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006508505321107805\n",
            "step: 10, loss: 0.0064972504042088985\n",
            "step: 20, loss: 0.002296508988365531\n",
            "step: 30, loss: 0.0064212060533463955\n",
            "step: 40, loss: 0.0015996466390788555\n",
            "step: 50, loss: 0.0018677779007703066\n",
            "step: 60, loss: 0.0007823217893019319\n",
            "step: 70, loss: 0.00020107986347284168\n",
            "step: 80, loss: 0.0008340264903381467\n",
            "step: 90, loss: 0.0007843866478651762\n",
            "step: 100, loss: 0.018373508006334305\n",
            "step: 110, loss: 0.000495673215482384\n",
            "step: 120, loss: 0.000768486235756427\n",
            "step: 130, loss: 0.0026725674979388714\n",
            "step: 140, loss: 0.0006584412767551839\n",
            "step: 150, loss: 0.09081470221281052\n",
            "step: 160, loss: 0.017400117591023445\n",
            "step: 170, loss: 0.03162234276533127\n",
            "step: 180, loss: 0.0006535037537105381\n",
            "step: 190, loss: 0.05729709193110466\n",
            "step: 200, loss: 0.025675207376480103\n",
            "step: 210, loss: 0.0032567856833338737\n",
            "step: 220, loss: 0.003949559759348631\n",
            "step: 230, loss: 0.00039063376607373357\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9876543209876544, f1=0.9787234042553192, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03715415298938751\n",
            "step: 10, loss: 0.0028109776321798563\n",
            "step: 20, loss: 0.002743246266618371\n",
            "step: 30, loss: 0.0018276766641065478\n",
            "step: 40, loss: 0.008672214113175869\n",
            "step: 50, loss: 0.0017756689339876175\n",
            "step: 60, loss: 0.0010865116491913795\n",
            "step: 70, loss: 0.2042893022298813\n",
            "step: 80, loss: 0.004293039441108704\n",
            "step: 90, loss: 0.0068870424292981625\n",
            "step: 100, loss: 0.0005076596862636507\n",
            "step: 110, loss: 0.0004555476480163634\n",
            "step: 120, loss: 0.014038858003914356\n",
            "step: 130, loss: 0.0007709752535447478\n",
            "step: 140, loss: 0.0010096885962411761\n",
            "step: 150, loss: 0.0015745348064228892\n",
            "step: 160, loss: 0.007854324765503407\n",
            "step: 170, loss: 0.0002579969877842814\n",
            "step: 180, loss: 0.0007386141223832965\n",
            "step: 190, loss: 0.0002623525506351143\n",
            "step: 200, loss: 0.0007638201350346208\n",
            "step: 210, loss: 0.001573361805640161\n",
            "step: 220, loss: 0.0009297685464844108\n",
            "step: 230, loss: 0.0010098294587805867\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9876543209876544, f1=0.9853438556933484, best_f1=0.9787234042553192\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008185406913980842\n",
            "step: 10, loss: 0.0010524599347263575\n",
            "step: 20, loss: 0.000683176564052701\n",
            "step: 30, loss: 0.00022073967556934804\n",
            "step: 40, loss: 0.002787386067211628\n",
            "step: 50, loss: 0.0001962301175808534\n",
            "step: 60, loss: 0.0010282143484801054\n",
            "step: 70, loss: 0.0011245274217799306\n",
            "step: 80, loss: 0.0003850744978990406\n",
            "step: 90, loss: 0.000617283396422863\n",
            "step: 100, loss: 0.00022502342471852899\n",
            "step: 110, loss: 0.04433847591280937\n",
            "step: 120, loss: 0.00017904682317748666\n",
            "step: 130, loss: 0.0003715687489602715\n",
            "step: 140, loss: 0.002391585148870945\n",
            "step: 150, loss: 0.0010426640510559082\n",
            "step: 160, loss: 7.873111462686211e-05\n",
            "step: 170, loss: 0.00029463093960657716\n",
            "step: 180, loss: 0.001989183248952031\n",
            "step: 190, loss: 0.001084622461348772\n",
            "step: 200, loss: 0.0012138671008870006\n",
            "step: 210, loss: 0.0042327274568378925\n",
            "step: 220, loss: 0.03871351480484009\n",
            "step: 230, loss: 0.0007156369392760098\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 10: dev_f1=0.9887640449438202, f1=0.9841986455981941, best_f1=0.9841986455981941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000320831430144608\n",
            "step: 10, loss: 0.0005528887850232422\n",
            "step: 20, loss: 0.0003988258249592036\n",
            "step: 30, loss: 0.0010666688904166222\n",
            "step: 40, loss: 5.7037985243368894e-05\n",
            "step: 50, loss: 0.00019516557222232223\n",
            "step: 60, loss: 0.011149386875331402\n",
            "step: 70, loss: 0.0001831393310567364\n",
            "step: 80, loss: 0.034148313105106354\n",
            "step: 90, loss: 0.18987905979156494\n",
            "step: 100, loss: 0.0006712324684485793\n",
            "step: 110, loss: 0.002592929406091571\n",
            "step: 120, loss: 0.0006136226584203541\n",
            "step: 130, loss: 0.0003511887334752828\n",
            "step: 140, loss: 0.0021084609907120466\n",
            "step: 150, loss: 0.0006460124859586358\n",
            "step: 160, loss: 0.0017138270195573568\n",
            "step: 170, loss: 0.000745861092582345\n",
            "step: 180, loss: 0.004192701540887356\n",
            "step: 190, loss: 0.0006881873123347759\n",
            "step: 200, loss: 0.0052424268797039986\n",
            "step: 210, loss: 0.0008760349592193961\n",
            "step: 220, loss: 0.03435801342129707\n",
            "step: 230, loss: 0.00098376942332834\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9876819708846584, f1=0.9865168539325843, best_f1=0.9841986455981941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0012395662488415837\n",
            "step: 10, loss: 0.00043616723269224167\n",
            "step: 20, loss: 0.019214246422052383\n",
            "step: 30, loss: 0.013919529505074024\n",
            "step: 40, loss: 0.002620579907670617\n",
            "step: 50, loss: 0.0020379803609102964\n",
            "step: 60, loss: 0.001431406126357615\n",
            "step: 70, loss: 0.0016249152831733227\n",
            "step: 80, loss: 0.0005633291439153254\n",
            "step: 90, loss: 0.002284703077748418\n",
            "step: 100, loss: 0.00047561427345499396\n",
            "step: 110, loss: 0.0003077318542636931\n",
            "step: 120, loss: 0.000613208394497633\n",
            "step: 130, loss: 0.0004718206764664501\n",
            "step: 140, loss: 0.0009639980271458626\n",
            "step: 150, loss: 0.0007547828718088567\n",
            "step: 160, loss: 0.0022861575707793236\n",
            "step: 170, loss: 0.0007839511963538826\n",
            "step: 180, loss: 0.0005501165287569165\n",
            "step: 190, loss: 0.006864774040877819\n",
            "step: 200, loss: 0.0004032420401927084\n",
            "step: 210, loss: 0.006938362028449774\n",
            "step: 220, loss: 0.008938937447965145\n",
            "step: 230, loss: 0.0007575266645289958\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9887892376681614, f1=0.9841986455981941, best_f1=0.9841986455981941\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002165335463359952\n",
            "step: 10, loss: 0.0007073446759022772\n",
            "step: 20, loss: 0.0011911054607480764\n",
            "step: 30, loss: 0.005970723927021027\n",
            "step: 40, loss: 0.0007538119680248201\n",
            "step: 50, loss: 0.00723624462261796\n",
            "step: 60, loss: 0.0003951282415073365\n",
            "step: 70, loss: 0.028048550710082054\n",
            "step: 80, loss: 0.0016286105383187532\n",
            "step: 90, loss: 0.0009622075594961643\n",
            "step: 100, loss: 0.0058470843359827995\n",
            "step: 110, loss: 0.0010559860384091735\n",
            "step: 120, loss: 0.004100009799003601\n",
            "step: 130, loss: 0.0005801782826893032\n",
            "step: 140, loss: 0.0005985188181512058\n",
            "step: 150, loss: 0.005566069856286049\n",
            "step: 160, loss: 0.0012505323393270373\n",
            "step: 170, loss: 0.0013845707289874554\n",
            "step: 180, loss: 0.026494689285755157\n",
            "step: 190, loss: 0.0012888303026556969\n",
            "step: 200, loss: 0.00015976183931343257\n",
            "step: 210, loss: 0.0010490627028048038\n",
            "step: 220, loss: 0.0005887798615731299\n",
            "step: 230, loss: 0.000607528374530375\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.9898762654668166, f1=0.9853107344632768, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004006966482847929\n",
            "step: 10, loss: 0.0005075340159237385\n",
            "step: 20, loss: 0.0005587634514085948\n",
            "step: 30, loss: 0.0012180466437712312\n",
            "step: 40, loss: 0.0015400117263197899\n",
            "step: 50, loss: 0.0004640059487428516\n",
            "step: 60, loss: 0.00041065734694711864\n",
            "step: 70, loss: 0.0007803748594596982\n",
            "step: 80, loss: 0.0006839905981905758\n",
            "step: 90, loss: 0.004675634671002626\n",
            "step: 100, loss: 0.0016163254622370005\n",
            "step: 110, loss: 0.012905655428767204\n",
            "step: 120, loss: 0.0001480287901358679\n",
            "step: 130, loss: 0.00103627925273031\n",
            "step: 140, loss: 0.00044894011807627976\n",
            "step: 150, loss: 0.00028247429872862995\n",
            "step: 160, loss: 0.0009209493873640895\n",
            "step: 170, loss: 0.0005593475070782006\n",
            "step: 180, loss: 0.00038031325675547123\n",
            "step: 190, loss: 0.0004829212848562747\n",
            "step: 200, loss: 0.0005914190551266074\n",
            "step: 210, loss: 0.0003386717871762812\n",
            "step: 220, loss: 0.0010005729272961617\n",
            "step: 230, loss: 0.0003631417639553547\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9854748603351955, f1=0.9865168539325843, best_f1=0.9853107344632768\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004545886069536209\n",
            "step: 10, loss: 0.000571529264561832\n",
            "step: 20, loss: 0.0009373924694955349\n",
            "step: 30, loss: 0.00032586450106464326\n",
            "step: 40, loss: 0.00017783220391720533\n",
            "step: 50, loss: 0.0002621747553348541\n",
            "step: 60, loss: 0.04307000711560249\n",
            "step: 70, loss: 0.0005605124752037227\n",
            "step: 80, loss: 0.0009366576559841633\n",
            "step: 90, loss: 0.0005454298225231469\n",
            "step: 100, loss: 0.00018407913739793003\n",
            "step: 110, loss: 0.0005265597719699144\n",
            "step: 120, loss: 0.16194123029708862\n",
            "step: 130, loss: 0.0006033480749465525\n",
            "step: 140, loss: 0.007320030592381954\n",
            "step: 150, loss: 0.0006643315427936614\n",
            "step: 160, loss: 0.0044508823193609715\n",
            "step: 170, loss: 0.00031144253443926573\n",
            "step: 180, loss: 0.0008411650196649134\n",
            "step: 190, loss: 0.0005973249790258706\n",
            "step: 200, loss: 0.0017487986478954554\n",
            "step: 210, loss: 0.08493306487798691\n",
            "step: 220, loss: 0.0020809208508580923\n",
            "step: 230, loss: 0.0005402371753007174\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9876265466816648, f1=0.9830124575311437, best_f1=0.9853107344632768\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:15, 156.54it/s]\n",
            "load_f1 = 0.9888392857142857\n",
            "real_f1 = 0.9865771812080537\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 144.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "CrVM9KP9hstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "7aLntP6ehstt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33db68db-c4db-48d1-cdf9-0c01c00eda56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6235836744308472\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4949053227901459\n",
            "step: 20, loss: 0.3340202569961548\n",
            "step: 30, loss: 0.4121380150318146\n",
            "step: 40, loss: 0.3581545650959015\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 50, loss: 0.6965097188949585\n",
            "step: 60, loss: 0.24889063835144043\n",
            "step: 70, loss: 0.24465960264205933\n",
            "step: 80, loss: 0.09544917196035385\n",
            "step: 90, loss: 0.1156267300248146\n",
            "step: 100, loss: 0.23837728798389435\n",
            "step: 110, loss: 0.15472526848316193\n",
            "step: 120, loss: 0.11126413941383362\n",
            "step: 130, loss: 0.1844794750213623\n",
            "step: 140, loss: 0.3023279011249542\n",
            "step: 150, loss: 0.08560624718666077\n",
            "step: 160, loss: 0.10514306277036667\n",
            "step: 170, loss: 0.04337035492062569\n",
            "step: 180, loss: 0.10512343049049377\n",
            "step: 190, loss: 0.20227575302124023\n",
            "step: 200, loss: 0.07045090943574905\n",
            "step: 210, loss: 0.05319429188966751\n",
            "step: 220, loss: 0.03645522892475128\n",
            "step: 230, loss: 0.21052931249141693\n",
            "step: 240, loss: 0.09561100602149963\n",
            "step: 250, loss: 0.014085793867707253\n",
            "step: 260, loss: 0.4553062915802002\n",
            "step: 270, loss: 0.15290024876594543\n",
            "step: 280, loss: 0.048270802944898605\n",
            "step: 290, loss: 0.10857444256544113\n",
            "step: 300, loss: 0.09599959850311279\n",
            "step: 310, loss: 0.218829944729805\n",
            "step: 320, loss: 0.1403767168521881\n",
            "step: 330, loss: 0.110983707010746\n",
            "step: 340, loss: 0.32631105184555054\n",
            "step: 350, loss: 0.10203611850738525\n",
            "step: 360, loss: 0.10357322543859482\n",
            "step: 370, loss: 0.022863758727908134\n",
            "step: 380, loss: 0.16043059527873993\n",
            "step: 390, loss: 0.018944796174764633\n",
            "step: 400, loss: 0.06757479161024094\n",
            "step: 410, loss: 0.2513650953769684\n",
            "step: 420, loss: 0.025812076404690742\n",
            "step: 430, loss: 0.06081555783748627\n",
            "step: 440, loss: 0.10213037580251694\n",
            "step: 450, loss: 0.09652339667081833\n",
            "step: 460, loss: 0.04755681753158569\n",
            "step: 470, loss: 0.044860854744911194\n",
            "step: 480, loss: 0.11536581814289093\n",
            "step: 490, loss: 0.1033681109547615\n",
            "step: 500, loss: 0.16103078424930573\n",
            "step: 510, loss: 0.06694255769252777\n",
            "step: 520, loss: 0.10467365384101868\n",
            "step: 530, loss: 0.010547861456871033\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.935258500232883, f1=0.9306197964847363, best_f1=0.9306197964847363\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09582428634166718\n",
            "step: 10, loss: 0.018332544714212418\n",
            "step: 20, loss: 0.021919649094343185\n",
            "step: 30, loss: 0.1496172547340393\n",
            "step: 40, loss: 0.1462516337633133\n",
            "step: 50, loss: 0.044466353952884674\n",
            "step: 60, loss: 0.04049420729279518\n",
            "step: 70, loss: 0.019451886415481567\n",
            "step: 80, loss: 0.04054459556937218\n",
            "step: 90, loss: 0.08869384974241257\n",
            "step: 100, loss: 0.05370495095849037\n",
            "step: 110, loss: 0.06632164865732193\n",
            "step: 120, loss: 0.20177429914474487\n",
            "step: 130, loss: 0.008638659492135048\n",
            "step: 140, loss: 0.08073996752500534\n",
            "step: 150, loss: 0.007055394817143679\n",
            "step: 160, loss: 0.030512487515807152\n",
            "step: 170, loss: 0.20308759808540344\n",
            "step: 180, loss: 0.05874735489487648\n",
            "step: 190, loss: 0.042556680738925934\n",
            "step: 200, loss: 0.26576703786849976\n",
            "step: 210, loss: 0.06991517543792725\n",
            "step: 220, loss: 0.003947316203266382\n",
            "step: 230, loss: 0.004117238335311413\n",
            "step: 240, loss: 0.10693518817424774\n",
            "step: 250, loss: 0.015532131306827068\n",
            "step: 260, loss: 0.05641206353902817\n",
            "step: 270, loss: 0.02098824456334114\n",
            "step: 280, loss: 0.03441765159368515\n",
            "step: 290, loss: 0.08671742677688599\n",
            "step: 300, loss: 0.032240044325590134\n",
            "step: 310, loss: 0.033461686223745346\n",
            "step: 320, loss: 0.028369951993227005\n",
            "step: 330, loss: 0.1982576996088028\n",
            "step: 340, loss: 0.2154167890548706\n",
            "step: 350, loss: 0.005421780049800873\n",
            "step: 360, loss: 0.05652381107211113\n",
            "step: 370, loss: 0.01398501917719841\n",
            "step: 380, loss: 0.11026860028505325\n",
            "step: 390, loss: 0.003193051554262638\n",
            "step: 400, loss: 0.07814624160528183\n",
            "step: 410, loss: 0.01275002770125866\n",
            "step: 420, loss: 0.00949656218290329\n",
            "step: 430, loss: 0.027875907719135284\n",
            "step: 440, loss: 0.003884970908984542\n",
            "step: 450, loss: 0.15890249609947205\n",
            "step: 460, loss: 0.06907449662685394\n",
            "step: 470, loss: 0.0959114357829094\n",
            "step: 480, loss: 0.006042566616088152\n",
            "step: 490, loss: 0.14660440385341644\n",
            "step: 500, loss: 0.02451857179403305\n",
            "step: 510, loss: 0.016116462647914886\n",
            "step: 520, loss: 0.4197255074977875\n",
            "step: 530, loss: 0.08540182560682297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.945387792565397, f1=0.9414434861552429, best_f1=0.9414434861552429\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05260690674185753\n",
            "step: 10, loss: 0.0632961094379425\n",
            "step: 20, loss: 0.005261018872261047\n",
            "step: 30, loss: 0.02502172254025936\n",
            "step: 40, loss: 0.028407253324985504\n",
            "step: 50, loss: 0.10066527128219604\n",
            "step: 60, loss: 0.009313737042248249\n",
            "step: 70, loss: 0.019796079024672508\n",
            "step: 80, loss: 0.15216825902462006\n",
            "step: 90, loss: 0.003626737743616104\n",
            "step: 100, loss: 0.06222368776798248\n",
            "step: 110, loss: 0.03705534338951111\n",
            "step: 120, loss: 0.10895058512687683\n",
            "step: 130, loss: 0.13897910714149475\n",
            "step: 140, loss: 0.006940280552953482\n",
            "step: 150, loss: 0.20764833688735962\n",
            "step: 160, loss: 0.03386836126446724\n",
            "step: 170, loss: 0.007794264703989029\n",
            "step: 180, loss: 0.047894712537527084\n",
            "step: 190, loss: 0.003980153240263462\n",
            "step: 200, loss: 0.010016162879765034\n",
            "step: 210, loss: 0.041795071214437485\n",
            "step: 220, loss: 0.1832878589630127\n",
            "step: 230, loss: 0.03317577391862869\n",
            "step: 240, loss: 0.09732095152139664\n",
            "step: 250, loss: 0.0949598029255867\n",
            "step: 260, loss: 0.14024777710437775\n",
            "step: 270, loss: 0.01759517751634121\n",
            "step: 280, loss: 0.0073523554019629955\n",
            "step: 290, loss: 0.012108864262700081\n",
            "step: 300, loss: 0.09706930816173553\n",
            "step: 310, loss: 0.025597859174013138\n",
            "step: 320, loss: 0.15774889290332794\n",
            "step: 330, loss: 0.0009175438899546862\n",
            "step: 340, loss: 0.018206311389803886\n",
            "step: 350, loss: 0.0507844015955925\n",
            "step: 360, loss: 0.12619304656982422\n",
            "step: 370, loss: 0.018044201657176018\n",
            "step: 380, loss: 0.12240937352180481\n",
            "step: 390, loss: 0.021274380385875702\n",
            "step: 400, loss: 0.28800106048583984\n",
            "step: 410, loss: 0.06016786769032478\n",
            "step: 420, loss: 0.021100779995322227\n",
            "step: 430, loss: 0.03653917461633682\n",
            "step: 440, loss: 0.191927969455719\n",
            "step: 450, loss: 0.02814873866736889\n",
            "step: 460, loss: 0.09389020502567291\n",
            "step: 470, loss: 0.11802579462528229\n",
            "step: 480, loss: 0.08167220652103424\n",
            "step: 490, loss: 0.04813679680228233\n",
            "step: 500, loss: 0.017613140866160393\n",
            "step: 510, loss: 0.018917903304100037\n",
            "step: 520, loss: 0.007866577245295048\n",
            "step: 530, loss: 0.0075886426493525505\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9493908153701968, f1=0.9411764705882353, best_f1=0.9411764705882353\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028023866936564445\n",
            "step: 10, loss: 0.0024997955188155174\n",
            "step: 20, loss: 0.07087399065494537\n",
            "step: 30, loss: 0.0022688601166009903\n",
            "step: 40, loss: 0.01610160991549492\n",
            "step: 50, loss: 0.045915354043245316\n",
            "step: 60, loss: 0.005387059412896633\n",
            "step: 70, loss: 0.08782260119915009\n",
            "step: 80, loss: 0.0449925996363163\n",
            "step: 90, loss: 0.004318659659475088\n",
            "step: 100, loss: 0.002604636363685131\n",
            "step: 110, loss: 0.04165353253483772\n",
            "step: 120, loss: 0.006599645595997572\n",
            "step: 130, loss: 0.06432680040597916\n",
            "step: 140, loss: 0.02352091670036316\n",
            "step: 150, loss: 0.04949076473712921\n",
            "step: 160, loss: 0.005528190638870001\n",
            "step: 170, loss: 0.04989666864275932\n",
            "step: 180, loss: 0.11802433431148529\n",
            "step: 190, loss: 0.08080728352069855\n",
            "step: 200, loss: 0.09884238243103027\n",
            "step: 210, loss: 0.030457496643066406\n",
            "step: 220, loss: 0.017180971801280975\n",
            "step: 230, loss: 0.043381791561841965\n",
            "step: 240, loss: 0.0018419591942802072\n",
            "step: 250, loss: 0.14692561328411102\n",
            "step: 260, loss: 0.0054425448179244995\n",
            "step: 270, loss: 0.040234703570604324\n",
            "step: 280, loss: 0.011392883025109768\n",
            "step: 290, loss: 0.009281409904360771\n",
            "step: 300, loss: 0.00291931489482522\n",
            "step: 310, loss: 0.001789191970601678\n",
            "step: 320, loss: 0.1697242558002472\n",
            "step: 330, loss: 0.08307495713233948\n",
            "step: 340, loss: 0.0006609621341340244\n",
            "step: 350, loss: 0.08502323180437088\n",
            "step: 360, loss: 0.09184720367193222\n",
            "step: 370, loss: 0.025057155638933182\n",
            "step: 380, loss: 0.007504351902753115\n",
            "step: 390, loss: 0.0009105968056246638\n",
            "step: 400, loss: 0.019754279404878616\n",
            "step: 410, loss: 0.0023252139799296856\n",
            "step: 420, loss: 0.013550798408687115\n",
            "step: 430, loss: 0.028574461117386818\n",
            "step: 440, loss: 0.019593805074691772\n",
            "step: 450, loss: 0.01832343451678753\n",
            "step: 460, loss: 0.04798406362533569\n",
            "step: 470, loss: 0.004980182740837336\n",
            "step: 480, loss: 0.0021088537760078907\n",
            "step: 490, loss: 0.001919249538332224\n",
            "step: 500, loss: 0.014987965114414692\n",
            "step: 510, loss: 0.019153447821736336\n",
            "step: 520, loss: 0.09914856404066086\n",
            "step: 530, loss: 0.048940956592559814\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9510166358595193, f1=0.9436555199267065, best_f1=0.9436555199267065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02266533672809601\n",
            "step: 10, loss: 0.009817478246986866\n",
            "step: 20, loss: 0.00254686176776886\n",
            "step: 30, loss: 0.02472858875989914\n",
            "step: 40, loss: 0.004479541443288326\n",
            "step: 50, loss: 0.025629179552197456\n",
            "step: 60, loss: 0.07385016977787018\n",
            "step: 70, loss: 0.020811205729842186\n",
            "step: 80, loss: 0.011165948584675789\n",
            "step: 90, loss: 0.20665138959884644\n",
            "step: 100, loss: 0.07086271047592163\n",
            "step: 110, loss: 0.016911599785089493\n",
            "step: 120, loss: 0.20884905755519867\n",
            "step: 130, loss: 0.017984818667173386\n",
            "step: 140, loss: 0.11472906917333603\n",
            "step: 150, loss: 0.012680836021900177\n",
            "step: 160, loss: 0.032800011336803436\n",
            "step: 170, loss: 0.17019322514533997\n",
            "step: 180, loss: 0.005546013358980417\n",
            "step: 190, loss: 0.005796527024358511\n",
            "step: 200, loss: 0.11048237234354019\n",
            "step: 210, loss: 0.004921692423522472\n",
            "step: 220, loss: 0.005955337546765804\n",
            "step: 230, loss: 0.0008949417970143259\n",
            "step: 240, loss: 0.0015959113370627165\n",
            "step: 250, loss: 0.07869889587163925\n",
            "step: 260, loss: 0.011438580229878426\n",
            "step: 270, loss: 0.004259144421666861\n",
            "step: 280, loss: 0.025299008935689926\n",
            "step: 290, loss: 0.07353639602661133\n",
            "step: 300, loss: 0.17018449306488037\n",
            "step: 310, loss: 0.12413433194160461\n",
            "step: 320, loss: 0.10103917121887207\n",
            "step: 330, loss: 0.005605505313724279\n",
            "step: 340, loss: 0.007850767113268375\n",
            "step: 350, loss: 0.0010093750897794962\n",
            "step: 360, loss: 0.0025097569450736046\n",
            "step: 370, loss: 0.000961722747888416\n",
            "step: 380, loss: 0.0002694054855965078\n",
            "step: 390, loss: 0.028902972117066383\n",
            "step: 400, loss: 0.02707536704838276\n",
            "step: 410, loss: 0.031101934611797333\n",
            "step: 420, loss: 0.164592906832695\n",
            "step: 430, loss: 0.012056000530719757\n",
            "step: 440, loss: 0.005521767772734165\n",
            "step: 450, loss: 0.007034240290522575\n",
            "step: 460, loss: 0.009679294191300869\n",
            "step: 470, loss: 0.006590495351701975\n",
            "step: 480, loss: 0.25284844636917114\n",
            "step: 490, loss: 0.03138194978237152\n",
            "step: 500, loss: 0.008004941046237946\n",
            "step: 510, loss: 0.03136828541755676\n",
            "step: 520, loss: 0.2608211636543274\n",
            "step: 530, loss: 0.007124943193048239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 5: dev_f1=0.9452181987000929, f1=0.9441624365482235, best_f1=0.9436555199267065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.021157963201403618\n",
            "step: 10, loss: 0.0011301658814772964\n",
            "step: 20, loss: 0.001694376114755869\n",
            "step: 30, loss: 0.0010107861598953605\n",
            "step: 40, loss: 0.11088994890451431\n",
            "step: 50, loss: 0.0014327903045341372\n",
            "step: 60, loss: 0.02135009877383709\n",
            "step: 70, loss: 0.004161171615123749\n",
            "step: 80, loss: 0.001410163240507245\n",
            "step: 90, loss: 0.011088227853178978\n",
            "step: 100, loss: 0.009389967657625675\n",
            "step: 110, loss: 0.0007488572737202048\n",
            "step: 120, loss: 0.004124436527490616\n",
            "step: 130, loss: 0.007475285325199366\n",
            "step: 140, loss: 0.008461706340312958\n",
            "step: 150, loss: 0.0006228543934412301\n",
            "step: 160, loss: 0.12607845664024353\n",
            "step: 170, loss: 0.0030604840721935034\n",
            "step: 180, loss: 0.0029731320682913065\n",
            "step: 190, loss: 0.02307809330523014\n",
            "step: 200, loss: 0.0012019178830087185\n",
            "step: 210, loss: 0.007180306129157543\n",
            "step: 220, loss: 0.008365863002836704\n",
            "step: 230, loss: 0.05858977511525154\n",
            "step: 240, loss: 0.004042036831378937\n",
            "step: 250, loss: 0.07252518087625504\n",
            "step: 260, loss: 0.0018563789781183004\n",
            "step: 270, loss: 0.007205397356301546\n",
            "step: 280, loss: 0.005075590219348669\n",
            "step: 290, loss: 0.033137135207653046\n",
            "step: 300, loss: 0.005095894914120436\n",
            "step: 310, loss: 0.09291652590036392\n",
            "step: 320, loss: 0.022174958139657974\n",
            "step: 330, loss: 0.011602725833654404\n",
            "step: 340, loss: 0.0009759392123669386\n",
            "step: 350, loss: 0.0036624413914978504\n",
            "step: 360, loss: 0.005359257105737925\n",
            "step: 370, loss: 0.03751003369688988\n",
            "step: 380, loss: 0.03056665137410164\n",
            "step: 390, loss: 0.0019015532452613115\n",
            "step: 400, loss: 0.0010952157899737358\n",
            "step: 410, loss: 0.007587744854390621\n",
            "step: 420, loss: 0.009405074641108513\n",
            "step: 430, loss: 0.02358388900756836\n",
            "step: 440, loss: 0.0017113022040575743\n",
            "step: 450, loss: 0.2108297049999237\n",
            "step: 460, loss: 0.015197362750768661\n",
            "step: 470, loss: 0.030763709917664528\n",
            "step: 480, loss: 0.0017920583486557007\n",
            "step: 490, loss: 0.12949055433273315\n",
            "step: 500, loss: 0.001414356753230095\n",
            "step: 510, loss: 0.21860335767269135\n",
            "step: 520, loss: 0.0020564058795571327\n",
            "step: 530, loss: 0.00880943238735199\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9445234708392602, f1=0.9360902255639098, best_f1=0.9436555199267065\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010279174894094467\n",
            "step: 10, loss: 0.0024592143017798662\n",
            "step: 20, loss: 0.0011889757588505745\n",
            "step: 30, loss: 0.0015884889289736748\n",
            "step: 40, loss: 0.0021037752740085125\n",
            "step: 50, loss: 0.007994696497917175\n",
            "step: 60, loss: 0.0007136373315006495\n",
            "step: 70, loss: 0.01040969043970108\n",
            "step: 80, loss: 0.002080831676721573\n",
            "step: 90, loss: 0.0034299970138818026\n",
            "step: 100, loss: 0.004238254390656948\n",
            "step: 110, loss: 0.0001396243751514703\n",
            "step: 120, loss: 0.0037690112367272377\n",
            "step: 130, loss: 0.0024839674588292837\n",
            "step: 140, loss: 0.04041123762726784\n",
            "step: 150, loss: 0.0037307031452655792\n",
            "step: 160, loss: 0.0002804319665301591\n",
            "step: 170, loss: 0.006065718829631805\n",
            "step: 180, loss: 0.009348534978926182\n",
            "step: 190, loss: 0.03468746691942215\n",
            "step: 200, loss: 0.001135934959165752\n",
            "step: 210, loss: 0.05332978069782257\n",
            "step: 220, loss: 0.01075077336281538\n",
            "step: 230, loss: 0.0016394013073295355\n",
            "step: 240, loss: 0.06976805627346039\n",
            "step: 250, loss: 0.021069487556815147\n",
            "step: 260, loss: 0.01440854649990797\n",
            "step: 270, loss: 0.011119052767753601\n",
            "step: 280, loss: 0.010085185058414936\n",
            "step: 290, loss: 0.01460309885442257\n",
            "step: 300, loss: 0.00036515164538286626\n",
            "step: 310, loss: 0.0025708256289362907\n",
            "step: 320, loss: 0.047289565205574036\n",
            "step: 330, loss: 0.0010710892966017127\n",
            "step: 340, loss: 0.001059570349752903\n",
            "step: 350, loss: 0.0032098705414682627\n",
            "step: 360, loss: 0.06967556476593018\n",
            "step: 370, loss: 0.06507119536399841\n",
            "step: 380, loss: 0.01658480055630207\n",
            "step: 390, loss: 0.002820270834490657\n",
            "step: 400, loss: 0.03501391410827637\n",
            "step: 410, loss: 4.703840386355296e-05\n",
            "step: 420, loss: 0.0034527124371379614\n",
            "step: 430, loss: 0.02587594836950302\n",
            "step: 440, loss: 0.05131955444812775\n",
            "step: 450, loss: 0.018963390961289406\n",
            "step: 460, loss: 0.007889987900853157\n",
            "step: 470, loss: 0.1426657736301422\n",
            "step: 480, loss: 0.002230652840808034\n",
            "step: 490, loss: 0.10332522541284561\n",
            "step: 500, loss: 0.0016081830253824592\n",
            "step: 510, loss: 0.0015366899315267801\n",
            "step: 520, loss: 0.00297342031262815\n",
            "step: 530, loss: 0.010443910956382751\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.9513108614232209, f1=0.95, best_f1=0.95\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.000772496045101434\n",
            "step: 10, loss: 0.0043389322236180305\n",
            "step: 20, loss: 0.02406550571322441\n",
            "step: 30, loss: 0.0007350765517912805\n",
            "step: 40, loss: 0.000801900343503803\n",
            "step: 50, loss: 0.002097371267154813\n",
            "step: 60, loss: 0.0008880756213329732\n",
            "step: 70, loss: 0.0008062673732638359\n",
            "step: 80, loss: 0.0641806572675705\n",
            "step: 90, loss: 0.0016698699910193682\n",
            "step: 100, loss: 0.0018006074242293835\n",
            "step: 110, loss: 0.013004492968320847\n",
            "step: 120, loss: 0.008821860887110233\n",
            "step: 130, loss: 0.00029076123610138893\n",
            "step: 140, loss: 0.07359228283166885\n",
            "step: 150, loss: 0.09565090388059616\n",
            "step: 160, loss: 0.0007748173666186631\n",
            "step: 170, loss: 0.14982785284519196\n",
            "step: 180, loss: 0.0017470415914431214\n",
            "step: 190, loss: 0.01755860447883606\n",
            "step: 200, loss: 0.013243254274129868\n",
            "step: 210, loss: 0.06592364609241486\n",
            "step: 220, loss: 0.00042951543582603335\n",
            "step: 230, loss: 0.019570855423808098\n",
            "step: 240, loss: 0.0074874162673950195\n",
            "step: 250, loss: 0.0009507669601589441\n",
            "step: 260, loss: 4.835258368984796e-05\n",
            "step: 270, loss: 0.00043049734085798264\n",
            "step: 280, loss: 0.009898432530462742\n",
            "step: 290, loss: 0.0005415361956693232\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 300, loss: 3.1452032999368384e-05\n",
            "step: 310, loss: 0.0009855936514213681\n",
            "step: 320, loss: 0.0023305302020162344\n",
            "step: 330, loss: 0.012019430287182331\n",
            "step: 340, loss: 0.06425531953573227\n",
            "step: 350, loss: 0.0002159438154194504\n",
            "step: 360, loss: 0.012928225100040436\n",
            "step: 370, loss: 0.0330202542245388\n",
            "step: 380, loss: 0.000526130199432373\n",
            "step: 390, loss: 0.001809529378078878\n",
            "step: 400, loss: 0.0008437116630375385\n",
            "step: 410, loss: 0.000482113566249609\n",
            "step: 420, loss: 0.000675915100146085\n",
            "step: 430, loss: 0.005288252141326666\n",
            "step: 440, loss: 0.005705762188881636\n",
            "step: 450, loss: 0.00039834174094721675\n",
            "step: 460, loss: 0.00036809747689403594\n",
            "step: 470, loss: 0.08715634793043137\n",
            "step: 480, loss: 0.004937636200338602\n",
            "step: 490, loss: 0.0028464121278375387\n",
            "step: 500, loss: 0.0047203670255839825\n",
            "step: 510, loss: 0.005644166376441717\n",
            "step: 520, loss: 6.775084330001846e-05\n",
            "step: 530, loss: 0.008831147104501724\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.9462563160312356, f1=0.9408553230209282, best_f1=0.95\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004548340570181608\n",
            "step: 10, loss: 0.0062348004430532455\n",
            "step: 20, loss: 0.07622257620096207\n",
            "step: 30, loss: 0.0896143987774849\n",
            "step: 40, loss: 0.0012209531851112843\n",
            "step: 50, loss: 0.00011625434854067862\n",
            "step: 60, loss: 0.00013885054795537144\n",
            "step: 70, loss: 0.011355586349964142\n",
            "step: 80, loss: 0.0026114131323993206\n",
            "step: 90, loss: 0.10451869666576385\n",
            "step: 100, loss: 0.0021872995421290398\n",
            "step: 110, loss: 0.009515291079878807\n",
            "step: 120, loss: 0.007536483462899923\n",
            "step: 130, loss: 0.0031119631603360176\n",
            "step: 140, loss: 0.0006420818390324712\n",
            "step: 150, loss: 0.0009867135668173432\n",
            "step: 160, loss: 0.015688972547650337\n",
            "step: 170, loss: 0.010277907364070415\n",
            "step: 180, loss: 0.0011454442283138633\n",
            "step: 190, loss: 0.0009532590629532933\n",
            "step: 200, loss: 0.0033748962450772524\n",
            "step: 210, loss: 0.01511395163834095\n",
            "step: 220, loss: 0.013437763787806034\n",
            "step: 230, loss: 0.005444212816655636\n",
            "step: 240, loss: 0.0038297364953905344\n",
            "step: 250, loss: 0.018725676462054253\n",
            "step: 260, loss: 0.0030791384633630514\n",
            "step: 270, loss: 0.0005099251866340637\n",
            "step: 280, loss: 0.0013514540623873472\n",
            "step: 290, loss: 0.00013859830505680293\n",
            "step: 300, loss: 0.006061120890080929\n",
            "step: 310, loss: 0.03535695746541023\n",
            "step: 320, loss: 0.0005842375103384256\n",
            "step: 330, loss: 0.0002999693970195949\n",
            "step: 340, loss: 0.016353148967027664\n",
            "step: 350, loss: 0.012911507859826088\n",
            "step: 360, loss: 0.043553270399570465\n",
            "step: 370, loss: 0.018377266824245453\n",
            "step: 380, loss: 2.3956719815032557e-05\n",
            "step: 390, loss: 0.006325086113065481\n",
            "step: 400, loss: 0.0006746512372046709\n",
            "step: 410, loss: 0.0021374376956373453\n",
            "step: 420, loss: 0.002210944192484021\n",
            "step: 430, loss: 0.00012068988871760666\n",
            "step: 440, loss: 1.804508974601049e-05\n",
            "step: 450, loss: 0.0006520616007037461\n",
            "step: 460, loss: 4.9015794502338395e-05\n",
            "step: 470, loss: 0.000318643928039819\n",
            "step: 480, loss: 5.685196447302587e-05\n",
            "step: 490, loss: 0.0010527550475671887\n",
            "step: 500, loss: 0.024060113355517387\n",
            "step: 510, loss: 0.0013213410275056958\n",
            "step: 520, loss: 0.002006059978157282\n",
            "step: 530, loss: 0.0016742937732487917\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9468283582089553, f1=0.9455216989843028, best_f1=0.95\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0058599053882062435\n",
            "step: 10, loss: 0.0005327855469658971\n",
            "step: 20, loss: 0.00025167909916490316\n",
            "step: 30, loss: 0.007295406423509121\n",
            "step: 40, loss: 0.030244935303926468\n",
            "step: 50, loss: 0.0016971547156572342\n",
            "step: 60, loss: 0.0021999890450388193\n",
            "step: 70, loss: 7.353570981649682e-05\n",
            "step: 80, loss: 0.00040820121648721397\n",
            "step: 90, loss: 0.00021291121083777398\n",
            "step: 100, loss: 0.00840213242918253\n",
            "step: 110, loss: 0.0019243784481659532\n",
            "step: 120, loss: 0.00030165465432219207\n",
            "step: 130, loss: 0.037605687975883484\n",
            "step: 140, loss: 0.0001875099987955764\n",
            "step: 150, loss: 3.41322083841078e-05\n",
            "step: 160, loss: 0.09757696837186813\n",
            "step: 170, loss: 0.0004987125284969807\n",
            "step: 180, loss: 0.0036615480203181505\n",
            "step: 190, loss: 0.00020964100258424878\n",
            "step: 200, loss: 0.0005173460813239217\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 210, loss: 0.005257163662463427\n",
            "step: 220, loss: 0.003874304937198758\n",
            "step: 230, loss: 0.0008407766581512988\n",
            "step: 240, loss: 0.00199851649813354\n",
            "step: 250, loss: 0.000438264716649428\n",
            "step: 260, loss: 0.003949700854718685\n",
            "step: 270, loss: 0.00062279321718961\n",
            "step: 280, loss: 0.045356158167123795\n",
            "step: 290, loss: 0.0013152190949767828\n",
            "step: 300, loss: 0.0003746877482626587\n",
            "step: 310, loss: 0.004370194859802723\n",
            "step: 320, loss: 0.0031296424567699432\n",
            "step: 330, loss: 0.00557226687669754\n",
            "step: 340, loss: 0.0003653440799098462\n",
            "step: 350, loss: 8.872504986356944e-05\n",
            "step: 360, loss: 3.5528406442608684e-05\n",
            "step: 370, loss: 0.035682160407304764\n",
            "step: 380, loss: 0.005738281179219484\n",
            "step: 390, loss: 0.00011243780318181962\n",
            "step: 400, loss: 0.0002716686576604843\n",
            "step: 410, loss: 0.009931320324540138\n",
            "step: 420, loss: 0.00016985024558380246\n",
            "step: 430, loss: 0.003015211783349514\n",
            "step: 440, loss: 3.809203553828411e-05\n",
            "step: 450, loss: 0.000335186836309731\n",
            "step: 460, loss: 0.00587307708337903\n",
            "step: 470, loss: 0.09667254239320755\n",
            "step: 480, loss: 0.00549622206017375\n",
            "step: 490, loss: 0.0004741852171719074\n",
            "step: 500, loss: 0.0018278028583154082\n",
            "step: 510, loss: 0.0003354644577484578\n",
            "step: 520, loss: 0.009797856211662292\n",
            "step: 530, loss: 0.010814250446856022\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9457221711131555, f1=0.939517962710323, best_f1=0.95\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003175294492393732\n",
            "step: 10, loss: 0.043146781623363495\n",
            "step: 20, loss: 0.0018075520638376474\n",
            "step: 30, loss: 0.0004297386039979756\n",
            "step: 40, loss: 0.0003414057719055563\n",
            "step: 50, loss: 7.661127165192738e-05\n",
            "step: 60, loss: 0.0003225121763534844\n",
            "step: 70, loss: 0.00019964342936873436\n",
            "step: 80, loss: 0.00016730561037547886\n",
            "step: 90, loss: 9.833327931119129e-05\n",
            "step: 100, loss: 9.032276284415275e-05\n",
            "step: 110, loss: 0.0022703446447849274\n",
            "step: 120, loss: 0.008223219774663448\n",
            "step: 130, loss: 0.0008745890227146447\n",
            "step: 140, loss: 0.0006019773427397013\n",
            "step: 150, loss: 0.004774518311023712\n",
            "step: 160, loss: 0.001484146574512124\n",
            "step: 170, loss: 0.006059739738702774\n",
            "step: 180, loss: 2.044402208412066e-05\n",
            "step: 190, loss: 0.001624459051527083\n",
            "step: 200, loss: 0.0013023227220401168\n",
            "step: 210, loss: 0.0024124435149133205\n",
            "step: 220, loss: 0.02365308813750744\n",
            "step: 230, loss: 6.967597437324002e-05\n",
            "step: 240, loss: 0.003758892184123397\n",
            "step: 250, loss: 8.547628385713324e-05\n",
            "step: 260, loss: 0.00022178140352480114\n",
            "step: 270, loss: 0.000288918090518564\n",
            "step: 280, loss: 0.003853085683658719\n",
            "step: 290, loss: 0.0005326664540916681\n",
            "step: 300, loss: 0.0007826388464309275\n",
            "step: 310, loss: 0.18291479349136353\n",
            "step: 320, loss: 0.0011756322346627712\n",
            "step: 330, loss: 2.5525227101752535e-05\n",
            "step: 340, loss: 0.001550432643853128\n",
            "step: 350, loss: 0.0006128768436610699\n",
            "step: 360, loss: 0.0022007657680660486\n",
            "step: 370, loss: 0.004709990229457617\n",
            "step: 380, loss: 0.0014528505271300673\n",
            "step: 390, loss: 0.002252202946692705\n",
            "step: 400, loss: 0.0014675408601760864\n",
            "step: 410, loss: 0.00020364332885947078\n",
            "step: 420, loss: 0.002036611782386899\n",
            "step: 430, loss: 0.0010564300464466214\n",
            "step: 440, loss: 0.04959004744887352\n",
            "step: 450, loss: 0.00023292495461646467\n",
            "step: 460, loss: 0.03767430782318115\n",
            "step: 470, loss: 0.00036945455940440297\n",
            "step: 480, loss: 0.0002740486233960837\n",
            "step: 490, loss: 0.027629978954792023\n",
            "step: 500, loss: 0.00343683990649879\n",
            "step: 510, loss: 0.0041128406301140785\n",
            "step: 520, loss: 0.0004601321998052299\n",
            "step: 530, loss: 0.012972353026270866\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.9465861588481189, f1=0.9425393883225208, best_f1=0.95\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005972345359623432\n",
            "step: 10, loss: 0.02638235129415989\n",
            "step: 20, loss: 0.027058426290750504\n",
            "step: 30, loss: 0.06820325553417206\n",
            "step: 40, loss: 0.0014013029867783189\n",
            "step: 50, loss: 5.621005402645096e-05\n",
            "step: 60, loss: 0.003664308227598667\n",
            "step: 70, loss: 0.00016602958203293383\n",
            "step: 80, loss: 0.0015956986462697387\n",
            "step: 90, loss: 0.004346094094216824\n",
            "step: 100, loss: 0.03731871396303177\n",
            "step: 110, loss: 0.007837495766580105\n",
            "step: 120, loss: 0.0001260935969185084\n",
            "step: 130, loss: 0.0013189062010496855\n",
            "step: 140, loss: 1.6357420463464223e-05\n",
            "step: 150, loss: 4.307659764890559e-05\n",
            "step: 160, loss: 0.0010279933921992779\n",
            "step: 170, loss: 0.0010151407914236188\n",
            "step: 180, loss: 0.0004741016891784966\n",
            "step: 190, loss: 0.00029375177109614015\n",
            "step: 200, loss: 0.0054284436628222466\n",
            "step: 210, loss: 0.0002439113159198314\n",
            "step: 220, loss: 6.923756882315502e-05\n",
            "step: 230, loss: 0.0006436521653085947\n",
            "step: 240, loss: 0.0008425275445915759\n",
            "step: 250, loss: 2.5191795430146158e-05\n",
            "step: 260, loss: 2.119550117640756e-05\n",
            "step: 270, loss: 0.001718074199743569\n",
            "step: 280, loss: 1.9463732314761728e-05\n",
            "step: 290, loss: 5.3990013839211315e-05\n",
            "step: 300, loss: 9.951119864126667e-05\n",
            "step: 310, loss: 0.00011793010344263166\n",
            "step: 320, loss: 0.0015667888801544905\n",
            "step: 330, loss: 0.010615733452141285\n",
            "step: 340, loss: 0.002060387749224901\n",
            "step: 350, loss: 0.00013641898112837225\n",
            "step: 360, loss: 0.00014340992493089288\n",
            "step: 370, loss: 0.00041257188422605395\n",
            "step: 380, loss: 0.00013947866682428867\n",
            "step: 390, loss: 0.0015071934321895242\n",
            "step: 400, loss: 0.0007506146212108433\n",
            "step: 410, loss: 0.00024080014554783702\n",
            "step: 420, loss: 0.030215125530958176\n",
            "step: 430, loss: 0.00268577691167593\n",
            "step: 440, loss: 0.07936178892850876\n",
            "step: 450, loss: 0.09339699149131775\n",
            "step: 460, loss: 0.003335206536576152\n",
            "step: 470, loss: 0.0019072560826316476\n",
            "step: 480, loss: 0.010502557270228863\n",
            "step: 490, loss: 0.00015257063205353916\n",
            "step: 500, loss: 7.67539459047839e-05\n",
            "step: 510, loss: 0.00026743128546513617\n",
            "step: 520, loss: 0.0020840608049184084\n",
            "step: 530, loss: 0.0011899442179128528\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.9527374824520355, f1=0.9456572224802601, best_f1=0.9456572224802601\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.002130664186552167\n",
            "step: 10, loss: 0.0007848880486562848\n",
            "step: 20, loss: 4.457665636437014e-05\n",
            "step: 30, loss: 5.281100675347261e-05\n",
            "step: 40, loss: 0.003620416158810258\n",
            "step: 50, loss: 0.0007877820753492415\n",
            "step: 60, loss: 7.903968798927963e-05\n",
            "step: 70, loss: 0.002000214299187064\n",
            "step: 80, loss: 0.002225875621661544\n",
            "step: 90, loss: 0.002908776979893446\n",
            "step: 100, loss: 9.38984812819399e-05\n",
            "step: 110, loss: 8.879297820385545e-05\n",
            "step: 120, loss: 2.8637852665269747e-05\n",
            "step: 130, loss: 0.00023128892644308507\n",
            "step: 140, loss: 0.00015243800589814782\n",
            "step: 150, loss: 0.0005635959678329527\n",
            "step: 160, loss: 0.005108403507620096\n",
            "step: 170, loss: 0.002676059491932392\n",
            "step: 180, loss: 0.009639966301620007\n",
            "step: 190, loss: 0.00020087792654521763\n",
            "step: 200, loss: 0.0018255740869790316\n",
            "step: 210, loss: 2.48506275966065e-05\n",
            "step: 220, loss: 0.0003786521847359836\n",
            "step: 230, loss: 0.00047906310646794736\n",
            "step: 240, loss: 0.00025518645998090506\n",
            "step: 250, loss: 7.939319766592234e-05\n",
            "step: 260, loss: 0.014623979106545448\n",
            "step: 270, loss: 0.0013089502463117242\n",
            "step: 280, loss: 0.00012471203808672726\n",
            "step: 290, loss: 0.0011271955445408821\n",
            "step: 300, loss: 0.00016514110029675066\n",
            "step: 310, loss: 0.00015885189350228757\n",
            "step: 320, loss: 2.1162655684747733e-05\n",
            "step: 330, loss: 9.475507977185771e-05\n",
            "step: 340, loss: 8.44743917696178e-05\n",
            "step: 350, loss: 0.0009531527175568044\n",
            "step: 360, loss: 0.05459558963775635\n",
            "step: 370, loss: 0.009688341058790684\n",
            "step: 380, loss: 8.056867955019698e-05\n",
            "step: 390, loss: 0.00229735323227942\n",
            "step: 400, loss: 0.00011439144145697355\n",
            "step: 410, loss: 0.0454443022608757\n",
            "step: 420, loss: 0.017566030845046043\n",
            "step: 430, loss: 0.00018304074183106422\n",
            "step: 440, loss: 0.00013004337961319834\n",
            "step: 450, loss: 0.0001737626298563555\n",
            "step: 460, loss: 0.012643427588045597\n",
            "step: 470, loss: 0.008930277079343796\n",
            "step: 480, loss: 4.281242217984982e-05\n",
            "step: 490, loss: 0.00015091565728653222\n",
            "step: 500, loss: 0.012858616188168526\n",
            "step: 510, loss: 0.00012264770339243114\n",
            "step: 520, loss: 5.822117964271456e-05\n",
            "step: 530, loss: 6.013858364894986e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9482185273159144, f1=0.940570893776322, best_f1=0.9456572224802601\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0003288751468062401\n",
            "step: 10, loss: 0.0012420808197930455\n",
            "step: 20, loss: 2.9650340366060846e-05\n",
            "step: 30, loss: 0.00018484247266314924\n",
            "step: 40, loss: 5.3659689001506194e-05\n",
            "step: 50, loss: 0.0011508205207064748\n",
            "step: 60, loss: 0.000313875381834805\n",
            "step: 70, loss: 4.5187811338109896e-05\n",
            "step: 80, loss: 0.0004439519834704697\n",
            "step: 90, loss: 2.9098122467985377e-05\n",
            "step: 100, loss: 0.00045101181603968143\n",
            "step: 110, loss: 0.0004503658274188638\n",
            "step: 120, loss: 1.244587929249974e-05\n",
            "step: 130, loss: 3.596628812374547e-05\n",
            "step: 140, loss: 0.004680579528212547\n",
            "step: 150, loss: 0.036050863564014435\n",
            "step: 160, loss: 7.938810449559242e-05\n",
            "step: 170, loss: 4.4942586100660264e-05\n",
            "step: 180, loss: 0.001097927917726338\n",
            "step: 190, loss: 6.496047717519104e-05\n",
            "step: 200, loss: 0.14441686868667603\n",
            "step: 210, loss: 0.0009187874966301024\n",
            "step: 220, loss: 0.0002108602930093184\n",
            "step: 230, loss: 6.583008507732302e-05\n",
            "step: 240, loss: 0.022205207496881485\n",
            "step: 250, loss: 0.000302773347357288\n",
            "step: 260, loss: 0.00018084126350004226\n",
            "step: 270, loss: 0.0008468671003356576\n",
            "step: 280, loss: 0.0006038869614712894\n",
            "step: 290, loss: 2.4935779947554693e-05\n",
            "step: 300, loss: 1.2535383575595915e-05\n",
            "step: 310, loss: 0.025947585701942444\n",
            "step: 320, loss: 1.9891887859557755e-05\n",
            "step: 330, loss: 0.0007280200952664018\n",
            "step: 340, loss: 0.0002268410025862977\n",
            "step: 350, loss: 0.00032602716237306595\n",
            "step: 360, loss: 0.00022379684378392994\n",
            "step: 370, loss: 0.006703819613903761\n",
            "step: 380, loss: 0.11919849365949631\n",
            "step: 390, loss: 0.00032885049586184323\n",
            "step: 400, loss: 0.017314041033387184\n",
            "step: 410, loss: 5.935007720836438e-05\n",
            "step: 420, loss: 0.00013864960055798292\n",
            "step: 430, loss: 0.014630105346441269\n",
            "step: 440, loss: 0.020160747691988945\n",
            "step: 450, loss: 5.4985743190627545e-05\n",
            "step: 460, loss: 0.0195735115557909\n",
            "step: 470, loss: 6.582149944733828e-05\n",
            "step: 480, loss: 0.002849870827049017\n",
            "step: 490, loss: 9.245659748557955e-05\n",
            "step: 500, loss: 0.0002114035887643695\n",
            "step: 510, loss: 0.004911710508167744\n",
            "step: 520, loss: 0.00039928779006004333\n",
            "step: 530, loss: 0.00036606844514608383\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9499766245909304, f1=0.9440591770688858, best_f1=0.9456572224802601\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00013022871280554682\n",
            "step: 10, loss: 0.00020498427329584956\n",
            "step: 20, loss: 0.001351278042420745\n",
            "step: 30, loss: 0.011283784173429012\n",
            "step: 40, loss: 3.689081859192811e-05\n",
            "step: 50, loss: 0.02101592719554901\n",
            "step: 60, loss: 0.00020105403382331133\n",
            "step: 70, loss: 8.387610432691872e-05\n",
            "step: 80, loss: 0.0012229534331709146\n",
            "step: 90, loss: 0.0001582368859089911\n",
            "step: 100, loss: 2.4991399186546914e-05\n",
            "step: 110, loss: 0.011053268797695637\n",
            "step: 120, loss: 9.843255247687921e-05\n",
            "step: 130, loss: 0.0034037348814308643\n",
            "step: 140, loss: 2.332864642085042e-05\n",
            "step: 150, loss: 0.014923185110092163\n",
            "step: 160, loss: 0.002458878792822361\n",
            "step: 170, loss: 6.326310540316626e-05\n",
            "step: 180, loss: 0.002097364515066147\n",
            "step: 190, loss: 0.003370001446455717\n",
            "step: 200, loss: 0.001014654990285635\n",
            "step: 210, loss: 3.70833913621027e-05\n",
            "step: 220, loss: 0.12194394320249557\n",
            "step: 230, loss: 0.00041136686922982335\n",
            "step: 240, loss: 0.004681771155446768\n",
            "step: 250, loss: 0.00012061349116265774\n",
            "step: 260, loss: 7.167577859945595e-05\n",
            "step: 270, loss: 1.173443433799548e-05\n",
            "step: 280, loss: 1.3317597222339828e-05\n",
            "step: 290, loss: 0.00018675060709938407\n",
            "step: 300, loss: 0.00022080967028159648\n",
            "step: 310, loss: 0.030027145519852638\n",
            "step: 320, loss: 9.15553537197411e-05\n",
            "step: 330, loss: 5.950398917775601e-05\n",
            "step: 340, loss: 0.00013411005784291774\n",
            "step: 350, loss: 0.008476442657411098\n",
            "step: 360, loss: 0.00041608759784139693\n",
            "step: 370, loss: 0.005606547929346561\n",
            "step: 380, loss: 0.002771536586806178\n",
            "step: 390, loss: 0.00010235852096229792\n",
            "step: 400, loss: 0.006005843635648489\n",
            "step: 410, loss: 0.00015219573106151074\n",
            "step: 420, loss: 0.011139729991555214\n",
            "step: 430, loss: 1.340321159659652e-05\n",
            "step: 440, loss: 0.00022849300876259804\n",
            "step: 450, loss: 0.0018463494488969445\n",
            "step: 460, loss: 0.013735978864133358\n",
            "step: 470, loss: 0.00021382630802690983\n",
            "step: 480, loss: 0.011131701059639454\n",
            "step: 490, loss: 0.00011117690883111209\n",
            "step: 500, loss: 0.000841280969325453\n",
            "step: 510, loss: 0.00044048551353625953\n",
            "step: 520, loss: 0.00020855346519965678\n",
            "step: 530, loss: 0.034896865487098694\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.950681070925317, f1=0.9438515081206497, best_f1=0.9456572224802601\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:33, 171.34it/s]\n",
            "load_f1 = 0.9533146591970122\n",
            "real_f1 = 0.9517564402810305\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 144.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon-Google - Running the matcher"
      ],
      "metadata": {
        "id": "nyGyaWAphstt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Amazon-Google.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Amazon-Google \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "wWlGklS4hstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c6e86a-41ae-4bcf-da01-aa1027f13fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.5086256265640259\n",
            "step: 10, loss: 0.41612404584884644\n",
            "step: 20, loss: 0.4577934443950653\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 30, loss: 0.31788212060928345\n",
            "step: 40, loss: 0.3502870500087738\n",
            "step: 50, loss: 0.47081008553504944\n",
            "step: 60, loss: 0.5017970204353333\n",
            "step: 70, loss: 0.3349350094795227\n",
            "step: 80, loss: 0.38371917605400085\n",
            "step: 90, loss: 0.23197206854820251\n",
            "step: 100, loss: 0.295478880405426\n",
            "step: 110, loss: 0.28868332505226135\n",
            "step: 120, loss: 0.40088552236557007\n",
            "step: 130, loss: 0.26275891065597534\n",
            "step: 140, loss: 0.3985206186771393\n",
            "step: 150, loss: 0.14914818108081818\n",
            "step: 160, loss: 0.48064538836479187\n",
            "step: 170, loss: 0.19103705883026123\n",
            "step: 180, loss: 0.3250727355480194\n",
            "step: 190, loss: 0.4355223476886749\n",
            "step: 200, loss: 0.2202732264995575\n",
            "step: 210, loss: 0.21418413519859314\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.5007974481658691, f1=0.5333333333333332, best_f1=0.5333333333333332\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.2833678126335144\n",
            "step: 10, loss: 0.13413883745670319\n",
            "step: 20, loss: 0.4240352511405945\n",
            "step: 30, loss: 0.2806291878223419\n",
            "step: 40, loss: 0.34423211216926575\n",
            "step: 50, loss: 0.2329908013343811\n",
            "step: 60, loss: 0.3889244496822357\n",
            "step: 70, loss: 0.16056710481643677\n",
            "step: 80, loss: 0.20059970021247864\n",
            "step: 90, loss: 0.15279065072536469\n",
            "step: 100, loss: 0.4524977505207062\n",
            "step: 110, loss: 0.3548129200935364\n",
            "step: 120, loss: 0.12562336027622223\n",
            "step: 130, loss: 0.2517614960670471\n",
            "step: 140, loss: 0.15000911056995392\n",
            "step: 150, loss: 0.32492268085479736\n",
            "step: 160, loss: 0.09802205860614777\n",
            "step: 170, loss: 0.22028961777687073\n",
            "step: 180, loss: 0.1638701856136322\n",
            "step: 190, loss: 0.2911703884601593\n",
            "step: 200, loss: 0.11103343218564987\n",
            "step: 210, loss: 0.1968298852443695\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.592734225621415, f1=0.6188679245283019, best_f1=0.6188679245283019\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10362720489501953\n",
            "step: 10, loss: 0.09486681967973709\n",
            "step: 20, loss: 0.24764369428157806\n",
            "step: 30, loss: 0.05118637531995773\n",
            "step: 40, loss: 0.24419090151786804\n",
            "step: 50, loss: 0.20432782173156738\n",
            "step: 60, loss: 0.20156818628311157\n",
            "step: 70, loss: 0.08160605281591415\n",
            "step: 80, loss: 0.17485590279102325\n",
            "step: 90, loss: 0.07744058966636658\n",
            "step: 100, loss: 0.27071821689605713\n",
            "step: 110, loss: 0.06637992709875107\n",
            "step: 120, loss: 0.14198853075504303\n",
            "step: 130, loss: 0.15163016319274902\n",
            "step: 140, loss: 0.08196346461772919\n",
            "step: 150, loss: 0.1833205223083496\n",
            "step: 160, loss: 0.14225947856903076\n",
            "step: 170, loss: 0.30042317509651184\n",
            "step: 180, loss: 0.07738704234361649\n",
            "step: 190, loss: 0.023746401071548462\n",
            "step: 200, loss: 0.16656482219696045\n",
            "step: 210, loss: 0.15955860912799835\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.5826235093696762, f1=0.6388888888888888, best_f1=0.6188679245283019\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1673239916563034\n",
            "step: 10, loss: 0.13271470367908478\n",
            "step: 20, loss: 0.11635757982730865\n",
            "step: 30, loss: 0.14262953400611877\n",
            "step: 40, loss: 0.04938328638672829\n",
            "step: 50, loss: 0.10683782398700714\n",
            "step: 60, loss: 0.2054523527622223\n",
            "step: 70, loss: 0.14159086346626282\n",
            "step: 80, loss: 0.09451833367347717\n",
            "step: 90, loss: 0.07069920748472214\n",
            "step: 100, loss: 0.26775339245796204\n",
            "step: 110, loss: 0.558018684387207\n",
            "step: 120, loss: 0.19120392203330994\n",
            "step: 130, loss: 0.3959060311317444\n",
            "step: 140, loss: 0.3310644030570984\n",
            "step: 150, loss: 0.13553908467292786\n",
            "step: 160, loss: 0.13064563274383545\n",
            "step: 170, loss: 0.1727769523859024\n",
            "step: 180, loss: 0.07855533063411713\n",
            "step: 190, loss: 0.166444793343544\n",
            "step: 200, loss: 0.06627125293016434\n",
            "step: 210, loss: 0.4382694959640503\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.6232741617357003, f1=0.6571428571428571, best_f1=0.6571428571428571\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.1620785892009735\n",
            "step: 10, loss: 0.092039093375206\n",
            "step: 20, loss: 0.13871155679225922\n",
            "step: 30, loss: 0.03257761523127556\n",
            "step: 40, loss: 0.11371558904647827\n",
            "step: 50, loss: 0.12984120845794678\n",
            "step: 60, loss: 0.08277779817581177\n",
            "step: 70, loss: 0.22598829865455627\n",
            "step: 80, loss: 0.07315058261156082\n",
            "step: 90, loss: 0.13462108373641968\n",
            "step: 100, loss: 0.005983374547213316\n",
            "step: 110, loss: 0.1624094396829605\n",
            "step: 120, loss: 0.1283838152885437\n",
            "step: 130, loss: 0.10824644565582275\n",
            "step: 140, loss: 0.20408964157104492\n",
            "step: 150, loss: 0.07820635288953781\n",
            "step: 160, loss: 0.05931814759969711\n",
            "step: 170, loss: 0.04567905142903328\n",
            "step: 180, loss: 0.18310587108135223\n",
            "step: 190, loss: 0.24858027696609497\n",
            "step: 200, loss: 0.24896202981472015\n",
            "step: 210, loss: 0.07983982563018799\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.6345811051693404, f1=0.6815642458100558, best_f1=0.6815642458100558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.025912277400493622\n",
            "step: 10, loss: 0.06983970105648041\n",
            "step: 20, loss: 0.023040207102894783\n",
            "step: 30, loss: 0.15669141709804535\n",
            "step: 40, loss: 0.05259590595960617\n",
            "step: 50, loss: 0.13275399804115295\n",
            "step: 60, loss: 0.09776223450899124\n",
            "step: 70, loss: 0.11145090311765671\n",
            "step: 80, loss: 0.03064206801354885\n",
            "step: 90, loss: 0.03117545135319233\n",
            "step: 100, loss: 0.0526144914329052\n",
            "step: 110, loss: 0.15812356770038605\n",
            "step: 120, loss: 0.009191041812300682\n",
            "step: 130, loss: 0.02315087430179119\n",
            "step: 140, loss: 0.040473662316799164\n",
            "step: 150, loss: 0.05249851197004318\n",
            "step: 160, loss: 0.021952936425805092\n",
            "step: 170, loss: 0.03985228389501572\n",
            "step: 180, loss: 0.04401343688368797\n",
            "step: 190, loss: 0.0768163874745369\n",
            "step: 200, loss: 0.10867326706647873\n",
            "step: 210, loss: 0.09738139808177948\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.6335174953959485, f1=0.6979362101313321, best_f1=0.6815642458100558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.09554770588874817\n",
            "step: 10, loss: 0.012215590104460716\n",
            "step: 20, loss: 0.033800676465034485\n",
            "step: 30, loss: 0.10156688094139099\n",
            "step: 40, loss: 0.047386862337589264\n",
            "step: 50, loss: 0.0759609118103981\n",
            "step: 60, loss: 0.0591922253370285\n",
            "step: 70, loss: 0.09162511676549911\n",
            "step: 80, loss: 0.07628366351127625\n",
            "step: 90, loss: 0.037615928798913956\n",
            "step: 100, loss: 0.09471079707145691\n",
            "step: 110, loss: 0.35481172800064087\n",
            "step: 120, loss: 0.0949360802769661\n",
            "step: 130, loss: 0.07895660400390625\n",
            "step: 140, loss: 0.025700535625219345\n",
            "step: 150, loss: 0.0634094625711441\n",
            "step: 160, loss: 0.24449579417705536\n",
            "step: 170, loss: 0.08612587302923203\n",
            "step: 180, loss: 0.01004744041711092\n",
            "step: 190, loss: 0.07219813019037247\n",
            "step: 200, loss: 0.0110324053093791\n",
            "step: 210, loss: 0.2258138656616211\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.6295585412667946, f1=0.6826347305389221, best_f1=0.6815642458100558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10889778286218643\n",
            "step: 10, loss: 0.011621021665632725\n",
            "step: 20, loss: 0.020405253395438194\n",
            "step: 30, loss: 0.24676208198070526\n",
            "step: 40, loss: 0.10603626072406769\n",
            "step: 50, loss: 0.006709471810609102\n",
            "step: 60, loss: 0.1749172955751419\n",
            "step: 70, loss: 0.1559273898601532\n",
            "step: 80, loss: 0.1289476454257965\n",
            "step: 90, loss: 0.17774082720279694\n",
            "step: 100, loss: 0.17350465059280396\n",
            "step: 110, loss: 0.024716539308428764\n",
            "step: 120, loss: 0.053465597331523895\n",
            "step: 130, loss: 0.029750101268291473\n",
            "step: 140, loss: 0.03949691355228424\n",
            "step: 150, loss: 0.08861776441335678\n",
            "step: 160, loss: 0.14479117095470428\n",
            "step: 170, loss: 0.14755110442638397\n",
            "step: 180, loss: 0.04679175093770027\n",
            "step: 190, loss: 0.08343423157930374\n",
            "step: 200, loss: 0.06349232792854309\n",
            "step: 210, loss: 0.08800263702869415\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.6297709923664123, f1=0.6806083650190115, best_f1=0.6815642458100558\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.028116805478930473\n",
            "step: 10, loss: 0.08546948432922363\n",
            "step: 20, loss: 0.03428782522678375\n",
            "step: 30, loss: 0.07241374254226685\n",
            "step: 40, loss: 0.008911800570786\n",
            "step: 50, loss: 0.0729726180434227\n",
            "step: 60, loss: 0.014441462233662605\n",
            "step: 70, loss: 0.07282520085573196\n",
            "step: 80, loss: 0.034875158220529556\n",
            "step: 90, loss: 0.027835527434945107\n",
            "step: 100, loss: 0.010870789177715778\n",
            "step: 110, loss: 0.028193030506372452\n",
            "step: 120, loss: 0.035957515239715576\n",
            "step: 130, loss: 0.08026159554719925\n",
            "step: 140, loss: 0.058614734560251236\n",
            "step: 150, loss: 0.03103623166680336\n",
            "step: 160, loss: 0.0801723375916481\n",
            "step: 170, loss: 0.029368281364440918\n",
            "step: 180, loss: 0.03246400132775307\n",
            "step: 190, loss: 0.020919786766171455\n",
            "step: 200, loss: 0.06380905210971832\n",
            "step: 210, loss: 0.0765429213643074\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.6451612903225807, f1=0.6777546777546777, best_f1=0.6777546777546777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.030230725184082985\n",
            "step: 10, loss: 0.024766797199845314\n",
            "step: 20, loss: 0.0038660261780023575\n",
            "step: 30, loss: 0.0022501302883028984\n",
            "step: 40, loss: 0.02080022543668747\n",
            "step: 50, loss: 0.05003904923796654\n",
            "step: 60, loss: 0.09997828304767609\n",
            "step: 70, loss: 0.02751813642680645\n",
            "step: 80, loss: 0.025449078530073166\n",
            "step: 90, loss: 0.09174223989248276\n",
            "step: 100, loss: 0.015344563871622086\n",
            "step: 110, loss: 0.014450764283537865\n",
            "step: 120, loss: 0.07420209795236588\n",
            "step: 130, loss: 0.05687015876173973\n",
            "step: 140, loss: 0.00574226351454854\n",
            "step: 150, loss: 0.017070546746253967\n",
            "step: 160, loss: 0.034510258585214615\n",
            "step: 170, loss: 0.023218927904963493\n",
            "step: 180, loss: 0.012997672893106937\n",
            "step: 190, loss: 0.009633286856114864\n",
            "step: 200, loss: 0.039117030799388885\n",
            "step: 210, loss: 0.10332268476486206\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.6294964028776978, f1=0.6937269372693727, best_f1=0.6777546777546777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.049014050513505936\n",
            "step: 10, loss: 0.051695529371500015\n",
            "step: 20, loss: 0.04718824476003647\n",
            "step: 30, loss: 0.045065999031066895\n",
            "step: 40, loss: 0.01085655577480793\n",
            "step: 50, loss: 0.03385894000530243\n",
            "step: 60, loss: 0.023677000775933266\n",
            "step: 70, loss: 0.029623888432979584\n",
            "step: 80, loss: 0.010548214428126812\n",
            "step: 90, loss: 0.15955793857574463\n",
            "step: 100, loss: 0.16685663163661957\n",
            "step: 110, loss: 0.08332715928554535\n",
            "step: 120, loss: 0.06588653475046158\n",
            "step: 130, loss: 0.02110329456627369\n",
            "step: 140, loss: 0.26502475142478943\n",
            "step: 150, loss: 0.013838821090757847\n",
            "step: 160, loss: 0.015101809054613113\n",
            "step: 170, loss: 0.011301213875412941\n",
            "step: 180, loss: 0.00975011009722948\n",
            "step: 190, loss: 0.031968098133802414\n",
            "step: 200, loss: 0.013224445283412933\n",
            "step: 210, loss: 0.08711009472608566\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.6397058823529412, f1=0.6781609195402298, best_f1=0.6777546777546777\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005535597912967205\n",
            "step: 10, loss: 0.013644412159919739\n",
            "step: 20, loss: 0.3573288917541504\n",
            "step: 30, loss: 0.016289444640278816\n",
            "step: 40, loss: 0.008597902953624725\n",
            "step: 50, loss: 0.0031407459173351526\n",
            "step: 60, loss: 0.006734529975801706\n",
            "step: 70, loss: 0.002622544765472412\n",
            "step: 80, loss: 0.014879901893436909\n",
            "step: 90, loss: 0.01530485786497593\n",
            "step: 100, loss: 0.0012714904733002186\n",
            "step: 110, loss: 0.018779275938868523\n",
            "step: 120, loss: 0.0013781852321699262\n",
            "step: 130, loss: 0.004616851452738047\n",
            "step: 140, loss: 0.06133987009525299\n",
            "step: 150, loss: 0.0026671062223613262\n",
            "step: 160, loss: 0.018505502492189407\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 170, loss: 0.007756715174764395\n",
            "step: 180, loss: 0.05522908270359039\n",
            "step: 190, loss: 0.015857690945267677\n",
            "step: 200, loss: 0.004970876034349203\n",
            "step: 210, loss: 0.006349790841341019\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 12: dev_f1=0.646643109540636, f1=0.6810035842293907, best_f1=0.6810035842293907\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.004346881993114948\n",
            "step: 10, loss: 0.0555773600935936\n",
            "step: 20, loss: 0.0036798056680709124\n",
            "step: 30, loss: 0.0006894786492921412\n",
            "step: 40, loss: 0.05571551248431206\n",
            "step: 50, loss: 0.08007440716028214\n",
            "step: 60, loss: 0.011134816333651543\n",
            "step: 70, loss: 0.0034691286273300648\n",
            "step: 80, loss: 0.027584929019212723\n",
            "step: 90, loss: 0.09131988137960434\n",
            "step: 100, loss: 0.08931994438171387\n",
            "step: 110, loss: 0.06100132688879967\n",
            "step: 120, loss: 0.005594199988991022\n",
            "step: 130, loss: 0.0065441434271633625\n",
            "step: 140, loss: 0.013626376166939735\n",
            "step: 150, loss: 0.00111361313611269\n",
            "step: 160, loss: 0.026249175891280174\n",
            "step: 170, loss: 0.019527478143572807\n",
            "step: 180, loss: 0.01952027529478073\n",
            "step: 190, loss: 0.023280788213014603\n",
            "step: 200, loss: 0.041204459965229034\n",
            "step: 210, loss: 0.003424506401643157\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 13: dev_f1=0.655367231638418, f1=0.6879699248120301, best_f1=0.6879699248120301\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0066306209191679955\n",
            "step: 10, loss: 0.032840121537446976\n",
            "step: 20, loss: 0.004523843992501497\n",
            "step: 30, loss: 0.01375288050621748\n",
            "step: 40, loss: 0.0031594315078109503\n",
            "step: 50, loss: 0.0038564251735806465\n",
            "step: 60, loss: 0.023565182462334633\n",
            "step: 70, loss: 0.0021316497586667538\n",
            "step: 80, loss: 0.10048118233680725\n",
            "step: 90, loss: 0.0774931088089943\n",
            "step: 100, loss: 0.00571187399327755\n",
            "step: 110, loss: 0.0026479989755898714\n",
            "step: 120, loss: 0.005517073441296816\n",
            "step: 130, loss: 0.011539044789969921\n",
            "step: 140, loss: 0.014050778932869434\n",
            "step: 150, loss: 0.27494707703590393\n",
            "step: 160, loss: 0.001387935713864863\n",
            "step: 170, loss: 0.037699829787015915\n",
            "step: 180, loss: 0.007669058162719011\n",
            "step: 190, loss: 0.03411960229277611\n",
            "step: 200, loss: 0.07352373749017715\n",
            "step: 210, loss: 0.0125635527074337\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.6431226765799257, f1=0.6966292134831461, best_f1=0.6879699248120301\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.010348054580390453\n",
            "step: 10, loss: 0.024934416636824608\n",
            "step: 20, loss: 0.022256184369325638\n",
            "step: 30, loss: 0.00603345176205039\n",
            "step: 40, loss: 0.0028380309231579304\n",
            "step: 50, loss: 0.004381721839308739\n",
            "step: 60, loss: 0.0015780660323798656\n",
            "step: 70, loss: 0.04114622250199318\n",
            "step: 80, loss: 0.09721468389034271\n",
            "step: 90, loss: 0.005019229371100664\n",
            "step: 100, loss: 0.012900205329060555\n",
            "step: 110, loss: 0.03318193554878235\n",
            "step: 120, loss: 0.03394200652837753\n",
            "step: 130, loss: 0.005671001970767975\n",
            "step: 140, loss: 0.13387693464756012\n",
            "step: 150, loss: 0.0058832657523453236\n",
            "step: 160, loss: 0.008789795450866222\n",
            "step: 170, loss: 0.0031691037584096193\n",
            "step: 180, loss: 0.01616487093269825\n",
            "step: 190, loss: 0.0015557543374598026\n",
            "step: 200, loss: 0.0011582609731703997\n",
            "step: 210, loss: 0.05251605808734894\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.6450381679389313, f1=0.6835937499999999, best_f1=0.6879699248120301\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2293it [00:09, 241.85it/s]\n",
            "load_f1 = 0.6506469500924215\n",
            "real_f1 = 0.6407407407407407\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 143.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walmart-Amazon - Running the matcher"
      ],
      "metadata": {
        "id": "dL0eWrGYhstu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Walmart-Amazon.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Structured/Walmart-Amazon \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "PqrllyyZhstu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9a4a07-4d21-47c9-cba9-f8e04dcd9436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.46696722507476807\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5223172903060913\n",
            "step: 20, loss: 0.24930629134178162\n",
            "step: 30, loss: 0.3845055401325226\n",
            "step: 40, loss: 0.23351410031318665\n",
            "step: 50, loss: 0.31782785058021545\n",
            "step: 60, loss: 0.4599210023880005\n",
            "step: 70, loss: 0.4105948805809021\n",
            "step: 80, loss: 0.15496882796287537\n",
            "step: 90, loss: 0.3158591091632843\n",
            "step: 100, loss: 0.4332602024078369\n",
            "step: 110, loss: 0.25697416067123413\n",
            "step: 120, loss: 0.36120298504829407\n",
            "step: 130, loss: 0.3413851261138916\n",
            "step: 140, loss: 0.19719186425209045\n",
            "step: 150, loss: 0.30288997292518616\n",
            "step: 160, loss: 0.2397376298904419\n",
            "step: 170, loss: 0.3707473874092102\n",
            "step: 180, loss: 0.16259802877902985\n",
            "step: 190, loss: 0.16540496051311493\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.17216770740410348, f1=0.17216770740410348, best_f1=0.17216770740410348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.3863462507724762\n",
            "step: 10, loss: 0.3113236427307129\n",
            "step: 20, loss: 0.7209707498550415\n",
            "step: 30, loss: 0.20440295338630676\n",
            "step: 40, loss: 0.3835618495941162\n",
            "step: 50, loss: 0.31374743580818176\n",
            "step: 60, loss: 0.228581503033638\n",
            "step: 70, loss: 0.23896078765392303\n",
            "step: 80, loss: 0.04016793891787529\n",
            "step: 90, loss: 0.13847675919532776\n",
            "step: 100, loss: 0.16638565063476562\n",
            "step: 110, loss: 0.11875394731760025\n",
            "step: 120, loss: 0.180443674325943\n",
            "step: 130, loss: 0.19483977556228638\n",
            "step: 140, loss: 0.27253416180610657\n",
            "step: 150, loss: 0.17995043098926544\n",
            "step: 160, loss: 0.24568909406661987\n",
            "step: 170, loss: 0.02513565495610237\n",
            "step: 180, loss: 0.060027290135622025\n",
            "step: 190, loss: 0.1934477835893631\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7704918032786885, f1=0.7837837837837839, best_f1=0.7837837837837839\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18263676762580872\n",
            "step: 10, loss: 0.14082524180412292\n",
            "step: 20, loss: 0.19196194410324097\n",
            "step: 30, loss: 0.0111743975430727\n",
            "step: 40, loss: 0.06504873186349869\n",
            "step: 50, loss: 0.0681341215968132\n",
            "step: 60, loss: 0.07182851433753967\n",
            "step: 70, loss: 0.09554020315408707\n",
            "step: 80, loss: 0.24174344539642334\n",
            "step: 90, loss: 0.11490752547979355\n",
            "step: 100, loss: 0.10932406783103943\n",
            "step: 110, loss: 0.31550511717796326\n",
            "step: 120, loss: 0.16521939635276794\n",
            "step: 130, loss: 0.06895964592695236\n",
            "step: 140, loss: 0.08161978423595428\n",
            "step: 150, loss: 0.2245696783065796\n",
            "step: 160, loss: 0.04435766488313675\n",
            "step: 170, loss: 0.24203850328922272\n",
            "step: 180, loss: 0.16754846274852753\n",
            "step: 190, loss: 0.06236737221479416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.7989130434782609, f1=0.8116710875331565, best_f1=0.8116710875331565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008709488436579704\n",
            "step: 10, loss: 0.03711690381169319\n",
            "step: 20, loss: 0.03105505369603634\n",
            "step: 30, loss: 0.007215695921331644\n",
            "step: 40, loss: 0.15381385385990143\n",
            "step: 50, loss: 0.0792425349354744\n",
            "step: 60, loss: 0.0408676341176033\n",
            "step: 70, loss: 0.08146657794713974\n",
            "step: 80, loss: 0.03099505417048931\n",
            "step: 90, loss: 0.06680808961391449\n",
            "step: 100, loss: 0.16874663531780243\n",
            "step: 110, loss: 0.11272131651639938\n",
            "step: 120, loss: 0.07462608814239502\n",
            "step: 130, loss: 0.11983326077461243\n",
            "step: 140, loss: 0.16394075751304626\n",
            "step: 150, loss: 0.002871517790481448\n",
            "step: 160, loss: 0.04602091759443283\n",
            "step: 170, loss: 0.07033819705247879\n",
            "step: 180, loss: 0.023726776242256165\n",
            "step: 190, loss: 0.006071147508919239\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.7696476964769648, f1=0.7978436657681941, best_f1=0.8116710875331565\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.27108606696128845\n",
            "step: 10, loss: 0.10122024267911911\n",
            "step: 20, loss: 0.006466829217970371\n",
            "step: 30, loss: 0.04024134576320648\n",
            "step: 40, loss: 0.00347076915204525\n",
            "step: 50, loss: 0.04779309779405594\n",
            "step: 60, loss: 0.17122752964496613\n",
            "step: 70, loss: 0.12552428245544434\n",
            "step: 80, loss: 0.06797832250595093\n",
            "step: 90, loss: 0.16178052127361298\n",
            "step: 100, loss: 0.05709342285990715\n",
            "step: 110, loss: 0.050933029502630234\n",
            "step: 120, loss: 0.02942153438925743\n",
            "step: 130, loss: 0.2437765896320343\n",
            "step: 140, loss: 0.015143364667892456\n",
            "step: 150, loss: 0.1303461641073227\n",
            "step: 160, loss: 0.08576948940753937\n",
            "step: 170, loss: 0.014477788470685482\n",
            "step: 180, loss: 0.026930196210741997\n",
            "step: 190, loss: 0.013188430108129978\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.802030456852792, f1=0.785, best_f1=0.785\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008347606286406517\n",
            "step: 10, loss: 0.01576179824769497\n",
            "step: 20, loss: 0.19869111478328705\n",
            "step: 30, loss: 0.053692009299993515\n",
            "step: 40, loss: 0.017388703301548958\n",
            "step: 50, loss: 0.02389562502503395\n",
            "step: 60, loss: 0.022359240800142288\n",
            "step: 70, loss: 0.09221960604190826\n",
            "step: 80, loss: 0.08077005296945572\n",
            "step: 90, loss: 0.1616876870393753\n",
            "step: 100, loss: 0.07957003265619278\n",
            "step: 110, loss: 0.004353655967861414\n",
            "step: 120, loss: 0.021693546324968338\n",
            "step: 130, loss: 0.1288551241159439\n",
            "step: 140, loss: 0.010649819858372211\n",
            "step: 150, loss: 0.007614356465637684\n",
            "step: 160, loss: 0.08007282763719559\n",
            "step: 170, loss: 0.08015541732311249\n",
            "step: 180, loss: 0.0174605343490839\n",
            "step: 190, loss: 0.008439463563263416\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 6: dev_f1=0.8153846153846155, f1=0.8040712468193384, best_f1=0.8040712468193384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006419401615858078\n",
            "step: 10, loss: 0.044581279158592224\n",
            "step: 20, loss: 0.0019963327795267105\n",
            "step: 30, loss: 0.0896696224808693\n",
            "step: 40, loss: 0.008117806166410446\n",
            "step: 50, loss: 0.0007579482044093311\n",
            "step: 60, loss: 0.004185533616691828\n",
            "step: 70, loss: 0.001863638637587428\n",
            "step: 80, loss: 0.0007001321064308286\n",
            "step: 90, loss: 0.016353171318769455\n",
            "step: 100, loss: 0.26052626967430115\n",
            "step: 110, loss: 0.04222693294286728\n",
            "step: 120, loss: 0.01647946983575821\n",
            "step: 130, loss: 0.010208300314843655\n",
            "step: 140, loss: 0.01830548420548439\n",
            "step: 150, loss: 0.050125084817409515\n",
            "step: 160, loss: 0.007082927506417036\n",
            "step: 170, loss: 0.1996358186006546\n",
            "step: 180, loss: 0.022503923624753952\n",
            "step: 190, loss: 0.04495585337281227\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.8055555555555556, f1=0.8247978436657681, best_f1=0.8040712468193384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02118837833404541\n",
            "step: 10, loss: 0.011107198894023895\n",
            "step: 20, loss: 0.0024966890923678875\n",
            "step: 30, loss: 0.003667006967589259\n",
            "step: 40, loss: 0.0430489182472229\n",
            "step: 50, loss: 0.1652243733406067\n",
            "step: 60, loss: 0.1621386706829071\n",
            "step: 70, loss: 0.06441491842269897\n",
            "step: 80, loss: 0.05932936072349548\n",
            "step: 90, loss: 0.06195119395852089\n",
            "step: 100, loss: 0.0037224143743515015\n",
            "step: 110, loss: 0.03113877959549427\n",
            "step: 120, loss: 0.12038569152355194\n",
            "step: 130, loss: 0.0071009485982358456\n",
            "step: 140, loss: 0.022149566560983658\n",
            "step: 150, loss: 0.06692028790712357\n",
            "step: 160, loss: 0.001554093207232654\n",
            "step: 170, loss: 0.001410702825523913\n",
            "step: 180, loss: 0.1767594814300537\n",
            "step: 190, loss: 0.005693000741302967\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.8030303030303031, f1=0.8174807197943443, best_f1=0.8040712468193384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.020675053820014\n",
            "step: 10, loss: 0.004207057878375053\n",
            "step: 20, loss: 0.004140826407819986\n",
            "step: 30, loss: 0.006679190322756767\n",
            "step: 40, loss: 0.033071115612983704\n",
            "step: 50, loss: 0.016891557723283768\n",
            "step: 60, loss: 0.0020112693309783936\n",
            "step: 70, loss: 0.0011672850232571363\n",
            "step: 80, loss: 0.01631266623735428\n",
            "step: 90, loss: 0.0021055482793599367\n",
            "step: 100, loss: 0.001630711485631764\n",
            "step: 110, loss: 0.00036810286110267043\n",
            "step: 120, loss: 0.24578548967838287\n",
            "step: 130, loss: 0.025451447814702988\n",
            "step: 140, loss: 0.04346829280257225\n",
            "step: 150, loss: 0.001430911011993885\n",
            "step: 160, loss: 0.17255684733390808\n",
            "step: 170, loss: 0.003294250462204218\n",
            "step: 180, loss: 0.009233874268829823\n",
            "step: 190, loss: 0.00045531996875070035\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8126649076517151, f1=0.8010752688172043, best_f1=0.8040712468193384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00915826577693224\n",
            "step: 10, loss: 0.001334281638264656\n",
            "step: 20, loss: 0.0031534337904304266\n",
            "step: 30, loss: 0.0060263448394834995\n",
            "step: 40, loss: 0.0034292778000235558\n",
            "step: 50, loss: 0.002332995180040598\n",
            "step: 60, loss: 0.08085235208272934\n",
            "step: 70, loss: 0.0022318908013403416\n",
            "step: 80, loss: 0.032988496124744415\n",
            "step: 90, loss: 0.0029022949747741222\n",
            "step: 100, loss: 0.0011080458061769605\n",
            "step: 110, loss: 0.005872105713933706\n",
            "step: 120, loss: 0.014985660091042519\n",
            "step: 130, loss: 0.0005453511839732528\n",
            "step: 140, loss: 0.081398144364357\n",
            "step: 150, loss: 0.006149685941636562\n",
            "step: 160, loss: 0.0033001736737787724\n",
            "step: 170, loss: 0.0007656418601982296\n",
            "step: 180, loss: 0.0025102966465055943\n",
            "step: 190, loss: 0.005271937698125839\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.805, f1=0.7949367088607595, best_f1=0.8040712468193384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008985031396150589\n",
            "step: 10, loss: 0.0009015848627313972\n",
            "step: 20, loss: 0.0004721497534774244\n",
            "step: 30, loss: 0.07941319048404694\n",
            "step: 40, loss: 0.0011005300329998136\n",
            "step: 50, loss: 0.0006767690065316856\n",
            "step: 60, loss: 0.0018021779833361506\n",
            "step: 70, loss: 0.0015381646808236837\n",
            "step: 80, loss: 0.002774093532934785\n",
            "step: 90, loss: 0.001959946472197771\n",
            "step: 100, loss: 0.006926009897142649\n",
            "step: 110, loss: 0.11670556664466858\n",
            "step: 120, loss: 0.0014998794067651033\n",
            "step: 130, loss: 0.02736133337020874\n",
            "step: 140, loss: 0.0017731406260281801\n",
            "step: 150, loss: 0.0018685257527977228\n",
            "step: 160, loss: 0.0008127427427098155\n",
            "step: 170, loss: 0.019686002284288406\n",
            "step: 180, loss: 0.001339695998467505\n",
            "step: 190, loss: 0.0011456158244982362\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8131868131868132, f1=0.8142076502732241, best_f1=0.8040712468193384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.001189405913464725\n",
            "step: 10, loss: 0.0213627889752388\n",
            "step: 20, loss: 0.002918063197284937\n",
            "step: 30, loss: 0.05156689137220383\n",
            "step: 40, loss: 0.0017168591730296612\n",
            "step: 50, loss: 0.007530090399086475\n",
            "step: 60, loss: 0.006122680846601725\n",
            "step: 70, loss: 0.010824852623045444\n",
            "step: 80, loss: 0.04146299883723259\n",
            "step: 90, loss: 0.0015591385308653116\n",
            "step: 100, loss: 0.008015461266040802\n",
            "step: 110, loss: 0.017383961006999016\n",
            "step: 120, loss: 0.0015899017453193665\n",
            "step: 130, loss: 0.008983165957033634\n",
            "step: 140, loss: 0.0015756525099277496\n",
            "step: 150, loss: 0.005281913094222546\n",
            "step: 160, loss: 0.0017682545585557818\n",
            "step: 170, loss: 0.0029624488670378923\n",
            "step: 180, loss: 0.0006672506569884717\n",
            "step: 190, loss: 0.0005567196058109403\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8125, f1=0.8083989501312335, best_f1=0.8040712468193384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0005179121508263052\n",
            "step: 10, loss: 0.002682323567569256\n",
            "step: 20, loss: 0.0010137977078557014\n",
            "step: 30, loss: 0.008747152984142303\n",
            "step: 40, loss: 0.0030052028596401215\n",
            "step: 50, loss: 0.06583605706691742\n",
            "step: 60, loss: 0.03290007263422012\n",
            "step: 70, loss: 0.0023626491893082857\n",
            "step: 80, loss: 0.000919554615393281\n",
            "step: 90, loss: 0.00039817410288378596\n",
            "step: 100, loss: 0.00043479312444105744\n",
            "step: 110, loss: 0.0018136478029191494\n",
            "step: 120, loss: 0.001190091366879642\n",
            "step: 130, loss: 0.0006838831468485296\n",
            "step: 140, loss: 0.0005286234663799405\n",
            "step: 150, loss: 0.0004565089475363493\n",
            "step: 160, loss: 0.0004426779050845653\n",
            "step: 170, loss: 0.07488042861223221\n",
            "step: 180, loss: 0.0005265618674457073\n",
            "step: 190, loss: 0.12432362884283066\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.8150134048257373, f1=0.8096514745308311, best_f1=0.8040712468193384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0010790068190544844\n",
            "step: 10, loss: 0.04676591604948044\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 20, loss: 0.0016569752478972077\n",
            "step: 30, loss: 0.004561539739370346\n",
            "step: 40, loss: 0.006627959199249744\n",
            "step: 50, loss: 0.0008230841485783458\n",
            "step: 60, loss: 0.0005560893332585692\n",
            "step: 70, loss: 0.0025147427804768085\n",
            "step: 80, loss: 0.00210075662471354\n",
            "step: 90, loss: 0.00039628471131436527\n",
            "step: 100, loss: 0.0003150800766889006\n",
            "step: 110, loss: 0.0001331991225015372\n",
            "step: 120, loss: 0.0008365956018678844\n",
            "step: 130, loss: 0.005047863349318504\n",
            "step: 140, loss: 0.0007824301719665527\n",
            "step: 150, loss: 0.013677949085831642\n",
            "step: 160, loss: 0.008287813514471054\n",
            "step: 170, loss: 0.004704525228589773\n",
            "step: 180, loss: 0.004881961736828089\n",
            "step: 190, loss: 0.00247213919647038\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8108108108108106, f1=0.8219178082191781, best_f1=0.8040712468193384\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.003964603878557682\n",
            "step: 10, loss: 0.0011767070973291993\n",
            "step: 20, loss: 0.002154207555577159\n",
            "step: 30, loss: 0.0007602862897329032\n",
            "step: 40, loss: 0.0014341695932671428\n",
            "step: 50, loss: 0.0006939075537957251\n",
            "step: 60, loss: 0.001653047394938767\n",
            "step: 70, loss: 0.004817640408873558\n",
            "step: 80, loss: 0.0007841151673346758\n",
            "step: 90, loss: 0.047339677810668945\n",
            "step: 100, loss: 0.00046488255611620843\n",
            "step: 110, loss: 0.0017339319456368685\n",
            "step: 120, loss: 0.008439003489911556\n",
            "step: 130, loss: 0.0032167029567062855\n",
            "step: 140, loss: 0.0013194250641390681\n",
            "step: 150, loss: 0.0007945906836539507\n",
            "step: 160, loss: 0.0017711755353957415\n",
            "step: 170, loss: 0.03402891382575035\n",
            "step: 180, loss: 0.000878617400303483\n",
            "step: 190, loss: 0.005759032443165779\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8148148148148148, f1=0.8150134048257373, best_f1=0.8040712468193384\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2049it [00:12, 158.17it/s]\n",
            "load_f1 = 0.7855153203342619\n",
            "real_f1 = 0.784741144414169\n",
            "733it [00:00, 3465.43it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 143.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO TEXTUAL"
      ],
      "metadata": {
        "id": "zW6LV4zMhstv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abt-Buy - Running the matcher"
      ],
      "metadata": {
        "id": "3an30TrShstv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        " \n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/Abt-Buy.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Textual/Abt-Buy \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "HUjK4bwlhstv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be403e3b-ace7-49f3-b6d9-297e0e4ef1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "step: 0, loss: 0.46702054142951965\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.43212762475013733\n",
            "step: 20, loss: 0.31524422764778137\n",
            "step: 30, loss: 0.4114842116832733\n",
            "step: 40, loss: 0.511871874332428\n",
            "step: 50, loss: 0.3734758794307709\n",
            "step: 60, loss: 0.5921560525894165\n",
            "step: 70, loss: 0.31557798385620117\n",
            "step: 80, loss: 0.1997348517179489\n",
            "step: 90, loss: 0.22200199961662292\n",
            "step: 100, loss: 0.15738524496555328\n",
            "step: 110, loss: 0.43344467878341675\n",
            "step: 120, loss: 0.3240932524204254\n",
            "step: 130, loss: 0.33221378922462463\n",
            "step: 140, loss: 0.38768717646598816\n",
            "step: 150, loss: 0.3094812035560608\n",
            "step: 160, loss: 0.40023067593574524\n",
            "step: 170, loss: 0.33208024501800537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.24628820960698694, f1=0.25426944971537, best_f1=0.25426944971537\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.29884111881256104\n",
            "step: 10, loss: 0.42173853516578674\n",
            "step: 20, loss: 0.3173537850379944\n",
            "step: 30, loss: 0.2943190932273865\n",
            "step: 40, loss: 0.05133942887187004\n",
            "step: 50, loss: 0.3960120379924774\n",
            "step: 60, loss: 0.16536539793014526\n",
            "step: 70, loss: 0.4387056529521942\n",
            "step: 80, loss: 0.1861044019460678\n",
            "step: 90, loss: 0.2298939973115921\n",
            "step: 100, loss: 0.2857889235019684\n",
            "step: 110, loss: 0.2252410501241684\n",
            "step: 120, loss: 0.07917921245098114\n",
            "step: 130, loss: 0.1416466236114502\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 140, loss: 0.35057002305984497\n",
            "step: 150, loss: 0.08079265058040619\n",
            "step: 160, loss: 0.22037875652313232\n",
            "step: 170, loss: 0.09758653491735458\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.7714987714987714, f1=0.7632183908045976, best_f1=0.7632183908045976\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.5072116255760193\n",
            "step: 10, loss: 0.057101164013147354\n",
            "step: 20, loss: 0.04974523186683655\n",
            "step: 30, loss: 0.10113365203142166\n",
            "step: 40, loss: 0.09448252618312836\n",
            "step: 50, loss: 0.16401304304599762\n",
            "step: 60, loss: 0.12359840422868729\n",
            "step: 70, loss: 0.13256308436393738\n",
            "step: 80, loss: 0.09701098501682281\n",
            "step: 90, loss: 0.41328099370002747\n",
            "step: 100, loss: 0.07533479481935501\n",
            "step: 110, loss: 0.057480618357658386\n",
            "step: 120, loss: 0.06908250600099564\n",
            "step: 130, loss: 0.16402913630008698\n",
            "step: 140, loss: 0.0789942815899849\n",
            "step: 150, loss: 0.11252420395612717\n",
            "step: 160, loss: 0.09630647301673889\n",
            "step: 170, loss: 0.0644557997584343\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.8155844155844156, f1=0.827250608272506, best_f1=0.827250608272506\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.14784865081310272\n",
            "step: 10, loss: 0.17995993793010712\n",
            "step: 20, loss: 0.05379597842693329\n",
            "step: 30, loss: 0.1926402449607849\n",
            "step: 40, loss: 0.11940898001194\n",
            "step: 50, loss: 0.0674111545085907\n",
            "step: 60, loss: 0.13845562934875488\n",
            "step: 70, loss: 0.0013957304181531072\n",
            "step: 80, loss: 0.22567501664161682\n",
            "step: 90, loss: 0.30848047137260437\n",
            "step: 100, loss: 0.1236807331442833\n",
            "step: 110, loss: 0.11216096580028534\n",
            "step: 120, loss: 0.20351049304008484\n",
            "step: 130, loss: 0.264702171087265\n",
            "step: 140, loss: 0.14445461332798004\n",
            "step: 150, loss: 0.15337009727954865\n",
            "step: 160, loss: 0.004743530414998531\n",
            "step: 170, loss: 0.032914336770772934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.8287037037037036, f1=0.8232662192393736, best_f1=0.8232662192393736\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.11446776986122131\n",
            "step: 10, loss: 0.03455202281475067\n",
            "step: 20, loss: 0.05249044671654701\n",
            "step: 30, loss: 0.02644798718392849\n",
            "step: 40, loss: 0.07812058925628662\n",
            "step: 50, loss: 0.051957931369543076\n",
            "step: 60, loss: 0.17039409279823303\n",
            "step: 70, loss: 0.007698417641222477\n",
            "step: 80, loss: 0.00706544378772378\n",
            "step: 90, loss: 0.025460200384259224\n",
            "step: 100, loss: 0.010366717353463173\n",
            "step: 110, loss: 0.04673406854271889\n",
            "step: 120, loss: 0.1064995527267456\n",
            "step: 130, loss: 0.011769838631153107\n",
            "step: 140, loss: 0.1462889015674591\n",
            "step: 150, loss: 0.04579703137278557\n",
            "step: 160, loss: 0.006617054343223572\n",
            "step: 170, loss: 0.006122846622020006\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.8599033816425121, f1=0.8689320388349513, best_f1=0.8689320388349513\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.18298955261707306\n",
            "step: 10, loss: 0.017616644501686096\n",
            "step: 20, loss: 0.08549655228853226\n",
            "step: 30, loss: 0.004092440940439701\n",
            "step: 40, loss: 0.009209509007632732\n",
            "step: 50, loss: 0.07580183446407318\n",
            "step: 60, loss: 0.024612585082650185\n",
            "step: 70, loss: 0.02185145765542984\n",
            "step: 80, loss: 0.02352973259985447\n",
            "step: 90, loss: 0.16129498183727264\n",
            "step: 100, loss: 0.0034964478109031916\n",
            "step: 110, loss: 0.04800403118133545\n",
            "step: 120, loss: 0.04561310261487961\n",
            "step: 130, loss: 0.2084832489490509\n",
            "step: 140, loss: 0.02567823976278305\n",
            "step: 150, loss: 0.025315308943390846\n",
            "step: 160, loss: 0.034760359674692154\n",
            "step: 170, loss: 0.02918240800499916\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.8529411764705882, f1=0.8815165876777251, best_f1=0.8689320388349513\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03321656957268715\n",
            "step: 10, loss: 0.043316252529621124\n",
            "step: 20, loss: 0.011849420145154\n",
            "step: 30, loss: 0.08497006446123123\n",
            "step: 40, loss: 0.003312258282676339\n",
            "step: 50, loss: 0.0012185962405055761\n",
            "step: 60, loss: 0.09586720913648605\n",
            "step: 70, loss: 0.024712838232517242\n",
            "step: 80, loss: 0.05731869488954544\n",
            "step: 90, loss: 0.012879472225904465\n",
            "step: 100, loss: 0.02097908779978752\n",
            "step: 110, loss: 0.01208537444472313\n",
            "step: 120, loss: 0.0270168948918581\n",
            "step: 130, loss: 0.05763527378439903\n",
            "step: 140, loss: 0.03725564479827881\n",
            "step: 150, loss: 0.0822284072637558\n",
            "step: 160, loss: 0.06665146350860596\n",
            "step: 170, loss: 0.03240028768777847\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 7: dev_f1=0.8725490196078431, f1=0.8805620608899297, best_f1=0.8805620608899297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02026655152440071\n",
            "step: 10, loss: 0.12615245580673218\n",
            "step: 20, loss: 0.026357464492321014\n",
            "step: 30, loss: 0.0008285861113108695\n",
            "step: 40, loss: 0.04733579605817795\n",
            "step: 50, loss: 0.010808417573571205\n",
            "step: 60, loss: 0.006797115784138441\n",
            "step: 70, loss: 0.005133844446390867\n",
            "step: 80, loss: 0.00033281449577771127\n",
            "step: 90, loss: 0.023938676342368126\n",
            "step: 100, loss: 0.0007415566360577941\n",
            "step: 110, loss: 0.06897298246622086\n",
            "step: 120, loss: 0.18442879617214203\n",
            "step: 130, loss: 0.01657160557806492\n",
            "step: 140, loss: 0.03934534266591072\n",
            "step: 150, loss: 0.012430130504071712\n",
            "step: 160, loss: 0.0013489816337823868\n",
            "step: 170, loss: 0.0940350592136383\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 8: dev_f1=0.86, f1=0.8909952606635072, best_f1=0.8805620608899297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.03754720091819763\n",
            "step: 10, loss: 0.007230158895254135\n",
            "step: 20, loss: 0.0008941606502048671\n",
            "step: 30, loss: 0.00308199692517519\n",
            "step: 40, loss: 0.13937322795391083\n",
            "step: 50, loss: 0.00502235209569335\n",
            "step: 60, loss: 0.017397835850715637\n",
            "step: 70, loss: 0.006290370598435402\n",
            "step: 80, loss: 0.06116149201989174\n",
            "step: 90, loss: 0.01778327487409115\n",
            "step: 100, loss: 0.012199979275465012\n",
            "step: 110, loss: 0.07646695524454117\n",
            "step: 120, loss: 0.04166563227772713\n",
            "step: 130, loss: 0.0021077252458781004\n",
            "step: 140, loss: 0.01642446592450142\n",
            "step: 150, loss: 0.10485411435365677\n",
            "step: 160, loss: 0.00486669922247529\n",
            "step: 170, loss: 0.026890795677900314\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.8622448979591837, f1=0.8977556109725686, best_f1=0.8805620608899297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0038567918818444014\n",
            "step: 10, loss: 0.007039605174213648\n",
            "step: 20, loss: 0.07893287390470505\n",
            "step: 30, loss: 0.05030197277665138\n",
            "step: 40, loss: 0.006075473967939615\n",
            "step: 50, loss: 0.03747643902897835\n",
            "step: 60, loss: 0.009204705245792866\n",
            "step: 70, loss: 0.011469081044197083\n",
            "step: 80, loss: 0.009055336937308311\n",
            "step: 90, loss: 0.010066596791148186\n",
            "step: 100, loss: 0.01177978329360485\n",
            "step: 110, loss: 0.016942018643021584\n",
            "step: 120, loss: 0.0019268247997388244\n",
            "step: 130, loss: 0.002131915418431163\n",
            "step: 140, loss: 0.0011363404337316751\n",
            "step: 150, loss: 0.0004738192947115749\n",
            "step: 160, loss: 0.0003957203880418092\n",
            "step: 170, loss: 0.019883664324879646\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.8671679197994987, f1=0.8926829268292683, best_f1=0.8805620608899297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006896683014929295\n",
            "step: 10, loss: 0.011870814487338066\n",
            "step: 20, loss: 0.010909120552241802\n",
            "step: 30, loss: 0.008415794931352139\n",
            "step: 40, loss: 0.00796441175043583\n",
            "step: 50, loss: 0.008839775808155537\n",
            "step: 60, loss: 0.057856831699609756\n",
            "step: 70, loss: 0.0021301216911524534\n",
            "step: 80, loss: 0.016220588237047195\n",
            "step: 90, loss: 0.0026962023694068193\n",
            "step: 100, loss: 0.016339661553502083\n",
            "step: 110, loss: 0.07667868584394455\n",
            "step: 120, loss: 0.011627831496298313\n",
            "step: 130, loss: 0.007740046828985214\n",
            "step: 140, loss: 0.0025987846311181784\n",
            "step: 150, loss: 0.00013201843830756843\n",
            "step: 160, loss: 0.0029072873294353485\n",
            "step: 170, loss: 0.06981299072504044\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.8506329113924049, f1=0.8786407766990291, best_f1=0.8805620608899297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.04290195554494858\n",
            "step: 10, loss: 0.0014528670581057668\n",
            "step: 20, loss: 0.00729254400357604\n",
            "step: 30, loss: 0.048522476106882095\n",
            "step: 40, loss: 0.0020746837835758924\n",
            "step: 50, loss: 0.004453095141798258\n",
            "step: 60, loss: 0.019412271678447723\n",
            "step: 70, loss: 0.00039251145790331066\n",
            "step: 80, loss: 0.00022203702246770263\n",
            "step: 90, loss: 0.0023347786627709866\n",
            "step: 100, loss: 0.0004926255205646157\n",
            "step: 110, loss: 0.034367818385362625\n",
            "step: 120, loss: 0.03613396733999252\n",
            "step: 130, loss: 0.2125980406999588\n",
            "step: 140, loss: 0.0003191028372384608\n",
            "step: 150, loss: 0.021869447082281113\n",
            "step: 160, loss: 0.06568457186222076\n",
            "step: 170, loss: 0.002245865296572447\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.8720626631853786, f1=0.898989898989899, best_f1=0.8805620608899297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008640487678349018\n",
            "step: 10, loss: 0.004779363051056862\n",
            "step: 20, loss: 0.004425869323313236\n",
            "step: 30, loss: 0.04601927474141121\n",
            "step: 40, loss: 0.054583799093961716\n",
            "step: 50, loss: 0.018265748396515846\n",
            "step: 60, loss: 0.0017535793595016003\n",
            "step: 70, loss: 0.07917200773954391\n",
            "step: 80, loss: 0.0003798638063017279\n",
            "step: 90, loss: 0.02081366814672947\n",
            "step: 100, loss: 0.022054651752114296\n",
            "step: 110, loss: 0.0016345684416592121\n",
            "step: 120, loss: 0.02627178467810154\n",
            "step: 130, loss: 0.012339258566498756\n",
            "step: 140, loss: 0.020243804901838303\n",
            "step: 150, loss: 0.0003415604878682643\n",
            "step: 160, loss: 0.0004081043298356235\n",
            "step: 170, loss: 0.0016612531617283821\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.854922279792746, f1=0.8905472636815919, best_f1=0.8805620608899297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009834669530391693\n",
            "step: 10, loss: 0.0008931048214435577\n",
            "step: 20, loss: 0.004807529505342245\n",
            "step: 30, loss: 0.01282070018351078\n",
            "step: 40, loss: 0.011171282269060612\n",
            "step: 50, loss: 0.0017796439351513982\n",
            "step: 60, loss: 0.006344193127006292\n",
            "step: 70, loss: 0.006566516123712063\n",
            "step: 80, loss: 0.01904691569507122\n",
            "step: 90, loss: 0.0004782379255630076\n",
            "step: 100, loss: 0.09982948005199432\n",
            "step: 110, loss: 0.029451783746480942\n",
            "step: 120, loss: 0.0020483999978750944\n",
            "step: 130, loss: 0.07858239859342575\n",
            "step: 140, loss: 0.01542894821614027\n",
            "step: 150, loss: 0.01669330894947052\n",
            "step: 160, loss: 0.0010910759447142482\n",
            "step: 170, loss: 0.0360715314745903\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.8640776699029126, f1=0.8658823529411764, best_f1=0.8805620608899297\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00045429336023516953\n",
            "step: 10, loss: 0.0013478926848620176\n",
            "step: 20, loss: 0.2600279748439789\n",
            "step: 30, loss: 0.003836559597402811\n",
            "step: 40, loss: 0.00044029654236510396\n",
            "step: 50, loss: 0.0004140314122196287\n",
            "step: 60, loss: 0.011927993036806583\n",
            "step: 70, loss: 0.12843897938728333\n",
            "step: 80, loss: 0.0005036680959165096\n",
            "step: 90, loss: 0.002589755691587925\n",
            "step: 100, loss: 0.031446296721696854\n",
            "step: 110, loss: 0.0005056431982666254\n",
            "step: 120, loss: 0.0006990960100665689\n",
            "step: 130, loss: 0.00037476845318451524\n",
            "step: 140, loss: 0.002819425193592906\n",
            "step: 150, loss: 0.001456850441172719\n",
            "step: 160, loss: 0.00104794732760638\n",
            "step: 170, loss: 0.0013098722556605935\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.8634146341463416, f1=0.8741092636579572, best_f1=0.8805620608899297\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "1916it [00:08, 235.54it/s]\n",
            "load_f1 = 0.4487369985141158\n",
            "real_f1 = 0.4253521126760563\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 144.57it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DITTO DIRTY"
      ],
      "metadata": {
        "id": "VngEb4vfhstw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-ACM - Running the matcher"
      ],
      "metadata": {
        "id": "QfPaCqR4hstw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-ACM.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-ACM \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "AA1CawEthstw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "064da670-d783-4aa2-bf2c-90bbca661c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 481/481 [00:00<00:00, 404kB/s]\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 795kB/s]\n",
            "Downloading: 100% 456k/456k [00:01<00:00, 403kB/s] \n",
            "Downloading: 100% 501M/501M [00:06<00:00, 73.1MB/s]\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6029409170150757\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.4525490999221802\n",
            "step: 20, loss: 0.49699318408966064\n",
            "step: 30, loss: 0.3406045138835907\n",
            "step: 40, loss: 0.33596473932266235\n",
            "step: 50, loss: 0.47067373991012573\n",
            "step: 60, loss: 0.07124824076890945\n",
            "step: 70, loss: 0.06522604078054428\n",
            "step: 80, loss: 0.04914839193224907\n",
            "step: 90, loss: 0.105853371322155\n",
            "step: 100, loss: 0.23052705824375153\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 110, loss: 0.07264204323291779\n",
            "step: 120, loss: 0.027242165058851242\n",
            "step: 130, loss: 0.17191459238529205\n",
            "step: 140, loss: 0.0107550248503685\n",
            "step: 150, loss: 0.1220327615737915\n",
            "step: 160, loss: 0.028470950201153755\n",
            "step: 170, loss: 0.07560119032859802\n",
            "step: 180, loss: 0.21842187643051147\n",
            "step: 190, loss: 0.017669538035988808\n",
            "step: 200, loss: 0.03317601978778839\n",
            "step: 210, loss: 0.288647323846817\n",
            "step: 220, loss: 0.09639342874288559\n",
            "step: 230, loss: 0.006854568608105183\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9690949227373068, f1=0.9673790776152981, best_f1=0.9673790776152981\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.009777556173503399\n",
            "step: 10, loss: 0.10090525448322296\n",
            "step: 20, loss: 0.05571822449564934\n",
            "step: 30, loss: 0.014020557515323162\n",
            "step: 40, loss: 0.029889367520809174\n",
            "step: 50, loss: 0.0033335413318127394\n",
            "step: 60, loss: 0.007266242057085037\n",
            "step: 70, loss: 0.009220811538398266\n",
            "step: 80, loss: 0.10143912583589554\n",
            "step: 90, loss: 0.04078512638807297\n",
            "step: 100, loss: 0.003594406880438328\n",
            "step: 110, loss: 0.04511532559990883\n",
            "step: 120, loss: 0.03424602746963501\n",
            "step: 130, loss: 0.030947711318731308\n",
            "step: 140, loss: 0.005883266683667898\n",
            "step: 150, loss: 0.0846845805644989\n",
            "step: 160, loss: 0.048740457743406296\n",
            "step: 170, loss: 0.001252524321898818\n",
            "step: 180, loss: 0.018357649445533752\n",
            "step: 190, loss: 0.0037145738024264574\n",
            "step: 200, loss: 0.032419923692941666\n",
            "step: 210, loss: 0.015783078968524933\n",
            "step: 220, loss: 0.0056887115351855755\n",
            "step: 230, loss: 0.0020908862352371216\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9700996677740864, f1=0.9698324022346367, best_f1=0.9698324022346367\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.02803192101418972\n",
            "step: 10, loss: 0.055741190910339355\n",
            "step: 20, loss: 0.034613415598869324\n",
            "step: 30, loss: 0.017135318368673325\n",
            "step: 40, loss: 0.08839885145425797\n",
            "step: 50, loss: 0.013625426217913628\n",
            "step: 60, loss: 0.00865261536091566\n",
            "step: 70, loss: 0.005898303352296352\n",
            "step: 80, loss: 0.1224130243062973\n",
            "step: 90, loss: 0.01098044402897358\n",
            "step: 100, loss: 0.005069551058113575\n",
            "step: 110, loss: 0.0167231522500515\n",
            "step: 120, loss: 0.00707939313724637\n",
            "step: 130, loss: 0.009630982764065266\n",
            "step: 140, loss: 0.0050214440561831\n",
            "step: 150, loss: 0.08417918533086777\n",
            "step: 160, loss: 0.0013009726535528898\n",
            "step: 170, loss: 0.0008833333849906921\n",
            "step: 180, loss: 0.010821714997291565\n",
            "step: 190, loss: 0.009182856418192387\n",
            "step: 200, loss: 0.010008787736296654\n",
            "step: 210, loss: 0.002235592110082507\n",
            "step: 220, loss: 0.11520547419786453\n",
            "step: 230, loss: 0.014505358412861824\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 3: dev_f1=0.9755555555555556, f1=0.9699666295884317, best_f1=0.9699666295884317\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.016011981293559074\n",
            "step: 10, loss: 0.006189489737153053\n",
            "step: 20, loss: 0.010099017061293125\n",
            "step: 30, loss: 0.0010146981803700328\n",
            "step: 40, loss: 0.04963235929608345\n",
            "step: 50, loss: 0.04744022712111473\n",
            "step: 60, loss: 0.0011708062374964356\n",
            "step: 70, loss: 0.020251775160431862\n",
            "step: 80, loss: 0.057670775800943375\n",
            "step: 90, loss: 0.10048602521419525\n",
            "step: 100, loss: 0.02096804976463318\n",
            "step: 110, loss: 0.001782967709004879\n",
            "step: 120, loss: 0.027184145525097847\n",
            "step: 130, loss: 0.014467140659689903\n",
            "step: 140, loss: 0.007745147682726383\n",
            "step: 150, loss: 0.001658713212236762\n",
            "step: 160, loss: 0.008007261902093887\n",
            "step: 170, loss: 0.000576779420953244\n",
            "step: 180, loss: 0.1394500881433487\n",
            "step: 190, loss: 0.004811698570847511\n",
            "step: 200, loss: 0.05687907710671425\n",
            "step: 210, loss: 0.07579611241817474\n",
            "step: 220, loss: 0.017964735627174377\n",
            "step: 230, loss: 0.004844917915761471\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 4: dev_f1=0.9767441860465117, f1=0.9723756906077348, best_f1=0.9723756906077348\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0011431712191551924\n",
            "step: 10, loss: 0.005538526922464371\n",
            "step: 20, loss: 0.02080768346786499\n",
            "step: 30, loss: 0.002440342679619789\n",
            "step: 40, loss: 0.0006971890688873827\n",
            "step: 50, loss: 0.001393481739796698\n",
            "step: 60, loss: 0.0004062712541781366\n",
            "step: 70, loss: 0.004742393270134926\n",
            "step: 80, loss: 0.028818916529417038\n",
            "step: 90, loss: 0.1275143027305603\n",
            "step: 100, loss: 0.0008402207749895751\n",
            "step: 110, loss: 0.04008299484848976\n",
            "step: 120, loss: 0.001130227348767221\n",
            "step: 130, loss: 0.0005125732277520001\n",
            "step: 140, loss: 0.0033881859853863716\n",
            "step: 150, loss: 0.007427656091749668\n",
            "step: 160, loss: 0.012874292209744453\n",
            "step: 170, loss: 0.0022480112966150045\n",
            "step: 180, loss: 0.007183664944022894\n",
            "step: 190, loss: 0.203715980052948\n",
            "step: 200, loss: 0.005425562616437674\n",
            "step: 210, loss: 0.01585371047258377\n",
            "step: 220, loss: 0.005920805037021637\n",
            "step: 230, loss: 0.015024008229374886\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.980963045912654, f1=0.9796839729119639, best_f1=0.9796839729119639\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009670207509770989\n",
            "step: 10, loss: 0.00953495129942894\n",
            "step: 20, loss: 0.005000092554837465\n",
            "step: 30, loss: 0.0018335779896005988\n",
            "step: 40, loss: 0.0004391554684843868\n",
            "step: 50, loss: 0.00042973330710083246\n",
            "step: 60, loss: 0.0003775745281018317\n",
            "step: 70, loss: 0.12463563680648804\n",
            "step: 80, loss: 0.0013281282735988498\n",
            "step: 90, loss: 0.006905474700033665\n",
            "step: 100, loss: 0.00114630616735667\n",
            "step: 110, loss: 0.0030999062582850456\n",
            "step: 120, loss: 0.0006576831801794469\n",
            "step: 130, loss: 0.007723875343799591\n",
            "step: 140, loss: 0.0024883055593818426\n",
            "step: 150, loss: 0.002198756206780672\n",
            "step: 160, loss: 0.0037129665724933147\n",
            "step: 170, loss: 0.0004272288060747087\n",
            "step: 180, loss: 0.00034931403934024274\n",
            "step: 190, loss: 0.0006032600649632514\n",
            "step: 200, loss: 0.0007971973391249776\n",
            "step: 210, loss: 0.01033781748265028\n",
            "step: 220, loss: 0.0031644843984395266\n",
            "step: 230, loss: 0.0007460821070708334\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9809203142536477, f1=0.9699666295884317, best_f1=0.9796839729119639\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0009591256966814399\n",
            "step: 10, loss: 0.0005017781513743103\n",
            "step: 20, loss: 0.021455027163028717\n",
            "step: 30, loss: 0.0005108578479848802\n",
            "step: 40, loss: 0.0009030138608068228\n",
            "step: 50, loss: 0.0034112134017050266\n",
            "step: 60, loss: 0.0007213001372292638\n",
            "step: 70, loss: 0.0006213239976204932\n",
            "step: 80, loss: 0.003919612616300583\n",
            "step: 90, loss: 0.0007272300426848233\n",
            "step: 100, loss: 0.08060546964406967\n",
            "step: 110, loss: 0.004856579005718231\n",
            "step: 120, loss: 0.0009638870833441615\n",
            "step: 130, loss: 0.003619919531047344\n",
            "step: 140, loss: 0.0008702790946699679\n",
            "step: 150, loss: 0.01179694663733244\n",
            "step: 160, loss: 0.0014002377865836024\n",
            "step: 170, loss: 0.0031083389185369015\n",
            "step: 180, loss: 0.0003153184661641717\n",
            "step: 190, loss: 0.00013995503832120448\n",
            "step: 200, loss: 0.022265180945396423\n",
            "step: 210, loss: 0.0014297803863883018\n",
            "step: 220, loss: 0.0002574908430688083\n",
            "step: 230, loss: 0.004008038435131311\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9730941704035874, f1=0.9642058165548099, best_f1=0.9796839729119639\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0002949887712020427\n",
            "step: 10, loss: 0.021472115069627762\n",
            "step: 20, loss: 0.001247613807208836\n",
            "step: 30, loss: 0.0004635625227820128\n",
            "step: 40, loss: 0.000900403072591871\n",
            "step: 50, loss: 0.0010729653295129538\n",
            "step: 60, loss: 0.0014529806794598699\n",
            "step: 70, loss: 0.001092495396733284\n",
            "step: 80, loss: 0.021014899015426636\n",
            "step: 90, loss: 0.0029837272595614195\n",
            "step: 100, loss: 0.0007513427408412099\n",
            "step: 110, loss: 0.0012258231872692704\n",
            "step: 120, loss: 0.0019871562253683805\n",
            "step: 130, loss: 0.05302342399954796\n",
            "step: 140, loss: 0.0004836528387386352\n",
            "step: 150, loss: 0.1698484867811203\n",
            "step: 160, loss: 0.0015513699036091566\n",
            "step: 170, loss: 0.006984334904700518\n",
            "step: 180, loss: 0.0011089255567640066\n",
            "step: 190, loss: 0.003232580842450261\n",
            "step: 200, loss: 0.0029940796084702015\n",
            "step: 210, loss: 0.0021565889474004507\n",
            "step: 220, loss: 0.0005137945991009474\n",
            "step: 230, loss: 0.00022311304928734899\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.9831649831649831, f1=0.9831649831649831, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0017204009927809238\n",
            "step: 10, loss: 0.0005560991121456027\n",
            "step: 20, loss: 0.000722876749932766\n",
            "step: 30, loss: 0.0012642014771699905\n",
            "step: 40, loss: 0.003075425513088703\n",
            "step: 50, loss: 0.0008559454581700265\n",
            "step: 60, loss: 0.00019109617278445512\n",
            "step: 70, loss: 0.03882889449596405\n",
            "step: 80, loss: 0.0004537806671578437\n",
            "step: 90, loss: 0.005362921394407749\n",
            "step: 100, loss: 0.00021796040527988225\n",
            "step: 110, loss: 0.0001959790533874184\n",
            "step: 120, loss: 0.03330694139003754\n",
            "step: 130, loss: 0.002309044823050499\n",
            "step: 140, loss: 0.00011449614248704165\n",
            "step: 150, loss: 0.00019312751828692853\n",
            "step: 160, loss: 0.00012186180538265035\n",
            "step: 170, loss: 5.0190741603728384e-05\n",
            "step: 180, loss: 0.0007595678325742483\n",
            "step: 190, loss: 5.8051515225088224e-05\n",
            "step: 200, loss: 7.580640522064641e-05\n",
            "step: 210, loss: 0.0015759498346596956\n",
            "step: 220, loss: 0.00020349562691990286\n",
            "step: 230, loss: 0.0006511099054478109\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 9: dev_f1=0.9820224719101124, f1=0.9775280898876404, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.00032076984643936157\n",
            "step: 10, loss: 0.0001855433511082083\n",
            "step: 20, loss: 0.000348919682437554\n",
            "step: 30, loss: 5.745440284954384e-05\n",
            "step: 40, loss: 0.013020376674830914\n",
            "step: 50, loss: 6.204542296472937e-05\n",
            "step: 60, loss: 5.917880844208412e-05\n",
            "step: 70, loss: 0.003419417655095458\n",
            "step: 80, loss: 0.0001292603265028447\n",
            "step: 90, loss: 0.00011156481195939705\n",
            "step: 100, loss: 0.00035523815313354135\n",
            "step: 110, loss: 0.0005850454326719046\n",
            "step: 120, loss: 0.010843967087566853\n",
            "step: 130, loss: 0.0002792346349451691\n",
            "step: 140, loss: 0.00012242680531926453\n",
            "step: 150, loss: 0.00024633921566419303\n",
            "step: 160, loss: 5.2432114898692816e-05\n",
            "step: 170, loss: 0.000315526791382581\n",
            "step: 180, loss: 0.03772986680269241\n",
            "step: 190, loss: 0.00034204169060103595\n",
            "step: 200, loss: 0.0001518077915534377\n",
            "step: 210, loss: 0.048837751150131226\n",
            "step: 220, loss: 0.001952730817720294\n",
            "step: 230, loss: 7.600565731991082e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.9820224719101124, f1=0.9787234042553192, best_f1=0.9831649831649831\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.234704389702529e-05\n",
            "step: 10, loss: 0.00019648371380753815\n",
            "step: 20, loss: 0.01885083317756653\n",
            "step: 30, loss: 0.010779124684631824\n",
            "step: 40, loss: 0.0003961570910178125\n",
            "step: 50, loss: 0.0001042397998389788\n",
            "step: 60, loss: 0.002124491613358259\n",
            "step: 70, loss: 0.004246925935149193\n",
            "step: 80, loss: 0.00022942645591683686\n",
            "step: 90, loss: 0.21009008586406708\n",
            "step: 100, loss: 0.00022699045075569302\n",
            "step: 110, loss: 0.00047683980665169656\n",
            "step: 120, loss: 0.0002816602645907551\n",
            "step: 130, loss: 0.00020541799312923104\n",
            "step: 140, loss: 0.0026388908736407757\n",
            "step: 150, loss: 0.00028944460791535676\n",
            "step: 160, loss: 0.0011749346740543842\n",
            "step: 170, loss: 0.00029727473156526685\n",
            "step: 180, loss: 0.00015211898426059633\n",
            "step: 190, loss: 0.00010284734889864922\n",
            "step: 200, loss: 0.000710601918399334\n",
            "step: 210, loss: 0.0002587584313005209\n",
            "step: 220, loss: 0.0009695907356217504\n",
            "step: 230, loss: 0.00020419416250661016\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 11: dev_f1=0.9842696629213483, f1=0.9821029082774049, best_f1=0.9821029082774049\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0001188965470646508\n",
            "step: 10, loss: 5.6152195611502975e-05\n",
            "step: 20, loss: 0.0003830956411547959\n",
            "step: 30, loss: 0.02676595374941826\n",
            "step: 40, loss: 0.0242752842605114\n",
            "step: 50, loss: 0.00020288201631046832\n",
            "step: 60, loss: 0.0002589320356491953\n",
            "step: 70, loss: 9.300607052864507e-05\n",
            "step: 80, loss: 5.7460434618406e-05\n",
            "step: 90, loss: 0.0003615952155087143\n",
            "step: 100, loss: 7.22004406270571e-05\n",
            "step: 110, loss: 0.00011722334602382034\n",
            "step: 120, loss: 0.0002900389372371137\n",
            "step: 130, loss: 0.00029658753192052245\n",
            "step: 140, loss: 0.00016735473764128983\n",
            "step: 150, loss: 0.0001447979302611202\n",
            "step: 160, loss: 0.0013361723395064473\n",
            "step: 170, loss: 0.0020499706733971834\n",
            "step: 180, loss: 0.00019115947361569852\n",
            "step: 190, loss: 0.0009479449363425374\n",
            "step: 200, loss: 5.799436621600762e-05\n",
            "step: 210, loss: 0.007118409965187311\n",
            "step: 220, loss: 0.005465859081596136\n",
            "step: 230, loss: 0.00016300761490128934\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.9842696629213483, f1=0.980963045912654, best_f1=0.9821029082774049\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.98258333816193e-05\n",
            "step: 10, loss: 0.00018067171913571656\n",
            "step: 20, loss: 0.0015938645228743553\n",
            "step: 30, loss: 3.6708668631035835e-05\n",
            "step: 40, loss: 0.0008027477306313813\n",
            "step: 50, loss: 0.0017399401403963566\n",
            "step: 60, loss: 9.268642315873876e-05\n",
            "step: 70, loss: 0.05522292107343674\n",
            "step: 80, loss: 0.00016850225802045316\n",
            "step: 90, loss: 4.7079611249500886e-05\n",
            "step: 100, loss: 8.443260594503954e-05\n",
            "step: 110, loss: 0.0044048139825463295\n",
            "step: 120, loss: 7.111661398084834e-05\n",
            "step: 130, loss: 8.196304406737909e-05\n",
            "step: 140, loss: 0.0027332636527717113\n",
            "step: 150, loss: 4.724492828245275e-05\n",
            "step: 160, loss: 0.017187079414725304\n",
            "step: 170, loss: 0.0002803397364914417\n",
            "step: 180, loss: 0.007453208323568106\n",
            "step: 190, loss: 0.0003212160081602633\n",
            "step: 200, loss: 0.00010833463602466509\n",
            "step: 210, loss: 0.00036953395465388894\n",
            "step: 220, loss: 0.000245588191319257\n",
            "step: 230, loss: 7.688700134167448e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9831649831649831, f1=0.9798657718120806, best_f1=0.9821029082774049\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 7.521810766775161e-05\n",
            "step: 10, loss: 3.771914998651482e-05\n",
            "step: 20, loss: 0.00010488511907169595\n",
            "step: 30, loss: 0.00019077349861618131\n",
            "step: 40, loss: 4.537441418506205e-05\n",
            "step: 50, loss: 3.587586616049521e-05\n",
            "step: 60, loss: 6.185044185258448e-05\n",
            "step: 70, loss: 5.757052713306621e-05\n",
            "step: 80, loss: 7.456001185346395e-05\n",
            "step: 90, loss: 0.00015716379857622087\n",
            "step: 100, loss: 0.001025811186991632\n",
            "step: 110, loss: 0.00010063155787065625\n",
            "step: 120, loss: 4.3281452235532925e-05\n",
            "step: 130, loss: 0.00031963762012310326\n",
            "step: 140, loss: 0.0031930580735206604\n",
            "step: 150, loss: 6.047669376130216e-05\n",
            "step: 160, loss: 8.716571028344333e-05\n",
            "step: 170, loss: 0.0002698382595553994\n",
            "step: 180, loss: 0.00012010219506919384\n",
            "step: 190, loss: 0.0001946143456734717\n",
            "step: 200, loss: 0.00014575180830433965\n",
            "step: 210, loss: 0.00018850521882995963\n",
            "step: 220, loss: 0.004015225451439619\n",
            "step: 230, loss: 5.0714781536953524e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9830890642615557, f1=0.9809203142536477, best_f1=0.9821029082774049\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0018409028416499496\n",
            "step: 10, loss: 0.0001542306854389608\n",
            "step: 20, loss: 0.00029874240863136947\n",
            "step: 30, loss: 4.684249506681226e-05\n",
            "step: 40, loss: 3.4252698242198676e-05\n",
            "step: 50, loss: 4.932699084747583e-05\n",
            "step: 60, loss: 0.0239032544195652\n",
            "step: 70, loss: 0.0012622610665857792\n",
            "step: 80, loss: 5.6480981584172696e-05\n",
            "step: 90, loss: 3.874441244988702e-05\n",
            "step: 100, loss: 2.7249478080193512e-05\n",
            "step: 110, loss: 0.0062121194787323475\n",
            "step: 120, loss: 0.02157861553132534\n",
            "step: 130, loss: 7.028587424429134e-05\n",
            "step: 140, loss: 0.00055761105613783\n",
            "step: 150, loss: 5.6928667618194595e-05\n",
            "step: 160, loss: 0.0003412714577279985\n",
            "step: 170, loss: 2.574057725723833e-05\n",
            "step: 180, loss: 0.00019093864830210805\n",
            "step: 190, loss: 5.43282221769914e-05\n",
            "step: 200, loss: 0.0002752675791271031\n",
            "step: 210, loss: 0.0013603395782411098\n",
            "step: 220, loss: 4.447377068572678e-05\n",
            "step: 230, loss: 0.0003380323469173163\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 15: dev_f1=0.9843400447427293, f1=0.9787709497206705, best_f1=0.9787709497206705\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "2473it [00:16, 152.39it/s]\n",
            "load_f1 = 0.9831649831649831\n",
            "real_f1 = 0.9820627802690582\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:31, 139.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBLP-GoogleScholar - Running the matcher"
      ],
      "metadata": {
        "id": "QY0y_yZuhstx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:\n",
        "# train the wdc model with the save_model flag on\n",
        "# it will produce two files *_dev.pt and *_test.pt which are the \n",
        "# best checkpoints on the dev set and the test set.\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_ditto.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --batch_size 32 \\\n",
        "  --max_len 256 \\\n",
        "  --lr 3e-5 \\\n",
        "  --n_epochs 15 \\\n",
        "  --finetuning \\\n",
        "  --lm roberta \\\n",
        "  --fp16 \\\n",
        "  --save_model \\\n",
        "  --dk general  \\\n",
        "  --dk product \\\n",
        "  --summarize \\\n",
        "  --da entry_swap  \\\n",
        "  --da attr_del\n",
        "\n",
        "\n",
        "# Step 2: pick a checkpoint, rename it to wdc_all_small.pt\n",
        "!mv *_dev.pt checkpoints/DBLP-GoogleScholar/.pt\n",
        "\n",
        "# Step 3: run the matcher\n",
        "!CUDA_VISIBLE_DEVICES=0 python matcher.py \\\n",
        "  --task Dirty/DBLP-GoogleScholar \\\n",
        "  --input_path input/input_small.jsonl \\\n",
        "  --output_path output/output_small.jsonl \\\n",
        "  --lm roberta \\\n",
        "  --use_gpu \\\n",
        "  --fp16 \\\n",
        "  --checkpoint_path checkpoints/"
      ],
      "metadata": {
        "id": "4aUWH5sHhstx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e29922d-dd34-4060-ad19-a6df15371960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.6016530990600586\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 10, loss: 0.5408405065536499\n",
            "step: 20, loss: 0.28653261065483093\n",
            "step: 30, loss: 0.35872137546539307\n",
            "step: 40, loss: 0.41295287013053894\n",
            "step: 50, loss: 0.6875376105308533\n",
            "step: 60, loss: 0.3477814197540283\n",
            "step: 70, loss: 0.40336477756500244\n",
            "step: 80, loss: 0.4122371971607208\n",
            "step: 90, loss: 0.1399577409029007\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
            "step: 100, loss: 0.4617884159088135\n",
            "step: 110, loss: 0.22702954709529877\n",
            "step: 120, loss: 0.2952024042606354\n",
            "step: 130, loss: 0.4424716532230377\n",
            "step: 140, loss: 0.18787793815135956\n",
            "step: 150, loss: 0.0972527340054512\n",
            "step: 160, loss: 0.2083234190940857\n",
            "step: 170, loss: 0.0806606113910675\n",
            "step: 180, loss: 0.07299027591943741\n",
            "step: 190, loss: 0.22406448423862457\n",
            "step: 200, loss: 0.08400201052427292\n",
            "step: 210, loss: 0.08374384045600891\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
            "step: 220, loss: 0.1012105718255043\n",
            "step: 230, loss: 0.23348629474639893\n",
            "step: 240, loss: 0.05722294747829437\n",
            "step: 250, loss: 0.009426192380487919\n",
            "step: 260, loss: 0.20769068598747253\n",
            "step: 270, loss: 0.3662005364894867\n",
            "step: 280, loss: 0.0879683718085289\n",
            "step: 290, loss: 0.17358660697937012\n",
            "step: 300, loss: 0.08575130254030228\n",
            "step: 310, loss: 0.05794616788625717\n",
            "step: 320, loss: 0.08948354423046112\n",
            "step: 330, loss: 0.054789766669273376\n",
            "step: 340, loss: 0.2938365042209625\n",
            "step: 350, loss: 0.1354418694972992\n",
            "step: 360, loss: 0.009953401051461697\n",
            "step: 370, loss: 0.1263837218284607\n",
            "step: 380, loss: 0.09944333136081696\n",
            "step: 390, loss: 0.017864318564534187\n",
            "step: 400, loss: 0.021069254726171494\n",
            "step: 410, loss: 0.2636454701423645\n",
            "step: 420, loss: 0.00951365940272808\n",
            "step: 430, loss: 0.029558170586824417\n",
            "step: 440, loss: 0.07601604610681534\n",
            "step: 450, loss: 0.22567126154899597\n",
            "step: 460, loss: 0.1728018820285797\n",
            "step: 470, loss: 0.0456177219748497\n",
            "step: 480, loss: 0.27847304940223694\n",
            "step: 490, loss: 0.1206713393330574\n",
            "step: 500, loss: 0.09007709473371506\n",
            "step: 510, loss: 0.03325798735022545\n",
            "step: 520, loss: 0.19587314128875732\n",
            "step: 530, loss: 0.048381321132183075\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 1: dev_f1=0.9298653042266605, f1=0.933826931975937, best_f1=0.933826931975937\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.05784802883863449\n",
            "step: 10, loss: 0.06369255483150482\n",
            "step: 20, loss: 0.02335163950920105\n",
            "step: 30, loss: 0.12659195065498352\n",
            "step: 40, loss: 0.030319146811962128\n",
            "step: 50, loss: 0.24631290137767792\n",
            "step: 60, loss: 0.06275836378335953\n",
            "step: 70, loss: 0.05508187413215637\n",
            "step: 80, loss: 0.07162082195281982\n",
            "step: 90, loss: 0.009767288342118263\n",
            "step: 100, loss: 0.23254084587097168\n",
            "step: 110, loss: 0.037632569670677185\n",
            "step: 120, loss: 0.04403890296816826\n",
            "step: 130, loss: 0.01886136457324028\n",
            "step: 140, loss: 0.05292225629091263\n",
            "step: 150, loss: 0.03667594864964485\n",
            "step: 160, loss: 0.02141730859875679\n",
            "step: 170, loss: 0.03861039876937866\n",
            "step: 180, loss: 0.03241823986172676\n",
            "step: 190, loss: 0.05772625282406807\n",
            "step: 200, loss: 0.26222914457321167\n",
            "step: 210, loss: 0.048020586371421814\n",
            "step: 220, loss: 0.002724266843870282\n",
            "step: 230, loss: 0.05773240327835083\n",
            "step: 240, loss: 0.13335634768009186\n",
            "step: 250, loss: 0.09444093704223633\n",
            "step: 260, loss: 0.30314022302627563\n",
            "step: 270, loss: 0.09897645562887192\n",
            "step: 280, loss: 0.03409251570701599\n",
            "step: 290, loss: 0.03282182663679123\n",
            "step: 300, loss: 0.06005137786269188\n",
            "step: 310, loss: 0.1350526660680771\n",
            "step: 320, loss: 0.045950569212436676\n",
            "step: 330, loss: 0.03982783108949661\n",
            "step: 340, loss: 0.0903383195400238\n",
            "step: 350, loss: 0.001642780378460884\n",
            "step: 360, loss: 0.163279727101326\n",
            "step: 370, loss: 0.022730210795998573\n",
            "step: 380, loss: 0.10548748821020126\n",
            "step: 390, loss: 0.005326990969479084\n",
            "step: 400, loss: 0.06807754188776016\n",
            "step: 410, loss: 0.0716806948184967\n",
            "step: 420, loss: 0.07305973023176193\n",
            "step: 430, loss: 0.16609054803848267\n",
            "step: 440, loss: 0.0029850462451577187\n",
            "step: 450, loss: 0.09967481344938278\n",
            "step: 460, loss: 0.050001710653305054\n",
            "step: 470, loss: 0.03572541102766991\n",
            "step: 480, loss: 0.006130714900791645\n",
            "step: 490, loss: 0.07009946554899216\n",
            "step: 500, loss: 0.02521761879324913\n",
            "step: 510, loss: 0.05420561507344246\n",
            "step: 520, loss: 0.3604614734649658\n",
            "step: 530, loss: 0.11859466880559921\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 2: dev_f1=0.9479553903345725, f1=0.9455560725919032, best_f1=0.9455560725919032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.10970021039247513\n",
            "step: 10, loss: 0.11173954606056213\n",
            "step: 20, loss: 0.023615362122654915\n",
            "step: 30, loss: 0.03223143890500069\n",
            "step: 40, loss: 0.2618955075740814\n",
            "step: 50, loss: 0.0665571317076683\n",
            "step: 60, loss: 0.010638369247317314\n",
            "step: 70, loss: 0.0035402653738856316\n",
            "step: 80, loss: 0.1855698972940445\n",
            "step: 90, loss: 0.015388491563498974\n",
            "step: 100, loss: 0.020752031356096268\n",
            "step: 110, loss: 0.015686361119151115\n",
            "step: 120, loss: 0.20281414687633514\n",
            "step: 130, loss: 0.044531047344207764\n",
            "step: 140, loss: 0.03794972226023674\n",
            "step: 150, loss: 0.04908537119626999\n",
            "step: 160, loss: 0.021808039397001266\n",
            "step: 170, loss: 0.03843977674841881\n",
            "step: 180, loss: 0.05240754410624504\n",
            "step: 190, loss: 0.01150195486843586\n",
            "step: 200, loss: 0.06906747072935104\n",
            "step: 210, loss: 0.05263938382267952\n",
            "step: 220, loss: 0.08989366888999939\n",
            "step: 230, loss: 0.06610200554132462\n",
            "step: 240, loss: 0.12612955272197723\n",
            "step: 250, loss: 0.2356761246919632\n",
            "step: 260, loss: 0.08307333290576935\n",
            "step: 270, loss: 0.01218412071466446\n",
            "step: 280, loss: 0.09309328347444534\n",
            "step: 290, loss: 0.031319763511419296\n",
            "step: 300, loss: 0.10742229968309402\n",
            "step: 310, loss: 0.15073467791080475\n",
            "step: 320, loss: 0.01354017574340105\n",
            "step: 330, loss: 0.03713750094175339\n",
            "step: 340, loss: 0.012175904586911201\n",
            "step: 350, loss: 0.050819069147109985\n",
            "step: 360, loss: 0.0076941740699112415\n",
            "step: 370, loss: 0.028160452842712402\n",
            "step: 380, loss: 0.010294970124959946\n",
            "step: 390, loss: 0.06840109080076218\n",
            "step: 400, loss: 0.042622555047273636\n",
            "step: 410, loss: 0.06737484782934189\n",
            "step: 420, loss: 0.08204139024019241\n",
            "step: 430, loss: 0.048282936215400696\n",
            "step: 440, loss: 0.2508247494697571\n",
            "step: 450, loss: 0.1101219654083252\n",
            "step: 460, loss: 0.03355337306857109\n",
            "step: 470, loss: 0.031527355313301086\n",
            "step: 480, loss: 0.25817447900772095\n",
            "step: 490, loss: 0.04609078913927078\n",
            "step: 500, loss: 0.009882249869406223\n",
            "step: 510, loss: 0.019901009276509285\n",
            "step: 520, loss: 0.02001138962805271\n",
            "step: 530, loss: 0.023192306980490685\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 3: dev_f1=0.9452503509592888, f1=0.9411764705882353, best_f1=0.9455560725919032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0359351709485054\n",
            "step: 10, loss: 0.026728352531790733\n",
            "step: 20, loss: 0.042070422321558\n",
            "step: 30, loss: 0.04014396667480469\n",
            "step: 40, loss: 0.013134544715285301\n",
            "step: 50, loss: 0.06681036949157715\n",
            "step: 60, loss: 0.023093638941645622\n",
            "step: 70, loss: 0.06435362249612808\n",
            "step: 80, loss: 0.10143382847309113\n",
            "step: 90, loss: 0.12168386578559875\n",
            "step: 100, loss: 0.01207798719406128\n",
            "step: 110, loss: 0.07741712033748627\n",
            "step: 120, loss: 0.0010100057115778327\n",
            "step: 130, loss: 0.0035036751069128513\n",
            "step: 140, loss: 0.04455578327178955\n",
            "step: 150, loss: 0.014379926957190037\n",
            "step: 160, loss: 0.006537744775414467\n",
            "step: 170, loss: 0.038175102323293686\n",
            "step: 180, loss: 0.07914018630981445\n",
            "step: 190, loss: 0.043242208659648895\n",
            "step: 200, loss: 0.022722844034433365\n",
            "step: 210, loss: 0.004958782810717821\n",
            "step: 220, loss: 0.2164851576089859\n",
            "step: 230, loss: 0.015869295224547386\n",
            "step: 240, loss: 0.009892456233501434\n",
            "step: 250, loss: 0.09961816668510437\n",
            "step: 260, loss: 0.0019455589354038239\n",
            "step: 270, loss: 0.07434170693159103\n",
            "step: 280, loss: 0.003291697707027197\n",
            "step: 290, loss: 0.013788674026727676\n",
            "step: 300, loss: 0.0012893383391201496\n",
            "step: 310, loss: 0.002504087286069989\n",
            "step: 320, loss: 0.18358655273914337\n",
            "step: 330, loss: 0.03900253027677536\n",
            "step: 340, loss: 0.010082261636853218\n",
            "step: 350, loss: 0.08754180371761322\n",
            "step: 360, loss: 0.04675056412816048\n",
            "step: 370, loss: 0.009363174438476562\n",
            "step: 380, loss: 0.008976919576525688\n",
            "step: 390, loss: 0.0018760983366519213\n",
            "step: 400, loss: 0.04307993873953819\n",
            "step: 410, loss: 0.013529636897146702\n",
            "step: 420, loss: 0.010325999930500984\n",
            "step: 430, loss: 0.01909567043185234\n",
            "step: 440, loss: 0.018484922125935555\n",
            "step: 450, loss: 0.028417831286787987\n",
            "step: 460, loss: 0.009586052969098091\n",
            "step: 470, loss: 0.0013525920221582055\n",
            "step: 480, loss: 0.09407349675893784\n",
            "step: 490, loss: 0.024726012721657753\n",
            "step: 500, loss: 0.36926764249801636\n",
            "step: 510, loss: 0.0732816606760025\n",
            "step: 520, loss: 0.01948152668774128\n",
            "step: 530, loss: 0.35449549555778503\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 4: dev_f1=0.9452181987000929, f1=0.9416666666666667, best_f1=0.9455560725919032\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006918288301676512\n",
            "step: 10, loss: 0.004789547063410282\n",
            "step: 20, loss: 0.013705424033105373\n",
            "step: 30, loss: 0.023515785112977028\n",
            "step: 40, loss: 0.012093986384570599\n",
            "step: 50, loss: 0.00210145628079772\n",
            "step: 60, loss: 0.006938346195966005\n",
            "step: 70, loss: 0.020094215869903564\n",
            "step: 80, loss: 0.02079654112458229\n",
            "step: 90, loss: 0.05142612010240555\n",
            "step: 100, loss: 0.018386930227279663\n",
            "step: 110, loss: 0.0006750907632522285\n",
            "step: 120, loss: 0.23073774576187134\n",
            "step: 130, loss: 0.027179211378097534\n",
            "step: 140, loss: 0.020720748230814934\n",
            "step: 150, loss: 0.05981963127851486\n",
            "step: 160, loss: 0.008472807705402374\n",
            "step: 170, loss: 0.015034377574920654\n",
            "step: 180, loss: 0.015584232285618782\n",
            "step: 190, loss: 0.001842675032094121\n",
            "step: 200, loss: 0.0036947159096598625\n",
            "step: 210, loss: 0.02128755860030651\n",
            "step: 220, loss: 0.04454192891716957\n",
            "step: 230, loss: 0.0028236545622348785\n",
            "step: 240, loss: 0.030380673706531525\n",
            "step: 250, loss: 0.08878667652606964\n",
            "step: 260, loss: 0.00379247241653502\n",
            "step: 270, loss: 0.001432347111403942\n",
            "step: 280, loss: 0.009857376106083393\n",
            "step: 290, loss: 0.00976146012544632\n",
            "step: 300, loss: 0.08936517685651779\n",
            "step: 310, loss: 0.05314638838171959\n",
            "step: 320, loss: 0.0058546108193695545\n",
            "step: 330, loss: 0.0038691714871674776\n",
            "step: 340, loss: 0.011121603660285473\n",
            "step: 350, loss: 0.003881945740431547\n",
            "step: 360, loss: 0.0003224439569748938\n",
            "step: 370, loss: 0.00034039950696751475\n",
            "step: 380, loss: 0.014573422260582447\n",
            "step: 390, loss: 0.0012833827640861273\n",
            "step: 400, loss: 0.017811022698879242\n",
            "step: 410, loss: 0.07852821052074432\n",
            "step: 420, loss: 0.21845190227031708\n",
            "step: 430, loss: 0.07336302101612091\n",
            "step: 440, loss: 0.006760946940630674\n",
            "step: 450, loss: 0.010712899267673492\n",
            "step: 460, loss: 0.025615738704800606\n",
            "step: 470, loss: 0.0891864225268364\n",
            "step: 480, loss: 0.014612526632845402\n",
            "step: 490, loss: 0.0962894931435585\n",
            "step: 500, loss: 0.06243037059903145\n",
            "step: 510, loss: 0.0194003414362669\n",
            "step: 520, loss: 0.08815788477659225\n",
            "step: 530, loss: 0.030127136036753654\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 5: dev_f1=0.9500233535730968, f1=0.941286989196806, best_f1=0.941286989196806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.043854255229234695\n",
            "step: 10, loss: 0.0549180768430233\n",
            "step: 20, loss: 0.01309348177164793\n",
            "step: 30, loss: 0.0020928129088133574\n",
            "step: 40, loss: 0.00034825364127755165\n",
            "step: 50, loss: 0.001978342654183507\n",
            "step: 60, loss: 0.0002814355830196291\n",
            "step: 70, loss: 0.0013180356472730637\n",
            "step: 80, loss: 0.02076425589621067\n",
            "step: 90, loss: 0.10781989246606827\n",
            "step: 100, loss: 0.011818824335932732\n",
            "step: 110, loss: 0.015303621999919415\n",
            "step: 120, loss: 0.008018726482987404\n",
            "step: 130, loss: 0.0027926114853471518\n",
            "step: 140, loss: 0.004379339516162872\n",
            "step: 150, loss: 0.0007390871760435402\n",
            "step: 160, loss: 0.03754456713795662\n",
            "step: 170, loss: 0.004415706731379032\n",
            "step: 180, loss: 0.002336923498660326\n",
            "step: 190, loss: 0.02492458000779152\n",
            "step: 200, loss: 0.03368731588125229\n",
            "step: 210, loss: 0.01011132262647152\n",
            "step: 220, loss: 0.005515509285032749\n",
            "step: 230, loss: 0.04169200733304024\n",
            "step: 240, loss: 0.0011298198951408267\n",
            "step: 250, loss: 0.11767568439245224\n",
            "step: 260, loss: 0.0012717292411252856\n",
            "step: 270, loss: 0.012609170749783516\n",
            "step: 280, loss: 0.001657032291404903\n",
            "step: 290, loss: 0.0011653626570478082\n",
            "step: 300, loss: 0.013148698955774307\n",
            "step: 310, loss: 0.12386675179004669\n",
            "step: 320, loss: 0.0001459649356547743\n",
            "step: 330, loss: 0.02099902741611004\n",
            "step: 340, loss: 0.005090456455945969\n",
            "step: 350, loss: 0.01282986905425787\n",
            "step: 360, loss: 0.04420290142297745\n",
            "step: 370, loss: 0.002986110746860504\n",
            "step: 380, loss: 0.0002478230162523687\n",
            "step: 390, loss: 0.0026343849021941423\n",
            "step: 400, loss: 0.004384488333016634\n",
            "step: 410, loss: 0.002423298079520464\n",
            "step: 420, loss: 0.020233260467648506\n",
            "step: 430, loss: 0.020891990512609482\n",
            "step: 440, loss: 0.0038720250595360994\n",
            "step: 450, loss: 0.17996086180210114\n",
            "step: 460, loss: 0.002331545576453209\n",
            "step: 470, loss: 0.010562616400420666\n",
            "step: 480, loss: 0.007144487462937832\n",
            "step: 490, loss: 0.13781823217868805\n",
            "step: 500, loss: 0.0006835209205746651\n",
            "step: 510, loss: 0.2591519057750702\n",
            "step: 520, loss: 0.0035544000566005707\n",
            "step: 530, loss: 0.004062245599925518\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 6: dev_f1=0.9453162149310508, f1=0.9410085632730733, best_f1=0.941286989196806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.005555413663387299\n",
            "step: 10, loss: 0.0009964030468836427\n",
            "step: 20, loss: 0.013423729687929153\n",
            "step: 30, loss: 0.030157960951328278\n",
            "step: 40, loss: 0.009430057369172573\n",
            "step: 50, loss: 0.008510874584317207\n",
            "step: 60, loss: 0.00800918135792017\n",
            "step: 70, loss: 0.0018311613239347935\n",
            "step: 80, loss: 0.004161683842539787\n",
            "step: 90, loss: 0.0002657197474036366\n",
            "step: 100, loss: 0.005602334160357714\n",
            "step: 110, loss: 0.0006791893974877894\n",
            "step: 120, loss: 0.2753038704395294\n",
            "step: 130, loss: 0.008400698192417622\n",
            "step: 140, loss: 0.003291537519544363\n",
            "step: 150, loss: 0.0025641778483986855\n",
            "step: 160, loss: 0.016405753791332245\n",
            "step: 170, loss: 0.0011561751598492265\n",
            "step: 180, loss: 0.04825247824192047\n",
            "step: 190, loss: 0.021337559446692467\n",
            "step: 200, loss: 0.0019745659083127975\n",
            "step: 210, loss: 0.0029578381218016148\n",
            "step: 220, loss: 0.008158196695148945\n",
            "step: 230, loss: 0.002637979108840227\n",
            "step: 240, loss: 0.00414246367290616\n",
            "step: 250, loss: 0.007896088063716888\n",
            "step: 260, loss: 0.009378033690154552\n",
            "step: 270, loss: 0.012813379056751728\n",
            "step: 280, loss: 0.0041433777660131454\n",
            "step: 290, loss: 0.12041941285133362\n",
            "step: 300, loss: 0.002391788177192211\n",
            "step: 310, loss: 0.001387760043144226\n",
            "step: 320, loss: 0.02337006852030754\n",
            "step: 330, loss: 0.0006564960349351168\n",
            "step: 340, loss: 0.017182407900691032\n",
            "step: 350, loss: 0.004663480445742607\n",
            "step: 360, loss: 0.010886318981647491\n",
            "step: 370, loss: 0.011584371328353882\n",
            "step: 380, loss: 0.000515387044288218\n",
            "step: 390, loss: 0.007614050060510635\n",
            "step: 400, loss: 0.037287428975105286\n",
            "step: 410, loss: 0.00016042821516748518\n",
            "step: 420, loss: 0.0149949686601758\n",
            "step: 430, loss: 0.0013773980317637324\n",
            "step: 440, loss: 0.0004788915393874049\n",
            "step: 450, loss: 0.025299161672592163\n",
            "step: 460, loss: 0.002676486037671566\n",
            "step: 470, loss: 0.26857906579971313\n",
            "step: 480, loss: 0.0016864617355167866\n",
            "step: 490, loss: 0.005268121603876352\n",
            "step: 500, loss: 0.007861284539103508\n",
            "step: 510, loss: 0.0014681603061035275\n",
            "step: 520, loss: 0.0020440183579921722\n",
            "step: 530, loss: 0.032150767743587494\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 7: dev_f1=0.9402427637721755, f1=0.9342105263157895, best_f1=0.941286989196806\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008746355306357145\n",
            "step: 10, loss: 0.0004877602041233331\n",
            "step: 20, loss: 0.0007298318669199944\n",
            "step: 30, loss: 0.0018322198884561658\n",
            "step: 40, loss: 0.0004972138558514416\n",
            "step: 50, loss: 0.004275182262063026\n",
            "step: 60, loss: 0.0002898193197324872\n",
            "step: 70, loss: 0.0016361285233870149\n",
            "step: 80, loss: 0.022310560569167137\n",
            "step: 90, loss: 0.013630124740302563\n",
            "step: 100, loss: 0.007846484892070293\n",
            "step: 110, loss: 0.0003398441185709089\n",
            "step: 120, loss: 0.011694865301251411\n",
            "step: 130, loss: 0.0018024482997134328\n",
            "step: 140, loss: 0.000515044666826725\n",
            "step: 150, loss: 0.001080166082829237\n",
            "step: 160, loss: 0.002987568499520421\n",
            "step: 170, loss: 0.05419242009520531\n",
            "step: 180, loss: 0.0015480410074815154\n",
            "step: 190, loss: 0.0123860202729702\n",
            "step: 200, loss: 0.004001634661108255\n",
            "step: 210, loss: 0.05849594622850418\n",
            "step: 220, loss: 0.0031122767832130194\n",
            "step: 230, loss: 0.08431331068277359\n",
            "step: 240, loss: 0.11878979951143265\n",
            "step: 250, loss: 0.0005712403799407184\n",
            "step: 260, loss: 0.0020281802862882614\n",
            "step: 270, loss: 0.027611156925559044\n",
            "step: 280, loss: 0.002180828945711255\n",
            "step: 290, loss: 0.0022865913342684507\n",
            "step: 300, loss: 0.00034951846464537084\n",
            "step: 310, loss: 0.0014648103388026357\n",
            "step: 320, loss: 0.0029751486144959927\n",
            "step: 330, loss: 0.0013221633853390813\n",
            "step: 340, loss: 0.06280297040939331\n",
            "step: 350, loss: 0.0006355607183650136\n",
            "step: 360, loss: 0.08707458525896072\n",
            "step: 370, loss: 0.02056816592812538\n",
            "step: 380, loss: 0.0015494738472625613\n",
            "step: 390, loss: 0.007509011309593916\n",
            "step: 400, loss: 0.05490861460566521\n",
            "step: 410, loss: 0.11420269310474396\n",
            "step: 420, loss: 0.0001302588643739\n",
            "step: 430, loss: 0.005537188146263361\n",
            "step: 440, loss: 0.017094528302550316\n",
            "step: 450, loss: 0.003361625364050269\n",
            "step: 460, loss: 0.016246983781456947\n",
            "step: 470, loss: 0.08182860910892487\n",
            "step: 480, loss: 0.009460925124585629\n",
            "step: 490, loss: 0.07191771268844604\n",
            "step: 500, loss: 0.001842243131250143\n",
            "step: 510, loss: 0.02706834115087986\n",
            "step: 520, loss: 0.0003515793941915035\n",
            "step: 530, loss: 0.0004505084070842713\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 8: dev_f1=0.950187969924812, f1=0.9418439716312057, best_f1=0.9418439716312057\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0008860734524205327\n",
            "step: 10, loss: 0.004002880770713091\n",
            "step: 20, loss: 0.0015995565336197615\n",
            "step: 30, loss: 0.07992997020483017\n",
            "step: 40, loss: 0.01506322156637907\n",
            "step: 50, loss: 0.001921455143019557\n",
            "step: 60, loss: 0.0009824992157518864\n",
            "step: 70, loss: 0.0005767598049715161\n",
            "step: 80, loss: 0.014030566439032555\n",
            "step: 90, loss: 0.04659184068441391\n",
            "step: 100, loss: 0.0026293464470654726\n",
            "step: 110, loss: 0.002846753690391779\n",
            "step: 120, loss: 0.006421966943889856\n",
            "step: 130, loss: 0.003273813519626856\n",
            "step: 140, loss: 0.007793476339429617\n",
            "step: 150, loss: 0.002493505598977208\n",
            "step: 160, loss: 0.0025967333931475878\n",
            "step: 170, loss: 0.001107400981709361\n",
            "step: 180, loss: 0.009270939975976944\n",
            "step: 190, loss: 0.0025500894989818335\n",
            "step: 200, loss: 0.0008827592828311026\n",
            "step: 210, loss: 0.006448649801313877\n",
            "step: 220, loss: 0.09797403216362\n",
            "step: 230, loss: 0.000415524875279516\n",
            "step: 240, loss: 0.002384789753705263\n",
            "step: 250, loss: 0.0009487204952165484\n",
            "step: 260, loss: 0.021943707019090652\n",
            "step: 270, loss: 0.009214391000568867\n",
            "step: 280, loss: 0.0008350482094101608\n",
            "step: 290, loss: 0.0007018441101536155\n",
            "step: 300, loss: 0.007238366641104221\n",
            "step: 310, loss: 0.0029286423232406378\n",
            "step: 320, loss: 0.0011196386767551303\n",
            "step: 330, loss: 0.0018926130142062902\n",
            "step: 340, loss: 0.006761026568710804\n",
            "step: 350, loss: 0.020973296836018562\n",
            "step: 360, loss: 0.03646313026547432\n",
            "step: 370, loss: 0.002031535841524601\n",
            "step: 380, loss: 0.00106630299706012\n",
            "step: 390, loss: 0.0001816151780076325\n",
            "step: 400, loss: 0.031359512358903885\n",
            "step: 410, loss: 0.0008129371562972665\n",
            "step: 420, loss: 0.0011846349807456136\n",
            "step: 430, loss: 0.0027269686106592417\n",
            "step: 440, loss: 9.34306881390512e-05\n",
            "step: 450, loss: 0.029804568737745285\n",
            "step: 460, loss: 0.0030003751162439585\n",
            "step: 470, loss: 0.026970289647579193\n",
            "step: 480, loss: 0.003604867262765765\n",
            "step: 490, loss: 0.0010238748509436846\n",
            "step: 500, loss: 0.028632057830691338\n",
            "step: 510, loss: 0.10528609901666641\n",
            "step: 520, loss: 0.012703250162303448\n",
            "step: 530, loss: 0.040222782641649246\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "epoch 9: dev_f1=0.9527777777777777, f1=0.9469767441860464, best_f1=0.9469767441860464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.008707325905561447\n",
            "step: 10, loss: 0.001683301292359829\n",
            "step: 20, loss: 9.56230578594841e-05\n",
            "step: 30, loss: 0.007446676027029753\n",
            "step: 40, loss: 0.0015721881063655019\n",
            "step: 50, loss: 0.0005267660599201918\n",
            "step: 60, loss: 0.005307563580572605\n",
            "step: 70, loss: 0.03282836452126503\n",
            "step: 80, loss: 0.00012854867964051664\n",
            "step: 90, loss: 0.004134104121476412\n",
            "step: 100, loss: 0.02804214134812355\n",
            "step: 110, loss: 0.02552664466202259\n",
            "step: 120, loss: 0.0021253919694572687\n",
            "step: 130, loss: 0.006635759491473436\n",
            "step: 140, loss: 0.0013896750751882792\n",
            "step: 150, loss: 0.003103662049397826\n",
            "step: 160, loss: 0.026078738272190094\n",
            "step: 170, loss: 0.014943505637347698\n",
            "step: 180, loss: 0.02329309470951557\n",
            "step: 190, loss: 7.488032133551314e-05\n",
            "step: 200, loss: 0.0011645944323390722\n",
            "step: 210, loss: 0.0014243398327380419\n",
            "step: 220, loss: 0.0007537057972513139\n",
            "step: 230, loss: 0.005350407678633928\n",
            "step: 240, loss: 0.00035411881981417537\n",
            "step: 250, loss: 0.0009598026517778635\n",
            "step: 260, loss: 0.0011736978776752949\n",
            "step: 270, loss: 4.6813394874334335e-05\n",
            "step: 280, loss: 0.0748748630285263\n",
            "step: 290, loss: 0.0002711203705985099\n",
            "step: 300, loss: 0.0042902217246592045\n",
            "step: 310, loss: 0.0023204258177429438\n",
            "step: 320, loss: 0.061923619359731674\n",
            "step: 330, loss: 0.015371795743703842\n",
            "step: 340, loss: 0.0014038638910278678\n",
            "step: 350, loss: 0.05940777063369751\n",
            "step: 360, loss: 0.00017801392823457718\n",
            "step: 370, loss: 0.0012813986977562308\n",
            "step: 380, loss: 0.0004449300467967987\n",
            "step: 390, loss: 0.00877027865499258\n",
            "step: 400, loss: 0.004513143561780453\n",
            "step: 410, loss: 0.001803395338356495\n",
            "step: 420, loss: 9.14255651878193e-05\n",
            "step: 430, loss: 0.01376900915056467\n",
            "step: 440, loss: 0.00021173048298805952\n",
            "step: 450, loss: 0.017020365223288536\n",
            "step: 460, loss: 0.0003269918670412153\n",
            "step: 470, loss: 0.004391095135360956\n",
            "step: 480, loss: 0.0002873440971598029\n",
            "step: 490, loss: 0.0058835819363594055\n",
            "step: 500, loss: 0.009304195642471313\n",
            "step: 510, loss: 0.0003911121457349509\n",
            "step: 520, loss: 0.002476766239851713\n",
            "step: 530, loss: 0.010669856332242489\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 10: dev_f1=0.951851851851852, f1=0.9458082445576655, best_f1=0.9469767441860464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0006220873910933733\n",
            "step: 10, loss: 0.0005455577047541738\n",
            "step: 20, loss: 8.424015686614439e-05\n",
            "step: 30, loss: 0.00015521436580456793\n",
            "step: 40, loss: 0.00042618034058250487\n",
            "step: 50, loss: 0.00012750618043355644\n",
            "step: 60, loss: 0.0004109530127607286\n",
            "step: 70, loss: 0.0001407250383635983\n",
            "step: 80, loss: 0.007809455972164869\n",
            "step: 90, loss: 0.00152855203486979\n",
            "step: 100, loss: 0.010218752548098564\n",
            "step: 110, loss: 0.00014783612277824432\n",
            "step: 120, loss: 0.001217442099004984\n",
            "step: 130, loss: 0.00010385778296040371\n",
            "step: 140, loss: 0.00011118053225800395\n",
            "step: 150, loss: 0.00013456668239086866\n",
            "step: 160, loss: 0.0011762662325054407\n",
            "step: 170, loss: 0.003271055407822132\n",
            "step: 180, loss: 4.090202855877578e-05\n",
            "step: 190, loss: 0.007583215832710266\n",
            "step: 200, loss: 0.001232219859957695\n",
            "step: 210, loss: 8.195603732019663e-05\n",
            "step: 220, loss: 0.05285891145467758\n",
            "step: 230, loss: 0.0017947423039004207\n",
            "step: 240, loss: 0.0007421479094773531\n",
            "step: 250, loss: 0.00019150493608321995\n",
            "step: 260, loss: 0.001250464585609734\n",
            "step: 270, loss: 0.006090317387133837\n",
            "step: 280, loss: 0.0006934432312846184\n",
            "step: 290, loss: 0.0006638043560087681\n",
            "step: 300, loss: 3.751200347323902e-05\n",
            "step: 310, loss: 0.0030944321770220995\n",
            "step: 320, loss: 0.0019199518719688058\n",
            "step: 330, loss: 0.0003969414974562824\n",
            "step: 340, loss: 0.005832551047205925\n",
            "step: 350, loss: 0.004020940978080034\n",
            "step: 360, loss: 0.00017588349874131382\n",
            "step: 370, loss: 0.0011278066085651517\n",
            "step: 380, loss: 0.0016767096240073442\n",
            "step: 390, loss: 0.0011163491290062666\n",
            "step: 400, loss: 0.0021817830856889486\n",
            "step: 410, loss: 0.001760201994329691\n",
            "step: 420, loss: 0.01602233201265335\n",
            "step: 430, loss: 0.015472856350243092\n",
            "step: 440, loss: 0.00015566287038382143\n",
            "step: 450, loss: 0.002190578728914261\n",
            "step: 460, loss: 0.003143091220408678\n",
            "step: 470, loss: 0.0009275175398215652\n",
            "step: 480, loss: 0.0008460283279418945\n",
            "step: 490, loss: 0.07175615429878235\n",
            "step: 500, loss: 0.00066947570303455\n",
            "step: 510, loss: 0.0002052946074400097\n",
            "step: 520, loss: 0.0009446527110412717\n",
            "step: 530, loss: 0.007274319417774677\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 11: dev_f1=0.951048951048951, f1=0.9454545454545454, best_f1=0.9469767441860464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.006881266832351685\n",
            "step: 10, loss: 0.0015425619203597307\n",
            "step: 20, loss: 0.005845093633979559\n",
            "step: 30, loss: 5.5393986258422956e-05\n",
            "step: 40, loss: 0.0067327567376196384\n",
            "step: 50, loss: 0.00717034051194787\n",
            "step: 60, loss: 2.8430758902686648e-05\n",
            "step: 70, loss: 0.0006138192256912589\n",
            "step: 80, loss: 0.29764485359191895\n",
            "step: 90, loss: 0.004131283611059189\n",
            "step: 100, loss: 0.06776203960180283\n",
            "step: 110, loss: 0.001397539977915585\n",
            "step: 120, loss: 0.0009998325258493423\n",
            "step: 130, loss: 0.0032675613183528185\n",
            "step: 140, loss: 0.0003475412377156317\n",
            "step: 150, loss: 0.058091968297958374\n",
            "step: 160, loss: 0.0017860678490251303\n",
            "step: 170, loss: 0.00031096735619939864\n",
            "step: 180, loss: 0.0003809505724348128\n",
            "step: 190, loss: 0.018621359020471573\n",
            "step: 200, loss: 0.0006118231685832143\n",
            "step: 210, loss: 0.001108266063965857\n",
            "step: 220, loss: 0.0005734594888053834\n",
            "step: 230, loss: 0.029112551361322403\n",
            "step: 240, loss: 0.001098955748602748\n",
            "step: 250, loss: 0.00012672781303990632\n",
            "step: 260, loss: 0.0007082909578457475\n",
            "step: 270, loss: 0.0006672080489806831\n",
            "step: 280, loss: 0.0012799871619790792\n",
            "step: 290, loss: 7.595965871587396e-05\n",
            "step: 300, loss: 0.0014760105405002832\n",
            "step: 310, loss: 0.000369786168448627\n",
            "step: 320, loss: 0.21305634081363678\n",
            "step: 330, loss: 0.004410632885992527\n",
            "step: 340, loss: 0.0026179328560829163\n",
            "step: 350, loss: 0.0004993161419406533\n",
            "step: 360, loss: 0.0022885575890541077\n",
            "step: 370, loss: 0.0004593775956891477\n",
            "step: 380, loss: 0.0009006606414914131\n",
            "step: 390, loss: 0.014219379983842373\n",
            "step: 400, loss: 0.0004971189773641527\n",
            "step: 410, loss: 0.0006418824195861816\n",
            "step: 420, loss: 0.0008689446258358657\n",
            "step: 430, loss: 0.0005928716855123639\n",
            "step: 440, loss: 0.0019902470521628857\n",
            "step: 450, loss: 0.00057133415248245\n",
            "step: 460, loss: 0.00014841335359960794\n",
            "step: 470, loss: 0.001940669841133058\n",
            "step: 480, loss: 0.0037825063336640596\n",
            "step: 490, loss: 0.00225664465688169\n",
            "step: 500, loss: 0.0005140199209563434\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
            "step: 510, loss: 0.006364952772855759\n",
            "step: 520, loss: 0.0014259335584938526\n",
            "step: 530, loss: 0.0004474818706512451\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 12: dev_f1=0.948339483394834, f1=0.9426456984273821, best_f1=0.9469767441860464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0033618221059441566\n",
            "step: 10, loss: 0.0002762152871582657\n",
            "step: 20, loss: 0.0002758036716841161\n",
            "step: 30, loss: 0.0007317520794458687\n",
            "step: 40, loss: 0.0005944709409959614\n",
            "step: 50, loss: 0.0019438257440924644\n",
            "step: 60, loss: 0.00028361400472931564\n",
            "step: 70, loss: 0.0005441776011139154\n",
            "step: 80, loss: 0.0046615442261099815\n",
            "step: 90, loss: 0.0014098136452957988\n",
            "step: 100, loss: 0.00011108448234153911\n",
            "step: 110, loss: 5.64622096135281e-05\n",
            "step: 120, loss: 0.0003374250081833452\n",
            "step: 130, loss: 0.0007592585170641541\n",
            "step: 140, loss: 0.00027547471108846366\n",
            "step: 150, loss: 0.001108434284105897\n",
            "step: 160, loss: 0.011103007942438126\n",
            "step: 170, loss: 0.00186661456245929\n",
            "step: 180, loss: 0.00212098122574389\n",
            "step: 190, loss: 4.128316868445836e-05\n",
            "step: 200, loss: 5.562596197705716e-05\n",
            "step: 210, loss: 4.6442528400802985e-05\n",
            "step: 220, loss: 0.02166741155087948\n",
            "step: 230, loss: 0.00027848215540871024\n",
            "step: 240, loss: 0.002491488354280591\n",
            "step: 250, loss: 0.0012350452598184347\n",
            "step: 260, loss: 0.11679515987634659\n",
            "step: 270, loss: 0.00011211973469471559\n",
            "step: 280, loss: 0.0001266501349164173\n",
            "step: 290, loss: 0.0003446451446507126\n",
            "step: 300, loss: 1.690140925347805e-05\n",
            "step: 310, loss: 0.005304827820509672\n",
            "step: 320, loss: 0.00010957181075355038\n",
            "step: 330, loss: 0.0007471635472029448\n",
            "step: 340, loss: 0.10618622601032257\n",
            "step: 350, loss: 0.00013619671517517418\n",
            "step: 360, loss: 0.12331455945968628\n",
            "step: 370, loss: 0.00038178550312295556\n",
            "step: 380, loss: 0.00017766088421922177\n",
            "step: 390, loss: 0.00029959093080833554\n",
            "step: 400, loss: 0.027294989675283432\n",
            "step: 410, loss: 7.774818368488923e-05\n",
            "step: 420, loss: 0.00012903234164696187\n",
            "step: 430, loss: 0.0002716419694479555\n",
            "step: 440, loss: 3.016240407305304e-05\n",
            "step: 450, loss: 0.09091512113809586\n",
            "step: 460, loss: 0.019313951954245567\n",
            "step: 470, loss: 0.002042289823293686\n",
            "step: 480, loss: 1.647983845032286e-05\n",
            "step: 490, loss: 0.00020879674411844462\n",
            "step: 500, loss: 0.0002294548903591931\n",
            "step: 510, loss: 0.0005496318335644901\n",
            "step: 520, loss: 0.0012295338092371821\n",
            "step: 530, loss: 8.553521911380813e-05\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 13: dev_f1=0.9508041627246925, f1=0.9430740037950663, best_f1=0.9469767441860464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004180096439085901\n",
            "step: 10, loss: 0.00023069503367878497\n",
            "step: 20, loss: 0.0002184257609769702\n",
            "step: 30, loss: 0.0014226097846403718\n",
            "step: 40, loss: 0.00031757925171405077\n",
            "step: 50, loss: 0.016930969431996346\n",
            "step: 60, loss: 6.306605064310133e-05\n",
            "step: 70, loss: 0.001852954039350152\n",
            "step: 80, loss: 0.00021708168787881732\n",
            "step: 90, loss: 6.840629794169217e-05\n",
            "step: 100, loss: 0.000841341505292803\n",
            "step: 110, loss: 0.00029643491143360734\n",
            "step: 120, loss: 1.4766698768653441e-05\n",
            "step: 130, loss: 7.6185395300854e-05\n",
            "step: 140, loss: 0.00537909334525466\n",
            "step: 150, loss: 0.01537015289068222\n",
            "step: 160, loss: 8.834696200210601e-05\n",
            "step: 170, loss: 0.0007590291206724942\n",
            "step: 180, loss: 0.00020423824025783688\n",
            "step: 190, loss: 0.0019363496685400605\n",
            "step: 200, loss: 6.642203516094014e-05\n",
            "step: 210, loss: 0.006545147858560085\n",
            "step: 220, loss: 0.0002684843202587217\n",
            "step: 230, loss: 0.0007981043308973312\n",
            "step: 240, loss: 0.02576333098113537\n",
            "step: 250, loss: 0.0019871287513524294\n",
            "step: 260, loss: 0.0028307815082371235\n",
            "step: 270, loss: 0.0011756251333281398\n",
            "step: 280, loss: 0.00020629620121326298\n",
            "step: 290, loss: 0.00012292570318095386\n",
            "step: 300, loss: 0.0005088479374535382\n",
            "step: 310, loss: 0.0009560619364492595\n",
            "step: 320, loss: 0.0004425456572789699\n",
            "step: 330, loss: 0.001917450106702745\n",
            "step: 340, loss: 0.0005201316671445966\n",
            "step: 350, loss: 0.00013782037422060966\n",
            "step: 360, loss: 0.00011755935702240095\n",
            "step: 370, loss: 0.07307565957307816\n",
            "step: 380, loss: 0.0009464796748943627\n",
            "step: 390, loss: 0.006490449886769056\n",
            "step: 400, loss: 0.0028616918716579676\n",
            "step: 410, loss: 0.00013481736823450774\n",
            "step: 420, loss: 0.0008938032551668584\n",
            "step: 430, loss: 0.012774602510035038\n",
            "step: 440, loss: 5.5824020819272846e-05\n",
            "step: 450, loss: 7.203660061350092e-05\n",
            "step: 460, loss: 0.08575161546468735\n",
            "step: 470, loss: 1.7221518646692857e-05\n",
            "step: 480, loss: 0.0007638730457983911\n",
            "step: 490, loss: 0.0012852151412516832\n",
            "step: 500, loss: 0.0030509456992149353\n",
            "step: 510, loss: 0.04613456875085831\n",
            "step: 520, loss: 0.013317476958036423\n",
            "step: 530, loss: 0.0016738693229854107\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 14: dev_f1=0.9517564402810305, f1=0.947417840375587, best_f1=0.9469767441860464\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "step: 0, loss: 0.0004222436691634357\n",
            "step: 10, loss: 0.004627940244972706\n",
            "step: 20, loss: 0.0005342259537428617\n",
            "step: 30, loss: 0.005306811071932316\n",
            "step: 40, loss: 8.27194526209496e-05\n",
            "step: 50, loss: 0.0066964286379516125\n",
            "step: 60, loss: 0.001435297541320324\n",
            "step: 70, loss: 0.00044237170368433\n",
            "step: 80, loss: 0.0006843424052931368\n",
            "step: 90, loss: 0.0030790637247264385\n",
            "step: 100, loss: 8.658464503241703e-05\n",
            "step: 110, loss: 0.027746178209781647\n",
            "step: 120, loss: 0.0001402635534759611\n",
            "step: 130, loss: 0.006967463530600071\n",
            "step: 140, loss: 5.143204907653853e-05\n",
            "step: 150, loss: 0.0015041421866044402\n",
            "step: 160, loss: 0.0004850762488786131\n",
            "step: 170, loss: 6.934606790309772e-05\n",
            "step: 180, loss: 0.0008032795740291476\n",
            "step: 190, loss: 0.0033397350925952196\n",
            "step: 200, loss: 0.000947357271797955\n",
            "step: 210, loss: 0.001360374386422336\n",
            "step: 220, loss: 0.0015230294084176421\n",
            "step: 230, loss: 0.17688605189323425\n",
            "step: 240, loss: 0.00010334992111893371\n",
            "step: 250, loss: 0.0005162360030226409\n",
            "step: 260, loss: 1.8905371689470485e-05\n",
            "step: 270, loss: 1.9162380340276286e-05\n",
            "step: 280, loss: 3.488634683890268e-05\n",
            "step: 290, loss: 0.000692423083819449\n",
            "step: 300, loss: 0.0006359717808663845\n",
            "step: 310, loss: 0.0005335371824912727\n",
            "step: 320, loss: 9.516167483525351e-05\n",
            "step: 330, loss: 0.0006620539352297783\n",
            "step: 340, loss: 0.0002591015072539449\n",
            "step: 350, loss: 0.005798682104796171\n",
            "step: 360, loss: 0.0005548717454075813\n",
            "step: 370, loss: 0.01261928305029869\n",
            "step: 380, loss: 0.001973051344975829\n",
            "step: 390, loss: 0.0003705014241859317\n",
            "step: 400, loss: 0.002709152176976204\n",
            "step: 410, loss: 0.00035543672856874764\n",
            "step: 420, loss: 0.0005356731126084924\n",
            "step: 430, loss: 1.7686741557554342e-05\n",
            "step: 440, loss: 0.001336113316938281\n",
            "step: 450, loss: 0.0013364125043153763\n",
            "step: 460, loss: 0.0006142942584119737\n",
            "step: 470, loss: 5.720312401535921e-05\n",
            "step: 480, loss: 0.001149865915067494\n",
            "step: 490, loss: 0.002732646418735385\n",
            "step: 500, loss: 0.0005100140697322786\n",
            "step: 510, loss: 0.04236208647489548\n",
            "step: 520, loss: 3.2743886549724266e-05\n",
            "step: 530, loss: 0.0005925345467403531\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "epoch 15: dev_f1=0.9512308406874129, f1=0.9441340782122905, best_f1=0.9469767441860464\n",
            "mv: cannot stat '*_dev.pt': No such file or directory\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "5742it [00:33, 169.89it/s]\n",
            "load_f1 = 0.9527777777777777\n",
            "real_f1 = 0.9526022304832714\n",
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/apex/amp/_initialize.py:25: UserWarning: An input tensor was not cuda.\n",
            "  warnings.warn(\"An input tensor was not cuda.\")\n",
            "4398it [00:30, 142.49it/s]\n"
          ]
        }
      ]
    }
  ]
}